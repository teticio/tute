{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2be88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2243616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bcaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rlcard\n",
    "from rlcard.agents.dmc_agent import DMCTrainer\n",
    "from rlcard.agents import RandomAgent, DQNAgent\n",
    "from rlcard.agents.dmc_agent.model import DMCAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb592b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tute.rlcard\n",
    "from tute.rlcard.agent import TuteHumanAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tute.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b519ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlcard.make('tute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e1f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 40\n",
      "Number of players: 2\n",
      "Shape of state: [[9, 41], [9, 41]]\n",
      "Shape of action: [[40], [40]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of actions:\", env.num_actions)\n",
    "print(\"Number of players:\", env.num_players)\n",
    "print(\"Shape of state:\", env.state_shape)\n",
    "print(\"Shape of action:\", env.action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59040da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(num_actions=env.num_actions)\n",
    "human_agent = TuteHumanAgent(player=1, tute=env.game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ca94e",
   "metadata": {},
   "source": [
    "### Human vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41670f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agents([human_agent, random_agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e555d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 played el as de copas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el tres de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el as de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el cuatro de bastos\n",
      "2: el dos de espadas\n",
      "3: el cinco de oros\n",
      "4: el siete de copas\n",
      "5: el cinco de copas\n",
      "6: el cinco de espadas\n",
      "7: la sota de bastos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trajectories, player_wins \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/envs/env.py:144\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_over():\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Agent plays\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_training:\n\u001b[0;32m--> 144\u001b[0m         action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents[player_id]\u001b[38;5;241m.\u001b[39mstep(state)\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/agent.py:72\u001b[0m, in \u001b[0;36mTuteHumanAgent.eval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict the action given the curent state for evaluation. The same to step here.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        action (int): the action predicted (randomly chosen) by the random agent\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/agent.py:58\u001b[0m, in \u001b[0;36mTuteHumanAgent.step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtute\u001b[38;5;241m.\u001b[39mget_hand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer)\n\u001b[1;32m     54\u001b[0m possible_cards \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m     index \u001b[38;5;28;01mfor\u001b[39;00m index, card \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hand\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39miteritems())\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m card[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_legal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m ]\n\u001b[0;32m---> 58\u001b[0m card \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_card\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_cards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m card\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/tute.py:258\u001b[0m, in \u001b[0;36mTute.choose_card\u001b[0;34m(context, hand, possible_cards)\u001b[0m\n\u001b[1;32m    255\u001b[0m Tute\u001b[38;5;241m.\u001b[39mshow_cards(hand, with_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossible_cards\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m? \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(choice) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m possible_cards:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "trajectories, player_wins = env.run(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cecca14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46, 104])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593dcfd3",
   "metadata": {},
   "source": [
    "### Train Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb1e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(\n",
    "    num_actions=env.num_actions,\n",
    "    state_shape=env.state_shape[0],\n",
    "    mlp_layers=[64, 64],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc679cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agents([dqn_agent, dqn_agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90cd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlcard.utils import (\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d04c93d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "  timestep     |  513684\n",
      "  reward       |  56.81\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 100, rl-loss: 361.80242919921875\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 101, rl-loss: 375.0470886230469\r",
      "INFO - Step 102, rl-loss: 213.30157470703125\r",
      "INFO - Step 103, rl-loss: 148.837158203125\r",
      "INFO - Step 104, rl-loss: 637.160888671875\r",
      "INFO - Step 105, rl-loss: 10.243341445922852\r",
      "INFO - Step 106, rl-loss: 0.7929958701133728\r",
      "INFO - Step 107, rl-loss: 0.8531547784805298\r",
      "INFO - Step 108, rl-loss: 10.068684577941895\r",
      "INFO - Step 109, rl-loss: 254.6727294921875\r",
      "INFO - Step 110, rl-loss: 256.3238220214844\r",
      "INFO - Step 111, rl-loss: 254.6962890625\r",
      "INFO - Step 112, rl-loss: 148.3545379638672\r",
      "INFO - Step 113, rl-loss: 148.30032348632812\r",
      "INFO - Step 114, rl-loss: 359.1612548828125\r",
      "INFO - Step 115, rl-loss: 371.7384338378906\r",
      "INFO - Step 116, rl-loss: 0.612777829170227\r",
      "INFO - Step 117, rl-loss: 148.4524383544922\r",
      "INFO - Step 118, rl-loss: 371.0697021484375\r",
      "INFO - Step 119, rl-loss: 254.47251892089844\r",
      "INFO - Step 120, rl-loss: 0.5566394329071045"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 121, rl-loss: 221.5338134765625\r",
      "INFO - Step 122, rl-loss: 254.25881958007812\r",
      "INFO - Step 123, rl-loss: 0.41328704357147217\r",
      "INFO - Step 124, rl-loss: 772.404052734375\r",
      "INFO - Step 125, rl-loss: 148.32228088378906\r",
      "INFO - Step 126, rl-loss: 622.7211303710938\r",
      "INFO - Step 127, rl-loss: 0.5806055665016174\r",
      "INFO - Step 128, rl-loss: 0.521873414516449\r",
      "INFO - Step 129, rl-loss: 518.8018798828125\r",
      "INFO - Step 130, rl-loss: 193.407470703125\r",
      "INFO - Step 131, rl-loss: 194.32484436035156\r",
      "INFO - Step 132, rl-loss: 843.9041748046875\r",
      "INFO - Step 133, rl-loss: 253.93650817871094\r",
      "INFO - Step 134, rl-loss: 520.0762329101562\r",
      "INFO - Step 135, rl-loss: 0.6725288033485413\r",
      "INFO - Step 136, rl-loss: 253.33277893066406\r",
      "INFO - Step 137, rl-loss: 252.96011352539062\r",
      "INFO - Step 138, rl-loss: 0.4979710578918457\r",
      "INFO - Step 139, rl-loss: 211.34373474121094\r",
      "INFO - Step 140, rl-loss: 339.8842468261719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 141, rl-loss: 0.34479957818984985\r",
      "INFO - Step 142, rl-loss: 370.11090087890625\r",
      "INFO - Step 143, rl-loss: 148.069091796875\r",
      "INFO - Step 144, rl-loss: 1098.321533203125\r",
      "INFO - Step 145, rl-loss: 830.9721069335938\r",
      "INFO - Step 146, rl-loss: 787.4940795898438\r",
      "INFO - Step 147, rl-loss: 210.71751403808594\r",
      "INFO - Step 148, rl-loss: 579.3435668945312\r",
      "INFO - Step 149, rl-loss: 578.9736938476562\r",
      "INFO - Step 150, rl-loss: 194.14141845703125\r",
      "INFO - Step 151, rl-loss: 519.376708984375\r",
      "INFO - Step 152, rl-loss: 578.2573852539062\r",
      "INFO - Step 153, rl-loss: 729.343017578125\r",
      "INFO - Step 154, rl-loss: 781.4009399414062\r",
      "INFO - Step 155, rl-loss: 0.48150578141212463\r",
      "INFO - Step 156, rl-loss: 193.56016540527344\r",
      "INFO - Step 157, rl-loss: 931.9937133789062\r",
      "INFO - Step 158, rl-loss: 0.4909432530403137\r",
      "INFO - Step 159, rl-loss: 9.848676681518555\r",
      "INFO - Step 160, rl-loss: 123.23039245605469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 161, rl-loss: 155.8363800048828\r",
      "INFO - Step 162, rl-loss: 725.3585205078125\r",
      "INFO - Step 163, rl-loss: 373.194091796875\r",
      "INFO - Step 164, rl-loss: 770.281005859375\r",
      "INFO - Step 165, rl-loss: 0.4021030068397522\r",
      "INFO - Step 166, rl-loss: 194.13267517089844\r",
      "INFO - Step 167, rl-loss: 404.0779724121094\r",
      "INFO - Step 168, rl-loss: 123.30973815917969\r",
      "INFO - Step 169, rl-loss: 123.20237731933594\r",
      "INFO - Step 170, rl-loss: 576.3294067382812\r",
      "INFO - Step 171, rl-loss: 146.60777282714844\r",
      "INFO - Step 172, rl-loss: 838.33203125\r",
      "INFO - Step 173, rl-loss: 576.4534912109375\r",
      "INFO - Step 174, rl-loss: 574.8131713867188\r",
      "INFO - Step 175, rl-loss: 146.72857666015625\r",
      "INFO - Step 176, rl-loss: 583.7794799804688\r",
      "INFO - Step 177, rl-loss: 1196.27490234375\r",
      "INFO - Step 178, rl-loss: 372.386474609375\r",
      "INFO - Step 179, rl-loss: 0.4048307240009308\r",
      "INFO - Step 180, rl-loss: 663.8278198242188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 181, rl-loss: 218.23550415039062\r",
      "INFO - Step 182, rl-loss: 574.08349609375\r",
      "INFO - Step 183, rl-loss: 209.85284423828125\r",
      "INFO - Step 184, rl-loss: 0.6073968410491943\r",
      "INFO - Step 185, rl-loss: 514.422607421875\r",
      "INFO - Step 186, rl-loss: 209.38450622558594\r",
      "INFO - Step 187, rl-loss: 0.6428005695343018\r",
      "INFO - Step 188, rl-loss: 251.74636840820312\r",
      "INFO - Step 189, rl-loss: 400.24176025390625\r",
      "INFO - Step 190, rl-loss: 330.9918518066406\r",
      "INFO - Step 191, rl-loss: 1021.311767578125\r",
      "INFO - Step 192, rl-loss: 515.1452026367188\r",
      "INFO - Step 193, rl-loss: 303.3911437988281\r",
      "INFO - Step 194, rl-loss: 840.6712646484375\r",
      "INFO - Step 195, rl-loss: 131.9343719482422\r",
      "INFO - Step 196, rl-loss: 0.39194154739379883\r",
      "INFO - Step 197, rl-loss: 571.1549072265625\r",
      "INFO - Step 198, rl-loss: 571.3170166015625\r",
      "INFO - Step 199, rl-loss: 542.6361083984375\r",
      "INFO - Step 200, rl-loss: 0.49388784170150757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 201, rl-loss: 146.19619750976562\r",
      "INFO - Step 202, rl-loss: 0.5542975664138794\r",
      "INFO - Step 203, rl-loss: 347.6707763671875\r",
      "INFO - Step 204, rl-loss: 413.7142639160156\r",
      "INFO - Step 205, rl-loss: 191.68638610839844\r",
      "INFO - Step 206, rl-loss: 965.3258056640625\r",
      "INFO - Step 207, rl-loss: 922.5198974609375\r",
      "INFO - Step 208, rl-loss: 9.563940048217773\r",
      "INFO - Step 209, rl-loss: 121.721435546875\r",
      "INFO - Step 210, rl-loss: 474.69305419921875\r",
      "INFO - Step 211, rl-loss: 882.9598388671875\r",
      "INFO - Step 212, rl-loss: 0.4220123887062073\r",
      "INFO - Step 213, rl-loss: 0.5212868452072144\r",
      "INFO - Step 214, rl-loss: 1141.749267578125\r",
      "INFO - Step 215, rl-loss: 570.6285400390625\r",
      "INFO - Step 216, rl-loss: 0.30556607246398926\r",
      "INFO - Step 217, rl-loss: 0.4526255428791046\r",
      "INFO - Step 218, rl-loss: 0.45950114727020264\r",
      "INFO - Step 219, rl-loss: 9.612212181091309\r",
      "INFO - Step 220, rl-loss: 145.73971557617188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 221, rl-loss: 318.552978515625\r",
      "INFO - Step 222, rl-loss: 575.7188720703125\r",
      "INFO - Step 223, rl-loss: 293.5239562988281\r",
      "INFO - Step 224, rl-loss: 9.441399574279785\r",
      "INFO - Step 225, rl-loss: 569.8008422851562\r",
      "INFO - Step 226, rl-loss: 373.1382751464844\r",
      "INFO - Step 227, rl-loss: 201.0227813720703\r",
      "INFO - Step 228, rl-loss: 513.0993041992188\r",
      "INFO - Step 229, rl-loss: 560.077880859375\r",
      "INFO - Step 230, rl-loss: 533.3556518554688\r",
      "INFO - Step 231, rl-loss: 49.97999954223633\r",
      "INFO - Step 232, rl-loss: 0.44180357456207275\r",
      "INFO - Step 233, rl-loss: 249.36248779296875\r",
      "INFO - Step 234, rl-loss: 867.7877807617188\r",
      "INFO - Step 235, rl-loss: 583.1448974609375\r",
      "INFO - Step 236, rl-loss: 0.35244640707969666\r",
      "INFO - Step 237, rl-loss: 122.07064056396484\r",
      "INFO - Step 238, rl-loss: 300.4707336425781\r",
      "INFO - Step 239, rl-loss: 0.2272472083568573\r",
      "INFO - Step 240, rl-loss: 616.8482666015625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 241, rl-loss: 437.675537109375\r",
      "INFO - Step 242, rl-loss: 58.627323150634766\r",
      "INFO - Step 243, rl-loss: 448.0804748535156\r",
      "INFO - Step 244, rl-loss: 384.7709045410156\r",
      "INFO - Step 245, rl-loss: 206.9842529296875\r",
      "INFO - Step 246, rl-loss: 0.3222126364707947\r",
      "INFO - Step 247, rl-loss: 366.6984558105469\r",
      "INFO - Step 248, rl-loss: 583.23046875\r",
      "INFO - Step 249, rl-loss: 395.0835876464844\r",
      "INFO - Step 250, rl-loss: 58.81576156616211\r",
      "INFO - Step 251, rl-loss: 365.58551025390625\r",
      "INFO - Step 252, rl-loss: 183.56529235839844\r",
      "INFO - Step 253, rl-loss: 367.87982177734375\r",
      "INFO - Step 254, rl-loss: 212.8161163330078\r",
      "INFO - Step 255, rl-loss: 0.3983994722366333\r",
      "INFO - Step 256, rl-loss: 0.3725762367248535\r",
      "INFO - Step 257, rl-loss: 0.549304723739624\r",
      "INFO - Step 258, rl-loss: 466.7950439453125\r",
      "INFO - Step 259, rl-loss: 658.99072265625\r",
      "INFO - Step 260, rl-loss: 0.4894622564315796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 261, rl-loss: 548.1828002929688\r",
      "INFO - Step 262, rl-loss: 163.46237182617188\r",
      "INFO - Step 263, rl-loss: 0.44080549478530884\r",
      "INFO - Step 264, rl-loss: 480.9642639160156\r",
      "INFO - Step 265, rl-loss: 49.36484909057617\r",
      "INFO - Step 266, rl-loss: 163.36448669433594\r",
      "INFO - Step 267, rl-loss: 377.2105712890625\r",
      "INFO - Step 268, rl-loss: 352.21343994140625\r",
      "INFO - Step 269, rl-loss: 0.7071136832237244\r",
      "INFO - Step 270, rl-loss: 216.96810913085938\r",
      "INFO - Step 271, rl-loss: 392.4851379394531\r",
      "INFO - Step 272, rl-loss: 49.58294677734375\r",
      "INFO - Step 273, rl-loss: 438.568603515625\r",
      "INFO - Step 274, rl-loss: 0.47207021713256836\r",
      "INFO - Step 275, rl-loss: 232.3209228515625\r",
      "INFO - Step 276, rl-loss: 0.2407243549823761\r",
      "INFO - Step 277, rl-loss: 364.9359436035156\r",
      "INFO - Step 278, rl-loss: 87.08650207519531\r",
      "INFO - Step 279, rl-loss: 709.7476806640625\r",
      "INFO - Step 280, rl-loss: 453.16571044921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 281, rl-loss: 364.2354431152344\r",
      "INFO - Step 282, rl-loss: 449.77362060546875\r",
      "INFO - Step 283, rl-loss: 368.987548828125\r",
      "INFO - Step 284, rl-loss: 535.480712890625\r",
      "INFO - Step 285, rl-loss: 371.250244140625\r",
      "INFO - Step 286, rl-loss: 87.05796813964844\r",
      "INFO - Step 287, rl-loss: 0.46592217683792114\r",
      "INFO - Step 288, rl-loss: 0.5405522584915161\r",
      "INFO - Step 289, rl-loss: 553.896240234375\r",
      "INFO - Step 290, rl-loss: 639.7079467773438\r",
      "INFO - Step 291, rl-loss: 379.36322021484375\r",
      "INFO - Step 292, rl-loss: 181.81063842773438\r",
      "INFO - Step 293, rl-loss: 368.8343505859375\r",
      "INFO - Step 294, rl-loss: 593.4387817382812\r",
      "INFO - Step 295, rl-loss: 863.4610595703125\r",
      "INFO - Step 296, rl-loss: 290.294677734375\r",
      "INFO - Step 297, rl-loss: 0.32070332765579224\r",
      "INFO - Step 298, rl-loss: 0.5633524656295776\r",
      "INFO - Step 299, rl-loss: 0.4958590269088745\r",
      "INFO - Step 300, rl-loss: 1066.38916015625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 301, rl-loss: 770.123291015625\r",
      "INFO - Step 302, rl-loss: 458.3781433105469\r",
      "INFO - Step 303, rl-loss: 170.4039306640625\r",
      "INFO - Step 304, rl-loss: 870.208984375\r",
      "INFO - Step 305, rl-loss: 0.5197238922119141\r",
      "INFO - Step 306, rl-loss: 0.2957661747932434\r",
      "INFO - Step 307, rl-loss: 730.4830932617188\r",
      "INFO - Step 308, rl-loss: 130.29795837402344\r",
      "INFO - Step 309, rl-loss: 0.5141567587852478\r",
      "INFO - Step 310, rl-loss: 868.1405029296875\r",
      "INFO - Step 311, rl-loss: 253.81838989257812\r",
      "INFO - Step 312, rl-loss: 0.488517165184021\r",
      "INFO - Step 313, rl-loss: 687.5980224609375\r",
      "INFO - Step 314, rl-loss: 48.6430778503418\r",
      "INFO - Step 315, rl-loss: 0.5471493005752563\r",
      "INFO - Step 316, rl-loss: 0.4000000059604645\r",
      "INFO - Step 317, rl-loss: 247.17408752441406\r",
      "INFO - Step 318, rl-loss: 0.4308953285217285\r",
      "INFO - Step 319, rl-loss: 362.27276611328125\r",
      "INFO - Step 320, rl-loss: 0.29441142082214355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 321, rl-loss: 468.8668212890625\r",
      "INFO - Step 322, rl-loss: 850.81591796875\r",
      "INFO - Step 323, rl-loss: 0.3831549882888794\r",
      "INFO - Step 324, rl-loss: 432.99072265625\r",
      "INFO - Step 325, rl-loss: 240.02603149414062\r",
      "INFO - Step 326, rl-loss: 434.246337890625\r",
      "INFO - Step 327, rl-loss: 294.32464599609375\r",
      "INFO - Step 328, rl-loss: 454.7055358886719\r",
      "INFO - Step 329, rl-loss: 86.55413055419922\r",
      "INFO - Step 330, rl-loss: 737.8770751953125\r",
      "INFO - Step 331, rl-loss: 48.146209716796875\r",
      "INFO - Step 332, rl-loss: 694.1621704101562\r",
      "INFO - Step 333, rl-loss: 679.9373779296875\r",
      "INFO - Step 334, rl-loss: 0.6078910827636719\r",
      "INFO - Step 335, rl-loss: 798.9659423828125\r",
      "INFO - Step 336, rl-loss: 48.56016540527344\r",
      "INFO - Step 337, rl-loss: 207.8982391357422\r",
      "INFO - Step 338, rl-loss: 0.4133250415325165\r",
      "INFO - Step 339, rl-loss: 366.609130859375\r",
      "INFO - Step 340, rl-loss: 484.11004638671875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 341, rl-loss: 244.57427978515625\r",
      "INFO - Step 342, rl-loss: 293.8210144042969\r",
      "INFO - Step 343, rl-loss: 386.9649353027344\r",
      "INFO - Step 344, rl-loss: 325.4312744140625\r",
      "INFO - Step 345, rl-loss: 376.68829345703125\r",
      "INFO - Step 346, rl-loss: 326.5208740234375\r",
      "INFO - Step 347, rl-loss: 520.0755615234375\r",
      "INFO - Step 348, rl-loss: 179.88565063476562\r",
      "INFO - Step 349, rl-loss: 80.88613891601562\r",
      "INFO - Step 350, rl-loss: 161.4388427734375\r",
      "INFO - Step 351, rl-loss: 144.7579803466797\r",
      "INFO - Step 352, rl-loss: 503.9478759765625\r",
      "INFO - Step 353, rl-loss: 0.4073448181152344\r",
      "INFO - Step 354, rl-loss: 0.3594026267528534\r",
      "INFO - Step 355, rl-loss: 540.1917114257812\r",
      "INFO - Step 356, rl-loss: 676.7044677734375\r",
      "INFO - Step 357, rl-loss: 0.495967298746109\r",
      "INFO - Step 358, rl-loss: 323.2827453613281\r",
      "INFO - Step 359, rl-loss: 47.39813995361328\r",
      "INFO - Step 360, rl-loss: 450.3296203613281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 361, rl-loss: 308.69366455078125\r",
      "INFO - Step 362, rl-loss: 205.00717163085938\r",
      "INFO - Step 363, rl-loss: 318.8800964355469\r",
      "INFO - Step 364, rl-loss: 244.93714904785156\r",
      "INFO - Step 365, rl-loss: 0.4322596788406372\r",
      "INFO - Step 366, rl-loss: 548.8217163085938\r",
      "INFO - Step 367, rl-loss: 559.05859375\r",
      "INFO - Step 368, rl-loss: 668.618408203125\r",
      "INFO - Step 369, rl-loss: 332.02191162109375\r",
      "INFO - Step 370, rl-loss: 244.86282348632812\r",
      "INFO - Step 371, rl-loss: 47.94777297973633\r",
      "INFO - Step 372, rl-loss: 363.6076354980469\r",
      "INFO - Step 373, rl-loss: 1024.423583984375\r",
      "INFO - Step 374, rl-loss: 604.1776733398438\r",
      "INFO - Step 375, rl-loss: 432.5555725097656\r",
      "INFO - Step 376, rl-loss: 131.71939086914062\r",
      "INFO - Step 377, rl-loss: 0.36441195011138916\r",
      "INFO - Step 378, rl-loss: 0.6286877393722534\r",
      "INFO - Step 379, rl-loss: 187.65159606933594\r",
      "INFO - Step 380, rl-loss: 241.8648223876953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 381, rl-loss: 136.5771484375\r",
      "INFO - Step 382, rl-loss: 0.5435082912445068\r",
      "INFO - Step 383, rl-loss: 0.509415864944458\r",
      "INFO - Step 384, rl-loss: 658.3533325195312\r",
      "INFO - Step 385, rl-loss: 816.6968994140625\r",
      "INFO - Step 386, rl-loss: 842.2083129882812\r",
      "INFO - Step 387, rl-loss: 252.35183715820312\r",
      "INFO - Step 388, rl-loss: 671.7734375\r",
      "INFO - Step 389, rl-loss: 218.61085510253906\r",
      "INFO - Step 390, rl-loss: 47.8068733215332\r",
      "INFO - Step 391, rl-loss: 95.69377136230469\r",
      "INFO - Step 392, rl-loss: 126.53675842285156\r",
      "INFO - Step 393, rl-loss: 49.09686279296875\r",
      "INFO - Step 394, rl-loss: 512.3767700195312\r",
      "INFO - Step 395, rl-loss: 0.501134991645813\r",
      "INFO - Step 396, rl-loss: 587.4659423828125\r",
      "INFO - Step 397, rl-loss: 252.5912322998047\r",
      "INFO - Step 398, rl-loss: 280.4527587890625\r",
      "INFO - Step 399, rl-loss: 49.15798568725586\r",
      "INFO - Step 400, rl-loss: 677.4237670898438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 401, rl-loss: 93.69265747070312\r",
      "INFO - Step 402, rl-loss: 391.96282958984375\r",
      "INFO - Step 403, rl-loss: 94.85771179199219\r",
      "INFO - Step 404, rl-loss: 0.3017045259475708\r",
      "INFO - Step 405, rl-loss: 0.4988645613193512\r",
      "INFO - Step 406, rl-loss: 0.4797441363334656\r",
      "INFO - Step 407, rl-loss: 0.4141201972961426\r",
      "INFO - Step 408, rl-loss: 490.8121643066406\r",
      "INFO - Step 409, rl-loss: 400.89764404296875\r",
      "INFO - Step 410, rl-loss: 9.60542106628418\r",
      "INFO - Step 411, rl-loss: 234.4772491455078\r",
      "INFO - Step 412, rl-loss: 493.3373107910156\r",
      "INFO - Step 413, rl-loss: 285.7512512207031\r",
      "INFO - Step 414, rl-loss: 179.82635498046875\r",
      "INFO - Step 415, rl-loss: 226.4427490234375\r",
      "INFO - Step 416, rl-loss: 97.54164123535156\r",
      "INFO - Step 417, rl-loss: 204.0707244873047\r",
      "INFO - Step 418, rl-loss: 9.89438533782959\r",
      "INFO - Step 419, rl-loss: 0.6383830308914185\r",
      "INFO - Step 420, rl-loss: 356.50390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 421, rl-loss: 0.47513413429260254\r",
      "INFO - Step 422, rl-loss: 544.7002563476562\r",
      "INFO - Step 423, rl-loss: 880.429443359375\r",
      "INFO - Step 424, rl-loss: 442.3631896972656\r",
      "INFO - Step 425, rl-loss: 749.4664916992188\r",
      "INFO - Step 426, rl-loss: 660.7963256835938\r",
      "INFO - Step 427, rl-loss: 490.3106384277344\r",
      "INFO - Step 428, rl-loss: 563.6223754882812\r",
      "INFO - Step 429, rl-loss: 376.1138610839844\r",
      "INFO - Step 430, rl-loss: 204.4796905517578\r",
      "INFO - Step 431, rl-loss: 284.23223876953125\r",
      "INFO - Step 432, rl-loss: 204.16311645507812\r",
      "INFO - Step 433, rl-loss: 328.0301818847656\r",
      "INFO - Step 434, rl-loss: 302.7430419921875\r",
      "INFO - Step 435, rl-loss: 332.0735778808594\r",
      "INFO - Step 436, rl-loss: 0.5750318765640259\r",
      "INFO - Step 437, rl-loss: 747.7933349609375\r",
      "INFO - Step 438, rl-loss: 183.2653350830078\r",
      "INFO - Step 439, rl-loss: 362.59405517578125\r",
      "INFO - Step 440, rl-loss: 93.74578094482422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 441, rl-loss: 143.49220275878906\r",
      "INFO - Step 442, rl-loss: 48.57817077636719\r",
      "INFO - Step 443, rl-loss: 0.3997006416320801\r",
      "INFO - Step 444, rl-loss: 92.18858337402344\r",
      "INFO - Step 445, rl-loss: 31.478729248046875\r",
      "INFO - Step 446, rl-loss: 46.8916130065918\r",
      "INFO - Step 447, rl-loss: 173.55703735351562\r",
      "INFO - Step 448, rl-loss: 435.6568603515625\r",
      "INFO - Step 449, rl-loss: 508.4750671386719\r",
      "INFO - Step 450, rl-loss: 414.04425048828125\r",
      "INFO - Step 451, rl-loss: 140.4598388671875\r",
      "INFO - Step 452, rl-loss: 283.77557373046875\r",
      "INFO - Step 453, rl-loss: 77.9562759399414\r",
      "INFO - Step 454, rl-loss: 85.23052978515625\r",
      "INFO - Step 455, rl-loss: 0.3950601816177368\r",
      "INFO - Step 456, rl-loss: 319.392578125\r",
      "INFO - Step 457, rl-loss: 46.66683578491211\r",
      "INFO - Step 458, rl-loss: 343.93408203125\r",
      "INFO - Step 459, rl-loss: 108.90290832519531\r",
      "INFO - Step 460, rl-loss: 650.4490966796875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 461, rl-loss: 119.35800170898438\r",
      "INFO - Step 462, rl-loss: 124.49292755126953\r",
      "INFO - Step 463, rl-loss: 421.6600646972656\r",
      "INFO - Step 464, rl-loss: 241.64955139160156\r",
      "INFO - Step 465, rl-loss: 286.09197998046875\r",
      "INFO - Step 466, rl-loss: 402.0357971191406\r",
      "INFO - Step 467, rl-loss: 0.4723501205444336\r",
      "INFO - Step 468, rl-loss: 93.51708221435547\r",
      "INFO - Step 469, rl-loss: 332.95068359375\r",
      "INFO - Step 470, rl-loss: 282.44622802734375\r",
      "INFO - Step 471, rl-loss: 167.93472290039062\r",
      "INFO - Step 472, rl-loss: 82.55104064941406\r",
      "INFO - Step 473, rl-loss: 138.13340759277344\r",
      "INFO - Step 474, rl-loss: 561.01806640625\r",
      "INFO - Step 475, rl-loss: 376.50677490234375\r",
      "INFO - Step 476, rl-loss: 0.4050600528717041\r",
      "INFO - Step 477, rl-loss: 328.3250732421875\r",
      "INFO - Step 478, rl-loss: 186.08811950683594\r",
      "INFO - Step 479, rl-loss: 272.8601989746094\r",
      "INFO - Step 480, rl-loss: 584.8975830078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 481, rl-loss: 0.31940194964408875\r",
      "INFO - Step 482, rl-loss: 262.2746276855469\r",
      "INFO - Step 483, rl-loss: 0.3251763582229614\r",
      "INFO - Step 484, rl-loss: 277.7528076171875\r",
      "INFO - Step 485, rl-loss: 561.8280639648438\r",
      "INFO - Step 486, rl-loss: 184.12271118164062\r",
      "INFO - Step 487, rl-loss: 515.714599609375\r",
      "INFO - Step 488, rl-loss: 97.47772979736328\r",
      "INFO - Step 489, rl-loss: 85.0252685546875\r",
      "INFO - Step 490, rl-loss: 45.926490783691406\r",
      "INFO - Step 491, rl-loss: 0.5046409368515015\r",
      "INFO - Step 492, rl-loss: 368.5704040527344\r",
      "INFO - Step 493, rl-loss: 142.88174438476562\r",
      "INFO - Step 494, rl-loss: 321.2667236328125\r",
      "INFO - Step 495, rl-loss: 739.5330810546875\r",
      "INFO - Step 496, rl-loss: 0.7107406854629517\r",
      "INFO - Step 497, rl-loss: 280.8547668457031\r",
      "INFO - Step 498, rl-loss: 158.61094665527344\r",
      "INFO - Step 499, rl-loss: 46.01106262207031\r",
      "INFO - Step 500, rl-loss: 664.531494140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 501, rl-loss: 0.9320893883705139\r",
      "INFO - Step 502, rl-loss: 0.5442203283309937\r",
      "INFO - Step 503, rl-loss: 66.77318572998047\r",
      "INFO - Step 504, rl-loss: 0.430726557970047\r",
      "INFO - Step 505, rl-loss: 0.48913878202438354\r",
      "INFO - Step 506, rl-loss: 142.73275756835938\r",
      "INFO - Step 507, rl-loss: 185.97352600097656\r",
      "INFO - Step 508, rl-loss: 75.80628967285156\r",
      "INFO - Step 509, rl-loss: 334.3245544433594\r",
      "INFO - Step 510, rl-loss: 166.55935668945312\r",
      "INFO - Step 511, rl-loss: 45.90717697143555\r",
      "INFO - Step 512, rl-loss: 0.3979780077934265\r",
      "INFO - Step 513, rl-loss: 137.33897399902344\r",
      "INFO - Step 514, rl-loss: 0.6814834475517273\r",
      "INFO - Step 515, rl-loss: 270.0566711425781\r",
      "INFO - Step 516, rl-loss: 194.7502899169922\r",
      "INFO - Step 517, rl-loss: 376.15533447265625\r",
      "INFO - Step 518, rl-loss: 85.02149200439453\r",
      "INFO - Step 519, rl-loss: 174.56561279296875\r",
      "INFO - Step 520, rl-loss: 239.73251342773438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 521, rl-loss: 185.7744903564453\r",
      "INFO - Step 522, rl-loss: 0.3581371605396271\r",
      "INFO - Step 523, rl-loss: 253.26370239257812\r",
      "INFO - Step 524, rl-loss: 569.3178100585938\r",
      "INFO - Step 525, rl-loss: 282.2767028808594\r",
      "INFO - Step 526, rl-loss: 90.52179718017578\r",
      "INFO - Step 527, rl-loss: 0.4522256851196289\r",
      "INFO - Step 528, rl-loss: 651.3390502929688\r",
      "INFO - Step 529, rl-loss: 979.3119506835938\r",
      "INFO - Step 530, rl-loss: 237.85968017578125\r",
      "INFO - Step 531, rl-loss: 185.30833435058594\r",
      "INFO - Step 532, rl-loss: 31.663463592529297\r",
      "INFO - Step 533, rl-loss: 326.1289978027344\r",
      "INFO - Step 534, rl-loss: 189.32107543945312\r",
      "INFO - Step 535, rl-loss: 31.898923873901367\r",
      "INFO - Step 536, rl-loss: 31.46847152709961\r",
      "INFO - Step 537, rl-loss: 120.85490417480469\r",
      "INFO - Step 538, rl-loss: 0.612045168876648\r",
      "INFO - Step 539, rl-loss: 0.5540319681167603\r",
      "INFO - Step 540, rl-loss: 0.3584609031677246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 541, rl-loss: 203.3409881591797\r",
      "INFO - Step 542, rl-loss: 98.33260345458984\r",
      "INFO - Step 543, rl-loss: 777.0911254882812\r",
      "INFO - Step 544, rl-loss: 91.47721862792969\r",
      "INFO - Step 545, rl-loss: 0.46781083941459656\r",
      "INFO - Step 546, rl-loss: 772.21044921875\r",
      "INFO - Step 547, rl-loss: 0.5042299628257751\r",
      "INFO - Step 548, rl-loss: 0.48433271050453186\r",
      "INFO - Step 549, rl-loss: 0.6858886480331421\r",
      "INFO - Step 550, rl-loss: 390.4952697753906\r",
      "INFO - Step 551, rl-loss: 250.77850341796875\r",
      "INFO - Step 552, rl-loss: 158.82351684570312\r",
      "INFO - Step 553, rl-loss: 143.07688903808594\r",
      "INFO - Step 554, rl-loss: 0.32532674074172974\r",
      "INFO - Step 555, rl-loss: 142.17730712890625\r",
      "INFO - Step 556, rl-loss: 190.9975128173828\r",
      "INFO - Step 557, rl-loss: 385.474609375\r",
      "INFO - Step 558, rl-loss: 606.3511962890625\r",
      "INFO - Step 559, rl-loss: 524.9414672851562\r",
      "INFO - Step 560, rl-loss: 182.88522338867188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 561, rl-loss: 47.371620178222656\r",
      "INFO - Step 562, rl-loss: 675.0861206054688\r",
      "INFO - Step 563, rl-loss: 605.2550659179688\r",
      "INFO - Step 564, rl-loss: 121.06885528564453\r",
      "INFO - Step 565, rl-loss: 302.9047546386719\r",
      "INFO - Step 566, rl-loss: 0.5120328664779663\r",
      "INFO - Step 567, rl-loss: 128.4246368408203\r",
      "INFO - Step 568, rl-loss: 256.6165771484375\r",
      "INFO - Step 569, rl-loss: 385.8148193359375\r",
      "INFO - Step 570, rl-loss: 233.99252319335938\r",
      "INFO - Step 571, rl-loss: 177.5107421875\r",
      "INFO - Step 572, rl-loss: 87.51749420166016\r",
      "INFO - Step 573, rl-loss: 0.8915493488311768\r",
      "INFO - Step 574, rl-loss: 0.537260890007019\r",
      "INFO - Step 575, rl-loss: 325.53802490234375\r",
      "INFO - Step 576, rl-loss: 158.52392578125\r",
      "INFO - Step 577, rl-loss: 330.6941833496094\r",
      "INFO - Step 578, rl-loss: 732.494873046875\r",
      "INFO - Step 579, rl-loss: 358.1135559082031\r",
      "INFO - Step 580, rl-loss: 230.19337463378906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 581, rl-loss: 146.74964904785156\r",
      "INFO - Step 582, rl-loss: 185.75596618652344\r",
      "INFO - Step 583, rl-loss: 560.2318115234375\r",
      "INFO - Step 584, rl-loss: 737.6856079101562\r",
      "INFO - Step 585, rl-loss: 202.2447509765625\r",
      "INFO - Step 586, rl-loss: 455.586181640625\r",
      "INFO - Step 587, rl-loss: 80.39437103271484\r",
      "INFO - Step 588, rl-loss: 364.32550048828125\r",
      "INFO - Step 589, rl-loss: 236.69287109375\r",
      "INFO - Step 590, rl-loss: 603.94677734375\r",
      "INFO - Step 591, rl-loss: 678.0543212890625\r",
      "INFO - Step 592, rl-loss: 0.6402735710144043\r",
      "INFO - Step 593, rl-loss: 0.3947269022464752\r",
      "INFO - Step 594, rl-loss: 0.2816033959388733\r",
      "INFO - Step 595, rl-loss: 141.8400421142578\r",
      "INFO - Step 596, rl-loss: 414.17059326171875\r",
      "INFO - Step 597, rl-loss: 75.79352569580078\r",
      "INFO - Step 598, rl-loss: 760.3568725585938\r",
      "INFO - Step 599, rl-loss: 0.4858701825141907\r",
      "INFO - Step 600, rl-loss: 0.7638550400733948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 601, rl-loss: 144.19439697265625\r",
      "INFO - Step 602, rl-loss: 0.5862365365028381\r",
      "INFO - Step 603, rl-loss: 267.89501953125\r",
      "INFO - Step 604, rl-loss: 0.5312876105308533\r",
      "INFO - Step 605, rl-loss: 440.2591552734375\r",
      "INFO - Step 606, rl-loss: 0.39333367347717285\r",
      "INFO - Step 607, rl-loss: 136.68563842773438\r",
      "INFO - Step 608, rl-loss: 393.5473937988281\r",
      "INFO - Step 609, rl-loss: 289.68585205078125\r",
      "INFO - Step 610, rl-loss: 216.04286193847656\r",
      "INFO - Step 611, rl-loss: 86.82945251464844\r",
      "INFO - Step 612, rl-loss: 224.5161590576172\r",
      "INFO - Step 613, rl-loss: 240.1046905517578\r",
      "INFO - Step 614, rl-loss: 294.25970458984375\r",
      "INFO - Step 615, rl-loss: 356.96337890625\r",
      "INFO - Step 616, rl-loss: 185.61627197265625\r",
      "INFO - Step 617, rl-loss: 0.8616906404495239\r",
      "INFO - Step 618, rl-loss: 480.0029296875\r",
      "INFO - Step 619, rl-loss: 97.06122589111328\r",
      "INFO - Step 620, rl-loss: 727.6053466796875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 621, rl-loss: 348.9187927246094\r",
      "INFO - Step 622, rl-loss: 743.7900390625\r",
      "INFO - Step 623, rl-loss: 96.40543365478516\r",
      "INFO - Step 624, rl-loss: 17.319482803344727\r",
      "INFO - Step 625, rl-loss: 9.326260566711426\r",
      "INFO - Step 626, rl-loss: 0.6550102233886719\r",
      "INFO - Step 627, rl-loss: 253.11016845703125\r",
      "INFO - Step 628, rl-loss: 367.41107177734375\r",
      "INFO - Step 629, rl-loss: 0.5632518529891968\r",
      "INFO - Step 630, rl-loss: 9.454296112060547\r",
      "INFO - Step 631, rl-loss: 120.17491149902344\r",
      "INFO - Step 632, rl-loss: 276.8323974609375\r",
      "INFO - Step 633, rl-loss: 282.5957946777344\r",
      "INFO - Step 634, rl-loss: 0.8482170104980469\r",
      "INFO - Step 635, rl-loss: 222.74261474609375\r",
      "INFO - Step 636, rl-loss: 0.34706228971481323\r",
      "INFO - Step 637, rl-loss: 128.681396484375\r",
      "INFO - Step 638, rl-loss: 165.62063598632812\r",
      "INFO - Step 639, rl-loss: 0.31854909658432007\r",
      "INFO - Step 640, rl-loss: 193.18524169921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 641, rl-loss: 66.19610595703125\r",
      "INFO - Step 642, rl-loss: 0.6178754568099976\r",
      "INFO - Step 643, rl-loss: 158.3546905517578\r",
      "INFO - Step 644, rl-loss: 141.5442352294922\r",
      "INFO - Step 645, rl-loss: 1333.41552734375\r",
      "INFO - Step 646, rl-loss: 359.78216552734375\r",
      "INFO - Step 647, rl-loss: 0.4877604842185974\r",
      "INFO - Step 648, rl-loss: 235.5036163330078\r",
      "INFO - Step 649, rl-loss: 503.67083740234375\r",
      "INFO - Step 650, rl-loss: 382.49859619140625\r",
      "INFO - Step 651, rl-loss: 323.6984558105469\r",
      "INFO - Step 652, rl-loss: 119.37030029296875\r",
      "INFO - Step 653, rl-loss: 505.82843017578125\r",
      "INFO - Step 654, rl-loss: 331.8887939453125\r",
      "INFO - Step 655, rl-loss: 256.75189208984375\r",
      "INFO - Step 656, rl-loss: 0.28011372685432434\r",
      "INFO - Step 657, rl-loss: 0.7100528478622437\r",
      "INFO - Step 658, rl-loss: 190.0338134765625\r",
      "INFO - Step 659, rl-loss: 295.87103271484375\r",
      "INFO - Step 660, rl-loss: 0.40207439661026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 661, rl-loss: 0.6264575719833374\r",
      "INFO - Step 662, rl-loss: 453.2510681152344\r",
      "INFO - Step 663, rl-loss: 402.0412902832031\r",
      "INFO - Step 664, rl-loss: 684.2113647460938\r",
      "INFO - Step 665, rl-loss: 0.879278302192688\r",
      "INFO - Step 666, rl-loss: 289.72906494140625\r",
      "INFO - Step 667, rl-loss: 72.06645202636719\r",
      "INFO - Step 668, rl-loss: 95.4993667602539\r",
      "INFO - Step 669, rl-loss: 458.572021484375\r",
      "INFO - Step 670, rl-loss: 125.61705017089844\r",
      "INFO - Step 671, rl-loss: 9.52092456817627\r",
      "INFO - Step 672, rl-loss: 0.3988230526447296\r",
      "INFO - Step 673, rl-loss: 44.537139892578125\r",
      "INFO - Step 674, rl-loss: 200.49732971191406\r",
      "INFO - Step 675, rl-loss: 235.5070037841797\r",
      "INFO - Step 676, rl-loss: 122.60877990722656\r",
      "INFO - Step 677, rl-loss: 0.5727685689926147\r",
      "INFO - Step 678, rl-loss: 242.09957885742188\r",
      "INFO - Step 679, rl-loss: 333.1836242675781\r",
      "INFO - Step 680, rl-loss: 175.9907989501953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 681, rl-loss: 521.45068359375\r",
      "INFO - Step 682, rl-loss: 0.29373499751091003\r",
      "INFO - Step 683, rl-loss: 1478.4185791015625\r",
      "INFO - Step 684, rl-loss: 556.4335327148438\r",
      "INFO - Step 685, rl-loss: 200.55474853515625\r",
      "INFO - Step 686, rl-loss: 183.1798553466797\r",
      "INFO - Step 687, rl-loss: 0.49539923667907715\r",
      "INFO - Step 688, rl-loss: 257.87237548828125\r",
      "INFO - Step 689, rl-loss: 131.94815063476562\r",
      "INFO - Step 690, rl-loss: 126.09928131103516\r",
      "INFO - Step 691, rl-loss: 0.463902086019516\r",
      "INFO - Step 692, rl-loss: 423.1919250488281\r",
      "INFO - Step 693, rl-loss: 127.08805847167969\r",
      "INFO - Step 694, rl-loss: 0.5713946223258972\r",
      "INFO - Step 695, rl-loss: 967.083251953125\r",
      "INFO - Step 696, rl-loss: 729.1929321289062\r",
      "INFO - Step 697, rl-loss: 140.51583862304688\r",
      "INFO - Step 698, rl-loss: 95.60991668701172\r",
      "INFO - Step 699, rl-loss: 261.76470947265625\r",
      "INFO - Step 700, rl-loss: 0.3214227855205536"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 701, rl-loss: 232.1739501953125\r",
      "INFO - Step 702, rl-loss: 189.16172790527344\r",
      "INFO - Step 703, rl-loss: 112.29181671142578\r",
      "INFO - Step 704, rl-loss: 201.06622314453125\r",
      "INFO - Step 705, rl-loss: 0.7694374322891235\r",
      "INFO - Step 706, rl-loss: 226.14544677734375\r",
      "INFO - Step 707, rl-loss: 91.09007263183594\r",
      "INFO - Step 708, rl-loss: 125.04060363769531\r",
      "INFO - Step 709, rl-loss: 760.3001708984375\r",
      "INFO - Step 710, rl-loss: 200.4084014892578\r",
      "INFO - Step 711, rl-loss: 487.93994140625\r",
      "INFO - Step 712, rl-loss: 126.34478759765625\r",
      "INFO - Step 713, rl-loss: 239.66241455078125\r",
      "INFO - Step 714, rl-loss: 158.09506225585938\r",
      "INFO - Step 715, rl-loss: 189.73570251464844\r",
      "INFO - Step 716, rl-loss: 66.01042938232422\r",
      "INFO - Step 717, rl-loss: 184.0869140625\r",
      "INFO - Step 718, rl-loss: 436.4171447753906\r",
      "INFO - Step 719, rl-loss: 256.71026611328125\r",
      "INFO - Step 720, rl-loss: 121.45008850097656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 721, rl-loss: 366.31378173828125\r",
      "INFO - Step 722, rl-loss: 175.59410095214844\r",
      "INFO - Step 723, rl-loss: 0.8829281330108643\r",
      "INFO - Step 724, rl-loss: 313.9609680175781\r",
      "INFO - Step 725, rl-loss: 321.9712829589844\r",
      "INFO - Step 726, rl-loss: 381.0657958984375\r",
      "INFO - Step 727, rl-loss: 235.11776733398438\r",
      "INFO - Step 728, rl-loss: 281.5252990722656\r",
      "INFO - Step 729, rl-loss: 0.6199491620063782\r",
      "INFO - Step 730, rl-loss: 183.28109741210938\r",
      "INFO - Step 731, rl-loss: 165.97557067871094\r",
      "INFO - Step 732, rl-loss: 223.6232452392578\r",
      "INFO - Step 733, rl-loss: 0.5804370641708374\r",
      "INFO - Step 734, rl-loss: 0.5678913593292236\r",
      "INFO - Step 735, rl-loss: 140.4430389404297\r",
      "INFO - Step 736, rl-loss: 607.4693603515625\r",
      "INFO - Step 737, rl-loss: 65.1558837890625\r",
      "INFO - Step 738, rl-loss: 0.5669976472854614\r",
      "INFO - Step 739, rl-loss: 329.6727294921875\r",
      "INFO - Step 740, rl-loss: 180.78460693359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 741, rl-loss: 0.4890805780887604\r",
      "INFO - Step 742, rl-loss: 585.135498046875\r",
      "INFO - Step 743, rl-loss: 401.6632385253906\r",
      "INFO - Step 744, rl-loss: 693.4817504882812\r",
      "INFO - Step 745, rl-loss: 1175.445556640625\r",
      "INFO - Step 746, rl-loss: 361.0001525878906\r",
      "INFO - Step 747, rl-loss: 250.1328125\r",
      "INFO - Step 748, rl-loss: 0.5262285470962524\r",
      "INFO - Step 749, rl-loss: 170.07513427734375\r",
      "INFO - Step 750, rl-loss: 140.33360290527344\r",
      "INFO - Step 751, rl-loss: 201.5615997314453\r",
      "INFO - Step 752, rl-loss: 0.49213171005249023\r",
      "INFO - Step 753, rl-loss: 0.4270178973674774\r",
      "INFO - Step 754, rl-loss: 766.14501953125\r",
      "INFO - Step 755, rl-loss: 477.43310546875\r",
      "INFO - Step 756, rl-loss: 283.14227294921875\r",
      "INFO - Step 757, rl-loss: 46.85469055175781\r",
      "INFO - Step 758, rl-loss: 85.70262145996094\r",
      "INFO - Step 759, rl-loss: 86.12101745605469\r",
      "INFO - Step 760, rl-loss: 365.2788391113281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 761, rl-loss: 174.2885284423828\r",
      "INFO - Step 762, rl-loss: 220.8891143798828\r",
      "INFO - Step 763, rl-loss: 0.41807544231414795\r",
      "INFO - Step 764, rl-loss: 463.4458923339844\r",
      "INFO - Step 765, rl-loss: 111.02787780761719\r",
      "INFO - Step 766, rl-loss: 0.4297727942466736\r",
      "INFO - Step 767, rl-loss: 78.29149627685547\r",
      "INFO - Step 768, rl-loss: 140.03732299804688\r",
      "INFO - Step 769, rl-loss: 1363.5887451171875\r",
      "INFO - Step 770, rl-loss: 281.55047607421875\r",
      "INFO - Step 771, rl-loss: 156.4678192138672\r",
      "INFO - Step 772, rl-loss: 156.44651794433594\r",
      "INFO - Step 773, rl-loss: 199.48301696777344\r",
      "INFO - Step 774, rl-loss: 460.1010437011719\r",
      "INFO - Step 775, rl-loss: 260.463134765625\r",
      "INFO - Step 776, rl-loss: 157.3873748779297\r",
      "INFO - Step 777, rl-loss: 17.072410583496094\r",
      "INFO - Step 778, rl-loss: 211.89736938476562\r",
      "INFO - Step 779, rl-loss: 610.43408203125\r",
      "INFO - Step 780, rl-loss: 119.11016082763672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 781, rl-loss: 70.75656127929688\r",
      "INFO - Step 782, rl-loss: 184.9694366455078\r",
      "INFO - Step 783, rl-loss: 420.1110534667969\r",
      "INFO - Step 784, rl-loss: 184.99700927734375\r",
      "INFO - Step 785, rl-loss: 165.58425903320312\r",
      "INFO - Step 786, rl-loss: 167.05227661132812\r",
      "INFO - Step 787, rl-loss: 393.33782958984375\r",
      "INFO - Step 788, rl-loss: 0.7794965505599976\r",
      "INFO - Step 789, rl-loss: 355.2723083496094\r",
      "INFO - Step 790, rl-loss: 111.01026153564453\r",
      "INFO - Step 791, rl-loss: 0.3412437438964844\r",
      "INFO - Step 792, rl-loss: 355.4901428222656\r",
      "INFO - Step 793, rl-loss: 674.3336181640625\r",
      "INFO - Step 794, rl-loss: 347.11798095703125\r",
      "INFO - Step 795, rl-loss: 300.3670349121094\r",
      "INFO - Step 796, rl-loss: 516.1034545898438\r",
      "INFO - Step 797, rl-loss: 343.49066162109375\r",
      "INFO - Step 798, rl-loss: 80.18893432617188\r",
      "INFO - Step 799, rl-loss: 391.460205078125\r",
      "INFO - Step 800, rl-loss: 596.02685546875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 801, rl-loss: 518.2680053710938\r",
      "INFO - Step 802, rl-loss: 145.90591430664062\r",
      "INFO - Step 803, rl-loss: 0.4748692512512207\r",
      "INFO - Step 804, rl-loss: 31.531444549560547\r",
      "INFO - Step 805, rl-loss: 276.0196228027344\r",
      "INFO - Step 806, rl-loss: 0.8849673867225647\r",
      "INFO - Step 807, rl-loss: 119.13937377929688\r",
      "INFO - Step 808, rl-loss: 280.7835693359375\r",
      "INFO - Step 809, rl-loss: 198.71067810058594\r",
      "INFO - Step 810, rl-loss: 187.89060974121094\r",
      "INFO - Step 811, rl-loss: 359.5964660644531\r",
      "INFO - Step 812, rl-loss: 234.62229919433594\r",
      "INFO - Step 813, rl-loss: 131.5209197998047\r",
      "INFO - Step 814, rl-loss: 238.65345764160156\r",
      "INFO - Step 815, rl-loss: 118.69440460205078\r",
      "INFO - Step 816, rl-loss: 0.5926305055618286\r",
      "INFO - Step 817, rl-loss: 182.62928771972656\r",
      "INFO - Step 818, rl-loss: 0.3747277855873108\r",
      "INFO - Step 819, rl-loss: 140.98342895507812\r",
      "INFO - Step 820, rl-loss: 70.7371826171875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 821, rl-loss: 239.3221893310547\r",
      "INFO - Step 822, rl-loss: 44.710243225097656\r",
      "INFO - Step 823, rl-loss: 431.185302734375\r",
      "INFO - Step 824, rl-loss: 0.669794499874115\r",
      "INFO - Step 825, rl-loss: 0.35531139373779297\r",
      "INFO - Step 826, rl-loss: 121.58162689208984\r",
      "INFO - Step 827, rl-loss: 233.82542419433594\r",
      "INFO - Step 828, rl-loss: 691.1943969726562\r",
      "INFO - Step 829, rl-loss: 300.33837890625\r",
      "INFO - Step 830, rl-loss: 0.9156593680381775\r",
      "INFO - Step 831, rl-loss: 572.358154296875\r",
      "INFO - Step 832, rl-loss: 125.16104125976562\r",
      "INFO - Step 833, rl-loss: 234.65145874023438\r",
      "INFO - Step 834, rl-loss: 347.4903564453125\r",
      "INFO - Step 835, rl-loss: 212.15634155273438\r",
      "INFO - Step 836, rl-loss: 647.7946166992188\r",
      "INFO - Step 837, rl-loss: 45.34484100341797\r",
      "INFO - Step 838, rl-loss: 0.6287429332733154\r",
      "INFO - Step 839, rl-loss: 352.135986328125\r",
      "INFO - Step 840, rl-loss: 1167.7926025390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 841, rl-loss: 464.4205322265625\r",
      "INFO - Step 842, rl-loss: 269.40533447265625\r",
      "INFO - Step 843, rl-loss: 234.06536865234375\r",
      "INFO - Step 844, rl-loss: 335.531005859375\r",
      "INFO - Step 845, rl-loss: 254.71337890625\r",
      "INFO - Step 846, rl-loss: 0.41112232208251953\r",
      "INFO - Step 847, rl-loss: 280.3088073730469\r",
      "INFO - Step 848, rl-loss: 406.4535827636719\r",
      "INFO - Step 849, rl-loss: 241.74322509765625\r",
      "INFO - Step 850, rl-loss: 985.4833984375\r",
      "INFO - Step 851, rl-loss: 154.30184936523438\r",
      "INFO - Step 852, rl-loss: 79.06755828857422\r",
      "INFO - Step 853, rl-loss: 677.0336303710938\r",
      "INFO - Step 854, rl-loss: 273.6623229980469\r",
      "INFO - Step 855, rl-loss: 0.43879956007003784\r",
      "INFO - Step 856, rl-loss: 0.6667977571487427\r",
      "INFO - Step 857, rl-loss: 0.5727308392524719\r",
      "INFO - Step 858, rl-loss: 254.99168395996094\r",
      "INFO - Step 859, rl-loss: 352.6535949707031\r",
      "INFO - Step 860, rl-loss: 360.0381774902344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 861, rl-loss: 384.40032958984375\r",
      "INFO - Step 862, rl-loss: 69.90357208251953\r",
      "INFO - Step 863, rl-loss: 236.30136108398438\r",
      "INFO - Step 864, rl-loss: 297.1993408203125\r",
      "INFO - Step 865, rl-loss: 493.2965087890625\r",
      "INFO - Step 866, rl-loss: 129.69891357421875\r",
      "INFO - Step 867, rl-loss: 0.3745099902153015\r",
      "INFO - Step 868, rl-loss: 552.8206176757812\r",
      "INFO - Step 869, rl-loss: 156.33218383789062\r",
      "INFO - Step 870, rl-loss: 538.2908935546875\r",
      "INFO - Step 871, rl-loss: 156.17440795898438\r",
      "INFO - Step 872, rl-loss: 137.42147827148438\r",
      "INFO - Step 873, rl-loss: 816.1190185546875\r",
      "INFO - Step 874, rl-loss: 311.9980163574219\r",
      "INFO - Step 875, rl-loss: 45.18209457397461\r",
      "INFO - Step 876, rl-loss: 0.4540479779243469\r",
      "INFO - Step 877, rl-loss: 531.302001953125\r",
      "INFO - Step 878, rl-loss: 689.1358642578125\r",
      "INFO - Step 879, rl-loss: 618.5219116210938\r",
      "INFO - Step 880, rl-loss: 232.92807006835938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 881, rl-loss: 245.10604858398438\r",
      "INFO - Step 882, rl-loss: 454.14324951171875\r",
      "INFO - Step 883, rl-loss: 305.052490234375\r",
      "INFO - Step 884, rl-loss: 9.229921340942383\r",
      "INFO - Step 885, rl-loss: 45.01533889770508\r",
      "INFO - Step 886, rl-loss: 253.44204711914062\r",
      "INFO - Step 887, rl-loss: 77.69416046142578\r",
      "INFO - Step 888, rl-loss: 0.6870280504226685\r",
      "INFO - Step 889, rl-loss: 44.85553741455078\r",
      "INFO - Step 890, rl-loss: 323.7616271972656\r",
      "INFO - Step 891, rl-loss: 31.833255767822266\r",
      "INFO - Step 892, rl-loss: 270.5682373046875\r",
      "INFO - Step 893, rl-loss: 79.09341430664062\r",
      "INFO - Step 894, rl-loss: 0.5176163911819458\r",
      "INFO - Step 895, rl-loss: 1109.2322998046875\r",
      "INFO - Step 896, rl-loss: 199.65423583984375\r",
      "INFO - Step 897, rl-loss: 287.69573974609375\r",
      "INFO - Step 898, rl-loss: 337.69439697265625\r",
      "INFO - Step 899, rl-loss: 85.283935546875\r",
      "INFO - Step 900, rl-loss: 154.6781463623047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 901, rl-loss: 218.03077697753906\r",
      "INFO - Step 902, rl-loss: 154.7618408203125\r",
      "INFO - Step 903, rl-loss: 671.7533569335938\r",
      "INFO - Step 904, rl-loss: 0.9710959792137146\r",
      "INFO - Step 905, rl-loss: 268.9381103515625\r",
      "INFO - Step 906, rl-loss: 434.5669250488281\r",
      "INFO - Step 907, rl-loss: 401.9270324707031\r",
      "INFO - Step 908, rl-loss: 336.5147705078125\r",
      "INFO - Step 909, rl-loss: 43.69800567626953\r",
      "INFO - Step 910, rl-loss: 0.5959721803665161\r",
      "INFO - Step 911, rl-loss: 226.9244384765625\r",
      "INFO - Step 912, rl-loss: 104.04814147949219\r",
      "INFO - Step 913, rl-loss: 674.9298095703125\r",
      "INFO - Step 914, rl-loss: 287.13751220703125\r",
      "INFO - Step 915, rl-loss: 0.5514052510261536\r",
      "INFO - Step 916, rl-loss: 63.70180892944336\r",
      "INFO - Step 917, rl-loss: 182.7960205078125\r",
      "INFO - Step 918, rl-loss: 188.7005615234375\r",
      "INFO - Step 919, rl-loss: 382.6912536621094\r",
      "INFO - Step 920, rl-loss: 9.409236907958984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 921, rl-loss: 604.2296752929688\r",
      "INFO - Step 922, rl-loss: 0.41205936670303345\r",
      "INFO - Step 923, rl-loss: 278.4765625\r",
      "INFO - Step 924, rl-loss: 608.189453125\r",
      "INFO - Step 925, rl-loss: 45.21184158325195\r",
      "INFO - Step 926, rl-loss: 90.46554565429688\r",
      "INFO - Step 927, rl-loss: 227.33377075195312\r",
      "INFO - Step 928, rl-loss: 284.83477783203125\r",
      "INFO - Step 929, rl-loss: 213.94366455078125\r",
      "INFO - Step 930, rl-loss: 0.6883297562599182\r",
      "INFO - Step 931, rl-loss: 353.5078430175781\r",
      "INFO - Step 932, rl-loss: 0.33625632524490356\r",
      "INFO - Step 933, rl-loss: 439.93792724609375\r",
      "INFO - Step 934, rl-loss: 144.5008087158203\r",
      "INFO - Step 935, rl-loss: 172.32406616210938\r",
      "INFO - Step 936, rl-loss: 589.892578125\r",
      "INFO - Step 937, rl-loss: 74.43569946289062\r",
      "INFO - Step 938, rl-loss: 0.4874379634857178\r",
      "INFO - Step 939, rl-loss: 0.31770139932632446\r",
      "INFO - Step 940, rl-loss: 0.4664125442504883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 941, rl-loss: 0.48402920365333557\r",
      "INFO - Step 942, rl-loss: 90.63896942138672\r",
      "INFO - Step 943, rl-loss: 594.2489624023438\r",
      "INFO - Step 944, rl-loss: 0.5462799668312073\r",
      "INFO - Step 945, rl-loss: 353.9022521972656\r",
      "INFO - Step 946, rl-loss: 0.6121217012405396\r",
      "INFO - Step 947, rl-loss: 286.7801513671875\r",
      "INFO - Step 948, rl-loss: 156.52867126464844\r",
      "INFO - Step 949, rl-loss: 80.4023208618164\r",
      "INFO - Step 950, rl-loss: 73.78724670410156\r",
      "INFO - Step 951, rl-loss: 413.06060791015625\r",
      "INFO - Step 952, rl-loss: 278.0302429199219\r",
      "INFO - Step 953, rl-loss: 0.45651689171791077\r",
      "INFO - Step 954, rl-loss: 223.88848876953125\r",
      "INFO - Step 955, rl-loss: 0.5551977753639221\r",
      "INFO - Step 956, rl-loss: 120.1451644897461\r",
      "INFO - Step 957, rl-loss: 280.5673522949219\r",
      "INFO - Step 958, rl-loss: 83.97673034667969\r",
      "INFO - Step 959, rl-loss: 0.8253853917121887\r",
      "INFO - Step 960, rl-loss: 174.4802703857422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 961, rl-loss: 321.6360168457031\r",
      "INFO - Step 962, rl-loss: 198.204833984375\r",
      "INFO - Step 963, rl-loss: 641.6940307617188\r",
      "INFO - Step 964, rl-loss: 180.52676391601562\r",
      "INFO - Step 965, rl-loss: 84.01590728759766\r",
      "INFO - Step 966, rl-loss: 0.5050557255744934\r",
      "INFO - Step 967, rl-loss: 282.2974853515625\r",
      "INFO - Step 968, rl-loss: 139.58717346191406\r",
      "INFO - Step 969, rl-loss: 373.9012145996094\r",
      "INFO - Step 970, rl-loss: 155.1532440185547\r",
      "INFO - Step 971, rl-loss: 155.815673828125\r",
      "INFO - Step 972, rl-loss: 118.67931365966797\r",
      "INFO - Step 973, rl-loss: 0.5040270090103149\r",
      "INFO - Step 974, rl-loss: 0.49702003598213196\r",
      "INFO - Step 975, rl-loss: 69.331298828125\r",
      "INFO - Step 976, rl-loss: 0.7513941526412964\r",
      "INFO - Step 977, rl-loss: 425.51220703125\r",
      "INFO - Step 978, rl-loss: 103.81735229492188\r",
      "INFO - Step 979, rl-loss: 158.2570343017578\r",
      "INFO - Step 980, rl-loss: 300.7301330566406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 981, rl-loss: 149.04742431640625\r",
      "INFO - Step 982, rl-loss: 559.2102661132812\r",
      "INFO - Step 983, rl-loss: 557.3634033203125\r",
      "INFO - Step 984, rl-loss: 117.4437026977539\r",
      "INFO - Step 985, rl-loss: 273.4280090332031\r",
      "INFO - Step 986, rl-loss: 178.81918334960938\r",
      "INFO - Step 987, rl-loss: 178.57762145996094\r",
      "INFO - Step 988, rl-loss: 324.1468505859375\r",
      "INFO - Step 989, rl-loss: 196.9757537841797\r",
      "INFO - Step 990, rl-loss: 802.9866333007812\r",
      "INFO - Step 991, rl-loss: 0.4615512490272522\r",
      "INFO - Step 992, rl-loss: 163.50726318359375\r",
      "INFO - Step 993, rl-loss: 0.3514658212661743\r",
      "INFO - Step 994, rl-loss: 0.3497640788555145\r",
      "INFO - Step 995, rl-loss: 154.8689422607422\r",
      "INFO - Step 996, rl-loss: 410.30316162109375\r",
      "INFO - Step 997, rl-loss: 0.6859367489814758\r",
      "INFO - Step 998, rl-loss: 701.718505859375\r",
      "INFO - Step 999, rl-loss: 0.5979297757148743\r",
      "INFO - Step 1000, rl-loss: 539.6950073242188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 1020, rl-loss: 296.633819580078123\n",
      "----------------------------------------\n",
      "  timestep     |  519611\n",
      "  reward       |  64.15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 1040, rl-loss: 9.5240259170532236"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 1060, rl-loss: 202.259613037109387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1061, rl-loss: 305.266845703125\r",
      "INFO - Step 1062, rl-loss: 0.5706988573074341\r",
      "INFO - Step 1063, rl-loss: 223.91769409179688\r",
      "INFO - Step 1064, rl-loss: 355.02252197265625\r",
      "INFO - Step 1065, rl-loss: 122.72200775146484\r",
      "INFO - Step 1066, rl-loss: 369.9443664550781\r",
      "INFO - Step 1067, rl-loss: 234.0706787109375\r",
      "INFO - Step 1068, rl-loss: 234.38035583496094\r",
      "INFO - Step 1069, rl-loss: 847.8411865234375\r",
      "INFO - Step 1070, rl-loss: 88.69686126708984\r",
      "INFO - Step 1071, rl-loss: 339.3504943847656\r",
      "INFO - Step 1072, rl-loss: 71.25173950195312\r",
      "INFO - Step 1073, rl-loss: 179.33961486816406\r",
      "INFO - Step 1074, rl-loss: 419.2674255371094\r",
      "INFO - Step 1075, rl-loss: 147.98072814941406\r",
      "INFO - Step 1076, rl-loss: 5.107247352600098\r",
      "INFO - Step 1077, rl-loss: 0.3235050439834595\r",
      "INFO - Step 1078, rl-loss: 74.0787582397461\r",
      "INFO - Step 1079, rl-loss: 46.348270416259766\r",
      "INFO - Step 1080, rl-loss: 200.2603302001953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1081, rl-loss: 0.6396763920783997\r",
      "INFO - Step 1082, rl-loss: 79.98477935791016\r",
      "INFO - Step 1083, rl-loss: 148.0281982421875\r",
      "INFO - Step 1084, rl-loss: 353.9329833984375\r",
      "INFO - Step 1085, rl-loss: 290.0264892578125\r",
      "INFO - Step 1086, rl-loss: 494.09112548828125\r",
      "INFO - Step 1087, rl-loss: 705.0848999023438\r",
      "INFO - Step 1088, rl-loss: 155.8655548095703\r",
      "INFO - Step 1089, rl-loss: 453.45159912109375\r",
      "INFO - Step 1090, rl-loss: 313.69390869140625\r",
      "INFO - Step 1091, rl-loss: 0.43699216842651367\r",
      "INFO - Step 1092, rl-loss: 213.8822784423828\r",
      "INFO - Step 1093, rl-loss: 368.17822265625\r",
      "INFO - Step 1094, rl-loss: 751.79248046875\r",
      "INFO - Step 1095, rl-loss: 299.99755859375\r",
      "INFO - Step 1096, rl-loss: 360.2884521484375\r",
      "INFO - Step 1097, rl-loss: 178.96563720703125\r",
      "INFO - Step 1098, rl-loss: 449.0005187988281\r",
      "INFO - Step 1099, rl-loss: 0.7034196853637695\r",
      "INFO - Step 1100, rl-loss: 118.47535705566406\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1101, rl-loss: 372.8429260253906\r",
      "INFO - Step 1102, rl-loss: 641.0682983398438\r",
      "INFO - Step 1103, rl-loss: 138.84988403320312\r",
      "INFO - Step 1104, rl-loss: 0.8061742186546326\r",
      "INFO - Step 1105, rl-loss: 225.72991943359375\r",
      "INFO - Step 1106, rl-loss: 574.0571899414062\r",
      "INFO - Step 1107, rl-loss: 323.93951416015625\r",
      "INFO - Step 1108, rl-loss: 75.87450408935547\r",
      "INFO - Step 1109, rl-loss: 119.98233032226562\r",
      "INFO - Step 1110, rl-loss: 1.7760741710662842\r",
      "INFO - Step 1111, rl-loss: 119.1957015991211\r",
      "INFO - Step 1112, rl-loss: 222.12281799316406\r",
      "INFO - Step 1113, rl-loss: 361.1865234375\r",
      "INFO - Step 1114, rl-loss: 94.64298248291016\r",
      "INFO - Step 1115, rl-loss: 421.20556640625\r",
      "INFO - Step 1116, rl-loss: 293.67236328125\r",
      "INFO - Step 1117, rl-loss: 147.93109130859375\r",
      "INFO - Step 1118, rl-loss: 235.31072998046875\r",
      "INFO - Step 1119, rl-loss: 102.70913696289062\r",
      "INFO - Step 1120, rl-loss: 343.9450378417969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1121, rl-loss: 102.29814147949219\r",
      "INFO - Step 1122, rl-loss: 261.6871032714844\r",
      "INFO - Step 1123, rl-loss: 178.88246154785156\r",
      "INFO - Step 1124, rl-loss: 80.33001708984375\r",
      "INFO - Step 1125, rl-loss: 138.5207977294922\r",
      "INFO - Step 1126, rl-loss: 1.036203145980835\r",
      "INFO - Step 1127, rl-loss: 395.8003234863281\r",
      "INFO - Step 1128, rl-loss: 267.6817626953125\r",
      "INFO - Step 1129, rl-loss: 0.6589837074279785\r",
      "INFO - Step 1130, rl-loss: 19.5063533782959\r",
      "INFO - Step 1131, rl-loss: 0.7420269846916199\r",
      "INFO - Step 1132, rl-loss: 10.11577320098877\r",
      "INFO - Step 1133, rl-loss: 202.3639373779297\r",
      "INFO - Step 1134, rl-loss: 788.90625\r",
      "INFO - Step 1135, rl-loss: 122.2648696899414\r",
      "INFO - Step 1136, rl-loss: 272.892822265625\r",
      "INFO - Step 1137, rl-loss: 1.3164687156677246\r",
      "INFO - Step 1138, rl-loss: 1.4371590614318848\r",
      "INFO - Step 1139, rl-loss: 172.3008575439453\r",
      "INFO - Step 1140, rl-loss: 452.471923828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1141, rl-loss: 1.1894896030426025\r",
      "INFO - Step 1142, rl-loss: 358.2187805175781\r",
      "INFO - Step 1143, rl-loss: 85.9712905883789\r",
      "INFO - Step 1144, rl-loss: 1.3960871696472168\r",
      "INFO - Step 1145, rl-loss: 257.46087646484375\r",
      "INFO - Step 1146, rl-loss: 118.71514892578125\r",
      "INFO - Step 1147, rl-loss: 168.49951171875\r",
      "INFO - Step 1148, rl-loss: 44.26005172729492\r",
      "INFO - Step 1149, rl-loss: 96.4876480102539\r",
      "INFO - Step 1150, rl-loss: 89.62574005126953\r",
      "INFO - Step 1151, rl-loss: 75.24486541748047\r",
      "INFO - Step 1152, rl-loss: 412.2127380371094\r",
      "INFO - Step 1153, rl-loss: 1.0979493856430054\r",
      "INFO - Step 1154, rl-loss: 249.62440490722656\r",
      "INFO - Step 1155, rl-loss: 574.7363891601562\r",
      "INFO - Step 1156, rl-loss: 299.63079833984375\r",
      "INFO - Step 1157, rl-loss: 142.34263610839844\r",
      "INFO - Step 1158, rl-loss: 539.4743041992188\r",
      "INFO - Step 1159, rl-loss: 67.57071685791016\r",
      "INFO - Step 1160, rl-loss: 74.37936401367188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1161, rl-loss: 1.0897027254104614\r",
      "INFO - Step 1162, rl-loss: 618.4637451171875\r",
      "INFO - Step 1163, rl-loss: 538.0775146484375\r",
      "INFO - Step 1164, rl-loss: 337.2735290527344\r",
      "INFO - Step 1165, rl-loss: 562.94091796875\r",
      "INFO - Step 1166, rl-loss: 228.55715942382812\r",
      "INFO - Step 1167, rl-loss: 0.9654508829116821\r",
      "INFO - Step 1168, rl-loss: 674.6161499023438\r",
      "INFO - Step 1169, rl-loss: 1.1196333169937134\r",
      "INFO - Step 1170, rl-loss: 256.68804931640625\r",
      "INFO - Step 1171, rl-loss: 600.95556640625\r",
      "INFO - Step 1172, rl-loss: 666.2047119140625\r",
      "INFO - Step 1173, rl-loss: 70.96341705322266\r",
      "INFO - Step 1174, rl-loss: 237.3759002685547\r",
      "INFO - Step 1175, rl-loss: 360.2301025390625\r",
      "INFO - Step 1176, rl-loss: 1.353273868560791\r",
      "INFO - Step 1177, rl-loss: 223.0681915283203\r",
      "INFO - Step 1178, rl-loss: 372.1431579589844\r",
      "INFO - Step 1179, rl-loss: 70.49004364013672\r",
      "INFO - Step 1180, rl-loss: 0.8928616046905518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1181, rl-loss: 152.08258056640625\r",
      "INFO - Step 1182, rl-loss: 233.25930786132812\r",
      "INFO - Step 1183, rl-loss: 102.93318939208984\r",
      "INFO - Step 1184, rl-loss: 142.11001586914062\r",
      "INFO - Step 1185, rl-loss: 1.082131266593933\r",
      "INFO - Step 1186, rl-loss: 767.7682495117188\r",
      "INFO - Step 1187, rl-loss: 308.15679931640625\r",
      "INFO - Step 1188, rl-loss: 350.78009033203125\r",
      "INFO - Step 1189, rl-loss: 551.1768188476562\r",
      "INFO - Step 1190, rl-loss: 192.07652282714844\r",
      "INFO - Step 1191, rl-loss: 197.9425811767578\r",
      "INFO - Step 1192, rl-loss: 1.098541021347046\r",
      "INFO - Step 1193, rl-loss: 176.58859252929688\r",
      "INFO - Step 1194, rl-loss: 311.0906066894531\r",
      "INFO - Step 1195, rl-loss: 77.45618438720703\r",
      "INFO - Step 1196, rl-loss: 224.9661865234375\r",
      "INFO - Step 1197, rl-loss: 1.4995136260986328\r",
      "INFO - Step 1198, rl-loss: 193.37786865234375\r",
      "INFO - Step 1199, rl-loss: 174.25701904296875\r",
      "INFO - Step 1200, rl-loss: 638.1143798828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1201, rl-loss: 1.2112727165222168\r",
      "INFO - Step 1202, rl-loss: 1.2774394750595093\r",
      "INFO - Step 1203, rl-loss: 299.99530029296875\r",
      "INFO - Step 1204, rl-loss: 777.8681030273438\r",
      "INFO - Step 1205, rl-loss: 378.7240295410156\r",
      "INFO - Step 1206, rl-loss: 1.358701467514038\r",
      "INFO - Step 1207, rl-loss: 793.5341796875\r",
      "INFO - Step 1208, rl-loss: 322.54351806640625\r",
      "INFO - Step 1209, rl-loss: 297.3594665527344\r",
      "INFO - Step 1210, rl-loss: 62.88304901123047\r",
      "INFO - Step 1211, rl-loss: 84.679931640625\r",
      "INFO - Step 1212, rl-loss: 572.2850341796875\r",
      "INFO - Step 1213, rl-loss: 481.0496520996094\r",
      "INFO - Step 1214, rl-loss: 272.651123046875\r",
      "INFO - Step 1215, rl-loss: 247.74240112304688\r",
      "INFO - Step 1216, rl-loss: 0.9259990453720093\r",
      "INFO - Step 1217, rl-loss: 163.37583923339844\r",
      "INFO - Step 1218, rl-loss: 1.6708810329437256\r",
      "INFO - Step 1219, rl-loss: 558.3375854492188\r",
      "INFO - Step 1220, rl-loss: 213.79588317871094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1221, rl-loss: 377.25274658203125\r",
      "INFO - Step 1222, rl-loss: 504.0137939453125\r",
      "INFO - Step 1223, rl-loss: 534.0750122070312\r",
      "INFO - Step 1224, rl-loss: 154.65623474121094\r",
      "INFO - Step 1225, rl-loss: 280.55841064453125\r",
      "INFO - Step 1226, rl-loss: 1.595658540725708\r",
      "INFO - Step 1227, rl-loss: 117.81092071533203\r",
      "INFO - Step 1228, rl-loss: 585.6731567382812\r",
      "INFO - Step 1229, rl-loss: 187.85342407226562\r",
      "INFO - Step 1230, rl-loss: 10.228554725646973\r",
      "INFO - Step 1231, rl-loss: 148.27565002441406\r",
      "INFO - Step 1232, rl-loss: 230.75685119628906\r",
      "INFO - Step 1233, rl-loss: 993.9947509765625\r",
      "INFO - Step 1234, rl-loss: 223.83389282226562\r",
      "INFO - Step 1235, rl-loss: 137.44723510742188\r",
      "INFO - Step 1236, rl-loss: 69.3282699584961\r",
      "INFO - Step 1237, rl-loss: 0.9512710571289062\r",
      "INFO - Step 1238, rl-loss: 627.5830078125\r",
      "INFO - Step 1239, rl-loss: 551.230224609375\r",
      "INFO - Step 1240, rl-loss: 536.1917114257812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1241, rl-loss: 136.42190551757812\r",
      "INFO - Step 1242, rl-loss: 135.87704467773438\r",
      "INFO - Step 1243, rl-loss: 1.0556750297546387\r",
      "INFO - Step 1244, rl-loss: 0.7148788571357727\r",
      "INFO - Step 1245, rl-loss: 1.1982916593551636\r",
      "INFO - Step 1246, rl-loss: 102.7983169555664\r",
      "INFO - Step 1247, rl-loss: 247.2051544189453\r",
      "INFO - Step 1248, rl-loss: 279.77386474609375\r",
      "INFO - Step 1249, rl-loss: 0.9272347688674927\r",
      "INFO - Step 1250, rl-loss: 1.198075294494629\r",
      "INFO - Step 1251, rl-loss: 233.52444458007812\r",
      "INFO - Step 1252, rl-loss: 246.5743865966797\r",
      "INFO - Step 1253, rl-loss: 322.72509765625\r",
      "INFO - Step 1254, rl-loss: 196.80108642578125\r",
      "INFO - Step 1255, rl-loss: 297.1399841308594\r",
      "INFO - Step 1256, rl-loss: 1.0124417543411255\r",
      "INFO - Step 1257, rl-loss: 1.3667845726013184\r",
      "INFO - Step 1258, rl-loss: 101.83072662353516\r",
      "INFO - Step 1259, rl-loss: 144.07095336914062\r",
      "INFO - Step 1260, rl-loss: 353.3841552734375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1261, rl-loss: 118.04375457763672\r",
      "INFO - Step 1262, rl-loss: 79.95572662353516\r",
      "INFO - Step 1263, rl-loss: 148.1127471923828\r",
      "INFO - Step 1264, rl-loss: 117.92044830322266\r",
      "INFO - Step 1265, rl-loss: 14.273085594177246\r",
      "INFO - Step 1266, rl-loss: 290.4145812988281\r",
      "INFO - Step 1267, rl-loss: 33.30128479003906\r",
      "INFO - Step 1268, rl-loss: 235.07040405273438\r",
      "INFO - Step 1269, rl-loss: 279.73895263671875\r",
      "INFO - Step 1270, rl-loss: 370.1318054199219\r",
      "INFO - Step 1271, rl-loss: 136.47637939453125\r",
      "INFO - Step 1272, rl-loss: 106.50495910644531\r",
      "INFO - Step 1273, rl-loss: 1.1162798404693604\r",
      "INFO - Step 1274, rl-loss: 141.95367431640625\r",
      "INFO - Step 1275, rl-loss: 342.01617431640625\r",
      "INFO - Step 1276, rl-loss: 642.3765869140625\r",
      "INFO - Step 1277, rl-loss: 121.16545104980469\r",
      "INFO - Step 1278, rl-loss: 232.41323852539062\r",
      "INFO - Step 1279, rl-loss: 73.33818817138672\r",
      "INFO - Step 1280, rl-loss: 146.57456970214844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1281, rl-loss: 716.5108032226562\r",
      "INFO - Step 1282, rl-loss: 0.6973097324371338\r",
      "INFO - Step 1283, rl-loss: 183.22166442871094\r",
      "INFO - Step 1284, rl-loss: 70.44867706298828\r",
      "INFO - Step 1285, rl-loss: 1224.65478515625\r",
      "INFO - Step 1286, rl-loss: 265.2218017578125\r",
      "INFO - Step 1287, rl-loss: 1.031743049621582\r",
      "INFO - Step 1288, rl-loss: 0.7589617967605591\r",
      "INFO - Step 1289, rl-loss: 62.299041748046875\r",
      "INFO - Step 1290, rl-loss: 257.93487548828125\r",
      "INFO - Step 1291, rl-loss: 181.38157653808594\r",
      "INFO - Step 1292, rl-loss: 293.0723876953125\r",
      "INFO - Step 1293, rl-loss: 357.384765625\r",
      "INFO - Step 1294, rl-loss: 1.5505096912384033\r",
      "INFO - Step 1295, rl-loss: 118.15184020996094\r",
      "INFO - Step 1296, rl-loss: 217.1929931640625\r",
      "INFO - Step 1297, rl-loss: 431.2683410644531\r",
      "INFO - Step 1298, rl-loss: 1.1002463102340698\r",
      "INFO - Step 1299, rl-loss: 456.1935729980469\r",
      "INFO - Step 1300, rl-loss: 164.5358428955078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1301, rl-loss: 794.8793334960938\r",
      "INFO - Step 1302, rl-loss: 335.4521789550781\r",
      "INFO - Step 1303, rl-loss: 197.06472778320312\r",
      "INFO - Step 1304, rl-loss: 93.06903839111328\r",
      "INFO - Step 1305, rl-loss: 147.0542755126953\r",
      "INFO - Step 1306, rl-loss: 221.62240600585938\r",
      "INFO - Step 1307, rl-loss: 149.89053344726562\r",
      "INFO - Step 1308, rl-loss: 1.798201560974121\r",
      "INFO - Step 1309, rl-loss: 151.41334533691406\r",
      "INFO - Step 1310, rl-loss: 118.51573181152344\r",
      "INFO - Step 1311, rl-loss: 432.4809265136719\r",
      "INFO - Step 1312, rl-loss: 117.19886779785156\r",
      "INFO - Step 1313, rl-loss: 237.94998168945312\r",
      "INFO - Step 1314, rl-loss: 88.98532104492188\r",
      "INFO - Step 1315, rl-loss: 381.2677307128906\r",
      "INFO - Step 1316, rl-loss: 539.8175048828125\r",
      "INFO - Step 1317, rl-loss: 117.9542007446289\r",
      "INFO - Step 1318, rl-loss: 617.4627075195312\r",
      "INFO - Step 1319, rl-loss: 237.09927368164062\r",
      "INFO - Step 1320, rl-loss: 152.6727752685547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1321, rl-loss: 431.5030822753906\r",
      "INFO - Step 1322, rl-loss: 221.79649353027344\r",
      "INFO - Step 1323, rl-loss: 152.10360717773438\r",
      "INFO - Step 1324, rl-loss: 0.7254375219345093\r",
      "INFO - Step 1325, rl-loss: 118.44430541992188\r",
      "INFO - Step 1326, rl-loss: 156.6795654296875\r",
      "INFO - Step 1327, rl-loss: 0.879391610622406\r",
      "INFO - Step 1328, rl-loss: 77.0555191040039\r",
      "INFO - Step 1329, rl-loss: 88.84015655517578\r",
      "INFO - Step 1330, rl-loss: 0.8632301092147827\r",
      "INFO - Step 1331, rl-loss: 208.22142028808594\r",
      "INFO - Step 1332, rl-loss: 1.5885714292526245\r",
      "INFO - Step 1333, rl-loss: 285.5828857421875\r",
      "INFO - Step 1334, rl-loss: 293.76983642578125\r",
      "INFO - Step 1335, rl-loss: 1.1598737239837646\r",
      "INFO - Step 1336, rl-loss: 20.33757781982422\r",
      "INFO - Step 1337, rl-loss: 1.2572858333587646\r",
      "INFO - Step 1338, rl-loss: 74.00404357910156\r",
      "INFO - Step 1339, rl-loss: 123.42001342773438\r",
      "INFO - Step 1340, rl-loss: 0.8200556039810181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1341, rl-loss: 181.54652404785156\r",
      "INFO - Step 1342, rl-loss: 456.2789306640625\r",
      "INFO - Step 1343, rl-loss: 77.05115509033203\r",
      "INFO - Step 1344, rl-loss: 5.567079544067383\r",
      "INFO - Step 1345, rl-loss: 527.1947021484375\r",
      "INFO - Step 1346, rl-loss: 123.37110137939453\r",
      "INFO - Step 1347, rl-loss: 84.04712677001953\r",
      "INFO - Step 1348, rl-loss: 236.6751708984375\r",
      "INFO - Step 1349, rl-loss: 165.6738739013672\r",
      "INFO - Step 1350, rl-loss: 367.07568359375\r",
      "INFO - Step 1351, rl-loss: 1.0938184261322021\r",
      "INFO - Step 1352, rl-loss: 434.4190368652344\r",
      "INFO - Step 1353, rl-loss: 446.396484375\r",
      "INFO - Step 1354, rl-loss: 797.802490234375\r",
      "INFO - Step 1355, rl-loss: 528.1277465820312\r",
      "INFO - Step 1356, rl-loss: 141.8798828125\r",
      "INFO - Step 1357, rl-loss: 343.0286865234375\r",
      "INFO - Step 1358, rl-loss: 528.2509765625\r",
      "INFO - Step 1359, rl-loss: 0.7820160984992981\r",
      "INFO - Step 1360, rl-loss: 0.8910370469093323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1361, rl-loss: 1.368959665298462\r",
      "INFO - Step 1362, rl-loss: 799.9443359375\r",
      "INFO - Step 1363, rl-loss: 270.3126220703125\r",
      "INFO - Step 1364, rl-loss: 17.14704704284668\r",
      "INFO - Step 1365, rl-loss: 643.0819091796875\r",
      "INFO - Step 1366, rl-loss: 210.29624938964844\r",
      "INFO - Step 1367, rl-loss: 465.2559509277344\r",
      "INFO - Step 1368, rl-loss: 415.3869323730469\r",
      "INFO - Step 1369, rl-loss: 17.105127334594727\r",
      "INFO - Step 1370, rl-loss: 147.24072265625\r",
      "INFO - Step 1371, rl-loss: 140.48741149902344\r",
      "INFO - Step 1372, rl-loss: 140.7394561767578\r",
      "INFO - Step 1373, rl-loss: 417.10614013671875\r",
      "INFO - Step 1374, rl-loss: 188.051513671875\r",
      "INFO - Step 1375, rl-loss: 1.5023539066314697\r",
      "INFO - Step 1376, rl-loss: 119.3058853149414\r",
      "INFO - Step 1377, rl-loss: 78.27164459228516\r",
      "INFO - Step 1378, rl-loss: 1.2343966960906982\r",
      "INFO - Step 1379, rl-loss: 368.0072021484375\r",
      "INFO - Step 1380, rl-loss: 154.1612091064453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1381, rl-loss: 175.20510864257812\r",
      "INFO - Step 1382, rl-loss: 179.7427520751953\r",
      "INFO - Step 1383, rl-loss: 227.77163696289062\r",
      "INFO - Step 1384, rl-loss: 78.80450439453125\r",
      "INFO - Step 1385, rl-loss: 10.95618724822998\r",
      "INFO - Step 1386, rl-loss: 200.26593017578125\r",
      "INFO - Step 1387, rl-loss: 319.1541442871094\r",
      "INFO - Step 1388, rl-loss: 146.24075317382812\r",
      "INFO - Step 1389, rl-loss: 162.84915161132812\r",
      "INFO - Step 1390, rl-loss: 260.4061279296875\r",
      "INFO - Step 1391, rl-loss: 0.6769018173217773\r",
      "INFO - Step 1392, rl-loss: 5.849212646484375\r",
      "INFO - Step 1393, rl-loss: 70.68366241455078\r",
      "INFO - Step 1394, rl-loss: 0.9811016321182251\r",
      "INFO - Step 1395, rl-loss: 303.4039001464844\r",
      "INFO - Step 1396, rl-loss: 1.358159065246582\r",
      "INFO - Step 1397, rl-loss: 318.21258544921875\r",
      "INFO - Step 1398, rl-loss: 293.4366149902344\r",
      "INFO - Step 1399, rl-loss: 378.0185852050781\r",
      "INFO - Step 1400, rl-loss: 1.8844420909881592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1401, rl-loss: 525.12060546875\r",
      "INFO - Step 1402, rl-loss: 118.42262268066406\r",
      "INFO - Step 1403, rl-loss: 394.86260986328125\r",
      "INFO - Step 1404, rl-loss: 1.1176799535751343\r",
      "INFO - Step 1405, rl-loss: 242.35244750976562\r",
      "INFO - Step 1406, rl-loss: 79.3001937866211\r",
      "INFO - Step 1407, rl-loss: 1.193128228187561\r",
      "INFO - Step 1408, rl-loss: 72.71105194091797\r",
      "INFO - Step 1409, rl-loss: 535.3253784179688\r",
      "INFO - Step 1410, rl-loss: 908.314697265625\r",
      "INFO - Step 1411, rl-loss: 552.0081176757812\r",
      "INFO - Step 1412, rl-loss: 555.1673583984375\r",
      "INFO - Step 1413, rl-loss: 5.892502307891846\r",
      "INFO - Step 1414, rl-loss: 320.2877197265625\r",
      "INFO - Step 1415, rl-loss: 185.29273986816406\r",
      "INFO - Step 1416, rl-loss: 1.1817561388015747\r",
      "INFO - Step 1417, rl-loss: 1.3262293338775635\r",
      "INFO - Step 1418, rl-loss: 193.85830688476562\r",
      "INFO - Step 1419, rl-loss: 62.502872467041016\r",
      "INFO - Step 1420, rl-loss: 162.2923583984375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1421, rl-loss: 222.1904296875\r",
      "INFO - Step 1422, rl-loss: 0.9816749691963196\r",
      "INFO - Step 1423, rl-loss: 342.44976806640625\r",
      "INFO - Step 1424, rl-loss: 358.4815979003906\r",
      "INFO - Step 1425, rl-loss: 521.4783325195312\r",
      "INFO - Step 1426, rl-loss: 162.2914276123047\r",
      "INFO - Step 1427, rl-loss: 344.63885498046875\r",
      "INFO - Step 1428, rl-loss: 866.4947509765625\r",
      "INFO - Step 1429, rl-loss: 123.13528442382812\r",
      "INFO - Step 1430, rl-loss: 158.90301513671875\r",
      "INFO - Step 1431, rl-loss: 119.06233978271484\r",
      "INFO - Step 1432, rl-loss: 429.9730224609375\r",
      "INFO - Step 1433, rl-loss: 180.42294311523438\r",
      "INFO - Step 1434, rl-loss: 406.73455810546875\r",
      "INFO - Step 1435, rl-loss: 120.07044982910156\r",
      "INFO - Step 1436, rl-loss: 0.7885252237319946\r",
      "INFO - Step 1437, rl-loss: 72.53662872314453\r",
      "INFO - Step 1438, rl-loss: 146.22329711914062\r",
      "INFO - Step 1439, rl-loss: 61.436588287353516\r",
      "INFO - Step 1440, rl-loss: 244.0220184326172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1441, rl-loss: 146.79910278320312\r",
      "INFO - Step 1442, rl-loss: 9.533585548400879\r",
      "INFO - Step 1443, rl-loss: 212.93150329589844\r",
      "INFO - Step 1444, rl-loss: 118.06657409667969\r",
      "INFO - Step 1445, rl-loss: 235.33766174316406\r",
      "INFO - Step 1446, rl-loss: 1.638916015625\r",
      "INFO - Step 1447, rl-loss: 160.37051391601562\r",
      "INFO - Step 1448, rl-loss: 1.4706008434295654\r",
      "INFO - Step 1449, rl-loss: 44.346153259277344\r",
      "INFO - Step 1450, rl-loss: 99.52196502685547\r",
      "INFO - Step 1451, rl-loss: 5.474843978881836\r",
      "INFO - Step 1452, rl-loss: 1.022016167640686\r",
      "INFO - Step 1453, rl-loss: 68.93785858154297\r",
      "INFO - Step 1454, rl-loss: 1.1520280838012695\r",
      "INFO - Step 1455, rl-loss: 271.4947204589844\r",
      "INFO - Step 1456, rl-loss: 402.4945983886719\r",
      "INFO - Step 1457, rl-loss: 101.56439971923828\r",
      "INFO - Step 1458, rl-loss: 232.19097900390625\r",
      "INFO - Step 1459, rl-loss: 420.2576904296875\r",
      "INFO - Step 1460, rl-loss: 179.9241485595703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1461, rl-loss: 380.73480224609375\r",
      "INFO - Step 1462, rl-loss: 1.3565162420272827\r",
      "INFO - Step 1463, rl-loss: 539.4902954101562\r",
      "INFO - Step 1464, rl-loss: 365.6976318359375\r",
      "INFO - Step 1465, rl-loss: 396.42510986328125\r",
      "INFO - Step 1466, rl-loss: 0.9547927379608154\r",
      "INFO - Step 1467, rl-loss: 172.763427734375\r",
      "INFO - Step 1468, rl-loss: 156.93963623046875\r",
      "INFO - Step 1469, rl-loss: 122.79861450195312\r",
      "INFO - Step 1470, rl-loss: 417.32183837890625\r",
      "INFO - Step 1471, rl-loss: 1025.3292236328125\r",
      "INFO - Step 1472, rl-loss: 159.15965270996094\r",
      "INFO - Step 1473, rl-loss: 311.8128356933594\r",
      "INFO - Step 1474, rl-loss: 10.497042655944824\r",
      "INFO - Step 1475, rl-loss: 43.74641418457031\r",
      "INFO - Step 1476, rl-loss: 470.2236328125\r",
      "INFO - Step 1477, rl-loss: 306.6533203125\r",
      "INFO - Step 1478, rl-loss: 16.123626708984375\r",
      "INFO - Step 1479, rl-loss: 1.0521444082260132\r",
      "INFO - Step 1480, rl-loss: 175.83804321289062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1481, rl-loss: 122.90902709960938\r",
      "INFO - Step 1482, rl-loss: 1.0170689821243286\r",
      "INFO - Step 1483, rl-loss: 122.33061218261719\r",
      "INFO - Step 1484, rl-loss: 219.08694458007812\r",
      "INFO - Step 1485, rl-loss: 322.2469482421875\r",
      "INFO - Step 1486, rl-loss: 525.4537353515625\r",
      "INFO - Step 1487, rl-loss: 244.4579315185547\r",
      "INFO - Step 1488, rl-loss: 165.1297607421875\r",
      "INFO - Step 1489, rl-loss: 19.9876766204834\r",
      "INFO - Step 1490, rl-loss: 186.93597412109375\r",
      "INFO - Step 1491, rl-loss: 1.2134902477264404\r",
      "INFO - Step 1492, rl-loss: 279.7031555175781\r",
      "INFO - Step 1493, rl-loss: 68.99382019042969\r",
      "INFO - Step 1494, rl-loss: 232.11239624023438\r",
      "INFO - Step 1495, rl-loss: 172.48345947265625\r",
      "INFO - Step 1496, rl-loss: 145.07826232910156\r",
      "INFO - Step 1497, rl-loss: 1.1544952392578125\r",
      "INFO - Step 1498, rl-loss: 273.4399108886719\r",
      "INFO - Step 1499, rl-loss: 355.9617004394531\r",
      "INFO - Step 1500, rl-loss: 244.58766174316406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1501, rl-loss: 382.84442138671875\r",
      "INFO - Step 1502, rl-loss: 241.8417510986328\r",
      "INFO - Step 1503, rl-loss: 476.44189453125\r",
      "INFO - Step 1504, rl-loss: 61.55280685424805\r",
      "INFO - Step 1505, rl-loss: 83.0860824584961\r",
      "INFO - Step 1506, rl-loss: 247.8873291015625\r",
      "INFO - Step 1507, rl-loss: 82.39388275146484\r",
      "INFO - Step 1508, rl-loss: 1.0337393283843994\r",
      "INFO - Step 1509, rl-loss: 118.35161590576172\r",
      "INFO - Step 1510, rl-loss: 194.9664306640625\r",
      "INFO - Step 1511, rl-loss: 244.94813537597656\r",
      "INFO - Step 1512, rl-loss: 180.56838989257812\r",
      "INFO - Step 1513, rl-loss: 250.33612060546875\r",
      "INFO - Step 1514, rl-loss: 1.1503463983535767\r",
      "INFO - Step 1515, rl-loss: 267.1459045410156\r",
      "INFO - Step 1516, rl-loss: 46.62346267700195\r",
      "INFO - Step 1517, rl-loss: 480.7804870605469\r",
      "INFO - Step 1518, rl-loss: 255.97994995117188\r",
      "INFO - Step 1519, rl-loss: 1.6289132833480835\r",
      "INFO - Step 1520, rl-loss: 763.2905883789062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1521, rl-loss: 1.0934042930603027\r",
      "INFO - Step 1522, rl-loss: 17.126544952392578\r",
      "INFO - Step 1523, rl-loss: 259.4438781738281\r",
      "INFO - Step 1524, rl-loss: 404.101806640625\r",
      "INFO - Step 1525, rl-loss: 695.044677734375\r",
      "INFO - Step 1526, rl-loss: 77.114013671875\r",
      "INFO - Step 1527, rl-loss: 0.8476849794387817\r",
      "INFO - Step 1528, rl-loss: 1.1359339952468872\r",
      "INFO - Step 1529, rl-loss: 337.2967834472656\r",
      "INFO - Step 1530, rl-loss: 136.43435668945312\r",
      "INFO - Step 1531, rl-loss: 220.55735778808594\r",
      "INFO - Step 1532, rl-loss: 311.29083251953125\r",
      "INFO - Step 1533, rl-loss: 396.3886413574219\r",
      "INFO - Step 1534, rl-loss: 636.2515258789062\r",
      "INFO - Step 1535, rl-loss: 80.1502914428711\r",
      "INFO - Step 1536, rl-loss: 143.6106719970703\r",
      "INFO - Step 1537, rl-loss: 169.59524536132812\r",
      "INFO - Step 1538, rl-loss: 87.30587005615234\r",
      "INFO - Step 1539, rl-loss: 153.5122833251953\r",
      "INFO - Step 1540, rl-loss: 1.1379750967025757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1541, rl-loss: 1.2294477224349976\r",
      "INFO - Step 1542, rl-loss: 1.0705689191818237\r",
      "INFO - Step 1543, rl-loss: 0.962000846862793\r",
      "INFO - Step 1544, rl-loss: 79.03285217285156\r",
      "INFO - Step 1545, rl-loss: 451.543701171875\r",
      "INFO - Step 1546, rl-loss: 214.349365234375\r",
      "INFO - Step 1547, rl-loss: 186.95980834960938\r",
      "INFO - Step 1548, rl-loss: 293.118408203125\r",
      "INFO - Step 1549, rl-loss: 123.95024871826172\r",
      "INFO - Step 1550, rl-loss: 338.93939208984375\r",
      "INFO - Step 1551, rl-loss: 118.59877014160156\r",
      "INFO - Step 1552, rl-loss: 157.87368774414062\r",
      "INFO - Step 1553, rl-loss: 329.8891906738281\r",
      "INFO - Step 1554, rl-loss: 604.6748657226562\r",
      "INFO - Step 1555, rl-loss: 174.86549377441406\r",
      "INFO - Step 1556, rl-loss: 32.32359313964844\r",
      "INFO - Step 1557, rl-loss: 103.00601196289062\r",
      "INFO - Step 1558, rl-loss: 64.32830810546875\r",
      "INFO - Step 1559, rl-loss: 519.6243896484375\r",
      "INFO - Step 1560, rl-loss: 193.0279541015625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1561, rl-loss: 285.645263671875\r",
      "INFO - Step 1562, rl-loss: 244.86700439453125\r",
      "INFO - Step 1563, rl-loss: 232.77157592773438\r",
      "INFO - Step 1564, rl-loss: 10.519401550292969\r",
      "INFO - Step 1565, rl-loss: 259.35931396484375\r",
      "INFO - Step 1566, rl-loss: 173.8531951904297\r",
      "INFO - Step 1567, rl-loss: 365.7763977050781\r",
      "INFO - Step 1568, rl-loss: 1.1471173763275146\r",
      "INFO - Step 1569, rl-loss: 236.9766387939453\r",
      "INFO - Step 1570, rl-loss: 364.4534606933594\r",
      "INFO - Step 1571, rl-loss: 470.1527099609375\r",
      "INFO - Step 1572, rl-loss: 163.73072814941406\r",
      "INFO - Step 1573, rl-loss: 189.337158203125\r",
      "INFO - Step 1574, rl-loss: 116.43594360351562\r",
      "INFO - Step 1575, rl-loss: 654.0065307617188\r",
      "INFO - Step 1576, rl-loss: 188.63922119140625\r",
      "INFO - Step 1577, rl-loss: 281.7156677246094\r",
      "INFO - Step 1578, rl-loss: 259.6159362792969\r",
      "INFO - Step 1579, rl-loss: 87.61190795898438\r",
      "INFO - Step 1580, rl-loss: 516.8516235351562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1581, rl-loss: 383.3019714355469\r",
      "INFO - Step 1582, rl-loss: 1.238106608390808\r",
      "INFO - Step 1583, rl-loss: 253.04251098632812\r",
      "INFO - Step 1584, rl-loss: 286.8304443359375\r",
      "INFO - Step 1585, rl-loss: 210.9943084716797\r",
      "INFO - Step 1586, rl-loss: 598.0062866210938\r",
      "INFO - Step 1587, rl-loss: 450.8577880859375\r",
      "INFO - Step 1588, rl-loss: 663.3599853515625\r",
      "INFO - Step 1589, rl-loss: 140.43386840820312\r",
      "INFO - Step 1590, rl-loss: 309.2515869140625\r",
      "INFO - Step 1591, rl-loss: 190.3714141845703\r",
      "INFO - Step 1592, rl-loss: 379.6364440917969\r",
      "INFO - Step 1593, rl-loss: 92.35379028320312\r",
      "INFO - Step 1594, rl-loss: 34.27524185180664\r",
      "INFO - Step 1595, rl-loss: 30.0897216796875\r",
      "INFO - Step 1596, rl-loss: 1.0271838903427124\r",
      "INFO - Step 1597, rl-loss: 31.989686965942383\r",
      "INFO - Step 1598, rl-loss: 0.6717454195022583\r",
      "INFO - Step 1599, rl-loss: 446.3258972167969\r",
      "INFO - Step 1600, rl-loss: 416.30780029296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1601, rl-loss: 0.9835824370384216\r",
      "INFO - Step 1602, rl-loss: 277.928955078125\r",
      "INFO - Step 1603, rl-loss: 595.4713134765625\r",
      "INFO - Step 1604, rl-loss: 348.5138244628906\r",
      "INFO - Step 1605, rl-loss: 308.6419982910156\r",
      "INFO - Step 1606, rl-loss: 212.42694091796875\r",
      "INFO - Step 1607, rl-loss: 74.34768676757812\r",
      "INFO - Step 1608, rl-loss: 74.05278015136719\r",
      "INFO - Step 1609, rl-loss: 381.05010986328125\r",
      "INFO - Step 1610, rl-loss: 292.8762512207031\r",
      "INFO - Step 1611, rl-loss: 1.1313005685806274\r",
      "INFO - Step 1612, rl-loss: 135.8345489501953\r",
      "INFO - Step 1613, rl-loss: 124.99214172363281\r",
      "INFO - Step 1614, rl-loss: 746.0927124023438\r",
      "INFO - Step 1615, rl-loss: 179.756103515625\r",
      "INFO - Step 1616, rl-loss: 1.9393675327301025\r",
      "INFO - Step 1617, rl-loss: 72.02739715576172\r",
      "INFO - Step 1618, rl-loss: 190.37098693847656\r",
      "INFO - Step 1619, rl-loss: 264.89227294921875\r",
      "INFO - Step 1620, rl-loss: 156.95216369628906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1621, rl-loss: 1.2589529752731323\r",
      "INFO - Step 1622, rl-loss: 1.3810012340545654\r",
      "INFO - Step 1623, rl-loss: 18.990158081054688\r",
      "INFO - Step 1624, rl-loss: 1.011566400527954\r",
      "INFO - Step 1625, rl-loss: 161.53494262695312\r",
      "INFO - Step 1626, rl-loss: 0.9248026013374329\r",
      "INFO - Step 1627, rl-loss: 783.8330078125\r",
      "INFO - Step 1628, rl-loss: 138.47396850585938\r",
      "INFO - Step 1629, rl-loss: 427.7960510253906\r",
      "INFO - Step 1630, rl-loss: 1.163912057876587\r",
      "INFO - Step 1631, rl-loss: 315.454345703125\r",
      "INFO - Step 1632, rl-loss: 300.8420715332031\r",
      "INFO - Step 1633, rl-loss: 0.9696274995803833\r",
      "INFO - Step 1634, rl-loss: 162.69500732421875\r",
      "INFO - Step 1635, rl-loss: 145.83168029785156\r",
      "INFO - Step 1636, rl-loss: 518.6315307617188\r",
      "INFO - Step 1637, rl-loss: 533.3556518554688\r",
      "INFO - Step 1638, rl-loss: 228.40013122558594\r",
      "INFO - Step 1639, rl-loss: 0.9032213687896729\r",
      "INFO - Step 1640, rl-loss: 155.10757446289062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1641, rl-loss: 1.2204244136810303\r",
      "INFO - Step 1642, rl-loss: 11.244935989379883\r",
      "INFO - Step 1643, rl-loss: 218.50601196289062\r",
      "INFO - Step 1644, rl-loss: 1.2674930095672607\r",
      "INFO - Step 1645, rl-loss: 1.0207500457763672\r",
      "INFO - Step 1646, rl-loss: 154.5796356201172\r",
      "INFO - Step 1647, rl-loss: 408.2173156738281\r",
      "INFO - Step 1648, rl-loss: 198.8456573486328\r",
      "INFO - Step 1649, rl-loss: 230.66033935546875\r",
      "INFO - Step 1650, rl-loss: 199.14759826660156\r",
      "INFO - Step 1651, rl-loss: 486.7607421875\r",
      "INFO - Step 1652, rl-loss: 490.18756103515625\r",
      "INFO - Step 1653, rl-loss: 488.54705810546875\r",
      "INFO - Step 1654, rl-loss: 100.2890625\r",
      "INFO - Step 1655, rl-loss: 522.564453125\r",
      "INFO - Step 1656, rl-loss: 454.3540954589844\r",
      "INFO - Step 1657, rl-loss: 136.96340942382812\r",
      "INFO - Step 1658, rl-loss: 312.2032165527344\r",
      "INFO - Step 1659, rl-loss: 751.2481689453125\r",
      "INFO - Step 1660, rl-loss: 386.7267761230469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 1680, rl-loss: 1.1928458213806152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1681, rl-loss: 374.1052551269531\r",
      "INFO - Step 1682, rl-loss: 76.53163146972656\r",
      "INFO - Step 1683, rl-loss: 43.53944778442383\r",
      "INFO - Step 1684, rl-loss: 0.6027160286903381\r",
      "INFO - Step 1685, rl-loss: 0.977098286151886\r",
      "INFO - Step 1686, rl-loss: 29.970943450927734\r",
      "INFO - Step 1687, rl-loss: 434.401611328125\r",
      "INFO - Step 1688, rl-loss: 308.0494079589844\r",
      "INFO - Step 1689, rl-loss: 1.1217583417892456\r",
      "INFO - Step 1690, rl-loss: 19.304643630981445\r",
      "INFO - Step 1691, rl-loss: 0.8648908138275146\r",
      "INFO - Step 1692, rl-loss: 34.84663009643555\r",
      "INFO - Step 1693, rl-loss: 272.3525390625\r",
      "INFO - Step 1694, rl-loss: 367.2315368652344\r",
      "INFO - Step 1695, rl-loss: 184.9062042236328\r",
      "INFO - Step 1696, rl-loss: 1.3422149419784546\r",
      "INFO - Step 1697, rl-loss: 7.020840644836426\r",
      "INFO - Step 1698, rl-loss: 173.0018768310547\r",
      "INFO - Step 1699, rl-loss: 244.6174774169922\r",
      "INFO - Step 1700, rl-loss: 0.9638193845748901"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1701, rl-loss: 747.9152221679688\r",
      "INFO - Step 1702, rl-loss: 312.58203125\r",
      "INFO - Step 1703, rl-loss: 1.07245934009552\r",
      "INFO - Step 1704, rl-loss: 154.70458984375\r",
      "INFO - Step 1705, rl-loss: 0.7540541291236877\r",
      "INFO - Step 1706, rl-loss: 199.21759033203125\r",
      "INFO - Step 1707, rl-loss: 0.8824125528335571\r",
      "INFO - Step 1708, rl-loss: 0.8862284421920776\r",
      "INFO - Step 1709, rl-loss: 43.517086029052734\r",
      "INFO - Step 1710, rl-loss: 207.01165771484375\r",
      "INFO - Step 1711, rl-loss: 131.62840270996094\r",
      "INFO - Step 1712, rl-loss: 333.1310119628906\r",
      "INFO - Step 1713, rl-loss: 262.4676513671875\r",
      "INFO - Step 1714, rl-loss: 598.5934448242188\r",
      "INFO - Step 1715, rl-loss: 699.97216796875\r",
      "INFO - Step 1716, rl-loss: 284.9532775878906\r",
      "INFO - Step 1717, rl-loss: 323.34027099609375\r",
      "INFO - Step 1718, rl-loss: 0.8155182600021362\r",
      "INFO - Step 1719, rl-loss: 130.7970428466797\r",
      "INFO - Step 1720, rl-loss: 1.1012556552886963"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1721, rl-loss: 369.7490234375\r",
      "INFO - Step 1722, rl-loss: 187.74493408203125\r",
      "INFO - Step 1723, rl-loss: 117.35687255859375\r",
      "INFO - Step 1724, rl-loss: 217.78643798828125\r",
      "INFO - Step 1725, rl-loss: 435.45074462890625\r",
      "INFO - Step 1726, rl-loss: 151.39743041992188\r",
      "INFO - Step 1727, rl-loss: 1.3932561874389648\r",
      "INFO - Step 1728, rl-loss: 144.40811157226562\r",
      "INFO - Step 1729, rl-loss: 9.622977256774902\r",
      "INFO - Step 1730, rl-loss: 527.2847290039062\r",
      "INFO - Step 1731, rl-loss: 674.3959350585938\r",
      "INFO - Step 1732, rl-loss: 259.345458984375\r",
      "INFO - Step 1733, rl-loss: 6.5522260665893555\r",
      "INFO - Step 1734, rl-loss: 144.42234802246094\r",
      "INFO - Step 1735, rl-loss: 226.3667755126953\r",
      "INFO - Step 1736, rl-loss: 624.1231079101562\r",
      "INFO - Step 1737, rl-loss: 1.2997779846191406\r",
      "INFO - Step 1738, rl-loss: 52.17200469970703\r",
      "INFO - Step 1739, rl-loss: 1.7721118927001953\r",
      "INFO - Step 1740, rl-loss: 1.003798484802246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1741, rl-loss: 116.52243041992188\r",
      "INFO - Step 1742, rl-loss: 533.1839599609375\r",
      "INFO - Step 1743, rl-loss: 133.6342010498047\r",
      "INFO - Step 1744, rl-loss: 138.45098876953125\r",
      "INFO - Step 1745, rl-loss: 78.52259063720703\r",
      "INFO - Step 1746, rl-loss: 366.70635986328125\r",
      "INFO - Step 1747, rl-loss: 125.95410919189453\r",
      "INFO - Step 1748, rl-loss: 185.0276641845703\r",
      "INFO - Step 1749, rl-loss: 264.54132080078125\r",
      "INFO - Step 1750, rl-loss: 201.72119140625\r",
      "INFO - Step 1751, rl-loss: 297.761962890625\r",
      "INFO - Step 1752, rl-loss: 171.12701416015625\r",
      "INFO - Step 1753, rl-loss: 75.99748992919922\r",
      "INFO - Step 1754, rl-loss: 445.1649169921875\r",
      "INFO - Step 1755, rl-loss: 195.71856689453125\r",
      "INFO - Step 1756, rl-loss: 407.72137451171875\r",
      "INFO - Step 1757, rl-loss: 0.8695143461227417\r",
      "INFO - Step 1758, rl-loss: 0.8825401067733765\r",
      "INFO - Step 1759, rl-loss: 474.28997802734375\r",
      "INFO - Step 1760, rl-loss: 231.2560272216797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1761, rl-loss: 226.1845245361328\r",
      "INFO - Step 1762, rl-loss: 162.0279998779297\r",
      "INFO - Step 1763, rl-loss: 282.69866943359375\r",
      "INFO - Step 1764, rl-loss: 1.6717872619628906\r",
      "INFO - Step 1765, rl-loss: 68.03050231933594\r",
      "INFO - Step 1766, rl-loss: 501.8074951171875\r",
      "INFO - Step 1767, rl-loss: 304.43994140625\r",
      "INFO - Step 1768, rl-loss: 826.3146362304688\r",
      "INFO - Step 1769, rl-loss: 229.9925537109375\r",
      "INFO - Step 1770, rl-loss: 490.08441162109375\r",
      "INFO - Step 1771, rl-loss: 1.0143930912017822\r",
      "INFO - Step 1772, rl-loss: 139.14956665039062\r",
      "INFO - Step 1773, rl-loss: 239.64410400390625\r",
      "INFO - Step 1774, rl-loss: 138.817626953125\r",
      "INFO - Step 1775, rl-loss: 0.9592333436012268\r",
      "INFO - Step 1776, rl-loss: 0.6847331523895264\r",
      "INFO - Step 1777, rl-loss: 804.8875732421875\r",
      "INFO - Step 1778, rl-loss: 232.39956665039062\r",
      "INFO - Step 1779, rl-loss: 184.6603546142578\r",
      "INFO - Step 1780, rl-loss: 91.99111938476562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1781, rl-loss: 403.7054443359375\r",
      "INFO - Step 1782, rl-loss: 0.9033257961273193\r",
      "INFO - Step 1783, rl-loss: 127.74171447753906\r",
      "INFO - Step 1784, rl-loss: 428.9317932128906\r",
      "INFO - Step 1785, rl-loss: 117.35053253173828\r",
      "INFO - Step 1786, rl-loss: 78.97920227050781\r",
      "INFO - Step 1787, rl-loss: 161.36717224121094\r",
      "INFO - Step 1788, rl-loss: 520.5880126953125\r",
      "INFO - Step 1789, rl-loss: 1.4170364141464233\r",
      "INFO - Step 1790, rl-loss: 346.5649108886719\r",
      "INFO - Step 1791, rl-loss: 211.5392303466797\r",
      "INFO - Step 1792, rl-loss: 168.14974975585938\r",
      "INFO - Step 1793, rl-loss: 1.0828146934509277\r",
      "INFO - Step 1794, rl-loss: 0.7590383291244507\r",
      "INFO - Step 1795, rl-loss: 0.9010446667671204\r",
      "INFO - Step 1796, rl-loss: 43.37136459350586\r",
      "INFO - Step 1797, rl-loss: 360.60369873046875\r",
      "INFO - Step 1798, rl-loss: 399.8929443359375\r",
      "INFO - Step 1799, rl-loss: 603.3563232421875\r",
      "INFO - Step 1800, rl-loss: 131.01400756835938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1801, rl-loss: 539.0140380859375\r",
      "INFO - Step 1802, rl-loss: 479.11102294921875\r",
      "INFO - Step 1803, rl-loss: 1.7090857028961182\r",
      "INFO - Step 1804, rl-loss: 501.43536376953125\r",
      "INFO - Step 1805, rl-loss: 175.41659545898438\r",
      "INFO - Step 1806, rl-loss: 179.2600860595703\r",
      "INFO - Step 1807, rl-loss: 1.1757620573043823\r",
      "INFO - Step 1808, rl-loss: 115.62468719482422\r",
      "INFO - Step 1809, rl-loss: 162.13613891601562\r",
      "INFO - Step 1810, rl-loss: 81.67462921142578\r",
      "INFO - Step 1811, rl-loss: 575.0068359375\r",
      "INFO - Step 1812, rl-loss: 77.60157775878906\r",
      "INFO - Step 1813, rl-loss: 183.83535766601562\r",
      "INFO - Step 1814, rl-loss: 1.4442638158798218\r",
      "INFO - Step 1815, rl-loss: 0.5552668571472168\r",
      "INFO - Step 1816, rl-loss: 1.42942476272583\r",
      "INFO - Step 1817, rl-loss: 1023.848876953125\r",
      "INFO - Step 1818, rl-loss: 191.11724853515625\r",
      "INFO - Step 1819, rl-loss: 265.69085693359375\r",
      "INFO - Step 1820, rl-loss: 0.9343667030334473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1821, rl-loss: 510.9271240234375\r",
      "INFO - Step 1822, rl-loss: 159.38037109375\r",
      "INFO - Step 1823, rl-loss: 479.99871826171875\r",
      "INFO - Step 1824, rl-loss: 379.59228515625\r",
      "INFO - Step 1825, rl-loss: 327.12322998046875\r",
      "INFO - Step 1826, rl-loss: 701.5824584960938\r",
      "INFO - Step 1827, rl-loss: 1.1217036247253418\r",
      "INFO - Step 1828, rl-loss: 297.87213134765625\r",
      "INFO - Step 1829, rl-loss: 1.9197138547897339\r",
      "INFO - Step 1830, rl-loss: 182.8633270263672\r",
      "INFO - Step 1831, rl-loss: 1.2697556018829346\r",
      "INFO - Step 1832, rl-loss: 283.6947326660156\r",
      "INFO - Step 1833, rl-loss: 1.1666200160980225\r",
      "INFO - Step 1834, rl-loss: 54.763031005859375\r",
      "INFO - Step 1835, rl-loss: 61.47438049316406\r",
      "INFO - Step 1836, rl-loss: 1.6546365022659302\r",
      "INFO - Step 1837, rl-loss: 349.588134765625\r",
      "INFO - Step 1838, rl-loss: 0.8213402032852173\r",
      "INFO - Step 1839, rl-loss: 198.16075134277344\r",
      "INFO - Step 1840, rl-loss: 152.6066436767578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1841, rl-loss: 265.4261169433594\r",
      "INFO - Step 1842, rl-loss: 1.1809309720993042\r",
      "INFO - Step 1843, rl-loss: 139.41790771484375\r",
      "INFO - Step 1844, rl-loss: 5.175448417663574\r",
      "INFO - Step 1845, rl-loss: 143.80874633789062\r",
      "INFO - Step 1846, rl-loss: 308.8184814453125\r",
      "INFO - Step 1847, rl-loss: 131.22760009765625\r",
      "INFO - Step 1848, rl-loss: 122.38066101074219\r",
      "INFO - Step 1849, rl-loss: 294.9681091308594\r",
      "INFO - Step 1850, rl-loss: 97.83190155029297\r",
      "INFO - Step 1851, rl-loss: 587.90673828125\r",
      "INFO - Step 1852, rl-loss: 202.85614013671875\r",
      "INFO - Step 1853, rl-loss: 874.1905517578125\r",
      "INFO - Step 1854, rl-loss: 1.0253008604049683\r",
      "INFO - Step 1855, rl-loss: 89.30463409423828\r",
      "INFO - Step 1856, rl-loss: 206.5978240966797\r",
      "INFO - Step 1857, rl-loss: 488.3097229003906\r",
      "INFO - Step 1858, rl-loss: 134.19937133789062\r",
      "INFO - Step 1859, rl-loss: 350.1502380371094\r",
      "INFO - Step 1860, rl-loss: 1.390354037284851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1861, rl-loss: 143.32696533203125\r",
      "INFO - Step 1862, rl-loss: 6.952284812927246\r",
      "INFO - Step 1863, rl-loss: 355.6982727050781\r",
      "INFO - Step 1864, rl-loss: 183.06053161621094\r",
      "INFO - Step 1865, rl-loss: 1.2559211254119873\r",
      "INFO - Step 1866, rl-loss: 309.6070251464844\r",
      "INFO - Step 1867, rl-loss: 272.9270324707031\r",
      "INFO - Step 1868, rl-loss: 69.81843566894531\r",
      "INFO - Step 1869, rl-loss: 398.00360107421875\r",
      "INFO - Step 1870, rl-loss: 208.61077880859375\r",
      "INFO - Step 1871, rl-loss: 84.21995544433594\r",
      "INFO - Step 1872, rl-loss: 215.0278778076172\r",
      "INFO - Step 1873, rl-loss: 0.6223260760307312\r",
      "INFO - Step 1874, rl-loss: 0.9453901052474976\r",
      "INFO - Step 1875, rl-loss: 298.4217834472656\r",
      "INFO - Step 1876, rl-loss: 0.8760820627212524\r",
      "INFO - Step 1877, rl-loss: 0.7740854024887085\r",
      "INFO - Step 1878, rl-loss: 367.92779541015625\r",
      "INFO - Step 1879, rl-loss: 1.212304711341858\r",
      "INFO - Step 1880, rl-loss: 296.7464599609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1881, rl-loss: 0.9297896027565002\r",
      "INFO - Step 1882, rl-loss: 125.12297821044922\r",
      "INFO - Step 1883, rl-loss: 1.4773935079574585\r",
      "INFO - Step 1884, rl-loss: 138.71286010742188\r",
      "INFO - Step 1885, rl-loss: 50.67382049560547\r",
      "INFO - Step 1886, rl-loss: 437.72509765625\r",
      "INFO - Step 1887, rl-loss: 1.2671282291412354\r",
      "INFO - Step 1888, rl-loss: 179.20098876953125\r",
      "INFO - Step 1889, rl-loss: 292.57427978515625\r",
      "INFO - Step 1890, rl-loss: 285.67840576171875\r",
      "INFO - Step 1891, rl-loss: 0.7719334959983826\r",
      "INFO - Step 1892, rl-loss: 216.60650634765625\r",
      "INFO - Step 1893, rl-loss: 230.58245849609375\r",
      "INFO - Step 1894, rl-loss: 1.2836768627166748\r",
      "INFO - Step 1895, rl-loss: 232.2406768798828\r",
      "INFO - Step 1896, rl-loss: 332.005615234375\r",
      "INFO - Step 1897, rl-loss: 371.5633239746094\r",
      "INFO - Step 1898, rl-loss: 318.4181823730469\r",
      "INFO - Step 1899, rl-loss: 73.89371490478516\r",
      "INFO - Step 1900, rl-loss: 217.89862060546875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1901, rl-loss: 796.0090942382812\r",
      "INFO - Step 1902, rl-loss: 354.890625\r",
      "INFO - Step 1903, rl-loss: 1.0112509727478027\r",
      "INFO - Step 1904, rl-loss: 446.2620544433594\r",
      "INFO - Step 1905, rl-loss: 340.1849670410156\r",
      "INFO - Step 1906, rl-loss: 270.373046875\r",
      "INFO - Step 1907, rl-loss: 43.71853256225586\r",
      "INFO - Step 1908, rl-loss: 74.64710998535156\r",
      "INFO - Step 1909, rl-loss: 289.8930358886719\r",
      "INFO - Step 1910, rl-loss: 73.69593811035156\r",
      "INFO - Step 1911, rl-loss: 152.51296997070312\r",
      "INFO - Step 1912, rl-loss: 153.83177185058594\r",
      "INFO - Step 1913, rl-loss: 0.6724504828453064\r",
      "INFO - Step 1914, rl-loss: 607.8999633789062\r",
      "INFO - Step 1915, rl-loss: 287.5674743652344\r",
      "INFO - Step 1916, rl-loss: 79.12800598144531\r",
      "INFO - Step 1917, rl-loss: 343.08447265625\r",
      "INFO - Step 1918, rl-loss: 154.60546875\r",
      "INFO - Step 1919, rl-loss: 87.57686614990234\r",
      "INFO - Step 1920, rl-loss: 128.3256378173828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1921, rl-loss: 0.8440045714378357\r",
      "INFO - Step 1922, rl-loss: 117.4217529296875\r",
      "INFO - Step 1923, rl-loss: 159.6920166015625\r",
      "INFO - Step 1924, rl-loss: 0.8759211897850037\r",
      "INFO - Step 1925, rl-loss: 144.9487762451172\r",
      "INFO - Step 1926, rl-loss: 0.8794585466384888\r",
      "INFO - Step 1927, rl-loss: 0.8458041548728943\r",
      "INFO - Step 1928, rl-loss: 72.03858947753906\r",
      "INFO - Step 1929, rl-loss: 362.9604187011719\r",
      "INFO - Step 1930, rl-loss: 88.40531921386719\r",
      "INFO - Step 1931, rl-loss: 193.6419677734375\r",
      "INFO - Step 1932, rl-loss: 70.84833526611328\r",
      "INFO - Step 1933, rl-loss: 283.386474609375\r",
      "INFO - Step 1934, rl-loss: 264.44244384765625\r",
      "INFO - Step 1935, rl-loss: 236.42642211914062\r",
      "INFO - Step 1936, rl-loss: 0.7974902987480164\r",
      "INFO - Step 1937, rl-loss: 373.9859619140625\r",
      "INFO - Step 1938, rl-loss: 492.94189453125\r",
      "INFO - Step 1939, rl-loss: 324.4581298828125\r",
      "INFO - Step 1940, rl-loss: 1.4105019569396973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1941, rl-loss: 594.4310302734375\r",
      "INFO - Step 1942, rl-loss: 0.6807615756988525\r",
      "INFO - Step 1943, rl-loss: 341.20458984375\r",
      "INFO - Step 1944, rl-loss: 0.9910587072372437\r",
      "INFO - Step 1945, rl-loss: 230.03240966796875\r",
      "INFO - Step 1946, rl-loss: 1.2152862548828125\r",
      "INFO - Step 1947, rl-loss: 1.0692137479782104\r",
      "INFO - Step 1948, rl-loss: 1.0809211730957031\r",
      "INFO - Step 1949, rl-loss: 1.1326367855072021\r",
      "INFO - Step 1950, rl-loss: 81.78825378417969\r",
      "INFO - Step 1951, rl-loss: 0.6356927156448364\r",
      "INFO - Step 1952, rl-loss: 116.23615264892578\r",
      "INFO - Step 1953, rl-loss: 329.23828125\r",
      "INFO - Step 1954, rl-loss: 299.6994323730469\r",
      "INFO - Step 1955, rl-loss: 200.96792602539062\r",
      "INFO - Step 1956, rl-loss: 78.99795532226562\r",
      "INFO - Step 1957, rl-loss: 481.003662109375\r",
      "INFO - Step 1958, rl-loss: 195.9503173828125\r",
      "INFO - Step 1959, rl-loss: 0.743388831615448\r",
      "INFO - Step 1960, rl-loss: 842.3724365234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1961, rl-loss: 59.30647277832031\r",
      "INFO - Step 1962, rl-loss: 188.92535400390625\r",
      "INFO - Step 1963, rl-loss: 43.888641357421875\r",
      "INFO - Step 1964, rl-loss: 182.44857788085938\r",
      "INFO - Step 1965, rl-loss: 511.5503845214844\r",
      "INFO - Step 1966, rl-loss: 129.93490600585938\r",
      "INFO - Step 1967, rl-loss: 144.89292907714844\r",
      "INFO - Step 1968, rl-loss: 449.1750183105469\r",
      "INFO - Step 1969, rl-loss: 189.2577362060547\r",
      "INFO - Step 1970, rl-loss: 235.63772583007812\r",
      "INFO - Step 1971, rl-loss: 0.8403565287590027\r",
      "INFO - Step 1972, rl-loss: 0.8434280157089233\r",
      "INFO - Step 1973, rl-loss: 603.4277954101562\r",
      "INFO - Step 1974, rl-loss: 1.1655181646347046\r",
      "INFO - Step 1975, rl-loss: 116.93746948242188\r",
      "INFO - Step 1976, rl-loss: 74.01459503173828\r",
      "INFO - Step 1977, rl-loss: 361.0005187988281\r",
      "INFO - Step 1978, rl-loss: 289.33160400390625\r",
      "INFO - Step 1979, rl-loss: 277.2944641113281\r",
      "INFO - Step 1980, rl-loss: 0.6500571966171265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 1981, rl-loss: 288.3022766113281\r",
      "INFO - Step 1982, rl-loss: 370.7118835449219\r",
      "INFO - Step 1983, rl-loss: 9.386235237121582\r",
      "INFO - Step 1984, rl-loss: 73.33589935302734\r",
      "INFO - Step 1985, rl-loss: 174.3201904296875\r",
      "INFO - Step 1986, rl-loss: 88.753662109375\r",
      "INFO - Step 1987, rl-loss: 338.8769226074219\r",
      "INFO - Step 1988, rl-loss: 305.8924255371094\r",
      "INFO - Step 1989, rl-loss: 216.7387237548828\r",
      "INFO - Step 1990, rl-loss: 143.75759887695312\r",
      "INFO - Step 1991, rl-loss: 116.13462829589844\r",
      "INFO - Step 1992, rl-loss: 367.7320861816406\r",
      "INFO - Step 1993, rl-loss: 466.2905578613281\r",
      "INFO - Step 1994, rl-loss: 1.1347242593765259\r",
      "INFO - Step 1995, rl-loss: 1.0386589765548706\r",
      "INFO - Step 1996, rl-loss: 327.9375305175781\r",
      "INFO - Step 1997, rl-loss: 130.36322021484375\r",
      "INFO - Step 1998, rl-loss: 335.0063171386719\r",
      "INFO - Step 1999, rl-loss: 160.17562866210938\r",
      "INFO - Step 2000, rl-loss: 172.51185607910156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 2020, rl-loss: 197.33390808105475\n",
      "----------------------------------------\n",
      "  timestep     |  525541\n",
      "  reward       |  59.74\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 2040, rl-loss: 143.86463928222656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2041, rl-loss: 31.386600494384766\r",
      "INFO - Step 2042, rl-loss: 1.3151477575302124\r",
      "INFO - Step 2043, rl-loss: 432.24072265625\r",
      "INFO - Step 2044, rl-loss: 682.3277587890625\r",
      "INFO - Step 2045, rl-loss: 412.4621276855469\r",
      "INFO - Step 2046, rl-loss: 626.2098999023438\r",
      "INFO - Step 2047, rl-loss: 237.615966796875\r",
      "INFO - Step 2048, rl-loss: 200.14076232910156\r",
      "INFO - Step 2049, rl-loss: 344.47314453125\r",
      "INFO - Step 2050, rl-loss: 27.65045928955078\r",
      "INFO - Step 2051, rl-loss: 195.01092529296875\r",
      "INFO - Step 2052, rl-loss: 1.1486774682998657\r",
      "INFO - Step 2053, rl-loss: 282.3035888671875\r",
      "INFO - Step 2054, rl-loss: 290.62530517578125\r",
      "INFO - Step 2055, rl-loss: 0.7144457697868347\r",
      "INFO - Step 2056, rl-loss: 60.95543670654297\r",
      "INFO - Step 2057, rl-loss: 299.04937744140625\r",
      "INFO - Step 2058, rl-loss: 138.1030731201172\r",
      "INFO - Step 2059, rl-loss: 416.74652099609375\r",
      "INFO - Step 2060, rl-loss: 294.56781005859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2061, rl-loss: 753.8731689453125\r",
      "INFO - Step 2062, rl-loss: 323.00848388671875\r",
      "INFO - Step 2063, rl-loss: 5.508438587188721\r",
      "INFO - Step 2064, rl-loss: 115.56047821044922\r",
      "INFO - Step 2065, rl-loss: 0.8793377876281738\r",
      "INFO - Step 2066, rl-loss: 70.04434967041016\r",
      "INFO - Step 2067, rl-loss: 161.86532592773438\r",
      "INFO - Step 2068, rl-loss: 122.70938110351562\r",
      "INFO - Step 2069, rl-loss: 325.330078125\r",
      "INFO - Step 2070, rl-loss: 313.8957824707031\r",
      "INFO - Step 2071, rl-loss: 100.33662414550781\r",
      "INFO - Step 2072, rl-loss: 1.191746473312378\r",
      "INFO - Step 2073, rl-loss: 378.0575256347656\r",
      "INFO - Step 2074, rl-loss: 87.38408660888672\r",
      "INFO - Step 2075, rl-loss: 363.8141174316406\r",
      "INFO - Step 2076, rl-loss: 214.11500549316406\r",
      "INFO - Step 2077, rl-loss: 127.51896667480469\r",
      "INFO - Step 2078, rl-loss: 120.7023696899414\r",
      "INFO - Step 2079, rl-loss: 143.697998046875\r",
      "INFO - Step 2080, rl-loss: 53.6319465637207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2081, rl-loss: 105.27587890625\r",
      "INFO - Step 2082, rl-loss: 138.81602478027344\r",
      "INFO - Step 2083, rl-loss: 304.8561706542969\r",
      "INFO - Step 2084, rl-loss: 1.3231430053710938\r",
      "INFO - Step 2085, rl-loss: 215.60687255859375\r",
      "INFO - Step 2086, rl-loss: 0.8016185164451599\r",
      "INFO - Step 2087, rl-loss: 1.0603077411651611\r",
      "INFO - Step 2088, rl-loss: 1.307993769645691\r",
      "INFO - Step 2089, rl-loss: 779.298095703125\r",
      "INFO - Step 2090, rl-loss: 311.6129150390625\r",
      "INFO - Step 2091, rl-loss: 348.96405029296875\r",
      "INFO - Step 2092, rl-loss: 1.5639045238494873\r",
      "INFO - Step 2093, rl-loss: 1.2911252975463867\r",
      "INFO - Step 2094, rl-loss: 1.2342371940612793\r",
      "INFO - Step 2095, rl-loss: 1.1140166521072388\r",
      "INFO - Step 2096, rl-loss: 231.56051635742188\r",
      "INFO - Step 2097, rl-loss: 113.03067779541016\r",
      "INFO - Step 2098, rl-loss: 152.3970489501953\r",
      "INFO - Step 2099, rl-loss: 159.73228454589844\r",
      "INFO - Step 2100, rl-loss: 0.90093594789505\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2101, rl-loss: 412.2444763183594\r",
      "INFO - Step 2102, rl-loss: 647.068115234375\r",
      "INFO - Step 2103, rl-loss: 115.58692932128906\r",
      "INFO - Step 2104, rl-loss: 1.4808636903762817\r",
      "INFO - Step 2105, rl-loss: 227.8331756591797\r",
      "INFO - Step 2106, rl-loss: 9.433473587036133\r",
      "INFO - Step 2107, rl-loss: 430.8970031738281\r",
      "INFO - Step 2108, rl-loss: 193.1656494140625\r",
      "INFO - Step 2109, rl-loss: 1.4996297359466553\r",
      "INFO - Step 2110, rl-loss: 155.2646942138672\r",
      "INFO - Step 2111, rl-loss: 541.1953125\r",
      "INFO - Step 2112, rl-loss: 44.04551315307617\r",
      "INFO - Step 2113, rl-loss: 167.51263427734375\r",
      "INFO - Step 2114, rl-loss: 1.2432293891906738\r",
      "INFO - Step 2115, rl-loss: 403.5729064941406\r",
      "INFO - Step 2116, rl-loss: 582.531494140625\r",
      "INFO - Step 2117, rl-loss: 1.2664986848831177\r",
      "INFO - Step 2118, rl-loss: 97.23353576660156\r",
      "INFO - Step 2119, rl-loss: 180.05819702148438\r",
      "INFO - Step 2120, rl-loss: 1.34879469871521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2121, rl-loss: 45.8933219909668\r",
      "INFO - Step 2122, rl-loss: 110.87092590332031\r",
      "INFO - Step 2123, rl-loss: 1.6813422441482544\r",
      "INFO - Step 2124, rl-loss: 53.872100830078125\r",
      "INFO - Step 2125, rl-loss: 116.53411865234375\r",
      "INFO - Step 2126, rl-loss: 75.02247619628906\r",
      "INFO - Step 2127, rl-loss: 334.5881652832031\r",
      "INFO - Step 2128, rl-loss: 371.1275939941406\r",
      "INFO - Step 2129, rl-loss: 409.92352294921875\r",
      "INFO - Step 2130, rl-loss: 561.6348266601562\r",
      "INFO - Step 2131, rl-loss: 130.234130859375\r",
      "INFO - Step 2132, rl-loss: 209.31724548339844\r",
      "INFO - Step 2133, rl-loss: 447.089111328125\r",
      "INFO - Step 2134, rl-loss: 8.764760971069336\r",
      "INFO - Step 2135, rl-loss: 243.7222137451172\r",
      "INFO - Step 2136, rl-loss: 236.27455139160156\r",
      "INFO - Step 2137, rl-loss: 1.1211838722229004\r",
      "INFO - Step 2138, rl-loss: 325.37030029296875\r",
      "INFO - Step 2139, rl-loss: 365.20184326171875\r",
      "INFO - Step 2140, rl-loss: 1.3565860986709595"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2141, rl-loss: 321.266357421875\r",
      "INFO - Step 2142, rl-loss: 339.4133605957031\r",
      "INFO - Step 2143, rl-loss: 68.15103149414062\r",
      "INFO - Step 2144, rl-loss: 91.7455825805664\r",
      "INFO - Step 2145, rl-loss: 525.34326171875\r",
      "INFO - Step 2146, rl-loss: 75.88993072509766\r",
      "INFO - Step 2147, rl-loss: 420.0758361816406\r",
      "INFO - Step 2148, rl-loss: 16.709022521972656\r",
      "INFO - Step 2149, rl-loss: 100.20120239257812\r",
      "INFO - Step 2150, rl-loss: 143.9812774658203\r",
      "INFO - Step 2151, rl-loss: 201.4838104248047\r",
      "INFO - Step 2152, rl-loss: 362.5501708984375\r",
      "INFO - Step 2153, rl-loss: 1.3327010869979858\r",
      "INFO - Step 2154, rl-loss: 361.56512451171875\r",
      "INFO - Step 2155, rl-loss: 33.39748001098633\r",
      "INFO - Step 2156, rl-loss: 457.9354553222656\r",
      "INFO - Step 2157, rl-loss: 1.1326544284820557\r",
      "INFO - Step 2158, rl-loss: 516.6270751953125\r",
      "INFO - Step 2159, rl-loss: 239.47547912597656\r",
      "INFO - Step 2160, rl-loss: 359.746826171875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2161, rl-loss: 116.43968963623047\r",
      "INFO - Step 2162, rl-loss: 124.15155029296875\r",
      "INFO - Step 2163, rl-loss: 71.24063110351562\r",
      "INFO - Step 2164, rl-loss: 116.4920425415039\r",
      "INFO - Step 2165, rl-loss: 189.13308715820312\r",
      "INFO - Step 2166, rl-loss: 589.6541137695312\r",
      "INFO - Step 2167, rl-loss: 71.99657440185547\r",
      "INFO - Step 2168, rl-loss: 8.283515930175781\r",
      "INFO - Step 2169, rl-loss: 252.584228515625\r",
      "INFO - Step 2170, rl-loss: 0.9575303792953491\r",
      "INFO - Step 2171, rl-loss: 587.1121826171875\r",
      "INFO - Step 2172, rl-loss: 27.54841423034668\r",
      "INFO - Step 2173, rl-loss: 310.5143127441406\r",
      "INFO - Step 2174, rl-loss: 307.36907958984375\r",
      "INFO - Step 2175, rl-loss: 279.8556823730469\r",
      "INFO - Step 2176, rl-loss: 1.195671558380127\r",
      "INFO - Step 2177, rl-loss: 1.7101736068725586\r",
      "INFO - Step 2178, rl-loss: 1.8032264709472656\r",
      "INFO - Step 2179, rl-loss: 659.0541381835938\r",
      "INFO - Step 2180, rl-loss: 534.615234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2181, rl-loss: 289.0186462402344\r",
      "INFO - Step 2182, rl-loss: 72.13086700439453\r",
      "INFO - Step 2183, rl-loss: 32.177894592285156\r",
      "INFO - Step 2184, rl-loss: 264.282958984375\r",
      "INFO - Step 2185, rl-loss: 10.775135040283203\r",
      "INFO - Step 2186, rl-loss: 0.9701125621795654\r",
      "INFO - Step 2187, rl-loss: 75.58588409423828\r",
      "INFO - Step 2188, rl-loss: 519.140380859375\r",
      "INFO - Step 2189, rl-loss: 74.20610809326172\r",
      "INFO - Step 2190, rl-loss: 213.51760864257812\r",
      "INFO - Step 2191, rl-loss: 0.8734092116355896\r",
      "INFO - Step 2192, rl-loss: 572.2720947265625\r",
      "INFO - Step 2193, rl-loss: 109.90914916992188\r",
      "INFO - Step 2194, rl-loss: 276.00115966796875\r",
      "INFO - Step 2195, rl-loss: 252.50564575195312\r",
      "INFO - Step 2196, rl-loss: 249.44772338867188\r",
      "INFO - Step 2197, rl-loss: 43.798831939697266\r",
      "INFO - Step 2198, rl-loss: 265.228515625\r",
      "INFO - Step 2199, rl-loss: 501.39849853515625\r",
      "INFO - Step 2200, rl-loss: 64.67022705078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2201, rl-loss: 504.44183349609375\r",
      "INFO - Step 2202, rl-loss: 1.6097912788391113\r",
      "INFO - Step 2203, rl-loss: 1.686984896659851\r",
      "INFO - Step 2204, rl-loss: 361.8140563964844\r",
      "INFO - Step 2205, rl-loss: 1.7756423950195312\r",
      "INFO - Step 2206, rl-loss: 574.2854614257812\r",
      "INFO - Step 2207, rl-loss: 296.3371276855469\r",
      "INFO - Step 2208, rl-loss: 229.13580322265625\r",
      "INFO - Step 2209, rl-loss: 8.11440372467041\r",
      "INFO - Step 2210, rl-loss: 87.0630111694336\r",
      "INFO - Step 2211, rl-loss: 274.2887268066406\r",
      "INFO - Step 2212, rl-loss: 254.02734375\r",
      "INFO - Step 2213, rl-loss: 53.591793060302734\r",
      "INFO - Step 2214, rl-loss: 113.43252563476562\r",
      "INFO - Step 2215, rl-loss: 152.71481323242188\r",
      "INFO - Step 2216, rl-loss: 138.03341674804688\r",
      "INFO - Step 2217, rl-loss: 187.7417449951172\r",
      "INFO - Step 2218, rl-loss: 390.85650634765625\r",
      "INFO - Step 2219, rl-loss: 889.9907836914062\r",
      "INFO - Step 2220, rl-loss: 149.74539184570312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2221, rl-loss: 1.2720110416412354\r",
      "INFO - Step 2222, rl-loss: 228.34817504882812\r",
      "INFO - Step 2223, rl-loss: 140.3007354736328\r",
      "INFO - Step 2224, rl-loss: 118.3046875\r",
      "INFO - Step 2225, rl-loss: 603.3134765625\r",
      "INFO - Step 2226, rl-loss: 27.88629722595215\r",
      "INFO - Step 2227, rl-loss: 185.50198364257812\r",
      "INFO - Step 2228, rl-loss: 10.095569610595703\r",
      "INFO - Step 2229, rl-loss: 43.1288948059082\r",
      "INFO - Step 2230, rl-loss: 142.28448486328125\r",
      "INFO - Step 2231, rl-loss: 151.55813598632812\r",
      "INFO - Step 2232, rl-loss: 345.0877380371094\r",
      "INFO - Step 2233, rl-loss: 18.995162963867188\r",
      "INFO - Step 2234, rl-loss: 140.7098388671875\r",
      "INFO - Step 2235, rl-loss: 209.194091796875\r",
      "INFO - Step 2236, rl-loss: 31.814794540405273\r",
      "INFO - Step 2237, rl-loss: 0.7382586598396301\r",
      "INFO - Step 2238, rl-loss: 1.1865322589874268\r",
      "INFO - Step 2239, rl-loss: 363.5802001953125\r",
      "INFO - Step 2240, rl-loss: 69.78292083740234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2241, rl-loss: 17.02108383178711\r",
      "INFO - Step 2242, rl-loss: 192.06716918945312\r",
      "INFO - Step 2243, rl-loss: 2.108011484146118\r",
      "INFO - Step 2244, rl-loss: 325.90447998046875\r",
      "INFO - Step 2245, rl-loss: 1.144526481628418\r",
      "INFO - Step 2246, rl-loss: 1.0809814929962158\r",
      "INFO - Step 2247, rl-loss: 180.18978881835938\r",
      "INFO - Step 2248, rl-loss: 135.1360321044922\r",
      "INFO - Step 2249, rl-loss: 142.59437561035156\r",
      "INFO - Step 2250, rl-loss: 134.2308807373047\r",
      "INFO - Step 2251, rl-loss: 282.99053955078125\r",
      "INFO - Step 2252, rl-loss: 182.53558349609375\r",
      "INFO - Step 2253, rl-loss: 111.09021759033203\r",
      "INFO - Step 2254, rl-loss: 298.2860107421875\r",
      "INFO - Step 2255, rl-loss: 134.35108947753906\r",
      "INFO - Step 2256, rl-loss: 383.12109375\r",
      "INFO - Step 2257, rl-loss: 51.497703552246094\r",
      "INFO - Step 2258, rl-loss: 284.32373046875\r",
      "INFO - Step 2259, rl-loss: 344.4775695800781\r",
      "INFO - Step 2260, rl-loss: 61.222652435302734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2261, rl-loss: 875.0904541015625\r",
      "INFO - Step 2262, rl-loss: 376.0943603515625\r",
      "INFO - Step 2263, rl-loss: 228.58636474609375\r",
      "INFO - Step 2264, rl-loss: 108.59318542480469\r",
      "INFO - Step 2265, rl-loss: 1.3930151462554932\r",
      "INFO - Step 2266, rl-loss: 1.5667238235473633\r",
      "INFO - Step 2267, rl-loss: 407.8393859863281\r",
      "INFO - Step 2268, rl-loss: 469.5674743652344\r",
      "INFO - Step 2269, rl-loss: 441.73236083984375\r",
      "INFO - Step 2270, rl-loss: 475.6440124511719\r",
      "INFO - Step 2271, rl-loss: 395.7471923828125\r",
      "INFO - Step 2272, rl-loss: 307.28033447265625\r",
      "INFO - Step 2273, rl-loss: 268.1286315917969\r",
      "INFO - Step 2274, rl-loss: 442.66375732421875\r",
      "INFO - Step 2275, rl-loss: 360.2173156738281\r",
      "INFO - Step 2276, rl-loss: 194.8332061767578\r",
      "INFO - Step 2277, rl-loss: 1.3892643451690674\r",
      "INFO - Step 2278, rl-loss: 550.4821166992188\r",
      "INFO - Step 2279, rl-loss: 113.32027435302734\r",
      "INFO - Step 2280, rl-loss: 352.27398681640625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2281, rl-loss: 1.1543325185775757\r",
      "INFO - Step 2282, rl-loss: 1.3809314966201782\r",
      "INFO - Step 2283, rl-loss: 162.2600860595703\r",
      "INFO - Step 2284, rl-loss: 428.77099609375\r",
      "INFO - Step 2285, rl-loss: 326.68829345703125\r",
      "INFO - Step 2286, rl-loss: 246.35690307617188\r",
      "INFO - Step 2287, rl-loss: 117.26991271972656\r",
      "INFO - Step 2288, rl-loss: 287.35675048828125\r",
      "INFO - Step 2289, rl-loss: 207.1798553466797\r",
      "INFO - Step 2290, rl-loss: 1.1830819845199585\r",
      "INFO - Step 2291, rl-loss: 1.0696890354156494\r",
      "INFO - Step 2292, rl-loss: 59.02845764160156\r",
      "INFO - Step 2293, rl-loss: 630.5120239257812\r",
      "INFO - Step 2294, rl-loss: 497.8269348144531\r",
      "INFO - Step 2295, rl-loss: 530.9013671875\r",
      "INFO - Step 2296, rl-loss: 0.9333745241165161\r",
      "INFO - Step 2297, rl-loss: 313.7679748535156\r",
      "INFO - Step 2298, rl-loss: 53.45947265625\r",
      "INFO - Step 2299, rl-loss: 687.27294921875\r",
      "INFO - Step 2300, rl-loss: 117.44802856445312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2301, rl-loss: 267.7530212402344\r",
      "INFO - Step 2302, rl-loss: 228.92984008789062\r",
      "INFO - Step 2303, rl-loss: 179.83547973632812\r",
      "INFO - Step 2304, rl-loss: 324.63543701171875\r",
      "INFO - Step 2305, rl-loss: 346.2373046875\r",
      "INFO - Step 2306, rl-loss: 234.3856658935547\r",
      "INFO - Step 2307, rl-loss: 1.177273154258728\r",
      "INFO - Step 2308, rl-loss: 215.3708038330078\r",
      "INFO - Step 2309, rl-loss: 1.5275402069091797\r",
      "INFO - Step 2310, rl-loss: 197.1652374267578\r",
      "INFO - Step 2311, rl-loss: 1.0815420150756836\r",
      "INFO - Step 2312, rl-loss: 1.585620641708374\r",
      "INFO - Step 2313, rl-loss: 96.90467071533203\r",
      "INFO - Step 2314, rl-loss: 669.7030029296875\r",
      "INFO - Step 2315, rl-loss: 480.1527099609375\r",
      "INFO - Step 2316, rl-loss: 392.7546691894531\r",
      "INFO - Step 2317, rl-loss: 245.0234832763672\r",
      "INFO - Step 2318, rl-loss: 1.5099687576293945\r",
      "INFO - Step 2319, rl-loss: 549.6249389648438\r",
      "INFO - Step 2320, rl-loss: 1.8594437837600708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2321, rl-loss: 98.06275177001953\r",
      "INFO - Step 2322, rl-loss: 78.31407928466797\r",
      "INFO - Step 2323, rl-loss: 42.63017272949219\r",
      "INFO - Step 2324, rl-loss: 210.08587646484375\r",
      "INFO - Step 2325, rl-loss: 91.8381118774414\r",
      "INFO - Step 2326, rl-loss: 100.81475830078125\r",
      "INFO - Step 2327, rl-loss: 688.7844848632812\r",
      "INFO - Step 2328, rl-loss: 5.001272678375244\r",
      "INFO - Step 2329, rl-loss: 355.2850646972656\r",
      "INFO - Step 2330, rl-loss: 1.0215595960617065\r",
      "INFO - Step 2331, rl-loss: 285.45233154296875\r",
      "INFO - Step 2332, rl-loss: 256.803955078125\r",
      "INFO - Step 2333, rl-loss: 1.637137770652771\r",
      "INFO - Step 2334, rl-loss: 223.71461486816406\r",
      "INFO - Step 2335, rl-loss: 425.5414123535156\r",
      "INFO - Step 2336, rl-loss: 658.9236450195312\r",
      "INFO - Step 2337, rl-loss: 244.28598022460938\r",
      "INFO - Step 2338, rl-loss: 617.29931640625\r",
      "INFO - Step 2339, rl-loss: 1.4526216983795166\r",
      "INFO - Step 2340, rl-loss: 166.9075927734375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2341, rl-loss: 447.5188903808594\r",
      "INFO - Step 2342, rl-loss: 214.6514892578125\r",
      "INFO - Step 2343, rl-loss: 105.62890625\r",
      "INFO - Step 2344, rl-loss: 157.71868896484375\r",
      "INFO - Step 2345, rl-loss: 35.97798538208008\r",
      "INFO - Step 2346, rl-loss: 324.7047424316406\r",
      "INFO - Step 2347, rl-loss: 1.8055657148361206\r",
      "INFO - Step 2348, rl-loss: 1.91776442527771\r",
      "INFO - Step 2349, rl-loss: 42.621646881103516\r",
      "INFO - Step 2350, rl-loss: 121.56392669677734\r",
      "INFO - Step 2351, rl-loss: 42.05323791503906\r",
      "INFO - Step 2352, rl-loss: 42.013362884521484\r",
      "INFO - Step 2353, rl-loss: 452.6630859375\r",
      "INFO - Step 2354, rl-loss: 279.53948974609375\r",
      "INFO - Step 2355, rl-loss: 390.93280029296875\r",
      "INFO - Step 2356, rl-loss: 79.26158142089844\r",
      "INFO - Step 2357, rl-loss: 644.0708618164062\r",
      "INFO - Step 2358, rl-loss: 1.8565430641174316\r",
      "INFO - Step 2359, rl-loss: 1.1792237758636475\r",
      "INFO - Step 2360, rl-loss: 133.78421020507812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2361, rl-loss: 259.9759216308594\r",
      "INFO - Step 2362, rl-loss: 0.9821553230285645\r",
      "INFO - Step 2363, rl-loss: 1.189100742340088\r",
      "INFO - Step 2364, rl-loss: 227.3007354736328\r",
      "INFO - Step 2365, rl-loss: 1.5072760581970215\r",
      "INFO - Step 2366, rl-loss: 84.66905212402344\r",
      "INFO - Step 2367, rl-loss: 398.15625\r",
      "INFO - Step 2368, rl-loss: 1.3189390897750854\r",
      "INFO - Step 2369, rl-loss: 246.1935577392578\r",
      "INFO - Step 2370, rl-loss: 218.9209442138672\r",
      "INFO - Step 2371, rl-loss: 1.0385682582855225\r",
      "INFO - Step 2372, rl-loss: 133.515625\r",
      "INFO - Step 2373, rl-loss: 363.0777587890625\r",
      "INFO - Step 2374, rl-loss: 1.3535430431365967\r",
      "INFO - Step 2375, rl-loss: 1.2647042274475098\r",
      "INFO - Step 2376, rl-loss: 1.4429008960723877\r",
      "INFO - Step 2377, rl-loss: 15.878987312316895\r",
      "INFO - Step 2378, rl-loss: 263.70654296875\r",
      "INFO - Step 2379, rl-loss: 1.0149316787719727\r",
      "INFO - Step 2380, rl-loss: 21.92782974243164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2381, rl-loss: 486.80810546875\r",
      "INFO - Step 2382, rl-loss: 142.442138671875\r",
      "INFO - Step 2383, rl-loss: 69.01175689697266\r",
      "INFO - Step 2384, rl-loss: 1.3130772113800049\r",
      "INFO - Step 2385, rl-loss: 734.7325439453125\r",
      "INFO - Step 2386, rl-loss: 269.70538330078125\r",
      "INFO - Step 2387, rl-loss: 457.524169921875\r",
      "INFO - Step 2388, rl-loss: 428.12298583984375\r",
      "INFO - Step 2389, rl-loss: 256.69366455078125\r",
      "INFO - Step 2390, rl-loss: 58.27360534667969\r",
      "INFO - Step 2391, rl-loss: 73.48421478271484\r",
      "INFO - Step 2392, rl-loss: 69.49678802490234\r",
      "INFO - Step 2393, rl-loss: 243.07513427734375\r",
      "INFO - Step 2394, rl-loss: 119.61328887939453\r",
      "INFO - Step 2395, rl-loss: 27.309953689575195\r",
      "INFO - Step 2396, rl-loss: 259.81494140625\r",
      "INFO - Step 2397, rl-loss: 1.5338013172149658\r",
      "INFO - Step 2398, rl-loss: 186.39157104492188\r",
      "INFO - Step 2399, rl-loss: 230.73483276367188\r",
      "INFO - Step 2400, rl-loss: 813.2960205078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2401, rl-loss: 190.08721923828125\r",
      "INFO - Step 2402, rl-loss: 354.11444091796875\r",
      "INFO - Step 2403, rl-loss: 139.66818237304688\r",
      "INFO - Step 2404, rl-loss: 150.2578887939453\r",
      "INFO - Step 2405, rl-loss: 534.6626586914062\r",
      "INFO - Step 2406, rl-loss: 405.0196228027344\r",
      "INFO - Step 2407, rl-loss: 353.9515380859375\r",
      "INFO - Step 2408, rl-loss: 1.3861008882522583\r",
      "INFO - Step 2409, rl-loss: 1.4737931489944458\r",
      "INFO - Step 2410, rl-loss: 22.055212020874023\r",
      "INFO - Step 2411, rl-loss: 1.1485154628753662\r",
      "INFO - Step 2412, rl-loss: 230.06321716308594\r",
      "INFO - Step 2413, rl-loss: 132.8046112060547\r",
      "INFO - Step 2414, rl-loss: 18.779266357421875\r",
      "INFO - Step 2415, rl-loss: 364.7373046875\r",
      "INFO - Step 2416, rl-loss: 138.55982971191406\r",
      "INFO - Step 2417, rl-loss: 200.24508666992188\r",
      "INFO - Step 2418, rl-loss: 122.44877624511719\r",
      "INFO - Step 2419, rl-loss: 242.47564697265625\r",
      "INFO - Step 2420, rl-loss: 601.1588134765625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2421, rl-loss: 149.4528045654297\r",
      "INFO - Step 2422, rl-loss: 114.7777328491211\r",
      "INFO - Step 2423, rl-loss: 482.0620422363281\r",
      "INFO - Step 2424, rl-loss: 142.37657165527344\r",
      "INFO - Step 2425, rl-loss: 322.14202880859375\r",
      "INFO - Step 2426, rl-loss: 115.41793060302734\r",
      "INFO - Step 2427, rl-loss: 172.2569122314453\r",
      "INFO - Step 2428, rl-loss: 258.51239013671875\r",
      "INFO - Step 2429, rl-loss: 793.5859375\r",
      "INFO - Step 2430, rl-loss: 69.0114974975586\r",
      "INFO - Step 2431, rl-loss: 174.56072998046875\r",
      "INFO - Step 2432, rl-loss: 9.827861785888672\r",
      "INFO - Step 2433, rl-loss: 1.3115267753601074\r",
      "INFO - Step 2434, rl-loss: 412.05328369140625\r",
      "INFO - Step 2435, rl-loss: 170.7754669189453\r",
      "INFO - Step 2436, rl-loss: 11.319315910339355\r",
      "INFO - Step 2437, rl-loss: 1.3287758827209473\r",
      "INFO - Step 2438, rl-loss: 1.7868083715438843\r",
      "INFO - Step 2439, rl-loss: 1.2930463552474976\r",
      "INFO - Step 2440, rl-loss: 232.26319885253906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2441, rl-loss: 327.523681640625\r",
      "INFO - Step 2442, rl-loss: 274.7171936035156\r",
      "INFO - Step 2443, rl-loss: 120.2485122680664\r",
      "INFO - Step 2444, rl-loss: 542.4024047851562\r",
      "INFO - Step 2445, rl-loss: 335.1076965332031\r",
      "INFO - Step 2446, rl-loss: 119.91683959960938\r",
      "INFO - Step 2447, rl-loss: 254.51168823242188\r",
      "INFO - Step 2448, rl-loss: 92.45330047607422\r",
      "INFO - Step 2449, rl-loss: 9.26978874206543\r",
      "INFO - Step 2450, rl-loss: 561.6983032226562\r",
      "INFO - Step 2451, rl-loss: 377.21905517578125\r",
      "INFO - Step 2452, rl-loss: 112.42298126220703\r",
      "INFO - Step 2453, rl-loss: 873.2935180664062\r",
      "INFO - Step 2454, rl-loss: 1.213158369064331\r",
      "INFO - Step 2455, rl-loss: 0.8607587814331055\r",
      "INFO - Step 2456, rl-loss: 539.4360961914062\r",
      "INFO - Step 2457, rl-loss: 1.4915497303009033\r",
      "INFO - Step 2458, rl-loss: 215.8397674560547\r",
      "INFO - Step 2459, rl-loss: 151.5833740234375\r",
      "INFO - Step 2460, rl-loss: 363.1761474609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2461, rl-loss: 10.673542022705078\r",
      "INFO - Step 2462, rl-loss: 119.95248413085938\r",
      "INFO - Step 2463, rl-loss: 1.2337859869003296\r",
      "INFO - Step 2464, rl-loss: 150.92129516601562\r",
      "INFO - Step 2465, rl-loss: 203.59434509277344\r",
      "INFO - Step 2466, rl-loss: 190.08509826660156\r",
      "INFO - Step 2467, rl-loss: 1.2196383476257324\r",
      "INFO - Step 2468, rl-loss: 1.1107916831970215\r",
      "INFO - Step 2469, rl-loss: 1.4731829166412354\r",
      "INFO - Step 2470, rl-loss: 69.23229217529297\r",
      "INFO - Step 2471, rl-loss: 153.17001342773438\r",
      "INFO - Step 2472, rl-loss: 159.03797912597656\r",
      "INFO - Step 2473, rl-loss: 86.28824615478516\r",
      "INFO - Step 2474, rl-loss: 447.2091979980469\r",
      "INFO - Step 2475, rl-loss: 282.2279968261719\r",
      "INFO - Step 2476, rl-loss: 153.449951171875\r",
      "INFO - Step 2477, rl-loss: 1.1679408550262451\r",
      "INFO - Step 2478, rl-loss: 382.6523742675781\r",
      "INFO - Step 2479, rl-loss: 102.36550903320312\r",
      "INFO - Step 2480, rl-loss: 84.1100845336914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2481, rl-loss: 208.0381622314453\r",
      "INFO - Step 2482, rl-loss: 183.614990234375\r",
      "INFO - Step 2483, rl-loss: 158.33023071289062\r",
      "INFO - Step 2484, rl-loss: 540.724609375\r",
      "INFO - Step 2485, rl-loss: 518.37646484375\r",
      "INFO - Step 2486, rl-loss: 254.00051879882812\r",
      "INFO - Step 2487, rl-loss: 306.2228088378906\r",
      "INFO - Step 2488, rl-loss: 1127.86376953125\r",
      "INFO - Step 2489, rl-loss: 1.4144190549850464\r",
      "INFO - Step 2490, rl-loss: 74.88604736328125\r",
      "INFO - Step 2491, rl-loss: 231.31686401367188\r",
      "INFO - Step 2492, rl-loss: 120.7818832397461\r",
      "INFO - Step 2493, rl-loss: 1.2642242908477783\r",
      "INFO - Step 2494, rl-loss: 243.61192321777344\r",
      "INFO - Step 2495, rl-loss: 106.15998077392578\r",
      "INFO - Step 2496, rl-loss: 57.81836700439453\r",
      "INFO - Step 2497, rl-loss: 0.6997784376144409\r",
      "INFO - Step 2498, rl-loss: 315.6330261230469\r",
      "INFO - Step 2499, rl-loss: 581.0003662109375\r",
      "INFO - Step 2500, rl-loss: 1.8217405080795288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2501, rl-loss: 1.2772799730300903\r",
      "INFO - Step 2502, rl-loss: 1.835242748260498\r",
      "INFO - Step 2503, rl-loss: 324.5218200683594\r",
      "INFO - Step 2504, rl-loss: 187.5105438232422\r",
      "INFO - Step 2505, rl-loss: 109.97711181640625\r",
      "INFO - Step 2506, rl-loss: 171.97933959960938\r",
      "INFO - Step 2507, rl-loss: 363.82965087890625\r",
      "INFO - Step 2508, rl-loss: 782.8096923828125\r",
      "INFO - Step 2509, rl-loss: 139.4597625732422\r",
      "INFO - Step 2510, rl-loss: 29.362655639648438\r",
      "INFO - Step 2511, rl-loss: 259.3373107910156\r",
      "INFO - Step 2512, rl-loss: 206.34710693359375\r",
      "INFO - Step 2513, rl-loss: 31.335189819335938\r",
      "INFO - Step 2514, rl-loss: 362.9093322753906\r",
      "INFO - Step 2515, rl-loss: 226.55491638183594\r",
      "INFO - Step 2516, rl-loss: 75.47512817382812\r",
      "INFO - Step 2517, rl-loss: 449.6819152832031\r",
      "INFO - Step 2518, rl-loss: 437.03680419921875\r",
      "INFO - Step 2519, rl-loss: 0.9234573841094971\r",
      "INFO - Step 2520, rl-loss: 451.69189453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2521, rl-loss: 90.36122131347656\r",
      "INFO - Step 2522, rl-loss: 1.8956763744354248\r",
      "INFO - Step 2523, rl-loss: 70.71611022949219\r",
      "INFO - Step 2524, rl-loss: 1.577201247215271\r",
      "INFO - Step 2525, rl-loss: 216.00637817382812\r",
      "INFO - Step 2526, rl-loss: 226.06399536132812\r",
      "INFO - Step 2527, rl-loss: 1.659771203994751\r",
      "INFO - Step 2528, rl-loss: 260.7951354980469\r",
      "INFO - Step 2529, rl-loss: 1.5074810981750488\r",
      "INFO - Step 2530, rl-loss: 376.66363525390625\r",
      "INFO - Step 2531, rl-loss: 325.7186279296875\r",
      "INFO - Step 2532, rl-loss: 138.39352416992188\r",
      "INFO - Step 2533, rl-loss: 180.28915405273438\r",
      "INFO - Step 2534, rl-loss: 82.12024688720703\r",
      "INFO - Step 2535, rl-loss: 64.56593322753906\r",
      "INFO - Step 2536, rl-loss: 138.07276916503906\r",
      "INFO - Step 2537, rl-loss: 157.171142578125\r",
      "INFO - Step 2538, rl-loss: 1.3332526683807373\r",
      "INFO - Step 2539, rl-loss: 0.8776997923851013\r",
      "INFO - Step 2540, rl-loss: 1.1616703271865845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2541, rl-loss: 491.3491516113281\r",
      "INFO - Step 2542, rl-loss: 809.18115234375\r",
      "INFO - Step 2543, rl-loss: 212.32476806640625\r",
      "INFO - Step 2544, rl-loss: 132.7584228515625\r",
      "INFO - Step 2545, rl-loss: 802.1265258789062\r",
      "INFO - Step 2546, rl-loss: 564.3345947265625\r",
      "INFO - Step 2547, rl-loss: 120.67804718017578\r",
      "INFO - Step 2548, rl-loss: 235.87403869628906\r",
      "INFO - Step 2549, rl-loss: 201.85145568847656\r",
      "INFO - Step 2550, rl-loss: 114.67103576660156\r",
      "INFO - Step 2551, rl-loss: 95.72268676757812\r",
      "INFO - Step 2552, rl-loss: 443.185791015625\r",
      "INFO - Step 2553, rl-loss: 1.20144784450531\r",
      "INFO - Step 2554, rl-loss: 0.970248281955719\r",
      "INFO - Step 2555, rl-loss: 1.3150182962417603\r",
      "INFO - Step 2556, rl-loss: 498.5511779785156\r",
      "INFO - Step 2557, rl-loss: 1.0786612033843994\r",
      "INFO - Step 2558, rl-loss: 326.7552490234375\r",
      "INFO - Step 2559, rl-loss: 529.0911254882812\r",
      "INFO - Step 2560, rl-loss: 70.02200317382812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2561, rl-loss: 428.1385498046875\r",
      "INFO - Step 2562, rl-loss: 1.1880087852478027\r",
      "INFO - Step 2563, rl-loss: 1.6755938529968262\r",
      "INFO - Step 2564, rl-loss: 78.0732650756836\r",
      "INFO - Step 2565, rl-loss: 1.626224398612976\r",
      "INFO - Step 2566, rl-loss: 53.39945983886719\r",
      "INFO - Step 2567, rl-loss: 1.3088899850845337\r",
      "INFO - Step 2568, rl-loss: 1.249192476272583\r",
      "INFO - Step 2569, rl-loss: 231.68284606933594\r",
      "INFO - Step 2570, rl-loss: 114.7562026977539\r",
      "INFO - Step 2571, rl-loss: 82.2436752319336\r",
      "INFO - Step 2572, rl-loss: 137.4252166748047\r",
      "INFO - Step 2573, rl-loss: 1.9234817028045654\r",
      "INFO - Step 2574, rl-loss: 271.1670837402344\r",
      "INFO - Step 2575, rl-loss: 648.0374145507812\r",
      "INFO - Step 2576, rl-loss: 79.79713439941406\r",
      "INFO - Step 2577, rl-loss: 169.60044860839844\r",
      "INFO - Step 2578, rl-loss: 722.014404296875\r",
      "INFO - Step 2579, rl-loss: 427.30975341796875\r",
      "INFO - Step 2580, rl-loss: 94.96437072753906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2581, rl-loss: 531.6993408203125\r",
      "INFO - Step 2582, rl-loss: 108.84414672851562\r",
      "INFO - Step 2583, rl-loss: 80.08226013183594\r",
      "INFO - Step 2584, rl-loss: 333.736572265625\r",
      "INFO - Step 2585, rl-loss: 59.27732467651367\r",
      "INFO - Step 2586, rl-loss: 1.244584321975708\r",
      "INFO - Step 2587, rl-loss: 1.4092369079589844\r",
      "INFO - Step 2588, rl-loss: 1.2664055824279785\r",
      "INFO - Step 2589, rl-loss: 50.9002685546875\r",
      "INFO - Step 2590, rl-loss: 406.12738037109375\r",
      "INFO - Step 2591, rl-loss: 514.4403076171875\r",
      "INFO - Step 2592, rl-loss: 57.361114501953125\r",
      "INFO - Step 2593, rl-loss: 86.62821197509766\r",
      "INFO - Step 2594, rl-loss: 628.75146484375\r",
      "INFO - Step 2595, rl-loss: 0.871009349822998\r",
      "INFO - Step 2596, rl-loss: 1.562941551208496\r",
      "INFO - Step 2597, rl-loss: 33.887699127197266\r",
      "INFO - Step 2598, rl-loss: 121.61245727539062\r",
      "INFO - Step 2599, rl-loss: 0.9678852558135986\r",
      "INFO - Step 2600, rl-loss: 226.23312377929688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2601, rl-loss: 340.006591796875\r",
      "INFO - Step 2602, rl-loss: 15.978096961975098\r",
      "INFO - Step 2603, rl-loss: 8.816375732421875\r",
      "INFO - Step 2604, rl-loss: 1.9462207555770874\r",
      "INFO - Step 2605, rl-loss: 40.653228759765625\r",
      "INFO - Step 2606, rl-loss: 160.44320678710938\r",
      "INFO - Step 2607, rl-loss: 1.8415484428405762\r",
      "INFO - Step 2608, rl-loss: 216.3175506591797\r",
      "INFO - Step 2609, rl-loss: 0.7375390529632568\r",
      "INFO - Step 2610, rl-loss: 278.9225769042969\r",
      "INFO - Step 2611, rl-loss: 183.82215881347656\r",
      "INFO - Step 2612, rl-loss: 1.6817156076431274\r",
      "INFO - Step 2613, rl-loss: 10.350883483886719\r",
      "INFO - Step 2614, rl-loss: 542.0524291992188\r",
      "INFO - Step 2615, rl-loss: 1.397735834121704\r",
      "INFO - Step 2616, rl-loss: 111.69599151611328\r",
      "INFO - Step 2617, rl-loss: 3.070119857788086\r",
      "INFO - Step 2618, rl-loss: 211.08718872070312\r",
      "INFO - Step 2619, rl-loss: 550.2431640625\r",
      "INFO - Step 2620, rl-loss: 225.2451629638672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2621, rl-loss: 385.3500061035156\r",
      "INFO - Step 2622, rl-loss: 441.1976623535156\r",
      "INFO - Step 2623, rl-loss: 141.6537322998047\r",
      "INFO - Step 2624, rl-loss: 610.91748046875\r",
      "INFO - Step 2625, rl-loss: 114.26776885986328\r",
      "INFO - Step 2626, rl-loss: 1.183225393295288\r",
      "INFO - Step 2627, rl-loss: 426.1688537597656\r",
      "INFO - Step 2628, rl-loss: 550.7728271484375\r",
      "INFO - Step 2629, rl-loss: 179.20159912109375\r",
      "INFO - Step 2630, rl-loss: 568.8438720703125\r",
      "INFO - Step 2631, rl-loss: 38.284332275390625\r",
      "INFO - Step 2632, rl-loss: 551.2914428710938\r",
      "INFO - Step 2633, rl-loss: 1296.9052734375\r",
      "INFO - Step 2634, rl-loss: 0.8283803462982178\r",
      "INFO - Step 2635, rl-loss: 0.7653170824050903\r",
      "INFO - Step 2636, rl-loss: 178.8217010498047\r",
      "INFO - Step 2637, rl-loss: 1.030784010887146\r",
      "INFO - Step 2638, rl-loss: 1.1350377798080444\r",
      "INFO - Step 2639, rl-loss: 122.23455810546875\r",
      "INFO - Step 2640, rl-loss: 438.90692138671875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2641, rl-loss: 1.1644160747528076\r",
      "INFO - Step 2642, rl-loss: 137.65414428710938\r",
      "INFO - Step 2643, rl-loss: 1.3593556880950928\r",
      "INFO - Step 2644, rl-loss: 112.36717987060547\r",
      "INFO - Step 2645, rl-loss: 113.48748779296875\r",
      "INFO - Step 2646, rl-loss: 473.49371337890625\r",
      "INFO - Step 2647, rl-loss: 66.60472869873047\r",
      "INFO - Step 2648, rl-loss: 423.3879699707031\r",
      "INFO - Step 2649, rl-loss: 1.319450855255127\r",
      "INFO - Step 2650, rl-loss: 1.4239518642425537\r",
      "INFO - Step 2651, rl-loss: 9.811894416809082\r",
      "INFO - Step 2652, rl-loss: 9.602508544921875\r",
      "INFO - Step 2653, rl-loss: 1010.1726684570312\r",
      "INFO - Step 2654, rl-loss: 1.0265007019042969\r",
      "INFO - Step 2655, rl-loss: 1171.5709228515625\r",
      "INFO - Step 2656, rl-loss: 772.0531005859375\r",
      "INFO - Step 2657, rl-loss: 73.97035217285156\r",
      "INFO - Step 2658, rl-loss: 169.522705078125\r",
      "INFO - Step 2659, rl-loss: 135.1380615234375\r",
      "INFO - Step 2660, rl-loss: 415.9175109863281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2661, rl-loss: 143.00685119628906\r",
      "INFO - Step 2662, rl-loss: 0.8946341276168823\r",
      "INFO - Step 2663, rl-loss: 200.8651123046875\r",
      "INFO - Step 2664, rl-loss: 215.8620147705078\r",
      "INFO - Step 2665, rl-loss: 207.19070434570312\r",
      "INFO - Step 2666, rl-loss: 42.21235656738281\r",
      "INFO - Step 2667, rl-loss: 1.054088830947876\r",
      "INFO - Step 2668, rl-loss: 26.538515090942383\r",
      "INFO - Step 2669, rl-loss: 124.63565063476562\r",
      "INFO - Step 2670, rl-loss: 369.2488098144531\r",
      "INFO - Step 2671, rl-loss: 301.88494873046875\r",
      "INFO - Step 2672, rl-loss: 601.3447265625\r",
      "INFO - Step 2673, rl-loss: 564.7976684570312\r",
      "INFO - Step 2674, rl-loss: 179.22744750976562\r",
      "INFO - Step 2675, rl-loss: 106.4364242553711\r",
      "INFO - Step 2676, rl-loss: 232.04840087890625\r",
      "INFO - Step 2677, rl-loss: 430.1689453125\r",
      "INFO - Step 2678, rl-loss: 283.371826171875\r",
      "INFO - Step 2679, rl-loss: 214.52529907226562\r",
      "INFO - Step 2680, rl-loss: 1.1790390014648438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2681, rl-loss: 207.40988159179688\r",
      "INFO - Step 2682, rl-loss: 115.51827239990234\r",
      "INFO - Step 2683, rl-loss: 300.0809326171875\r",
      "INFO - Step 2684, rl-loss: 1.6832340955734253\r",
      "INFO - Step 2685, rl-loss: 1.394352912902832\r",
      "INFO - Step 2686, rl-loss: 238.6110382080078\r",
      "INFO - Step 2687, rl-loss: 93.05673217773438\r",
      "INFO - Step 2688, rl-loss: 321.90716552734375\r",
      "INFO - Step 2689, rl-loss: 345.3807678222656\r",
      "INFO - Step 2690, rl-loss: 222.1162567138672\r",
      "INFO - Step 2691, rl-loss: 8.865128517150879\r",
      "INFO - Step 2692, rl-loss: 76.9107666015625\r",
      "INFO - Step 2693, rl-loss: 130.27392578125\r",
      "INFO - Step 2694, rl-loss: 386.5453186035156\r",
      "INFO - Step 2695, rl-loss: 323.5489501953125\r",
      "INFO - Step 2696, rl-loss: 187.77708435058594\r",
      "INFO - Step 2697, rl-loss: 63.5181999206543\r",
      "INFO - Step 2698, rl-loss: 1050.9713134765625\r",
      "INFO - Step 2699, rl-loss: 592.4619750976562\r",
      "INFO - Step 2700, rl-loss: 188.4199981689453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2701, rl-loss: 235.61239624023438\r",
      "INFO - Step 2702, rl-loss: 112.29678344726562\r",
      "INFO - Step 2703, rl-loss: 8.175918579101562\r",
      "INFO - Step 2704, rl-loss: 82.14067077636719\r",
      "INFO - Step 2705, rl-loss: 1.4740601778030396\r",
      "INFO - Step 2706, rl-loss: 0.624974250793457\r",
      "INFO - Step 2707, rl-loss: 1.8077678680419922\r",
      "INFO - Step 2708, rl-loss: 180.96546936035156\r",
      "INFO - Step 2709, rl-loss: 1.2402374744415283\r",
      "INFO - Step 2710, rl-loss: 540.8385009765625\r",
      "INFO - Step 2711, rl-loss: 89.8904037475586\r",
      "INFO - Step 2712, rl-loss: 1.029395341873169\r",
      "INFO - Step 2713, rl-loss: 245.68092346191406\r",
      "INFO - Step 2714, rl-loss: 114.68842315673828\r",
      "INFO - Step 2715, rl-loss: 208.27394104003906\r",
      "INFO - Step 2716, rl-loss: 347.03326416015625\r",
      "INFO - Step 2717, rl-loss: 113.69184875488281\r",
      "INFO - Step 2718, rl-loss: 141.47377014160156\r",
      "INFO - Step 2719, rl-loss: 1.031128168106079\r",
      "INFO - Step 2720, rl-loss: 230.44345092773438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2721, rl-loss: 149.3867950439453\r",
      "INFO - Step 2722, rl-loss: 454.7077941894531\r",
      "INFO - Step 2723, rl-loss: 145.1730194091797\r",
      "INFO - Step 2724, rl-loss: 277.42828369140625\r",
      "INFO - Step 2725, rl-loss: 1.09652578830719\r",
      "INFO - Step 2726, rl-loss: 46.31175994873047\r",
      "INFO - Step 2727, rl-loss: 115.97063446044922\r",
      "INFO - Step 2728, rl-loss: 518.08154296875\r",
      "INFO - Step 2729, rl-loss: 118.59917449951172\r",
      "INFO - Step 2730, rl-loss: 163.7442169189453\r",
      "INFO - Step 2731, rl-loss: 1.2251286506652832\r",
      "INFO - Step 2732, rl-loss: 2.058521270751953\r",
      "INFO - Step 2733, rl-loss: 177.96734619140625\r",
      "INFO - Step 2734, rl-loss: 362.21075439453125\r",
      "INFO - Step 2735, rl-loss: 424.338623046875\r",
      "INFO - Step 2736, rl-loss: 194.8718719482422\r",
      "INFO - Step 2737, rl-loss: 1.0103449821472168\r",
      "INFO - Step 2738, rl-loss: 472.4797668457031\r",
      "INFO - Step 2739, rl-loss: 291.88671875\r",
      "INFO - Step 2740, rl-loss: 358.2418212890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2741, rl-loss: 147.8561248779297\r",
      "INFO - Step 2742, rl-loss: 137.6364288330078\r",
      "INFO - Step 2743, rl-loss: 228.62741088867188\r",
      "INFO - Step 2744, rl-loss: 105.56940460205078\r",
      "INFO - Step 2745, rl-loss: 401.6800231933594\r",
      "INFO - Step 2746, rl-loss: 0.824576735496521\r",
      "INFO - Step 2747, rl-loss: 140.85595703125\r",
      "INFO - Step 2748, rl-loss: 235.38177490234375\r",
      "INFO - Step 2749, rl-loss: 324.2374572753906\r",
      "INFO - Step 2750, rl-loss: 230.7073516845703\r",
      "INFO - Step 2751, rl-loss: 342.7052917480469\r",
      "INFO - Step 2752, rl-loss: 76.04802703857422\r",
      "INFO - Step 2753, rl-loss: 229.8993377685547\r",
      "INFO - Step 2754, rl-loss: 77.48554229736328\r",
      "INFO - Step 2755, rl-loss: 0.8999942541122437\r",
      "INFO - Step 2756, rl-loss: 105.60883331298828\r",
      "INFO - Step 2757, rl-loss: 75.63871002197266\r",
      "INFO - Step 2758, rl-loss: 139.1024627685547\r",
      "INFO - Step 2759, rl-loss: 401.7796936035156\r",
      "INFO - Step 2760, rl-loss: 636.9359130859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2761, rl-loss: 136.51866149902344\r",
      "INFO - Step 2762, rl-loss: 215.6664276123047\r",
      "INFO - Step 2763, rl-loss: 315.57489013671875\r",
      "INFO - Step 2764, rl-loss: 1.1020643711090088\r",
      "INFO - Step 2765, rl-loss: 493.9046936035156\r",
      "INFO - Step 2766, rl-loss: 26.730676651000977\r",
      "INFO - Step 2767, rl-loss: 252.83656311035156\r",
      "INFO - Step 2768, rl-loss: 8.874019622802734\r",
      "INFO - Step 2769, rl-loss: 166.66604614257812\r",
      "INFO - Step 2770, rl-loss: 235.2305908203125\r",
      "INFO - Step 2771, rl-loss: 623.8389892578125\r",
      "INFO - Step 2772, rl-loss: 361.0141296386719\r",
      "INFO - Step 2773, rl-loss: 128.8331298828125\r",
      "INFO - Step 2774, rl-loss: 0.982664942741394\r",
      "INFO - Step 2775, rl-loss: 208.44825744628906\r",
      "INFO - Step 2776, rl-loss: 5.031477928161621\r",
      "INFO - Step 2777, rl-loss: 1.1859160661697388\r",
      "INFO - Step 2778, rl-loss: 0.8994197845458984\r",
      "INFO - Step 2779, rl-loss: 306.8309631347656\r",
      "INFO - Step 2780, rl-loss: 42.834373474121094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2781, rl-loss: 369.7124938964844\r",
      "INFO - Step 2782, rl-loss: 1.4256219863891602\r",
      "INFO - Step 2783, rl-loss: 402.10528564453125\r",
      "INFO - Step 2784, rl-loss: 1.3171336650848389\r",
      "INFO - Step 2785, rl-loss: 1.6679083108901978\r",
      "INFO - Step 2786, rl-loss: 1.4321324825286865\r",
      "INFO - Step 2787, rl-loss: 186.51644897460938\r",
      "INFO - Step 2788, rl-loss: 147.42178344726562\r",
      "INFO - Step 2789, rl-loss: 1.0291855335235596\r",
      "INFO - Step 2790, rl-loss: 348.0718688964844\r",
      "INFO - Step 2791, rl-loss: 1.748936653137207\r",
      "INFO - Step 2792, rl-loss: 412.6383361816406\r",
      "INFO - Step 2793, rl-loss: 108.69246673583984\r",
      "INFO - Step 2794, rl-loss: 1.2598659992218018\r",
      "INFO - Step 2795, rl-loss: 225.71377563476562\r",
      "INFO - Step 2796, rl-loss: 1.367002010345459\r",
      "INFO - Step 2797, rl-loss: 345.33624267578125\r",
      "INFO - Step 2798, rl-loss: 197.91030883789062\r",
      "INFO - Step 2799, rl-loss: 646.0277709960938\r",
      "INFO - Step 2800, rl-loss: 308.6844787597656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2801, rl-loss: 396.18902587890625\r",
      "INFO - Step 2802, rl-loss: 131.09756469726562\r",
      "INFO - Step 2803, rl-loss: 372.5599670410156\r",
      "INFO - Step 2804, rl-loss: 148.65505981445312\r",
      "INFO - Step 2805, rl-loss: 535.93798828125\r",
      "INFO - Step 2806, rl-loss: 1.2009203433990479\r",
      "INFO - Step 2807, rl-loss: 1.3346035480499268\r",
      "INFO - Step 2808, rl-loss: 245.10055541992188\r",
      "INFO - Step 2809, rl-loss: 271.1077880859375\r",
      "INFO - Step 2810, rl-loss: 0.8380202651023865\r",
      "INFO - Step 2811, rl-loss: 1.4774315357208252\r",
      "INFO - Step 2812, rl-loss: 384.49554443359375\r",
      "INFO - Step 2813, rl-loss: 0.9348717927932739\r",
      "INFO - Step 2814, rl-loss: 1.6182321310043335\r",
      "INFO - Step 2815, rl-loss: 63.55804443359375\r",
      "INFO - Step 2816, rl-loss: 75.0350570678711\r",
      "INFO - Step 2817, rl-loss: 18.203815460205078\r",
      "INFO - Step 2818, rl-loss: 142.0602569580078\r",
      "INFO - Step 2819, rl-loss: 141.45285034179688\r",
      "INFO - Step 2820, rl-loss: 323.13812255859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2821, rl-loss: 412.31292724609375\r",
      "INFO - Step 2822, rl-loss: 284.0915832519531\r",
      "INFO - Step 2823, rl-loss: 446.2689208984375\r",
      "INFO - Step 2824, rl-loss: 547.2882690429688\r",
      "INFO - Step 2825, rl-loss: 1.3216716051101685\r",
      "INFO - Step 2826, rl-loss: 1.5681028366088867\r",
      "INFO - Step 2827, rl-loss: 521.3953857421875\r",
      "INFO - Step 2828, rl-loss: 0.6735650300979614\r",
      "INFO - Step 2829, rl-loss: 253.00521850585938\r",
      "INFO - Step 2830, rl-loss: 211.2066192626953\r",
      "INFO - Step 2831, rl-loss: 388.45166015625\r",
      "INFO - Step 2832, rl-loss: 227.97113037109375\r",
      "INFO - Step 2833, rl-loss: 233.4397735595703\r",
      "INFO - Step 2834, rl-loss: 491.721435546875\r",
      "INFO - Step 2835, rl-loss: 1.1859865188598633\r",
      "INFO - Step 2836, rl-loss: 1.3962204456329346\r",
      "INFO - Step 2837, rl-loss: 220.6872100830078\r",
      "INFO - Step 2838, rl-loss: 164.9611053466797\r",
      "INFO - Step 2839, rl-loss: 209.77650451660156\r",
      "INFO - Step 2840, rl-loss: 87.07952117919922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2841, rl-loss: 126.80779266357422\r",
      "INFO - Step 2842, rl-loss: 136.41802978515625\r",
      "INFO - Step 2843, rl-loss: 280.8363342285156\r",
      "INFO - Step 2844, rl-loss: 1.0165677070617676\r",
      "INFO - Step 2845, rl-loss: 1.4359866380691528\r",
      "INFO - Step 2846, rl-loss: 135.97549438476562\r",
      "INFO - Step 2847, rl-loss: 200.66465759277344\r",
      "INFO - Step 2848, rl-loss: 313.9738464355469\r",
      "INFO - Step 2849, rl-loss: 508.39251708984375\r",
      "INFO - Step 2850, rl-loss: 181.19215393066406\r",
      "INFO - Step 2851, rl-loss: 141.8505859375\r",
      "INFO - Step 2852, rl-loss: 184.67990112304688\r",
      "INFO - Step 2853, rl-loss: 86.64689636230469\r",
      "INFO - Step 2854, rl-loss: 183.3054962158203\r",
      "INFO - Step 2855, rl-loss: 670.8905029296875\r",
      "INFO - Step 2856, rl-loss: 42.16149139404297\r",
      "INFO - Step 2857, rl-loss: 141.22079467773438\r",
      "INFO - Step 2858, rl-loss: 1.5121244192123413\r",
      "INFO - Step 2859, rl-loss: 178.25164794921875\r",
      "INFO - Step 2860, rl-loss: 1.0415539741516113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2861, rl-loss: 250.48968505859375\r",
      "INFO - Step 2862, rl-loss: 69.63412475585938\r",
      "INFO - Step 2863, rl-loss: 293.3639831542969\r",
      "INFO - Step 2864, rl-loss: 0.8172575235366821\r",
      "INFO - Step 2865, rl-loss: 170.17686462402344\r",
      "INFO - Step 2866, rl-loss: 79.70372772216797\r",
      "INFO - Step 2867, rl-loss: 79.35302734375\r",
      "INFO - Step 2868, rl-loss: 610.15234375\r",
      "INFO - Step 2869, rl-loss: 76.32897186279297\r",
      "INFO - Step 2870, rl-loss: 462.69189453125\r",
      "INFO - Step 2871, rl-loss: 192.16867065429688\r",
      "INFO - Step 2872, rl-loss: 128.53570556640625\r",
      "INFO - Step 2873, rl-loss: 169.79791259765625\r",
      "INFO - Step 2874, rl-loss: 292.88238525390625\r",
      "INFO - Step 2875, rl-loss: 0.8324869871139526\r",
      "INFO - Step 2876, rl-loss: 383.59912109375\r",
      "INFO - Step 2877, rl-loss: 42.6264762878418\r",
      "INFO - Step 2878, rl-loss: 38.296905517578125\r",
      "INFO - Step 2879, rl-loss: 268.2848815917969\r",
      "INFO - Step 2880, rl-loss: 637.9618530273438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2881, rl-loss: 419.70037841796875\r",
      "INFO - Step 2882, rl-loss: 319.6159362792969\r",
      "INFO - Step 2883, rl-loss: 202.5750732421875\r",
      "INFO - Step 2884, rl-loss: 0.8891081213951111\r",
      "INFO - Step 2885, rl-loss: 121.30117797851562\r",
      "INFO - Step 2886, rl-loss: 1.3642237186431885\r",
      "INFO - Step 2887, rl-loss: 130.83848571777344\r",
      "INFO - Step 2888, rl-loss: 1.5166559219360352\r",
      "INFO - Step 2889, rl-loss: 502.4054870605469\r",
      "INFO - Step 2890, rl-loss: 31.533050537109375\r",
      "INFO - Step 2891, rl-loss: 14.935005187988281\r",
      "INFO - Step 2892, rl-loss: 71.21261596679688\r",
      "INFO - Step 2893, rl-loss: 0.9958902597427368\r",
      "INFO - Step 2894, rl-loss: 1.3733021020889282\r",
      "INFO - Step 2895, rl-loss: 578.1148681640625\r",
      "INFO - Step 2896, rl-loss: 1.3813512325286865\r",
      "INFO - Step 2897, rl-loss: 1.061847448348999\r",
      "INFO - Step 2898, rl-loss: 236.37498474121094\r",
      "INFO - Step 2899, rl-loss: 981.8519287109375\r",
      "INFO - Step 2900, rl-loss: 1.1311957836151123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2901, rl-loss: 485.68408203125\r",
      "INFO - Step 2902, rl-loss: 331.45782470703125\r",
      "INFO - Step 2903, rl-loss: 15.316059112548828\r",
      "INFO - Step 2904, rl-loss: 79.4000244140625\r",
      "INFO - Step 2905, rl-loss: 109.38592529296875\r",
      "INFO - Step 2906, rl-loss: 107.60706329345703\r",
      "INFO - Step 2907, rl-loss: 1.3994779586791992\r",
      "INFO - Step 2908, rl-loss: 145.7944793701172\r",
      "INFO - Step 2909, rl-loss: 1.2340418100357056\r",
      "INFO - Step 2910, rl-loss: 155.4298553466797\r",
      "INFO - Step 2911, rl-loss: 133.1990509033203\r",
      "INFO - Step 2912, rl-loss: 196.07757568359375\r",
      "INFO - Step 2913, rl-loss: 7.446199893951416\r",
      "INFO - Step 2914, rl-loss: 238.5205078125\r",
      "INFO - Step 2915, rl-loss: 1.0701799392700195\r",
      "INFO - Step 2916, rl-loss: 212.53297424316406\r",
      "INFO - Step 2917, rl-loss: 39.92163848876953\r",
      "INFO - Step 2918, rl-loss: 348.14178466796875\r",
      "INFO - Step 2919, rl-loss: 138.14773559570312\r",
      "INFO - Step 2920, rl-loss: 0.9379444122314453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2921, rl-loss: 377.8801574707031\r",
      "INFO - Step 2922, rl-loss: 68.92036437988281\r",
      "INFO - Step 2923, rl-loss: 7.9982099533081055\r",
      "INFO - Step 2924, rl-loss: 272.8565368652344\r",
      "INFO - Step 2925, rl-loss: 20.717422485351562\r",
      "INFO - Step 2926, rl-loss: 141.15455627441406\r",
      "INFO - Step 2927, rl-loss: 132.5104217529297\r",
      "INFO - Step 2928, rl-loss: 114.26014709472656\r",
      "INFO - Step 2929, rl-loss: 297.9496154785156\r",
      "INFO - Step 2930, rl-loss: 361.347412109375\r",
      "INFO - Step 2931, rl-loss: 314.85211181640625\r",
      "INFO - Step 2932, rl-loss: 72.48079681396484\r",
      "INFO - Step 2933, rl-loss: 206.04306030273438\r",
      "INFO - Step 2934, rl-loss: 1.4336849451065063\r",
      "INFO - Step 2935, rl-loss: 452.0641784667969\r",
      "INFO - Step 2936, rl-loss: 15.455368995666504\r",
      "INFO - Step 2937, rl-loss: 152.6566162109375\r",
      "INFO - Step 2938, rl-loss: 286.522216796875\r",
      "INFO - Step 2939, rl-loss: 314.465576171875\r",
      "INFO - Step 2940, rl-loss: 0.8298198580741882"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2941, rl-loss: 64.94986724853516\r",
      "INFO - Step 2942, rl-loss: 1.1726967096328735\r",
      "INFO - Step 2943, rl-loss: 133.75856018066406\r",
      "INFO - Step 2944, rl-loss: 110.94886016845703\r",
      "INFO - Step 2945, rl-loss: 8.330254554748535\r",
      "INFO - Step 2946, rl-loss: 114.29193878173828\r",
      "INFO - Step 2947, rl-loss: 800.7373046875\r",
      "INFO - Step 2948, rl-loss: 408.572021484375\r",
      "INFO - Step 2949, rl-loss: 214.14767456054688\r",
      "INFO - Step 2950, rl-loss: 111.5697250366211\r",
      "INFO - Step 2951, rl-loss: 698.3106689453125\r",
      "INFO - Step 2952, rl-loss: 182.9070587158203\r",
      "INFO - Step 2953, rl-loss: 211.24411010742188\r",
      "INFO - Step 2954, rl-loss: 182.36131286621094\r",
      "INFO - Step 2955, rl-loss: 7.618490695953369\r",
      "INFO - Step 2956, rl-loss: 371.3185729980469\r",
      "INFO - Step 2957, rl-loss: 330.365966796875\r",
      "INFO - Step 2958, rl-loss: 213.72235107421875\r",
      "INFO - Step 2959, rl-loss: 243.21163940429688\r",
      "INFO - Step 2960, rl-loss: 513.5935668945312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2961, rl-loss: 469.40863037109375\r",
      "INFO - Step 2962, rl-loss: 111.09281158447266\r",
      "INFO - Step 2963, rl-loss: 41.843204498291016\r",
      "INFO - Step 2964, rl-loss: 307.072021484375\r",
      "INFO - Step 2965, rl-loss: 1.1297643184661865\r",
      "INFO - Step 2966, rl-loss: 160.41159057617188\r",
      "INFO - Step 2967, rl-loss: 503.98602294921875\r",
      "INFO - Step 2968, rl-loss: 119.515380859375\r",
      "INFO - Step 2969, rl-loss: 110.46688079833984\r",
      "INFO - Step 2970, rl-loss: 265.65887451171875\r",
      "INFO - Step 2971, rl-loss: 180.8106231689453\r",
      "INFO - Step 2972, rl-loss: 371.8436584472656\r",
      "INFO - Step 2973, rl-loss: 1.2142059803009033\r",
      "INFO - Step 2974, rl-loss: 153.91554260253906\r",
      "INFO - Step 2975, rl-loss: 543.05126953125\r",
      "INFO - Step 2976, rl-loss: 596.9765625\r",
      "INFO - Step 2977, rl-loss: 1.6571044921875\r",
      "INFO - Step 2978, rl-loss: 320.7522888183594\r",
      "INFO - Step 2979, rl-loss: 1.0576581954956055\r",
      "INFO - Step 2980, rl-loss: 209.3594970703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 2981, rl-loss: 673.3492431640625\r",
      "INFO - Step 2982, rl-loss: 105.20667266845703\r",
      "INFO - Step 2983, rl-loss: 75.39878845214844\r",
      "INFO - Step 2984, rl-loss: 978.3233642578125\r",
      "INFO - Step 2985, rl-loss: 649.7210693359375\r",
      "INFO - Step 2986, rl-loss: 246.78138732910156\r",
      "INFO - Step 2987, rl-loss: 135.74046325683594\r",
      "INFO - Step 2988, rl-loss: 481.573974609375\r",
      "INFO - Step 2989, rl-loss: 73.66822814941406\r",
      "INFO - Step 2990, rl-loss: 547.6546020507812\r",
      "INFO - Step 2991, rl-loss: 41.480342864990234\r",
      "INFO - Step 2992, rl-loss: 503.3447265625\r",
      "INFO - Step 2993, rl-loss: 135.0265655517578\r",
      "INFO - Step 2994, rl-loss: 401.47589111328125\r",
      "INFO - Step 2995, rl-loss: 95.4729995727539\r",
      "INFO - Step 2996, rl-loss: 325.1241149902344\r",
      "INFO - Step 2997, rl-loss: 1.7142281532287598\r",
      "INFO - Step 2998, rl-loss: 353.3500061035156\r",
      "INFO - Step 2999, rl-loss: 1.0170061588287354\r",
      "INFO - Step 3000, rl-loss: 212.1836395263672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 3020, rl-loss: 391.33056640625752\n",
      "----------------------------------------\n",
      "  timestep     |  531467\n",
      "  reward       |  61.51\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 3040, rl-loss: 0.6427760124206543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3041, rl-loss: 0.8822108507156372\r",
      "INFO - Step 3042, rl-loss: 191.11386108398438\r",
      "INFO - Step 3043, rl-loss: 252.23336791992188\r",
      "INFO - Step 3044, rl-loss: 39.83833312988281\r",
      "INFO - Step 3045, rl-loss: 137.89491271972656\r",
      "INFO - Step 3046, rl-loss: 48.72243881225586\r",
      "INFO - Step 3047, rl-loss: 746.7012939453125\r",
      "INFO - Step 3048, rl-loss: 255.1703338623047\r",
      "INFO - Step 3049, rl-loss: 320.09869384765625\r",
      "INFO - Step 3050, rl-loss: 5.886360168457031\r",
      "INFO - Step 3051, rl-loss: 252.67681884765625\r",
      "INFO - Step 3052, rl-loss: 31.464035034179688\r",
      "INFO - Step 3053, rl-loss: 425.53094482421875\r",
      "INFO - Step 3054, rl-loss: 45.66497802734375\r",
      "INFO - Step 3055, rl-loss: 0.8109467625617981\r",
      "INFO - Step 3056, rl-loss: 160.23826599121094\r",
      "INFO - Step 3057, rl-loss: 428.40460205078125\r",
      "INFO - Step 3058, rl-loss: 1.6977012157440186\r",
      "INFO - Step 3059, rl-loss: 87.69384002685547\r",
      "INFO - Step 3060, rl-loss: 109.16968536376953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3061, rl-loss: 282.8375244140625\r",
      "INFO - Step 3062, rl-loss: 98.3336181640625\r",
      "INFO - Step 3063, rl-loss: 1.1117994785308838\r",
      "INFO - Step 3064, rl-loss: 0.9392615556716919\r",
      "INFO - Step 3065, rl-loss: 1.4277398586273193\r",
      "INFO - Step 3066, rl-loss: 397.16424560546875\r",
      "INFO - Step 3067, rl-loss: 159.13845825195312\r",
      "INFO - Step 3068, rl-loss: 74.02488708496094\r",
      "INFO - Step 3069, rl-loss: 112.85282135009766\r",
      "INFO - Step 3070, rl-loss: 248.81280517578125\r",
      "INFO - Step 3071, rl-loss: 462.95697021484375\r",
      "INFO - Step 3072, rl-loss: 1.39424729347229\r",
      "INFO - Step 3073, rl-loss: 177.46620178222656\r",
      "INFO - Step 3074, rl-loss: 279.76666259765625\r",
      "INFO - Step 3075, rl-loss: 81.99454498291016\r",
      "INFO - Step 3076, rl-loss: 312.8504943847656\r",
      "INFO - Step 3077, rl-loss: 280.75079345703125\r",
      "INFO - Step 3078, rl-loss: 72.21881866455078\r",
      "INFO - Step 3079, rl-loss: 0.9107072353363037\r",
      "INFO - Step 3080, rl-loss: 569.2682495117188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3081, rl-loss: 265.8263854980469\r",
      "INFO - Step 3082, rl-loss: 222.45376586914062\r",
      "INFO - Step 3083, rl-loss: 1.280413269996643\r",
      "INFO - Step 3084, rl-loss: 176.52468872070312\r",
      "INFO - Step 3085, rl-loss: 131.3336944580078\r",
      "INFO - Step 3086, rl-loss: 476.578125\r",
      "INFO - Step 3087, rl-loss: 1.4310641288757324\r",
      "INFO - Step 3088, rl-loss: 54.074398040771484\r",
      "INFO - Step 3089, rl-loss: 189.9269256591797\r",
      "INFO - Step 3090, rl-loss: 398.0927734375\r",
      "INFO - Step 3091, rl-loss: 0.727142870426178\r",
      "INFO - Step 3092, rl-loss: 72.0924301147461\r",
      "INFO - Step 3093, rl-loss: 112.44825744628906\r",
      "INFO - Step 3094, rl-loss: 99.13357543945312\r",
      "INFO - Step 3095, rl-loss: 160.688232421875\r",
      "INFO - Step 3096, rl-loss: 65.25746154785156\r",
      "INFO - Step 3097, rl-loss: 86.79340362548828\r",
      "INFO - Step 3098, rl-loss: 838.5537109375\r",
      "INFO - Step 3099, rl-loss: 0.940477728843689\r",
      "INFO - Step 3100, rl-loss: 239.22830200195312\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3101, rl-loss: 38.36399459838867\r",
      "INFO - Step 3102, rl-loss: 293.8879699707031\r",
      "INFO - Step 3103, rl-loss: 251.1525115966797\r",
      "INFO - Step 3104, rl-loss: 65.26976776123047\r",
      "INFO - Step 3105, rl-loss: 821.2843017578125\r",
      "INFO - Step 3106, rl-loss: 123.82199096679688\r",
      "INFO - Step 3107, rl-loss: 262.01214599609375\r",
      "INFO - Step 3108, rl-loss: 155.2150421142578\r",
      "INFO - Step 3109, rl-loss: 2.5788018703460693\r",
      "INFO - Step 3110, rl-loss: 165.67962646484375\r",
      "INFO - Step 3111, rl-loss: 1.3639434576034546\r",
      "INFO - Step 3112, rl-loss: 1.8854092359542847\r",
      "INFO - Step 3113, rl-loss: 378.5664367675781\r",
      "INFO - Step 3114, rl-loss: 268.8303527832031\r",
      "INFO - Step 3115, rl-loss: 155.3724365234375\r",
      "INFO - Step 3116, rl-loss: 0.8489697575569153\r",
      "INFO - Step 3117, rl-loss: 1.7519375085830688\r",
      "INFO - Step 3118, rl-loss: 1.0834367275238037\r",
      "INFO - Step 3119, rl-loss: 158.91517639160156\r",
      "INFO - Step 3120, rl-loss: 345.1887512207031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3121, rl-loss: 319.77264404296875\r",
      "INFO - Step 3122, rl-loss: 1.3316384553909302\r",
      "INFO - Step 3123, rl-loss: 1.5977246761322021\r",
      "INFO - Step 3124, rl-loss: 1.7330873012542725\r",
      "INFO - Step 3125, rl-loss: 187.66644287109375\r",
      "INFO - Step 3126, rl-loss: 307.1219787597656\r",
      "INFO - Step 3127, rl-loss: 2.034242630004883\r",
      "INFO - Step 3128, rl-loss: 21.385967254638672\r",
      "INFO - Step 3129, rl-loss: 723.8815307617188\r",
      "INFO - Step 3130, rl-loss: 68.85248565673828\r",
      "INFO - Step 3131, rl-loss: 1.6980843544006348\r",
      "INFO - Step 3132, rl-loss: 470.04058837890625\r",
      "INFO - Step 3133, rl-loss: 209.06324768066406\r",
      "INFO - Step 3134, rl-loss: 0.9117013812065125\r",
      "INFO - Step 3135, rl-loss: 1.2984192371368408\r",
      "INFO - Step 3136, rl-loss: 434.26177978515625\r",
      "INFO - Step 3137, rl-loss: 129.66563415527344\r",
      "INFO - Step 3138, rl-loss: 89.13816833496094\r",
      "INFO - Step 3139, rl-loss: 478.9984436035156\r",
      "INFO - Step 3140, rl-loss: 387.2234802246094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3141, rl-loss: 74.96759033203125\r",
      "INFO - Step 3142, rl-loss: 1.3470892906188965\r",
      "INFO - Step 3143, rl-loss: 510.6004333496094\r",
      "INFO - Step 3144, rl-loss: 368.3555603027344\r",
      "INFO - Step 3145, rl-loss: 390.7548522949219\r",
      "INFO - Step 3146, rl-loss: 277.76361083984375\r",
      "INFO - Step 3147, rl-loss: 771.389892578125\r",
      "INFO - Step 3148, rl-loss: 1.4940099716186523\r",
      "INFO - Step 3149, rl-loss: 419.05169677734375\r",
      "INFO - Step 3150, rl-loss: 564.2590942382812\r",
      "INFO - Step 3151, rl-loss: 298.3701477050781\r",
      "INFO - Step 3152, rl-loss: 130.82049560546875\r",
      "INFO - Step 3153, rl-loss: 620.2008666992188\r",
      "INFO - Step 3154, rl-loss: 194.8978729248047\r",
      "INFO - Step 3155, rl-loss: 156.39187622070312\r",
      "INFO - Step 3156, rl-loss: 424.6537170410156\r",
      "INFO - Step 3157, rl-loss: 106.8266830444336\r",
      "INFO - Step 3158, rl-loss: 1.6148842573165894\r",
      "INFO - Step 3159, rl-loss: 383.1934814453125\r",
      "INFO - Step 3160, rl-loss: 348.6802978515625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3161, rl-loss: 176.83970642089844\r",
      "INFO - Step 3162, rl-loss: 15.540213584899902\r",
      "INFO - Step 3163, rl-loss: 1.5655262470245361\r",
      "INFO - Step 3164, rl-loss: 1.4180357456207275\r",
      "INFO - Step 3165, rl-loss: 261.2956237792969\r",
      "INFO - Step 3166, rl-loss: 304.73651123046875\r",
      "INFO - Step 3167, rl-loss: 1.7009387016296387\r",
      "INFO - Step 3168, rl-loss: 391.84832763671875\r",
      "INFO - Step 3169, rl-loss: 1.1586289405822754\r",
      "INFO - Step 3170, rl-loss: 320.1035461425781\r",
      "INFO - Step 3171, rl-loss: 225.87506103515625\r",
      "INFO - Step 3172, rl-loss: 140.4954376220703\r",
      "INFO - Step 3173, rl-loss: 242.10206604003906\r",
      "INFO - Step 3174, rl-loss: 1.2115283012390137\r",
      "INFO - Step 3175, rl-loss: 224.592529296875\r",
      "INFO - Step 3176, rl-loss: 281.1269226074219\r",
      "INFO - Step 3177, rl-loss: 170.28082275390625\r",
      "INFO - Step 3178, rl-loss: 857.275390625\r",
      "INFO - Step 3179, rl-loss: 483.5068054199219\r",
      "INFO - Step 3180, rl-loss: 197.87234497070312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3181, rl-loss: 361.6548156738281\r",
      "INFO - Step 3182, rl-loss: 156.27932739257812\r",
      "INFO - Step 3183, rl-loss: 0.9163833856582642\r",
      "INFO - Step 3184, rl-loss: 1.0295830965042114\r",
      "INFO - Step 3185, rl-loss: 65.67848205566406\r",
      "INFO - Step 3186, rl-loss: 6.123634338378906\r",
      "INFO - Step 3187, rl-loss: 507.705078125\r",
      "INFO - Step 3188, rl-loss: 60.32206344604492\r",
      "INFO - Step 3189, rl-loss: 1.1416445970535278\r",
      "INFO - Step 3190, rl-loss: 263.84271240234375\r",
      "INFO - Step 3191, rl-loss: 25.40541648864746\r",
      "INFO - Step 3192, rl-loss: 1.7552804946899414\r",
      "INFO - Step 3193, rl-loss: 29.129009246826172\r",
      "INFO - Step 3194, rl-loss: 580.139404296875\r",
      "INFO - Step 3195, rl-loss: 380.37591552734375\r",
      "INFO - Step 3196, rl-loss: 329.3023681640625\r",
      "INFO - Step 3197, rl-loss: 289.2915954589844\r",
      "INFO - Step 3198, rl-loss: 361.86529541015625\r",
      "INFO - Step 3199, rl-loss: 1.4586173295974731\r",
      "INFO - Step 3200, rl-loss: 393.0501403808594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 3220, rl-loss: 242.33467102050786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 3240, rl-loss: 686.45153808593755"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3241, rl-loss: 876.1921997070312\r",
      "INFO - Step 3242, rl-loss: 221.29995727539062\r",
      "INFO - Step 3243, rl-loss: 1.7655060291290283\r",
      "INFO - Step 3244, rl-loss: 341.51019287109375\r",
      "INFO - Step 3245, rl-loss: 164.83047485351562\r",
      "INFO - Step 3246, rl-loss: 124.71147155761719\r",
      "INFO - Step 3247, rl-loss: 191.41407775878906\r",
      "INFO - Step 3248, rl-loss: 317.6692199707031\r",
      "INFO - Step 3249, rl-loss: 1.0676450729370117\r",
      "INFO - Step 3250, rl-loss: 301.8006591796875\r",
      "INFO - Step 3251, rl-loss: 119.29414367675781\r",
      "INFO - Step 3252, rl-loss: 67.1024398803711\r",
      "INFO - Step 3253, rl-loss: 190.56253051757812\r",
      "INFO - Step 3254, rl-loss: 299.0576171875\r",
      "INFO - Step 3255, rl-loss: 735.6793212890625\r",
      "INFO - Step 3256, rl-loss: 91.83135223388672\r",
      "INFO - Step 3257, rl-loss: 290.7493591308594\r",
      "INFO - Step 3258, rl-loss: 0.9800021052360535\r",
      "INFO - Step 3259, rl-loss: 95.63542938232422\r",
      "INFO - Step 3260, rl-loss: 239.34010314941406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3261, rl-loss: 1.5763678550720215\r",
      "INFO - Step 3262, rl-loss: 214.16319274902344\r",
      "INFO - Step 3263, rl-loss: 660.079833984375\r",
      "INFO - Step 3264, rl-loss: 113.16124725341797\r",
      "INFO - Step 3265, rl-loss: 76.08548736572266\r",
      "INFO - Step 3266, rl-loss: 303.7368469238281\r",
      "INFO - Step 3267, rl-loss: 308.09344482421875\r",
      "INFO - Step 3268, rl-loss: 57.61386489868164\r",
      "INFO - Step 3269, rl-loss: 1.3625882863998413\r",
      "INFO - Step 3270, rl-loss: 72.84664916992188\r",
      "INFO - Step 3271, rl-loss: 1.3153753280639648\r",
      "INFO - Step 3272, rl-loss: 552.6527099609375\r",
      "INFO - Step 3273, rl-loss: 413.9413757324219\r",
      "INFO - Step 3274, rl-loss: 170.3889923095703\r",
      "INFO - Step 3275, rl-loss: 260.775634765625\r",
      "INFO - Step 3276, rl-loss: 342.85223388671875\r",
      "INFO - Step 3277, rl-loss: 90.7227783203125\r",
      "INFO - Step 3278, rl-loss: 215.0447235107422\r",
      "INFO - Step 3279, rl-loss: 286.6958312988281\r",
      "INFO - Step 3280, rl-loss: 415.55242919921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 3300, rl-loss: 189.70884704589844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3301, rl-loss: 175.41610717773438\r",
      "INFO - Step 3302, rl-loss: 1.3995492458343506\r",
      "INFO - Step 3303, rl-loss: 313.8125915527344\r",
      "INFO - Step 3304, rl-loss: 161.03732299804688\r",
      "INFO - Step 3305, rl-loss: 1.4991004467010498\r",
      "INFO - Step 3306, rl-loss: 175.275634765625\r",
      "INFO - Step 3307, rl-loss: 213.40460205078125\r",
      "INFO - Step 3308, rl-loss: 61.70934295654297\r",
      "INFO - Step 3309, rl-loss: 41.18547821044922\r",
      "INFO - Step 3310, rl-loss: 847.696533203125\r",
      "INFO - Step 3311, rl-loss: 115.55224609375\r",
      "INFO - Step 3312, rl-loss: 1.4062719345092773\r",
      "INFO - Step 3313, rl-loss: 33.68507385253906\r",
      "INFO - Step 3314, rl-loss: 65.48963165283203\r",
      "INFO - Step 3315, rl-loss: 65.783203125\r",
      "INFO - Step 3316, rl-loss: 444.5332946777344\r",
      "INFO - Step 3317, rl-loss: 193.0238494873047\r",
      "INFO - Step 3318, rl-loss: 786.5416870117188\r",
      "INFO - Step 3319, rl-loss: 354.91351318359375\r",
      "INFO - Step 3320, rl-loss: 136.94390869140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3321, rl-loss: 1.719174861907959\r",
      "INFO - Step 3322, rl-loss: 359.686767578125\r",
      "INFO - Step 3323, rl-loss: 348.3949279785156\r",
      "INFO - Step 3324, rl-loss: 1.3968565464019775\r",
      "INFO - Step 3325, rl-loss: 1.0034470558166504\r",
      "INFO - Step 3326, rl-loss: 255.9276885986328\r",
      "INFO - Step 3327, rl-loss: 75.82971954345703\r",
      "INFO - Step 3328, rl-loss: 155.18150329589844\r",
      "INFO - Step 3329, rl-loss: 613.744384765625\r",
      "INFO - Step 3330, rl-loss: 226.4022216796875\r",
      "INFO - Step 3331, rl-loss: 147.7585906982422\r",
      "INFO - Step 3332, rl-loss: 1.3782942295074463\r",
      "INFO - Step 3333, rl-loss: 285.740234375\r",
      "INFO - Step 3334, rl-loss: 658.9863891601562\r",
      "INFO - Step 3335, rl-loss: 226.59841918945312\r",
      "INFO - Step 3336, rl-loss: 232.62554931640625\r",
      "INFO - Step 3337, rl-loss: 547.2906494140625\r",
      "INFO - Step 3338, rl-loss: 41.94012451171875\r",
      "INFO - Step 3339, rl-loss: 68.27757263183594\r",
      "INFO - Step 3340, rl-loss: 1.1987113952636719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3341, rl-loss: 213.44381713867188\r",
      "INFO - Step 3342, rl-loss: 163.80526733398438\r",
      "INFO - Step 3343, rl-loss: 149.7477569580078\r",
      "INFO - Step 3344, rl-loss: 276.9977722167969\r",
      "INFO - Step 3345, rl-loss: 1.337576150894165\r",
      "INFO - Step 3346, rl-loss: 798.988037109375\r",
      "INFO - Step 3347, rl-loss: 1.0986011028289795\r",
      "INFO - Step 3348, rl-loss: 1.4575775861740112\r",
      "INFO - Step 3349, rl-loss: 550.40625\r",
      "INFO - Step 3350, rl-loss: 120.83241271972656\r",
      "INFO - Step 3351, rl-loss: 411.4893493652344\r",
      "INFO - Step 3352, rl-loss: 0.758834958076477\r",
      "INFO - Step 3353, rl-loss: 386.1241455078125\r",
      "INFO - Step 3354, rl-loss: 239.76478576660156\r",
      "INFO - Step 3355, rl-loss: 227.8403778076172\r",
      "INFO - Step 3356, rl-loss: 238.0636749267578\r",
      "INFO - Step 3357, rl-loss: 86.088623046875\r",
      "INFO - Step 3358, rl-loss: 960.179931640625\r",
      "INFO - Step 3359, rl-loss: 590.32861328125\r",
      "INFO - Step 3360, rl-loss: 270.0975341796875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3361, rl-loss: 622.3458251953125\r",
      "INFO - Step 3362, rl-loss: 1.4226264953613281\r",
      "INFO - Step 3363, rl-loss: 1.5932679176330566\r",
      "INFO - Step 3364, rl-loss: 1.2630630731582642\r",
      "INFO - Step 3365, rl-loss: 42.67298126220703\r",
      "INFO - Step 3366, rl-loss: 189.466552734375\r",
      "INFO - Step 3367, rl-loss: 114.73004150390625\r",
      "INFO - Step 3368, rl-loss: 5.925370693206787\r",
      "INFO - Step 3369, rl-loss: 663.2611694335938\r",
      "INFO - Step 3370, rl-loss: 167.40928649902344\r",
      "INFO - Step 3371, rl-loss: 210.1971435546875\r",
      "INFO - Step 3372, rl-loss: 1.0841009616851807\r",
      "INFO - Step 3373, rl-loss: 73.0127182006836\r",
      "INFO - Step 3374, rl-loss: 1.4320411682128906\r",
      "INFO - Step 3375, rl-loss: 1.1344215869903564\r",
      "INFO - Step 3376, rl-loss: 74.92314910888672\r",
      "INFO - Step 3377, rl-loss: 397.7434997558594\r",
      "INFO - Step 3378, rl-loss: 193.42160034179688\r",
      "INFO - Step 3379, rl-loss: 373.473876953125\r",
      "INFO - Step 3380, rl-loss: 36.646705627441406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3381, rl-loss: 147.27955627441406\r",
      "INFO - Step 3382, rl-loss: 339.2804870605469\r",
      "INFO - Step 3383, rl-loss: 41.8627815246582\r",
      "INFO - Step 3384, rl-loss: 205.32496643066406\r",
      "INFO - Step 3385, rl-loss: 455.0957946777344\r",
      "INFO - Step 3386, rl-loss: 121.82791137695312\r",
      "INFO - Step 3387, rl-loss: 1.687807321548462\r",
      "INFO - Step 3388, rl-loss: 233.48126220703125\r",
      "INFO - Step 3389, rl-loss: 160.5887451171875\r",
      "INFO - Step 3390, rl-loss: 15.214720726013184\r",
      "INFO - Step 3391, rl-loss: 528.3544311523438\r",
      "INFO - Step 3392, rl-loss: 178.3144073486328\r",
      "INFO - Step 3393, rl-loss: 0.8167651891708374\r",
      "INFO - Step 3394, rl-loss: 248.29632568359375\r",
      "INFO - Step 3395, rl-loss: 111.54560852050781\r",
      "INFO - Step 3396, rl-loss: 81.72673797607422\r",
      "INFO - Step 3397, rl-loss: 8.801325798034668\r",
      "INFO - Step 3398, rl-loss: 333.13580322265625\r",
      "INFO - Step 3399, rl-loss: 335.0196228027344\r",
      "INFO - Step 3400, rl-loss: 202.9156494140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3401, rl-loss: 33.47868347167969\r",
      "INFO - Step 3402, rl-loss: 1.299525499343872\r",
      "INFO - Step 3403, rl-loss: 1.7837928533554077\r",
      "INFO - Step 3404, rl-loss: 825.274658203125\r",
      "INFO - Step 3405, rl-loss: 1.1955878734588623\r",
      "INFO - Step 3406, rl-loss: 4.224642753601074\r",
      "INFO - Step 3407, rl-loss: 1.3476781845092773\r",
      "INFO - Step 3408, rl-loss: 20.173051834106445\r",
      "INFO - Step 3409, rl-loss: 455.78106689453125\r",
      "INFO - Step 3410, rl-loss: 2.8850457668304443\r",
      "INFO - Step 3411, rl-loss: 526.5798950195312\r",
      "INFO - Step 3412, rl-loss: 260.9291687011719\r",
      "INFO - Step 3413, rl-loss: 1.5684635639190674\r",
      "INFO - Step 3414, rl-loss: 109.99321746826172\r",
      "INFO - Step 3415, rl-loss: 0.8188669681549072\r",
      "INFO - Step 3416, rl-loss: 1.8010849952697754\r",
      "INFO - Step 3417, rl-loss: 405.949951171875\r",
      "INFO - Step 3418, rl-loss: 0.7951517701148987\r",
      "INFO - Step 3419, rl-loss: 119.42647552490234\r",
      "INFO - Step 3420, rl-loss: 134.72067260742188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3421, rl-loss: 41.17131423950195\r",
      "INFO - Step 3422, rl-loss: 131.29519653320312\r",
      "INFO - Step 3423, rl-loss: 1.4768635034561157\r",
      "INFO - Step 3424, rl-loss: 1.3851574659347534\r",
      "INFO - Step 3425, rl-loss: 988.4085693359375\r",
      "INFO - Step 3426, rl-loss: 210.04815673828125\r",
      "INFO - Step 3427, rl-loss: 71.895263671875\r",
      "INFO - Step 3428, rl-loss: 1.352096676826477\r",
      "INFO - Step 3429, rl-loss: 69.98654174804688\r",
      "INFO - Step 3430, rl-loss: 1.1349867582321167\r",
      "INFO - Step 3431, rl-loss: 693.2376708984375\r",
      "INFO - Step 3432, rl-loss: 518.7188720703125\r",
      "INFO - Step 3433, rl-loss: 140.260498046875\r",
      "INFO - Step 3434, rl-loss: 653.2669677734375\r",
      "INFO - Step 3435, rl-loss: 276.1406555175781\r",
      "INFO - Step 3436, rl-loss: 213.0472869873047\r",
      "INFO - Step 3437, rl-loss: 111.85823822021484\r",
      "INFO - Step 3438, rl-loss: 191.24072265625\r",
      "INFO - Step 3439, rl-loss: 360.7867736816406\r",
      "INFO - Step 3440, rl-loss: 1.319092035293579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3441, rl-loss: 487.8481750488281\r",
      "INFO - Step 3442, rl-loss: 132.7351837158203\r",
      "INFO - Step 3443, rl-loss: 363.6552429199219\r",
      "INFO - Step 3444, rl-loss: 342.1713562011719\r",
      "INFO - Step 3445, rl-loss: 338.64306640625\r",
      "INFO - Step 3446, rl-loss: 117.83712768554688\r",
      "INFO - Step 3447, rl-loss: 341.0993957519531\r",
      "INFO - Step 3448, rl-loss: 1.9291307926177979\r",
      "INFO - Step 3449, rl-loss: 9.39443588256836\r",
      "INFO - Step 3450, rl-loss: 145.4447784423828\r",
      "INFO - Step 3451, rl-loss: 319.6042785644531\r",
      "INFO - Step 3452, rl-loss: 1.213179111480713\r",
      "INFO - Step 3453, rl-loss: 153.75250244140625\r",
      "INFO - Step 3454, rl-loss: 203.70523071289062\r",
      "INFO - Step 3455, rl-loss: 1.5173896551132202\r",
      "INFO - Step 3456, rl-loss: 168.9583740234375\r",
      "INFO - Step 3457, rl-loss: 1.9281504154205322\r",
      "INFO - Step 3458, rl-loss: 332.83154296875\r",
      "INFO - Step 3459, rl-loss: 633.1926879882812\r",
      "INFO - Step 3460, rl-loss: 274.97607421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3461, rl-loss: 213.2433319091797\r",
      "INFO - Step 3462, rl-loss: 1.2315582036972046\r",
      "INFO - Step 3463, rl-loss: 321.056640625\r",
      "INFO - Step 3464, rl-loss: 697.9556884765625\r",
      "INFO - Step 3465, rl-loss: 207.05224609375\r",
      "INFO - Step 3466, rl-loss: 1.099463701248169\r",
      "INFO - Step 3467, rl-loss: 76.38258361816406\r",
      "INFO - Step 3468, rl-loss: 77.85702514648438\r",
      "INFO - Step 3469, rl-loss: 4.625545024871826\r",
      "INFO - Step 3470, rl-loss: 416.9964599609375\r",
      "INFO - Step 3471, rl-loss: 64.3515396118164\r",
      "INFO - Step 3472, rl-loss: 213.06982421875\r",
      "INFO - Step 3473, rl-loss: 460.26861572265625\r",
      "INFO - Step 3474, rl-loss: 286.7625732421875\r",
      "INFO - Step 3475, rl-loss: 257.7530517578125\r",
      "INFO - Step 3476, rl-loss: 437.95556640625\r",
      "INFO - Step 3477, rl-loss: 40.70565414428711\r",
      "INFO - Step 3478, rl-loss: 160.37857055664062\r",
      "INFO - Step 3479, rl-loss: 191.2288818359375\r",
      "INFO - Step 3480, rl-loss: 1.2978650331497192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3481, rl-loss: 229.9458465576172\r",
      "INFO - Step 3482, rl-loss: 343.9027099609375\r",
      "INFO - Step 3483, rl-loss: 0.7351166009902954\r",
      "INFO - Step 3484, rl-loss: 1.3444757461547852\r",
      "INFO - Step 3485, rl-loss: 258.18017578125\r",
      "INFO - Step 3486, rl-loss: 1168.830810546875\r",
      "INFO - Step 3487, rl-loss: 400.2206726074219\r",
      "INFO - Step 3488, rl-loss: 1.0529427528381348\r",
      "INFO - Step 3489, rl-loss: 600.4749755859375\r",
      "INFO - Step 3490, rl-loss: 252.49542236328125\r",
      "INFO - Step 3491, rl-loss: 68.33616638183594\r",
      "INFO - Step 3492, rl-loss: 226.48770141601562\r",
      "INFO - Step 3493, rl-loss: 598.44189453125\r",
      "INFO - Step 3494, rl-loss: 177.962646484375\r",
      "INFO - Step 3495, rl-loss: 615.992431640625\r",
      "INFO - Step 3496, rl-loss: 138.5350799560547\r",
      "INFO - Step 3497, rl-loss: 1.22129487991333\r",
      "INFO - Step 3498, rl-loss: 1.7240233421325684\r",
      "INFO - Step 3499, rl-loss: 565.5917358398438\r",
      "INFO - Step 3500, rl-loss: 710.0208740234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3501, rl-loss: 620.1320190429688\r",
      "INFO - Step 3502, rl-loss: 0.8603398203849792\r",
      "INFO - Step 3503, rl-loss: 248.79652404785156\r",
      "INFO - Step 3504, rl-loss: 332.1311950683594\r",
      "INFO - Step 3505, rl-loss: 1.0274932384490967\r",
      "INFO - Step 3506, rl-loss: 299.3973388671875\r",
      "INFO - Step 3507, rl-loss: 559.27001953125\r",
      "INFO - Step 3508, rl-loss: 0.8173835277557373\r",
      "INFO - Step 3509, rl-loss: 378.2452697753906\r",
      "INFO - Step 3510, rl-loss: 7.332935333251953\r",
      "INFO - Step 3511, rl-loss: 265.17572021484375\r",
      "INFO - Step 3512, rl-loss: 180.27279663085938\r",
      "INFO - Step 3513, rl-loss: 413.3448486328125\r",
      "INFO - Step 3514, rl-loss: 0.7903022170066833\r",
      "INFO - Step 3515, rl-loss: 81.98008728027344\r",
      "INFO - Step 3516, rl-loss: 302.435302734375\r",
      "INFO - Step 3517, rl-loss: 390.8121643066406\r",
      "INFO - Step 3518, rl-loss: 322.1127014160156\r",
      "INFO - Step 3519, rl-loss: 85.84480285644531\r",
      "INFO - Step 3520, rl-loss: 307.9306335449219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3521, rl-loss: 62.652496337890625\r",
      "INFO - Step 3522, rl-loss: 193.67562866210938\r",
      "INFO - Step 3523, rl-loss: 0.743881106376648\r",
      "INFO - Step 3524, rl-loss: 85.63432312011719\r",
      "INFO - Step 3525, rl-loss: 144.8140869140625\r",
      "INFO - Step 3526, rl-loss: 40.80477523803711\r",
      "INFO - Step 3527, rl-loss: 21.100812911987305\r",
      "INFO - Step 3528, rl-loss: 584.7730712890625\r",
      "INFO - Step 3529, rl-loss: 418.6901550292969\r",
      "INFO - Step 3530, rl-loss: 225.1873779296875\r",
      "INFO - Step 3531, rl-loss: 56.43197250366211\r",
      "INFO - Step 3532, rl-loss: 581.9949340820312\r",
      "INFO - Step 3533, rl-loss: 396.9610595703125\r",
      "INFO - Step 3534, rl-loss: 1.0555821657180786\r",
      "INFO - Step 3535, rl-loss: 239.5040740966797\r",
      "INFO - Step 3536, rl-loss: 403.0130310058594\r",
      "INFO - Step 3537, rl-loss: 90.02394104003906\r",
      "INFO - Step 3538, rl-loss: 339.512939453125\r",
      "INFO - Step 3539, rl-loss: 172.7587432861328\r",
      "INFO - Step 3540, rl-loss: 622.910400390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3541, rl-loss: 50.76092529296875\r",
      "INFO - Step 3542, rl-loss: 114.76786041259766\r",
      "INFO - Step 3543, rl-loss: 334.13037109375\r",
      "INFO - Step 3544, rl-loss: 271.404296875\r",
      "INFO - Step 3545, rl-loss: 0.9430655241012573\r",
      "INFO - Step 3546, rl-loss: 1.2023646831512451\r",
      "INFO - Step 3547, rl-loss: 223.14622497558594\r",
      "INFO - Step 3548, rl-loss: 1.6067745685577393\r",
      "INFO - Step 3549, rl-loss: 1.1492937803268433\r",
      "INFO - Step 3550, rl-loss: 218.7606964111328\r",
      "INFO - Step 3551, rl-loss: 69.17118835449219\r",
      "INFO - Step 3552, rl-loss: 702.641357421875\r",
      "INFO - Step 3553, rl-loss: 244.30796813964844\r",
      "INFO - Step 3554, rl-loss: 495.8267517089844\r",
      "INFO - Step 3555, rl-loss: 7.6988844871521\r",
      "INFO - Step 3556, rl-loss: 384.83203125\r",
      "INFO - Step 3557, rl-loss: 1.8381105661392212\r",
      "INFO - Step 3558, rl-loss: 189.68325805664062\r",
      "INFO - Step 3559, rl-loss: 208.5381317138672\r",
      "INFO - Step 3560, rl-loss: 146.6109161376953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3561, rl-loss: 137.56076049804688\r",
      "INFO - Step 3562, rl-loss: 658.4559936523438\r",
      "INFO - Step 3563, rl-loss: 218.3040008544922\r",
      "INFO - Step 3564, rl-loss: 1.3136365413665771\r",
      "INFO - Step 3565, rl-loss: 136.10780334472656\r",
      "INFO - Step 3566, rl-loss: 1.3166978359222412\r",
      "INFO - Step 3567, rl-loss: 132.82229614257812\r",
      "INFO - Step 3568, rl-loss: 30.756319046020508\r",
      "INFO - Step 3569, rl-loss: 176.46041870117188\r",
      "INFO - Step 3570, rl-loss: 1.3037917613983154\r",
      "INFO - Step 3571, rl-loss: 380.6499938964844\r",
      "INFO - Step 3572, rl-loss: 68.5701675415039\r",
      "INFO - Step 3573, rl-loss: 140.10467529296875\r",
      "INFO - Step 3574, rl-loss: 293.16845703125\r",
      "INFO - Step 3575, rl-loss: 1.1119836568832397\r",
      "INFO - Step 3576, rl-loss: 158.64541625976562\r",
      "INFO - Step 3577, rl-loss: 304.42010498046875\r",
      "INFO - Step 3578, rl-loss: 327.9888916015625\r",
      "INFO - Step 3579, rl-loss: 254.19485473632812\r",
      "INFO - Step 3580, rl-loss: 282.1744079589844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3581, rl-loss: 119.0239028930664\r",
      "INFO - Step 3582, rl-loss: 0.8767750859260559\r",
      "INFO - Step 3583, rl-loss: 226.88912963867188\r",
      "INFO - Step 3584, rl-loss: 61.190025329589844\r",
      "INFO - Step 3585, rl-loss: 223.0253448486328\r",
      "INFO - Step 3586, rl-loss: 222.58071899414062\r",
      "INFO - Step 3587, rl-loss: 9.29271411895752\r",
      "INFO - Step 3588, rl-loss: 330.77490234375\r",
      "INFO - Step 3589, rl-loss: 368.5676574707031\r",
      "INFO - Step 3590, rl-loss: 633.7838134765625\r",
      "INFO - Step 3591, rl-loss: 590.3025512695312\r",
      "INFO - Step 3592, rl-loss: 0.9428776502609253\r",
      "INFO - Step 3593, rl-loss: 169.10995483398438\r",
      "INFO - Step 3594, rl-loss: 0.9333664178848267\r",
      "INFO - Step 3595, rl-loss: 152.9816436767578\r",
      "INFO - Step 3596, rl-loss: 95.25113677978516\r",
      "INFO - Step 3597, rl-loss: 956.8032836914062\r",
      "INFO - Step 3598, rl-loss: 189.4501190185547\r",
      "INFO - Step 3599, rl-loss: 1.0924255847930908\r",
      "INFO - Step 3600, rl-loss: 1.2171751260757446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3601, rl-loss: 319.0634765625\r",
      "INFO - Step 3602, rl-loss: 209.17393493652344\r",
      "INFO - Step 3603, rl-loss: 99.22955322265625\r",
      "INFO - Step 3604, rl-loss: 180.16265869140625\r",
      "INFO - Step 3605, rl-loss: 369.85333251953125\r",
      "INFO - Step 3606, rl-loss: 234.15740966796875\r",
      "INFO - Step 3607, rl-loss: 272.0030822753906\r",
      "INFO - Step 3608, rl-loss: 277.09661865234375\r",
      "INFO - Step 3609, rl-loss: 1.145970344543457\r",
      "INFO - Step 3610, rl-loss: 246.9652557373047\r",
      "INFO - Step 3611, rl-loss: 1126.630126953125\r",
      "INFO - Step 3612, rl-loss: 185.5078125\r",
      "INFO - Step 3613, rl-loss: 1.9926106929779053\r",
      "INFO - Step 3614, rl-loss: 424.1929931640625\r",
      "INFO - Step 3615, rl-loss: 111.69696044921875\r",
      "INFO - Step 3616, rl-loss: 67.72864532470703\r",
      "INFO - Step 3617, rl-loss: 359.787109375\r",
      "INFO - Step 3618, rl-loss: 1.3554465770721436\r",
      "INFO - Step 3619, rl-loss: 24.888761520385742\r",
      "INFO - Step 3620, rl-loss: 499.813232421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3621, rl-loss: 262.92138671875\r",
      "INFO - Step 3622, rl-loss: 372.26318359375\r",
      "INFO - Step 3623, rl-loss: 490.1539306640625\r",
      "INFO - Step 3624, rl-loss: 56.37786865234375\r",
      "INFO - Step 3625, rl-loss: 1.2428644895553589\r",
      "INFO - Step 3626, rl-loss: 2.3689794540405273\r",
      "INFO - Step 3627, rl-loss: 341.76287841796875\r",
      "INFO - Step 3628, rl-loss: 396.3302307128906\r",
      "INFO - Step 3629, rl-loss: 130.07559204101562\r",
      "INFO - Step 3630, rl-loss: 1.140620470046997\r",
      "INFO - Step 3631, rl-loss: 189.85702514648438\r",
      "INFO - Step 3632, rl-loss: 1.0262298583984375\r",
      "INFO - Step 3633, rl-loss: 257.9603576660156\r",
      "INFO - Step 3634, rl-loss: 409.0020446777344\r",
      "INFO - Step 3635, rl-loss: 344.54388427734375\r",
      "INFO - Step 3636, rl-loss: 227.40940856933594\r",
      "INFO - Step 3637, rl-loss: 1.0185372829437256\r",
      "INFO - Step 3638, rl-loss: 1.01071298122406\r",
      "INFO - Step 3639, rl-loss: 62.425254821777344\r",
      "INFO - Step 3640, rl-loss: 370.01116943359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3641, rl-loss: 209.17138671875\r",
      "INFO - Step 3642, rl-loss: 0.9830490946769714\r",
      "INFO - Step 3643, rl-loss: 390.898681640625\r",
      "INFO - Step 3644, rl-loss: 272.8013916015625\r",
      "INFO - Step 3645, rl-loss: 172.44793701171875\r",
      "INFO - Step 3646, rl-loss: 179.89903259277344\r",
      "INFO - Step 3647, rl-loss: 779.3599853515625\r",
      "INFO - Step 3648, rl-loss: 231.746826171875\r",
      "INFO - Step 3649, rl-loss: 230.43374633789062\r",
      "INFO - Step 3650, rl-loss: 208.94857788085938\r",
      "INFO - Step 3651, rl-loss: 328.2866516113281\r",
      "INFO - Step 3652, rl-loss: 48.972782135009766\r",
      "INFO - Step 3653, rl-loss: 103.17091369628906\r",
      "INFO - Step 3654, rl-loss: 350.0914306640625\r",
      "INFO - Step 3655, rl-loss: 607.8470458984375\r",
      "INFO - Step 3656, rl-loss: 128.857177734375\r",
      "INFO - Step 3657, rl-loss: 152.32467651367188\r",
      "INFO - Step 3658, rl-loss: 1.0622589588165283\r",
      "INFO - Step 3659, rl-loss: 2.165372848510742\r",
      "INFO - Step 3660, rl-loss: 1.1350483894348145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3661, rl-loss: 1.2085814476013184\r",
      "INFO - Step 3662, rl-loss: 278.87786865234375\r",
      "INFO - Step 3663, rl-loss: 491.7865905761719\r",
      "INFO - Step 3664, rl-loss: 438.53399658203125\r",
      "INFO - Step 3665, rl-loss: 84.60326385498047\r",
      "INFO - Step 3666, rl-loss: 412.9130554199219\r",
      "INFO - Step 3667, rl-loss: 172.99327087402344\r",
      "INFO - Step 3668, rl-loss: 182.3272247314453\r",
      "INFO - Step 3669, rl-loss: 1.070427656173706\r",
      "INFO - Step 3670, rl-loss: 1.2598742246627808\r",
      "INFO - Step 3671, rl-loss: 525.2360229492188\r",
      "INFO - Step 3672, rl-loss: 4.721472263336182\r",
      "INFO - Step 3673, rl-loss: 323.0343933105469\r",
      "INFO - Step 3674, rl-loss: 67.23468017578125\r",
      "INFO - Step 3675, rl-loss: 1.255065679550171\r",
      "INFO - Step 3676, rl-loss: 329.86285400390625\r",
      "INFO - Step 3677, rl-loss: 1.1737350225448608\r",
      "INFO - Step 3678, rl-loss: 43.693748474121094\r",
      "INFO - Step 3679, rl-loss: 241.23162841796875\r",
      "INFO - Step 3680, rl-loss: 175.07904052734375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3681, rl-loss: 233.6645050048828\r",
      "INFO - Step 3682, rl-loss: 237.40931701660156\r",
      "INFO - Step 3683, rl-loss: 51.29240036010742\r",
      "INFO - Step 3684, rl-loss: 195.69869995117188\r",
      "INFO - Step 3685, rl-loss: 92.32252502441406\r",
      "INFO - Step 3686, rl-loss: 325.305419921875\r",
      "INFO - Step 3687, rl-loss: 222.45217895507812\r",
      "INFO - Step 3688, rl-loss: 245.5194854736328\r",
      "INFO - Step 3689, rl-loss: 1.2440900802612305\r",
      "INFO - Step 3690, rl-loss: 47.105751037597656\r",
      "INFO - Step 3691, rl-loss: 318.99365234375\r",
      "INFO - Step 3692, rl-loss: 0.7782594561576843\r",
      "INFO - Step 3693, rl-loss: 1.1656410694122314\r",
      "INFO - Step 3694, rl-loss: 370.1827697753906\r",
      "INFO - Step 3695, rl-loss: 1.1640557050704956\r",
      "INFO - Step 3696, rl-loss: 149.98577880859375\r",
      "INFO - Step 3697, rl-loss: 187.4558868408203\r",
      "INFO - Step 3698, rl-loss: 177.01727294921875\r",
      "INFO - Step 3699, rl-loss: 140.3650665283203\r",
      "INFO - Step 3700, rl-loss: 68.73049926757812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3701, rl-loss: 0.762816309928894\r",
      "INFO - Step 3702, rl-loss: 53.3541145324707\r",
      "INFO - Step 3703, rl-loss: 0.8426834344863892\r",
      "INFO - Step 3704, rl-loss: 445.43255615234375\r",
      "INFO - Step 3705, rl-loss: 233.49488830566406\r",
      "INFO - Step 3706, rl-loss: 203.45370483398438\r",
      "INFO - Step 3707, rl-loss: 7.870201110839844\r",
      "INFO - Step 3708, rl-loss: 50.83664321899414\r",
      "INFO - Step 3709, rl-loss: 188.4899444580078\r",
      "INFO - Step 3710, rl-loss: 0.9069843888282776\r",
      "INFO - Step 3711, rl-loss: 1.0440455675125122\r",
      "INFO - Step 3712, rl-loss: 693.1951293945312\r",
      "INFO - Step 3713, rl-loss: 0.9149402976036072\r",
      "INFO - Step 3714, rl-loss: 215.98292541503906\r",
      "INFO - Step 3715, rl-loss: 150.75791931152344\r",
      "INFO - Step 3716, rl-loss: 464.1080322265625\r",
      "INFO - Step 3717, rl-loss: 433.7470397949219\r",
      "INFO - Step 3718, rl-loss: 1.52141273021698\r",
      "INFO - Step 3719, rl-loss: 150.41148376464844\r",
      "INFO - Step 3720, rl-loss: 682.3087158203125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3721, rl-loss: 172.383544921875\r",
      "INFO - Step 3722, rl-loss: 21.171852111816406\r",
      "INFO - Step 3723, rl-loss: 1.9260315895080566\r",
      "INFO - Step 3724, rl-loss: 100.86936950683594\r",
      "INFO - Step 3725, rl-loss: 104.5324478149414\r",
      "INFO - Step 3726, rl-loss: 123.95402526855469\r",
      "INFO - Step 3727, rl-loss: 909.2388305664062\r",
      "INFO - Step 3728, rl-loss: 188.36788940429688\r",
      "INFO - Step 3729, rl-loss: 1.5037330389022827\r",
      "INFO - Step 3730, rl-loss: 1.251603126525879\r",
      "INFO - Step 3731, rl-loss: 0.9368258714675903\r",
      "INFO - Step 3732, rl-loss: 344.5516357421875\r",
      "INFO - Step 3733, rl-loss: 166.20590209960938\r",
      "INFO - Step 3734, rl-loss: 603.1112670898438\r",
      "INFO - Step 3735, rl-loss: 177.48660278320312\r",
      "INFO - Step 3736, rl-loss: 254.43214416503906\r",
      "INFO - Step 3737, rl-loss: 1.074596643447876\r",
      "INFO - Step 3738, rl-loss: 887.9234619140625\r",
      "INFO - Step 3739, rl-loss: 60.26549530029297\r",
      "INFO - Step 3740, rl-loss: 1.3186664581298828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3741, rl-loss: 1.7054531574249268\r",
      "INFO - Step 3742, rl-loss: 212.29811096191406\r",
      "INFO - Step 3743, rl-loss: 1.0348187685012817\r",
      "INFO - Step 3744, rl-loss: 743.306884765625\r",
      "INFO - Step 3745, rl-loss: 216.28443908691406\r",
      "INFO - Step 3746, rl-loss: 17.56415367126465\r",
      "INFO - Step 3747, rl-loss: 388.08050537109375\r",
      "INFO - Step 3748, rl-loss: 55.7030143737793\r",
      "INFO - Step 3749, rl-loss: 452.4185791015625\r",
      "INFO - Step 3750, rl-loss: 257.9451904296875\r",
      "INFO - Step 3751, rl-loss: 248.66175842285156\r",
      "INFO - Step 3752, rl-loss: 171.25521850585938\r",
      "INFO - Step 3753, rl-loss: 504.47283935546875\r",
      "INFO - Step 3754, rl-loss: 1.0374150276184082\r",
      "INFO - Step 3755, rl-loss: 72.69915771484375\r",
      "INFO - Step 3756, rl-loss: 0.8698751926422119\r",
      "INFO - Step 3757, rl-loss: 343.99517822265625\r",
      "INFO - Step 3758, rl-loss: 0.8473478555679321\r",
      "INFO - Step 3759, rl-loss: 104.10673522949219\r",
      "INFO - Step 3760, rl-loss: 309.2484130859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3761, rl-loss: 639.9573974609375\r",
      "INFO - Step 3762, rl-loss: 187.12547302246094\r",
      "INFO - Step 3763, rl-loss: 518.7691650390625\r",
      "INFO - Step 3764, rl-loss: 63.270652770996094\r",
      "INFO - Step 3765, rl-loss: 167.47222900390625\r",
      "INFO - Step 3766, rl-loss: 20.523815155029297\r",
      "INFO - Step 3767, rl-loss: 152.2662811279297\r",
      "INFO - Step 3768, rl-loss: 90.8710708618164\r",
      "INFO - Step 3769, rl-loss: 0.7642276287078857\r",
      "INFO - Step 3770, rl-loss: 0.9166469573974609\r",
      "INFO - Step 3771, rl-loss: 433.2499694824219\r",
      "INFO - Step 3772, rl-loss: 38.34161376953125\r",
      "INFO - Step 3773, rl-loss: 39.584877014160156\r",
      "INFO - Step 3774, rl-loss: 246.95755004882812\r",
      "INFO - Step 3775, rl-loss: 310.6199951171875\r",
      "INFO - Step 3776, rl-loss: 1.101704716682434\r",
      "INFO - Step 3777, rl-loss: 220.47976684570312\r",
      "INFO - Step 3778, rl-loss: 20.589073181152344\r",
      "INFO - Step 3779, rl-loss: 278.2884826660156\r",
      "INFO - Step 3780, rl-loss: 349.50579833984375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3781, rl-loss: 140.0470428466797\r",
      "INFO - Step 3782, rl-loss: 557.6952514648438\r",
      "INFO - Step 3783, rl-loss: 407.1494445800781\r",
      "INFO - Step 3784, rl-loss: 205.4443817138672\r",
      "INFO - Step 3785, rl-loss: 242.49139404296875\r",
      "INFO - Step 3786, rl-loss: 404.53729248046875\r",
      "INFO - Step 3787, rl-loss: 0.9883864521980286\r",
      "INFO - Step 3788, rl-loss: 1.3342422246932983\r",
      "INFO - Step 3789, rl-loss: 638.0807495117188\r",
      "INFO - Step 3790, rl-loss: 133.38946533203125\r",
      "INFO - Step 3791, rl-loss: 707.606201171875\r",
      "INFO - Step 3792, rl-loss: 254.8678436279297\r",
      "INFO - Step 3793, rl-loss: 197.9539794921875\r",
      "INFO - Step 3794, rl-loss: 0.9563572406768799\r",
      "INFO - Step 3795, rl-loss: 448.2776794433594\r",
      "INFO - Step 3796, rl-loss: 315.00115966796875\r",
      "INFO - Step 3797, rl-loss: 545.6182861328125\r",
      "INFO - Step 3798, rl-loss: 423.4117431640625\r",
      "INFO - Step 3799, rl-loss: 309.2034912109375\r",
      "INFO - Step 3800, rl-loss: 2.1317853927612305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3801, rl-loss: 184.87144470214844\r",
      "INFO - Step 3802, rl-loss: 295.9969787597656\r",
      "INFO - Step 3803, rl-loss: 308.1467590332031\r",
      "INFO - Step 3804, rl-loss: 490.9846496582031\r",
      "INFO - Step 3805, rl-loss: 64.27204895019531\r",
      "INFO - Step 3806, rl-loss: 1.2740284204483032\r",
      "INFO - Step 3807, rl-loss: 1.419097661972046\r",
      "INFO - Step 3808, rl-loss: 359.9610595703125\r",
      "INFO - Step 3809, rl-loss: 1.432910680770874\r",
      "INFO - Step 3810, rl-loss: 435.22576904296875\r",
      "INFO - Step 3811, rl-loss: 262.07647705078125\r",
      "INFO - Step 3812, rl-loss: 1.1276426315307617\r",
      "INFO - Step 3813, rl-loss: 132.07693481445312\r",
      "INFO - Step 3814, rl-loss: 1.383710265159607\r",
      "INFO - Step 3815, rl-loss: 0.9575866460800171\r",
      "INFO - Step 3816, rl-loss: 136.46490478515625\r",
      "INFO - Step 3817, rl-loss: 138.66073608398438\r",
      "INFO - Step 3818, rl-loss: 0.8584715127944946\r",
      "INFO - Step 3819, rl-loss: 98.51850891113281\r",
      "INFO - Step 3820, rl-loss: 222.65699768066406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3821, rl-loss: 0.8219050168991089\r",
      "INFO - Step 3822, rl-loss: 559.215576171875\r",
      "INFO - Step 3823, rl-loss: 639.3728637695312\r",
      "INFO - Step 3824, rl-loss: 180.11209106445312\r",
      "INFO - Step 3825, rl-loss: 582.2205810546875\r",
      "INFO - Step 3826, rl-loss: 0.9669918417930603\r",
      "INFO - Step 3827, rl-loss: 503.0633239746094\r",
      "INFO - Step 3828, rl-loss: 361.735595703125\r",
      "INFO - Step 3829, rl-loss: 378.82110595703125\r",
      "INFO - Step 3830, rl-loss: 384.8248291015625\r",
      "INFO - Step 3831, rl-loss: 416.4339599609375\r",
      "INFO - Step 3832, rl-loss: 239.0469207763672\r",
      "INFO - Step 3833, rl-loss: 623.884033203125\r",
      "INFO - Step 3834, rl-loss: 81.0199966430664\r",
      "INFO - Step 3835, rl-loss: 1.000084638595581\r",
      "INFO - Step 3836, rl-loss: 152.7967071533203\r",
      "INFO - Step 3837, rl-loss: 357.4783935546875\r",
      "INFO - Step 3838, rl-loss: 0.6635922193527222\r",
      "INFO - Step 3839, rl-loss: 163.48202514648438\r",
      "INFO - Step 3840, rl-loss: 100.51268005371094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3841, rl-loss: 0.908719539642334\r",
      "INFO - Step 3842, rl-loss: 222.03408813476562\r",
      "INFO - Step 3843, rl-loss: 33.41407012939453\r",
      "INFO - Step 3844, rl-loss: 352.591796875\r",
      "INFO - Step 3845, rl-loss: 138.46861267089844\r",
      "INFO - Step 3846, rl-loss: 339.3955078125\r",
      "INFO - Step 3847, rl-loss: 428.16912841796875\r",
      "INFO - Step 3848, rl-loss: 8.05893611907959\r",
      "INFO - Step 3849, rl-loss: 405.2179870605469\r",
      "INFO - Step 3850, rl-loss: 14.843062400817871\r",
      "INFO - Step 3851, rl-loss: 0.8493126630783081\r",
      "INFO - Step 3852, rl-loss: 156.21075439453125\r",
      "INFO - Step 3853, rl-loss: 1.2602430582046509\r",
      "INFO - Step 3854, rl-loss: 156.15211486816406\r",
      "INFO - Step 3855, rl-loss: 308.5692138671875\r",
      "INFO - Step 3856, rl-loss: 92.3128662109375\r",
      "INFO - Step 3857, rl-loss: 314.840087890625\r",
      "INFO - Step 3858, rl-loss: 389.4250793457031\r",
      "INFO - Step 3859, rl-loss: 146.1597442626953\r",
      "INFO - Step 3860, rl-loss: 1.5343759059906006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3861, rl-loss: 118.54480743408203\r",
      "INFO - Step 3862, rl-loss: 169.19210815429688\r",
      "INFO - Step 3863, rl-loss: 445.4061584472656\r",
      "INFO - Step 3864, rl-loss: 398.267333984375\r",
      "INFO - Step 3865, rl-loss: 93.60897827148438\r",
      "INFO - Step 3866, rl-loss: 238.39044189453125\r",
      "INFO - Step 3867, rl-loss: 659.01513671875\r",
      "INFO - Step 3868, rl-loss: 133.18016052246094\r",
      "INFO - Step 3869, rl-loss: 24.023632049560547\r",
      "INFO - Step 3870, rl-loss: 459.4543151855469\r",
      "INFO - Step 3871, rl-loss: 92.35536193847656\r",
      "INFO - Step 3872, rl-loss: 152.59263610839844\r",
      "INFO - Step 3873, rl-loss: 0.850774884223938\r",
      "INFO - Step 3874, rl-loss: 468.3121337890625\r",
      "INFO - Step 3875, rl-loss: 50.64736557006836\r",
      "INFO - Step 3876, rl-loss: 1.182428240776062\r",
      "INFO - Step 3877, rl-loss: 299.4718017578125\r",
      "INFO - Step 3878, rl-loss: 1.0066821575164795\r",
      "INFO - Step 3879, rl-loss: 290.1535949707031\r",
      "INFO - Step 3880, rl-loss: 26.765148162841797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3881, rl-loss: 2.0209484100341797\r",
      "INFO - Step 3882, rl-loss: 208.17991638183594\r",
      "INFO - Step 3883, rl-loss: 24.430099487304688\r",
      "INFO - Step 3884, rl-loss: 1.0323044061660767\r",
      "INFO - Step 3885, rl-loss: 0.9243139028549194\r",
      "INFO - Step 3886, rl-loss: 939.0752563476562\r",
      "INFO - Step 3887, rl-loss: 24.162107467651367\r",
      "INFO - Step 3888, rl-loss: 62.076656341552734\r",
      "INFO - Step 3889, rl-loss: 541.7506103515625\r",
      "INFO - Step 3890, rl-loss: 394.720458984375\r",
      "INFO - Step 3891, rl-loss: 0.9243009686470032\r",
      "INFO - Step 3892, rl-loss: 531.6472778320312\r",
      "INFO - Step 3893, rl-loss: 218.86585998535156\r",
      "INFO - Step 3894, rl-loss: 90.01689910888672\r",
      "INFO - Step 3895, rl-loss: 247.5277099609375\r",
      "INFO - Step 3896, rl-loss: 185.99822998046875\r",
      "INFO - Step 3897, rl-loss: 13.670655250549316\r",
      "INFO - Step 3898, rl-loss: 321.1068115234375\r",
      "INFO - Step 3899, rl-loss: 390.7817687988281\r",
      "INFO - Step 3900, rl-loss: 77.56535339355469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3901, rl-loss: 585.5112915039062\r",
      "INFO - Step 3902, rl-loss: 1.1015841960906982\r",
      "INFO - Step 3903, rl-loss: 148.9232940673828\r",
      "INFO - Step 3904, rl-loss: 777.179931640625\r",
      "INFO - Step 3905, rl-loss: 14.309417724609375\r",
      "INFO - Step 3906, rl-loss: 1.1801908016204834\r",
      "INFO - Step 3907, rl-loss: 8.928535461425781\r",
      "INFO - Step 3908, rl-loss: 0.8372803330421448\r",
      "INFO - Step 3909, rl-loss: 1.3112766742706299\r",
      "INFO - Step 3910, rl-loss: 272.28704833984375\r",
      "INFO - Step 3911, rl-loss: 80.2884750366211\r",
      "INFO - Step 3912, rl-loss: 231.07781982421875\r",
      "INFO - Step 3913, rl-loss: 77.06266784667969\r",
      "INFO - Step 3914, rl-loss: 151.587890625\r",
      "INFO - Step 3915, rl-loss: 444.4013977050781\r",
      "INFO - Step 3916, rl-loss: 114.48282623291016\r",
      "INFO - Step 3917, rl-loss: 411.77203369140625\r",
      "INFO - Step 3918, rl-loss: 423.90631103515625\r",
      "INFO - Step 3919, rl-loss: 207.55902099609375\r",
      "INFO - Step 3920, rl-loss: 272.78106689453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3921, rl-loss: 330.65740966796875\r",
      "INFO - Step 3922, rl-loss: 1.0920108556747437\r",
      "INFO - Step 3923, rl-loss: 0.8583914041519165\r",
      "INFO - Step 3924, rl-loss: 173.39317321777344\r",
      "INFO - Step 3925, rl-loss: 0.8789052963256836\r",
      "INFO - Step 3926, rl-loss: 55.89544677734375\r",
      "INFO - Step 3927, rl-loss: 345.0694274902344\r",
      "INFO - Step 3928, rl-loss: 1.2107168436050415\r",
      "INFO - Step 3929, rl-loss: 289.77239990234375\r",
      "INFO - Step 3930, rl-loss: 131.56570434570312\r",
      "INFO - Step 3931, rl-loss: 95.72227478027344\r",
      "INFO - Step 3932, rl-loss: 1.3015375137329102\r",
      "INFO - Step 3933, rl-loss: 275.4820251464844\r",
      "INFO - Step 3934, rl-loss: 310.8135986328125\r",
      "INFO - Step 3935, rl-loss: 0.7005358934402466\r",
      "INFO - Step 3936, rl-loss: 84.88613891601562\r",
      "INFO - Step 3937, rl-loss: 714.9406127929688\r",
      "INFO - Step 3938, rl-loss: 51.11552047729492\r",
      "INFO - Step 3939, rl-loss: 782.3011474609375\r",
      "INFO - Step 3940, rl-loss: 90.99256896972656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3941, rl-loss: 0.9671518206596375\r",
      "INFO - Step 3942, rl-loss: 307.3458251953125\r",
      "INFO - Step 3943, rl-loss: 108.25838470458984\r",
      "INFO - Step 3944, rl-loss: 0.5910890698432922\r",
      "INFO - Step 3945, rl-loss: 0.8703604936599731\r",
      "INFO - Step 3946, rl-loss: 180.77273559570312\r",
      "INFO - Step 3947, rl-loss: 769.0343017578125\r",
      "INFO - Step 3948, rl-loss: 211.51531982421875\r",
      "INFO - Step 3949, rl-loss: 185.87088012695312\r",
      "INFO - Step 3950, rl-loss: 293.94873046875\r",
      "INFO - Step 3951, rl-loss: 168.2135467529297\r",
      "INFO - Step 3952, rl-loss: 46.364688873291016\r",
      "INFO - Step 3953, rl-loss: 1.5036625862121582\r",
      "INFO - Step 3954, rl-loss: 174.9534912109375\r",
      "INFO - Step 3955, rl-loss: 0.9005855321884155\r",
      "INFO - Step 3956, rl-loss: 138.80328369140625\r",
      "INFO - Step 3957, rl-loss: 0.9180721640586853\r",
      "INFO - Step 3958, rl-loss: 445.9626159667969\r",
      "INFO - Step 3959, rl-loss: 221.60252380371094\r",
      "INFO - Step 3960, rl-loss: 376.9701843261719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3961, rl-loss: 1.1423356533050537\r",
      "INFO - Step 3962, rl-loss: 430.42340087890625\r",
      "INFO - Step 3963, rl-loss: 277.1661071777344\r",
      "INFO - Step 3964, rl-loss: 2.156977891921997\r",
      "INFO - Step 3965, rl-loss: 543.5406494140625\r",
      "INFO - Step 3966, rl-loss: 291.6004638671875\r",
      "INFO - Step 3967, rl-loss: 0.8671282529830933\r",
      "INFO - Step 3968, rl-loss: 149.19053649902344\r",
      "INFO - Step 3969, rl-loss: 265.3938293457031\r",
      "INFO - Step 3970, rl-loss: 223.69961547851562\r",
      "INFO - Step 3971, rl-loss: 486.71295166015625\r",
      "INFO - Step 3972, rl-loss: 389.0889892578125\r",
      "INFO - Step 3973, rl-loss: 776.9728393554688\r",
      "INFO - Step 3974, rl-loss: 151.74868774414062\r",
      "INFO - Step 3975, rl-loss: 529.6187744140625\r",
      "INFO - Step 3976, rl-loss: 10.375324249267578\r",
      "INFO - Step 3977, rl-loss: 1.2732596397399902\r",
      "INFO - Step 3978, rl-loss: 193.89784240722656\r",
      "INFO - Step 3979, rl-loss: 21.18206787109375\r",
      "INFO - Step 3980, rl-loss: 749.5421752929688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 3981, rl-loss: 263.3769226074219\r",
      "INFO - Step 3982, rl-loss: 1.040169358253479\r",
      "INFO - Step 3983, rl-loss: 332.1459045410156\r",
      "INFO - Step 3984, rl-loss: 0.9354985952377319\r",
      "INFO - Step 3985, rl-loss: 74.42807006835938\r",
      "INFO - Step 3986, rl-loss: 597.2945556640625\r",
      "INFO - Step 3987, rl-loss: 145.19735717773438\r",
      "INFO - Step 3988, rl-loss: 1.0603007078170776\r",
      "INFO - Step 3989, rl-loss: 509.4630432128906\r",
      "INFO - Step 3990, rl-loss: 615.6373901367188\r",
      "INFO - Step 3991, rl-loss: 33.20847702026367\r",
      "INFO - Step 3992, rl-loss: 210.10284423828125\r",
      "INFO - Step 3993, rl-loss: 316.19110107421875\r",
      "INFO - Step 3994, rl-loss: 270.1781921386719\r",
      "INFO - Step 3995, rl-loss: 266.9254150390625\r",
      "INFO - Step 3996, rl-loss: 251.153564453125\r",
      "INFO - Step 3997, rl-loss: 0.8767300844192505\r",
      "INFO - Step 3998, rl-loss: 158.2501220703125\r",
      "INFO - Step 3999, rl-loss: 224.10447692871094\r",
      "INFO - Step 4000, rl-loss: 148.155029296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 4020, rl-loss: 0.6222033500671387\n",
      "----------------------------------------\n",
      "  timestep     |  537392\n",
      "  reward       |  60.45\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 4040, rl-loss: 737.72552490234383"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4041, rl-loss: 363.77447509765625\r",
      "INFO - Step 4042, rl-loss: 149.86805725097656\r",
      "INFO - Step 4043, rl-loss: 249.25680541992188\r",
      "INFO - Step 4044, rl-loss: 306.41937255859375\r",
      "INFO - Step 4045, rl-loss: 563.33935546875\r",
      "INFO - Step 4046, rl-loss: 779.2322998046875\r",
      "INFO - Step 4047, rl-loss: 137.3437957763672\r",
      "INFO - Step 4048, rl-loss: 57.28174591064453\r",
      "INFO - Step 4049, rl-loss: 128.9134063720703\r",
      "INFO - Step 4050, rl-loss: 433.298095703125\r",
      "INFO - Step 4051, rl-loss: 185.31422424316406\r",
      "INFO - Step 4052, rl-loss: 0.9023697376251221\r",
      "INFO - Step 4053, rl-loss: 115.34217071533203\r",
      "INFO - Step 4054, rl-loss: 158.93218994140625\r",
      "INFO - Step 4055, rl-loss: 109.68733215332031\r",
      "INFO - Step 4056, rl-loss: 209.52520751953125\r",
      "INFO - Step 4057, rl-loss: 281.62744140625\r",
      "INFO - Step 4058, rl-loss: 144.255615234375\r",
      "INFO - Step 4059, rl-loss: 106.55978393554688\r",
      "INFO - Step 4060, rl-loss: 110.19913482666016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4061, rl-loss: 5.942049980163574\r",
      "INFO - Step 4062, rl-loss: 394.9647521972656\r",
      "INFO - Step 4063, rl-loss: 1.2656915187835693\r",
      "INFO - Step 4064, rl-loss: 381.2107238769531\r",
      "INFO - Step 4065, rl-loss: 449.60284423828125\r",
      "INFO - Step 4066, rl-loss: 69.40676879882812\r",
      "INFO - Step 4067, rl-loss: 167.69430541992188\r",
      "INFO - Step 4068, rl-loss: 612.910888671875\r",
      "INFO - Step 4069, rl-loss: 135.21746826171875\r",
      "INFO - Step 4070, rl-loss: 0.7028688192367554\r",
      "INFO - Step 4071, rl-loss: 0.4392414093017578\r",
      "INFO - Step 4072, rl-loss: 63.15532684326172\r",
      "INFO - Step 4073, rl-loss: 1.1850314140319824\r",
      "INFO - Step 4074, rl-loss: 1.2243921756744385\r",
      "INFO - Step 4075, rl-loss: 491.4645080566406\r",
      "INFO - Step 4076, rl-loss: 215.77264404296875\r",
      "INFO - Step 4077, rl-loss: 92.10396575927734\r",
      "INFO - Step 4078, rl-loss: 114.69717407226562\r",
      "INFO - Step 4079, rl-loss: 112.46830749511719\r",
      "INFO - Step 4080, rl-loss: 146.23602294921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4081, rl-loss: 290.85076904296875\r",
      "INFO - Step 4082, rl-loss: 69.71331787109375\r",
      "INFO - Step 4083, rl-loss: 0.736690878868103\r",
      "INFO - Step 4084, rl-loss: 1.3575607538223267\r",
      "INFO - Step 4085, rl-loss: 274.7906188964844\r",
      "INFO - Step 4086, rl-loss: 6.136564254760742\r",
      "INFO - Step 4087, rl-loss: 549.432861328125\r",
      "INFO - Step 4088, rl-loss: 326.9290466308594\r",
      "INFO - Step 4089, rl-loss: 130.93284606933594\r",
      "INFO - Step 4090, rl-loss: 138.9603729248047\r",
      "INFO - Step 4091, rl-loss: 1.4872385263442993\r",
      "INFO - Step 4092, rl-loss: 0.9669829607009888\r",
      "INFO - Step 4093, rl-loss: 92.0873031616211\r",
      "INFO - Step 4094, rl-loss: 442.4176025390625\r",
      "INFO - Step 4095, rl-loss: 0.7435100078582764\r",
      "INFO - Step 4096, rl-loss: 32.64238739013672\r",
      "INFO - Step 4097, rl-loss: 483.36090087890625\r",
      "INFO - Step 4098, rl-loss: 784.19384765625\r",
      "INFO - Step 4099, rl-loss: 293.4352111816406\r",
      "INFO - Step 4100, rl-loss: 176.23672485351562\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4101, rl-loss: 645.1571655273438\r",
      "INFO - Step 4102, rl-loss: 276.2572937011719\r",
      "INFO - Step 4103, rl-loss: 847.709228515625\r",
      "INFO - Step 4104, rl-loss: 208.2122802734375\r",
      "INFO - Step 4105, rl-loss: 587.5064086914062\r",
      "INFO - Step 4106, rl-loss: 193.7935333251953\r",
      "INFO - Step 4107, rl-loss: 132.9630584716797\r",
      "INFO - Step 4108, rl-loss: 1.1828229427337646\r",
      "INFO - Step 4109, rl-loss: 396.5150146484375\r",
      "INFO - Step 4110, rl-loss: 176.21148681640625\r",
      "INFO - Step 4111, rl-loss: 137.7184295654297\r",
      "INFO - Step 4112, rl-loss: 418.2911376953125\r",
      "INFO - Step 4113, rl-loss: 93.02945709228516\r",
      "INFO - Step 4114, rl-loss: 267.7027282714844\r",
      "INFO - Step 4115, rl-loss: 108.72740936279297\r",
      "INFO - Step 4116, rl-loss: 508.54559326171875\r",
      "INFO - Step 4117, rl-loss: 175.64028930664062\r",
      "INFO - Step 4118, rl-loss: 282.77630615234375\r",
      "INFO - Step 4119, rl-loss: 1.3683353662490845\r",
      "INFO - Step 4120, rl-loss: 1.653651475906372"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4121, rl-loss: 385.4427185058594\r",
      "INFO - Step 4122, rl-loss: 468.8638000488281\r",
      "INFO - Step 4123, rl-loss: 0.9481360912322998\r",
      "INFO - Step 4124, rl-loss: 406.3108215332031\r",
      "INFO - Step 4125, rl-loss: 1.91352117061615\r",
      "INFO - Step 4126, rl-loss: 179.33253479003906\r",
      "INFO - Step 4127, rl-loss: 192.75877380371094\r",
      "INFO - Step 4128, rl-loss: 1.8545970916748047\r",
      "INFO - Step 4129, rl-loss: 303.7900695800781\r",
      "INFO - Step 4130, rl-loss: 235.41717529296875\r",
      "INFO - Step 4131, rl-loss: 2.249134063720703\r",
      "INFO - Step 4132, rl-loss: 1.4522581100463867\r",
      "INFO - Step 4133, rl-loss: 1.2303428649902344\r",
      "INFO - Step 4134, rl-loss: 194.65213012695312\r",
      "INFO - Step 4135, rl-loss: 147.27767944335938\r",
      "INFO - Step 4136, rl-loss: 1.1955244541168213\r",
      "INFO - Step 4137, rl-loss: 132.90721130371094\r",
      "INFO - Step 4138, rl-loss: 80.0548324584961\r",
      "INFO - Step 4139, rl-loss: 120.57361602783203\r",
      "INFO - Step 4140, rl-loss: 1.2797110080718994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4141, rl-loss: 93.09701538085938\r",
      "INFO - Step 4142, rl-loss: 80.4176025390625\r",
      "INFO - Step 4143, rl-loss: 223.55560302734375\r",
      "INFO - Step 4144, rl-loss: 150.53955078125\r",
      "INFO - Step 4145, rl-loss: 73.42521667480469\r",
      "INFO - Step 4146, rl-loss: 662.0738525390625\r",
      "INFO - Step 4147, rl-loss: 611.0411376953125\r",
      "INFO - Step 4148, rl-loss: 62.942962646484375\r",
      "INFO - Step 4149, rl-loss: 103.05622100830078\r",
      "INFO - Step 4150, rl-loss: 2.159766674041748\r",
      "INFO - Step 4151, rl-loss: 420.90911865234375\r",
      "INFO - Step 4152, rl-loss: 1.8540711402893066\r",
      "INFO - Step 4153, rl-loss: 215.2205352783203\r",
      "INFO - Step 4154, rl-loss: 310.6691589355469\r",
      "INFO - Step 4155, rl-loss: 203.49778747558594\r",
      "INFO - Step 4156, rl-loss: 92.45540618896484\r",
      "INFO - Step 4157, rl-loss: 424.11260986328125\r",
      "INFO - Step 4158, rl-loss: 32.5411491394043\r",
      "INFO - Step 4159, rl-loss: 193.98648071289062\r",
      "INFO - Step 4160, rl-loss: 242.01844787597656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4161, rl-loss: 132.94039916992188\r",
      "INFO - Step 4162, rl-loss: 0.930789589881897\r",
      "INFO - Step 4163, rl-loss: 1.487501621246338\r",
      "INFO - Step 4164, rl-loss: 117.98896026611328\r",
      "INFO - Step 4165, rl-loss: 202.58697509765625\r",
      "INFO - Step 4166, rl-loss: 463.00042724609375\r",
      "INFO - Step 4167, rl-loss: 0.9788951277732849\r",
      "INFO - Step 4168, rl-loss: 67.59461212158203\r",
      "INFO - Step 4169, rl-loss: 190.8525390625\r",
      "INFO - Step 4170, rl-loss: 161.92413330078125\r",
      "INFO - Step 4171, rl-loss: 98.0391845703125\r",
      "INFO - Step 4172, rl-loss: 289.9606018066406\r",
      "INFO - Step 4173, rl-loss: 1.430039882659912\r",
      "INFO - Step 4174, rl-loss: 91.2783203125\r",
      "INFO - Step 4175, rl-loss: 70.50408172607422\r",
      "INFO - Step 4176, rl-loss: 154.36839294433594\r",
      "INFO - Step 4177, rl-loss: 176.0500946044922\r",
      "INFO - Step 4178, rl-loss: 361.75018310546875\r",
      "INFO - Step 4179, rl-loss: 278.42974853515625\r",
      "INFO - Step 4180, rl-loss: 154.23666381835938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4181, rl-loss: 320.085693359375\r",
      "INFO - Step 4182, rl-loss: 146.68983459472656\r",
      "INFO - Step 4183, rl-loss: 1.060798168182373\r",
      "INFO - Step 4184, rl-loss: 223.2617645263672\r",
      "INFO - Step 4185, rl-loss: 1.7419952154159546\r",
      "INFO - Step 4186, rl-loss: 559.6417846679688\r",
      "INFO - Step 4187, rl-loss: 180.16123962402344\r",
      "INFO - Step 4188, rl-loss: 1.6055632829666138\r",
      "INFO - Step 4189, rl-loss: 0.8067209720611572\r",
      "INFO - Step 4190, rl-loss: 5.867710590362549\r",
      "INFO - Step 4191, rl-loss: 6.21343469619751\r",
      "INFO - Step 4192, rl-loss: 89.46826171875\r",
      "INFO - Step 4193, rl-loss: 146.10296630859375\r",
      "INFO - Step 4194, rl-loss: 185.349853515625\r",
      "INFO - Step 4195, rl-loss: 245.21676635742188\r",
      "INFO - Step 4196, rl-loss: 1.2493454217910767\r",
      "INFO - Step 4197, rl-loss: 72.59285736083984\r",
      "INFO - Step 4198, rl-loss: 799.9234619140625\r",
      "INFO - Step 4199, rl-loss: 0.7260844111442566\r",
      "INFO - Step 4200, rl-loss: 100.36508178710938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4201, rl-loss: 497.5486145019531\r",
      "INFO - Step 4202, rl-loss: 313.33184814453125\r",
      "INFO - Step 4203, rl-loss: 198.87437438964844\r",
      "INFO - Step 4204, rl-loss: 705.21044921875\r",
      "INFO - Step 4205, rl-loss: 0.8323401808738708\r",
      "INFO - Step 4206, rl-loss: 2.4731764793395996\r",
      "INFO - Step 4207, rl-loss: 341.0785217285156\r",
      "INFO - Step 4208, rl-loss: 161.71726989746094\r",
      "INFO - Step 4209, rl-loss: 565.3233642578125\r",
      "INFO - Step 4210, rl-loss: 334.26220703125\r",
      "INFO - Step 4211, rl-loss: 1013.9556274414062\r",
      "INFO - Step 4212, rl-loss: 339.85797119140625\r",
      "INFO - Step 4213, rl-loss: 63.64653778076172\r",
      "INFO - Step 4214, rl-loss: 106.1311264038086\r",
      "INFO - Step 4215, rl-loss: 392.923095703125\r",
      "INFO - Step 4216, rl-loss: 166.5799102783203\r",
      "INFO - Step 4217, rl-loss: 74.4610595703125\r",
      "INFO - Step 4218, rl-loss: 415.1300048828125\r",
      "INFO - Step 4219, rl-loss: 154.677001953125\r",
      "INFO - Step 4220, rl-loss: 55.439144134521484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4221, rl-loss: 1.7182261943817139\r",
      "INFO - Step 4222, rl-loss: 98.84194946289062\r",
      "INFO - Step 4223, rl-loss: 242.07858276367188\r",
      "INFO - Step 4224, rl-loss: 205.61956787109375\r",
      "INFO - Step 4225, rl-loss: 689.3658447265625\r",
      "INFO - Step 4226, rl-loss: 139.31251525878906\r",
      "INFO - Step 4227, rl-loss: 153.0482177734375\r",
      "INFO - Step 4228, rl-loss: 841.58203125\r",
      "INFO - Step 4229, rl-loss: 68.94007110595703\r",
      "INFO - Step 4230, rl-loss: 160.10064697265625\r",
      "INFO - Step 4231, rl-loss: 0.7926265001296997\r",
      "INFO - Step 4232, rl-loss: 104.25967407226562\r",
      "INFO - Step 4233, rl-loss: 301.716796875\r",
      "INFO - Step 4234, rl-loss: 204.8632049560547\r",
      "INFO - Step 4235, rl-loss: 1.4525929689407349\r",
      "INFO - Step 4236, rl-loss: 223.0219268798828\r",
      "INFO - Step 4237, rl-loss: 1.3757569789886475\r",
      "INFO - Step 4238, rl-loss: 364.9674072265625\r",
      "INFO - Step 4239, rl-loss: 1.4623682498931885\r",
      "INFO - Step 4240, rl-loss: 196.52850341796875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4241, rl-loss: 196.00390625\r",
      "INFO - Step 4242, rl-loss: 154.09603881835938\r",
      "INFO - Step 4243, rl-loss: 212.60931396484375\r",
      "INFO - Step 4244, rl-loss: 1.768078327178955\r",
      "INFO - Step 4245, rl-loss: 318.5135803222656\r",
      "INFO - Step 4246, rl-loss: 221.96278381347656\r",
      "INFO - Step 4247, rl-loss: 306.9628601074219\r",
      "INFO - Step 4248, rl-loss: 161.40533447265625\r",
      "INFO - Step 4249, rl-loss: 1.2036492824554443\r",
      "INFO - Step 4250, rl-loss: 306.802490234375\r",
      "INFO - Step 4251, rl-loss: 493.2387390136719\r",
      "INFO - Step 4252, rl-loss: 220.1258544921875\r",
      "INFO - Step 4253, rl-loss: 96.37621307373047\r",
      "INFO - Step 4254, rl-loss: 220.08148193359375\r",
      "INFO - Step 4255, rl-loss: 370.83270263671875\r",
      "INFO - Step 4256, rl-loss: 1.1772429943084717\r",
      "INFO - Step 4257, rl-loss: 244.71533203125\r",
      "INFO - Step 4258, rl-loss: 39.30857467651367\r",
      "INFO - Step 4259, rl-loss: 539.1420288085938\r",
      "INFO - Step 4260, rl-loss: 589.1777954101562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4261, rl-loss: 7.279338836669922\r",
      "INFO - Step 4262, rl-loss: 768.204833984375\r",
      "INFO - Step 4263, rl-loss: 322.11724853515625\r",
      "INFO - Step 4264, rl-loss: 60.438804626464844\r",
      "INFO - Step 4265, rl-loss: 1.0580508708953857\r",
      "INFO - Step 4266, rl-loss: 176.83395385742188\r",
      "INFO - Step 4267, rl-loss: 1.1318342685699463\r",
      "INFO - Step 4268, rl-loss: 7.056741714477539\r",
      "INFO - Step 4269, rl-loss: 60.06703567504883\r",
      "INFO - Step 4270, rl-loss: 85.74348449707031\r",
      "INFO - Step 4271, rl-loss: 482.1408996582031\r",
      "INFO - Step 4272, rl-loss: 109.67481231689453\r",
      "INFO - Step 4273, rl-loss: 285.4645080566406\r",
      "INFO - Step 4274, rl-loss: 271.01397705078125\r",
      "INFO - Step 4275, rl-loss: 235.60430908203125\r",
      "INFO - Step 4276, rl-loss: 296.4742736816406\r",
      "INFO - Step 4277, rl-loss: 109.69898223876953\r",
      "INFO - Step 4278, rl-loss: 1.1581227779388428\r",
      "INFO - Step 4279, rl-loss: 130.83815002441406\r",
      "INFO - Step 4280, rl-loss: 313.35980224609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4281, rl-loss: 211.64736938476562\r",
      "INFO - Step 4282, rl-loss: 1.3046892881393433\r",
      "INFO - Step 4283, rl-loss: 276.0508728027344\r",
      "INFO - Step 4284, rl-loss: 85.32865905761719\r",
      "INFO - Step 4285, rl-loss: 585.8525390625\r",
      "INFO - Step 4286, rl-loss: 207.90069580078125\r",
      "INFO - Step 4287, rl-loss: 0.7424287796020508\r",
      "INFO - Step 4288, rl-loss: 809.0687866210938\r",
      "INFO - Step 4289, rl-loss: 0.7757301330566406\r",
      "INFO - Step 4290, rl-loss: 102.68211364746094\r",
      "INFO - Step 4291, rl-loss: 508.89617919921875\r",
      "INFO - Step 4292, rl-loss: 270.8304443359375\r",
      "INFO - Step 4293, rl-loss: 357.29180908203125\r",
      "INFO - Step 4294, rl-loss: 237.74209594726562\r",
      "INFO - Step 4295, rl-loss: 511.0367736816406\r",
      "INFO - Step 4296, rl-loss: 0.9366388320922852\r",
      "INFO - Step 4297, rl-loss: 347.920654296875\r",
      "INFO - Step 4298, rl-loss: 306.9287109375\r",
      "INFO - Step 4299, rl-loss: 77.92399597167969\r",
      "INFO - Step 4300, rl-loss: 481.9111022949219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4301, rl-loss: 621.4107666015625\r",
      "INFO - Step 4302, rl-loss: 151.00257873535156\r",
      "INFO - Step 4303, rl-loss: 196.28012084960938\r",
      "INFO - Step 4304, rl-loss: 149.1806640625\r",
      "INFO - Step 4305, rl-loss: 1.8145115375518799\r",
      "INFO - Step 4306, rl-loss: 109.3843994140625\r",
      "INFO - Step 4307, rl-loss: 75.03958129882812\r",
      "INFO - Step 4308, rl-loss: 480.3514099121094\r",
      "INFO - Step 4309, rl-loss: 10.083247184753418\r",
      "INFO - Step 4310, rl-loss: 1.3855671882629395\r",
      "INFO - Step 4311, rl-loss: 78.36597442626953\r",
      "INFO - Step 4312, rl-loss: 198.7477264404297\r",
      "INFO - Step 4313, rl-loss: 235.52865600585938\r",
      "INFO - Step 4314, rl-loss: 266.42108154296875\r",
      "INFO - Step 4315, rl-loss: 269.3058776855469\r",
      "INFO - Step 4316, rl-loss: 1.832179069519043\r",
      "INFO - Step 4317, rl-loss: 164.95675659179688\r",
      "INFO - Step 4318, rl-loss: 42.988990783691406\r",
      "INFO - Step 4319, rl-loss: 0.6558128595352173\r",
      "INFO - Step 4320, rl-loss: 0.7458611726760864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4321, rl-loss: 68.96940612792969\r",
      "INFO - Step 4322, rl-loss: 1.3640310764312744\r",
      "INFO - Step 4323, rl-loss: 1.153505563735962\r",
      "INFO - Step 4324, rl-loss: 127.44432067871094\r",
      "INFO - Step 4325, rl-loss: 106.78385162353516\r",
      "INFO - Step 4326, rl-loss: 306.76123046875\r",
      "INFO - Step 4327, rl-loss: 111.38024139404297\r",
      "INFO - Step 4328, rl-loss: 215.52699279785156\r",
      "INFO - Step 4329, rl-loss: 200.06202697753906\r",
      "INFO - Step 4330, rl-loss: 59.999210357666016\r",
      "INFO - Step 4331, rl-loss: 419.64501953125\r",
      "INFO - Step 4332, rl-loss: 76.23574829101562\r",
      "INFO - Step 4333, rl-loss: 981.639404296875\r",
      "INFO - Step 4334, rl-loss: 52.921146392822266\r",
      "INFO - Step 4335, rl-loss: 356.13787841796875\r",
      "INFO - Step 4336, rl-loss: 368.0771179199219\r",
      "INFO - Step 4337, rl-loss: 26.201828002929688\r",
      "INFO - Step 4338, rl-loss: 1.525206446647644\r",
      "INFO - Step 4339, rl-loss: 1.4551202058792114\r",
      "INFO - Step 4340, rl-loss: 755.95703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4341, rl-loss: 342.1005554199219\r",
      "INFO - Step 4342, rl-loss: 132.67002868652344\r",
      "INFO - Step 4343, rl-loss: 425.46002197265625\r",
      "INFO - Step 4344, rl-loss: 1.555052399635315\r",
      "INFO - Step 4345, rl-loss: 140.14682006835938\r",
      "INFO - Step 4346, rl-loss: 522.2665405273438\r",
      "INFO - Step 4347, rl-loss: 445.99237060546875\r",
      "INFO - Step 4348, rl-loss: 82.68372344970703\r",
      "INFO - Step 4349, rl-loss: 86.10831451416016\r",
      "INFO - Step 4350, rl-loss: 227.25637817382812\r",
      "INFO - Step 4351, rl-loss: 127.85012817382812\r",
      "INFO - Step 4352, rl-loss: 199.66012573242188\r",
      "INFO - Step 4353, rl-loss: 364.1734619140625\r",
      "INFO - Step 4354, rl-loss: 19.170270919799805\r",
      "INFO - Step 4355, rl-loss: 0.8530424237251282\r",
      "INFO - Step 4356, rl-loss: 154.0025177001953\r",
      "INFO - Step 4357, rl-loss: 37.46337127685547\r",
      "INFO - Step 4358, rl-loss: 327.03021240234375\r",
      "INFO - Step 4359, rl-loss: 84.57514190673828\r",
      "INFO - Step 4360, rl-loss: 104.59741973876953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4361, rl-loss: 1038.607666015625\r",
      "INFO - Step 4362, rl-loss: 1.5308384895324707\r",
      "INFO - Step 4363, rl-loss: 1.3111603260040283\r",
      "INFO - Step 4364, rl-loss: 211.20181274414062\r",
      "INFO - Step 4365, rl-loss: 302.157958984375\r",
      "INFO - Step 4366, rl-loss: 462.6175537109375\r",
      "INFO - Step 4367, rl-loss: 212.945068359375\r",
      "INFO - Step 4368, rl-loss: 78.71295928955078\r",
      "INFO - Step 4369, rl-loss: 195.69444274902344\r",
      "INFO - Step 4370, rl-loss: 204.79176330566406\r",
      "INFO - Step 4371, rl-loss: 0.8834787011146545\r",
      "INFO - Step 4372, rl-loss: 152.27745056152344\r",
      "INFO - Step 4373, rl-loss: 275.8863220214844\r",
      "INFO - Step 4374, rl-loss: 570.9681396484375\r",
      "INFO - Step 4375, rl-loss: 391.5048522949219\r",
      "INFO - Step 4376, rl-loss: 354.0139465332031\r",
      "INFO - Step 4377, rl-loss: 1.1691211462020874\r",
      "INFO - Step 4378, rl-loss: 253.16795349121094\r",
      "INFO - Step 4379, rl-loss: 348.8560791015625\r",
      "INFO - Step 4380, rl-loss: 392.88153076171875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4381, rl-loss: 1.3662012815475464\r",
      "INFO - Step 4382, rl-loss: 183.701904296875\r",
      "INFO - Step 4383, rl-loss: 163.8119354248047\r",
      "INFO - Step 4384, rl-loss: 241.1000518798828\r",
      "INFO - Step 4385, rl-loss: 357.9377136230469\r",
      "INFO - Step 4386, rl-loss: 340.72808837890625\r",
      "INFO - Step 4387, rl-loss: 743.0100708007812\r",
      "INFO - Step 4388, rl-loss: 0.7641003131866455\r",
      "INFO - Step 4389, rl-loss: 15.877876281738281\r",
      "INFO - Step 4390, rl-loss: 151.22792053222656\r",
      "INFO - Step 4391, rl-loss: 68.07290649414062\r",
      "INFO - Step 4392, rl-loss: 532.264404296875\r",
      "INFO - Step 4393, rl-loss: 887.0625610351562\r",
      "INFO - Step 4394, rl-loss: 292.5586853027344\r",
      "INFO - Step 4395, rl-loss: 234.44139099121094\r",
      "INFO - Step 4396, rl-loss: 231.08409118652344\r",
      "INFO - Step 4397, rl-loss: 143.33871459960938\r",
      "INFO - Step 4398, rl-loss: 443.60272216796875\r",
      "INFO - Step 4399, rl-loss: 0.8677482604980469\r",
      "INFO - Step 4400, rl-loss: 0.5344370603561401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4401, rl-loss: 507.2655944824219\r",
      "INFO - Step 4402, rl-loss: 565.325927734375\r",
      "INFO - Step 4403, rl-loss: 474.2506408691406\r",
      "INFO - Step 4404, rl-loss: 1.141316533088684\r",
      "INFO - Step 4405, rl-loss: 499.9408874511719\r",
      "INFO - Step 4406, rl-loss: 1.9570621252059937\r",
      "INFO - Step 4407, rl-loss: 96.38969421386719\r",
      "INFO - Step 4408, rl-loss: 400.2005615234375\r",
      "INFO - Step 4409, rl-loss: 140.91567993164062\r",
      "INFO - Step 4410, rl-loss: 110.44918060302734\r",
      "INFO - Step 4411, rl-loss: 31.78557586669922\r",
      "INFO - Step 4412, rl-loss: 120.04867553710938\r",
      "INFO - Step 4413, rl-loss: 101.71688842773438\r",
      "INFO - Step 4414, rl-loss: 43.025001525878906\r",
      "INFO - Step 4415, rl-loss: 626.70361328125\r",
      "INFO - Step 4416, rl-loss: 7.74289608001709\r",
      "INFO - Step 4417, rl-loss: 231.294921875\r",
      "INFO - Step 4418, rl-loss: 166.15943908691406\r",
      "INFO - Step 4419, rl-loss: 346.6739196777344\r",
      "INFO - Step 4420, rl-loss: 104.76190948486328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4421, rl-loss: 423.0854797363281\r",
      "INFO - Step 4422, rl-loss: 329.3987121582031\r",
      "INFO - Step 4423, rl-loss: 988.8582153320312\r",
      "INFO - Step 4424, rl-loss: 600.101806640625\r",
      "INFO - Step 4425, rl-loss: 76.41118621826172\r",
      "INFO - Step 4426, rl-loss: 193.72640991210938\r",
      "INFO - Step 4427, rl-loss: 17.79475212097168\r",
      "INFO - Step 4428, rl-loss: 376.92572021484375\r",
      "INFO - Step 4429, rl-loss: 265.44464111328125\r",
      "INFO - Step 4430, rl-loss: 209.18692016601562\r",
      "INFO - Step 4431, rl-loss: 39.045692443847656\r",
      "INFO - Step 4432, rl-loss: 574.4110107421875\r",
      "INFO - Step 4433, rl-loss: 1.0965088605880737\r",
      "INFO - Step 4434, rl-loss: 0.9934898018836975\r",
      "INFO - Step 4435, rl-loss: 86.45221710205078\r",
      "INFO - Step 4436, rl-loss: 110.79106140136719\r",
      "INFO - Step 4437, rl-loss: 207.897705078125\r",
      "INFO - Step 4438, rl-loss: 168.93370056152344\r",
      "INFO - Step 4439, rl-loss: 294.1426086425781\r",
      "INFO - Step 4440, rl-loss: 188.51876831054688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4441, rl-loss: 356.5006103515625\r",
      "INFO - Step 4442, rl-loss: 130.3843536376953\r",
      "INFO - Step 4443, rl-loss: 91.41648864746094\r",
      "INFO - Step 4444, rl-loss: 173.00953674316406\r",
      "INFO - Step 4445, rl-loss: 2.4432146549224854\r",
      "INFO - Step 4446, rl-loss: 7.468292236328125\r",
      "INFO - Step 4447, rl-loss: 169.06410217285156\r",
      "INFO - Step 4448, rl-loss: 1.2191617488861084\r",
      "INFO - Step 4449, rl-loss: 202.10203552246094\r",
      "INFO - Step 4450, rl-loss: 188.58389282226562\r",
      "INFO - Step 4451, rl-loss: 106.79904174804688\r",
      "INFO - Step 4452, rl-loss: 0.774707555770874\r",
      "INFO - Step 4453, rl-loss: 153.0084991455078\r",
      "INFO - Step 4454, rl-loss: 45.541847229003906\r",
      "INFO - Step 4455, rl-loss: 241.11184692382812\r",
      "INFO - Step 4456, rl-loss: 406.8794250488281\r",
      "INFO - Step 4457, rl-loss: 20.418960571289062\r",
      "INFO - Step 4458, rl-loss: 575.3704223632812\r",
      "INFO - Step 4459, rl-loss: 0.977557897567749\r",
      "INFO - Step 4460, rl-loss: 293.61090087890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4461, rl-loss: 142.98045349121094\r",
      "INFO - Step 4462, rl-loss: 355.91290283203125\r",
      "INFO - Step 4463, rl-loss: 1.053380012512207\r",
      "INFO - Step 4464, rl-loss: 424.5946960449219\r",
      "INFO - Step 4465, rl-loss: 146.75323486328125\r",
      "INFO - Step 4466, rl-loss: 352.6669921875\r",
      "INFO - Step 4467, rl-loss: 0.9474651217460632\r",
      "INFO - Step 4468, rl-loss: 0.742408037185669\r",
      "INFO - Step 4469, rl-loss: 0.9033075571060181\r",
      "INFO - Step 4470, rl-loss: 404.9664001464844\r",
      "INFO - Step 4471, rl-loss: 676.4603881835938\r",
      "INFO - Step 4472, rl-loss: 208.10414123535156\r",
      "INFO - Step 4473, rl-loss: 108.73849487304688\r",
      "INFO - Step 4474, rl-loss: 0.977451503276825\r",
      "INFO - Step 4475, rl-loss: 178.676025390625\r",
      "INFO - Step 4476, rl-loss: 1.3814094066619873\r",
      "INFO - Step 4477, rl-loss: 71.1580581665039\r",
      "INFO - Step 4478, rl-loss: 369.7146301269531\r",
      "INFO - Step 4479, rl-loss: 226.86367797851562\r",
      "INFO - Step 4480, rl-loss: 54.769893646240234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4481, rl-loss: 330.64544677734375\r",
      "INFO - Step 4482, rl-loss: 352.2071533203125\r",
      "INFO - Step 4483, rl-loss: 413.0482177734375\r",
      "INFO - Step 4484, rl-loss: 258.74127197265625\r",
      "INFO - Step 4485, rl-loss: 723.1589965820312\r",
      "INFO - Step 4486, rl-loss: 25.144241333007812\r",
      "INFO - Step 4487, rl-loss: 96.06864929199219\r",
      "INFO - Step 4488, rl-loss: 244.5010223388672\r",
      "INFO - Step 4489, rl-loss: 34.38587188720703\r",
      "INFO - Step 4490, rl-loss: 120.01449584960938\r",
      "INFO - Step 4491, rl-loss: 137.47744750976562\r",
      "INFO - Step 4492, rl-loss: 289.1819152832031\r",
      "INFO - Step 4493, rl-loss: 117.84101867675781\r",
      "INFO - Step 4494, rl-loss: 311.64227294921875\r",
      "INFO - Step 4495, rl-loss: 78.67430114746094\r",
      "INFO - Step 4496, rl-loss: 1.375823974609375\r",
      "INFO - Step 4497, rl-loss: 248.5994415283203\r",
      "INFO - Step 4498, rl-loss: 915.4783935546875\r",
      "INFO - Step 4499, rl-loss: 88.60027313232422\r",
      "INFO - Step 4500, rl-loss: 59.62607192993164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4501, rl-loss: 420.8397521972656\r",
      "INFO - Step 4502, rl-loss: 302.15692138671875\r",
      "INFO - Step 4503, rl-loss: 37.96265411376953\r",
      "INFO - Step 4504, rl-loss: 263.94482421875\r",
      "INFO - Step 4505, rl-loss: 160.33975219726562\r",
      "INFO - Step 4506, rl-loss: 110.17695617675781\r",
      "INFO - Step 4507, rl-loss: 91.13130950927734\r",
      "INFO - Step 4508, rl-loss: 539.12109375\r",
      "INFO - Step 4509, rl-loss: 152.47874450683594\r",
      "INFO - Step 4510, rl-loss: 23.988929748535156\r",
      "INFO - Step 4511, rl-loss: 499.5046691894531\r",
      "INFO - Step 4512, rl-loss: 586.3707275390625\r",
      "INFO - Step 4513, rl-loss: 107.94821166992188\r",
      "INFO - Step 4514, rl-loss: 66.62693786621094\r",
      "INFO - Step 4515, rl-loss: 144.7061767578125\r",
      "INFO - Step 4516, rl-loss: 1.2814626693725586\r",
      "INFO - Step 4517, rl-loss: 156.31369018554688\r",
      "INFO - Step 4518, rl-loss: 1.1007148027420044\r",
      "INFO - Step 4519, rl-loss: 45.193546295166016\r",
      "INFO - Step 4520, rl-loss: 32.32146072387695"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4521, rl-loss: 54.25211715698242\r",
      "INFO - Step 4522, rl-loss: 59.84368133544922\r",
      "INFO - Step 4523, rl-loss: 121.6810302734375\r",
      "INFO - Step 4524, rl-loss: 400.2939453125\r",
      "INFO - Step 4525, rl-loss: 106.46910858154297\r",
      "INFO - Step 4526, rl-loss: 102.70309448242188\r",
      "INFO - Step 4527, rl-loss: 1.0068786144256592\r",
      "INFO - Step 4528, rl-loss: 253.76687622070312\r",
      "INFO - Step 4529, rl-loss: 550.3413696289062\r",
      "INFO - Step 4530, rl-loss: 216.71498107910156\r",
      "INFO - Step 4531, rl-loss: 285.30328369140625\r",
      "INFO - Step 4532, rl-loss: 112.46607208251953\r",
      "INFO - Step 4533, rl-loss: 586.15771484375\r",
      "INFO - Step 4534, rl-loss: 177.4115447998047\r",
      "INFO - Step 4535, rl-loss: 223.37374877929688\r",
      "INFO - Step 4536, rl-loss: 235.29238891601562\r",
      "INFO - Step 4537, rl-loss: 50.041351318359375\r",
      "INFO - Step 4538, rl-loss: 46.77123260498047\r",
      "INFO - Step 4539, rl-loss: 0.9267359972000122\r",
      "INFO - Step 4540, rl-loss: 90.31652069091797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4541, rl-loss: 187.478271484375\r",
      "INFO - Step 4542, rl-loss: 317.33599853515625\r",
      "INFO - Step 4543, rl-loss: 160.12734985351562\r",
      "INFO - Step 4544, rl-loss: 480.9947204589844\r",
      "INFO - Step 4545, rl-loss: 368.2470397949219\r",
      "INFO - Step 4546, rl-loss: 186.74830627441406\r",
      "INFO - Step 4547, rl-loss: 0.9786474108695984\r",
      "INFO - Step 4548, rl-loss: 137.70616149902344\r",
      "INFO - Step 4549, rl-loss: 1.469429612159729\r",
      "INFO - Step 4550, rl-loss: 1.002629280090332\r",
      "INFO - Step 4551, rl-loss: 112.00135803222656\r",
      "INFO - Step 4552, rl-loss: 220.33653259277344\r",
      "INFO - Step 4553, rl-loss: 597.54443359375\r",
      "INFO - Step 4554, rl-loss: 236.90647888183594\r",
      "INFO - Step 4555, rl-loss: 297.54071044921875\r",
      "INFO - Step 4556, rl-loss: 169.5985870361328\r",
      "INFO - Step 4557, rl-loss: 113.9508056640625\r",
      "INFO - Step 4558, rl-loss: 175.50811767578125\r",
      "INFO - Step 4559, rl-loss: 1.288101077079773\r",
      "INFO - Step 4560, rl-loss: 443.4168395996094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4561, rl-loss: 0.6554235219955444\r",
      "INFO - Step 4562, rl-loss: 278.8437194824219\r",
      "INFO - Step 4563, rl-loss: 574.6396484375\r",
      "INFO - Step 4564, rl-loss: 73.08370208740234\r",
      "INFO - Step 4565, rl-loss: 69.89016723632812\r",
      "INFO - Step 4566, rl-loss: 24.716970443725586\r",
      "INFO - Step 4567, rl-loss: 198.35882568359375\r",
      "INFO - Step 4568, rl-loss: 1.0901316404342651\r",
      "INFO - Step 4569, rl-loss: 136.0980987548828\r",
      "INFO - Step 4570, rl-loss: 105.15177917480469\r",
      "INFO - Step 4571, rl-loss: 62.569881439208984\r",
      "INFO - Step 4572, rl-loss: 9.224380493164062\r",
      "INFO - Step 4573, rl-loss: 77.59688568115234\r",
      "INFO - Step 4574, rl-loss: 775.6161499023438\r",
      "INFO - Step 4575, rl-loss: 104.6248550415039\r",
      "INFO - Step 4576, rl-loss: 185.9700164794922\r",
      "INFO - Step 4577, rl-loss: 201.6751708984375\r",
      "INFO - Step 4578, rl-loss: 0.7710364460945129\r",
      "INFO - Step 4579, rl-loss: 98.93034362792969\r",
      "INFO - Step 4580, rl-loss: 143.46022033691406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4581, rl-loss: 49.890228271484375\r",
      "INFO - Step 4582, rl-loss: 409.6689758300781\r",
      "INFO - Step 4583, rl-loss: 0.6959347128868103\r",
      "INFO - Step 4584, rl-loss: 164.6033477783203\r",
      "INFO - Step 4585, rl-loss: 0.6857962012290955\r",
      "INFO - Step 4586, rl-loss: 85.34351348876953\r",
      "INFO - Step 4587, rl-loss: 70.2553939819336\r",
      "INFO - Step 4588, rl-loss: 132.4193878173828\r",
      "INFO - Step 4589, rl-loss: 97.40047454833984\r",
      "INFO - Step 4590, rl-loss: 117.15096282958984\r",
      "INFO - Step 4591, rl-loss: 351.8050537109375\r",
      "INFO - Step 4592, rl-loss: 383.6624755859375\r",
      "INFO - Step 4593, rl-loss: 163.28619384765625\r",
      "INFO - Step 4594, rl-loss: 387.15606689453125\r",
      "INFO - Step 4595, rl-loss: 103.15982818603516\r",
      "INFO - Step 4596, rl-loss: 239.05503845214844\r",
      "INFO - Step 4597, rl-loss: 295.9168701171875\r",
      "INFO - Step 4598, rl-loss: 133.69073486328125\r",
      "INFO - Step 4599, rl-loss: 79.7460708618164\r",
      "INFO - Step 4600, rl-loss: 107.51593017578125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4601, rl-loss: 42.36627197265625\r",
      "INFO - Step 4602, rl-loss: 0.8281338214874268\r",
      "INFO - Step 4603, rl-loss: 1.0629223585128784\r",
      "INFO - Step 4604, rl-loss: 106.83536529541016\r",
      "INFO - Step 4605, rl-loss: 66.13548278808594\r",
      "INFO - Step 4606, rl-loss: 130.13388061523438\r",
      "INFO - Step 4607, rl-loss: 0.7157831788063049\r",
      "INFO - Step 4608, rl-loss: 399.29547119140625\r",
      "INFO - Step 4609, rl-loss: 459.611083984375\r",
      "INFO - Step 4610, rl-loss: 399.0602722167969\r",
      "INFO - Step 4611, rl-loss: 109.72652435302734\r",
      "INFO - Step 4612, rl-loss: 0.8933777809143066\r",
      "INFO - Step 4613, rl-loss: 128.74209594726562\r",
      "INFO - Step 4614, rl-loss: 0.8419318199157715\r",
      "INFO - Step 4615, rl-loss: 429.8863525390625\r",
      "INFO - Step 4616, rl-loss: 0.9065091013908386\r",
      "INFO - Step 4617, rl-loss: 311.31475830078125\r",
      "INFO - Step 4618, rl-loss: 0.8774277567863464\r",
      "INFO - Step 4619, rl-loss: 1.512380838394165\r",
      "INFO - Step 4620, rl-loss: 143.04766845703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4621, rl-loss: 248.65432739257812\r",
      "INFO - Step 4622, rl-loss: 110.55109405517578\r",
      "INFO - Step 4623, rl-loss: 109.5177230834961\r",
      "INFO - Step 4624, rl-loss: 38.3474235534668\r",
      "INFO - Step 4625, rl-loss: 7.154475212097168\r",
      "INFO - Step 4626, rl-loss: 0.7059873342514038\r",
      "INFO - Step 4627, rl-loss: 876.6784057617188\r",
      "INFO - Step 4628, rl-loss: 263.7796936035156\r",
      "INFO - Step 4629, rl-loss: 646.469970703125\r",
      "INFO - Step 4630, rl-loss: 221.46377563476562\r",
      "INFO - Step 4631, rl-loss: 500.308349609375\r",
      "INFO - Step 4632, rl-loss: 83.16770935058594\r",
      "INFO - Step 4633, rl-loss: 70.9544448852539\r",
      "INFO - Step 4634, rl-loss: 554.436279296875\r",
      "INFO - Step 4635, rl-loss: 5.997140884399414\r",
      "INFO - Step 4636, rl-loss: 2.3138279914855957\r",
      "INFO - Step 4637, rl-loss: 225.33766174316406\r",
      "INFO - Step 4638, rl-loss: 142.36648559570312\r",
      "INFO - Step 4639, rl-loss: 90.15171813964844\r",
      "INFO - Step 4640, rl-loss: 7.77296257019043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4641, rl-loss: 0.852777361869812\r",
      "INFO - Step 4642, rl-loss: 0.6244847178459167\r",
      "INFO - Step 4643, rl-loss: 42.805503845214844\r",
      "INFO - Step 4644, rl-loss: 321.9922180175781\r",
      "INFO - Step 4645, rl-loss: 265.41790771484375\r",
      "INFO - Step 4646, rl-loss: 244.99530029296875\r",
      "INFO - Step 4647, rl-loss: 435.3768310546875\r",
      "INFO - Step 4648, rl-loss: 91.13778686523438\r",
      "INFO - Step 4649, rl-loss: 0.7949445247650146\r",
      "INFO - Step 4650, rl-loss: 1.0065940618515015\r",
      "INFO - Step 4651, rl-loss: 79.03244018554688\r",
      "INFO - Step 4652, rl-loss: 1.718799114227295\r",
      "INFO - Step 4653, rl-loss: 1.0729533433914185\r",
      "INFO - Step 4654, rl-loss: 38.508880615234375\r",
      "INFO - Step 4655, rl-loss: 601.2830200195312\r",
      "INFO - Step 4656, rl-loss: 0.7284123301506042\r",
      "INFO - Step 4657, rl-loss: 318.0129089355469\r",
      "INFO - Step 4658, rl-loss: 1.1239607334136963\r",
      "INFO - Step 4659, rl-loss: 299.5047912597656\r",
      "INFO - Step 4660, rl-loss: 273.41485595703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4661, rl-loss: 627.315185546875\r",
      "INFO - Step 4662, rl-loss: 94.58941650390625\r",
      "INFO - Step 4663, rl-loss: 87.61883544921875\r",
      "INFO - Step 4664, rl-loss: 1.2178794145584106\r",
      "INFO - Step 4665, rl-loss: 5.90117883682251\r",
      "INFO - Step 4666, rl-loss: 396.79730224609375\r",
      "INFO - Step 4667, rl-loss: 158.27969360351562\r",
      "INFO - Step 4668, rl-loss: 108.52777862548828\r",
      "INFO - Step 4669, rl-loss: 405.7890930175781\r",
      "INFO - Step 4670, rl-loss: 153.2261505126953\r",
      "INFO - Step 4671, rl-loss: 618.758056640625\r",
      "INFO - Step 4672, rl-loss: 1260.153076171875\r",
      "INFO - Step 4673, rl-loss: 53.246273040771484\r",
      "INFO - Step 4674, rl-loss: 66.33836364746094\r",
      "INFO - Step 4675, rl-loss: 588.2537841796875\r",
      "INFO - Step 4676, rl-loss: 709.7393798828125\r",
      "INFO - Step 4677, rl-loss: 537.5294189453125\r",
      "INFO - Step 4678, rl-loss: 1.2103064060211182\r",
      "INFO - Step 4679, rl-loss: 130.1741485595703\r",
      "INFO - Step 4680, rl-loss: 312.5977478027344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4681, rl-loss: 521.1736450195312\r",
      "INFO - Step 4682, rl-loss: 274.71392822265625\r",
      "INFO - Step 4683, rl-loss: 83.6768569946289\r",
      "INFO - Step 4684, rl-loss: 339.6962890625\r",
      "INFO - Step 4685, rl-loss: 558.6342163085938\r",
      "INFO - Step 4686, rl-loss: 549.2576293945312\r",
      "INFO - Step 4687, rl-loss: 247.10586547851562\r",
      "INFO - Step 4688, rl-loss: 177.98828125\r",
      "INFO - Step 4689, rl-loss: 178.0077667236328\r",
      "INFO - Step 4690, rl-loss: 127.14729309082031\r",
      "INFO - Step 4691, rl-loss: 562.0623168945312\r",
      "INFO - Step 4692, rl-loss: 1.0926706790924072\r",
      "INFO - Step 4693, rl-loss: 565.399169921875\r",
      "INFO - Step 4694, rl-loss: 63.000999450683594\r",
      "INFO - Step 4695, rl-loss: 315.06512451171875\r",
      "INFO - Step 4696, rl-loss: 1.2573184967041016\r",
      "INFO - Step 4697, rl-loss: 159.47413635253906\r",
      "INFO - Step 4698, rl-loss: 147.2455596923828\r",
      "INFO - Step 4699, rl-loss: 216.9505615234375\r",
      "INFO - Step 4700, rl-loss: 659.2186279296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4701, rl-loss: 39.744293212890625\r",
      "INFO - Step 4702, rl-loss: 108.6933822631836\r",
      "INFO - Step 4703, rl-loss: 179.38131713867188\r",
      "INFO - Step 4704, rl-loss: 734.3203125\r",
      "INFO - Step 4705, rl-loss: 364.9364929199219\r",
      "INFO - Step 4706, rl-loss: 68.50040435791016\r",
      "INFO - Step 4707, rl-loss: 0.6773152351379395\r",
      "INFO - Step 4708, rl-loss: 1.4370652437210083\r",
      "INFO - Step 4709, rl-loss: 66.09292602539062\r",
      "INFO - Step 4710, rl-loss: 230.180908203125\r",
      "INFO - Step 4711, rl-loss: 232.8209228515625\r",
      "INFO - Step 4712, rl-loss: 444.4450378417969\r",
      "INFO - Step 4713, rl-loss: 424.11419677734375\r",
      "INFO - Step 4714, rl-loss: 0.9541614055633545\r",
      "INFO - Step 4715, rl-loss: 105.59288787841797\r",
      "INFO - Step 4716, rl-loss: 717.74169921875\r",
      "INFO - Step 4717, rl-loss: 348.94171142578125\r",
      "INFO - Step 4718, rl-loss: 89.97955322265625\r",
      "INFO - Step 4719, rl-loss: 230.35543823242188\r",
      "INFO - Step 4720, rl-loss: 0.9118492603302002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4721, rl-loss: 469.54150390625\r",
      "INFO - Step 4722, rl-loss: 34.620479583740234\r",
      "INFO - Step 4723, rl-loss: 179.11062622070312\r",
      "INFO - Step 4724, rl-loss: 443.2261962890625\r",
      "INFO - Step 4725, rl-loss: 205.90054321289062\r",
      "INFO - Step 4726, rl-loss: 394.7354736328125\r",
      "INFO - Step 4727, rl-loss: 107.75775146484375\r",
      "INFO - Step 4728, rl-loss: 962.5076293945312\r",
      "INFO - Step 4729, rl-loss: 10.95905876159668\r",
      "INFO - Step 4730, rl-loss: 148.14073181152344\r",
      "INFO - Step 4731, rl-loss: 363.92779541015625\r",
      "INFO - Step 4732, rl-loss: 402.42108154296875\r",
      "INFO - Step 4733, rl-loss: 171.13211059570312\r",
      "INFO - Step 4734, rl-loss: 1.3528928756713867\r",
      "INFO - Step 4735, rl-loss: 23.990585327148438\r",
      "INFO - Step 4736, rl-loss: 210.2694854736328\r",
      "INFO - Step 4737, rl-loss: 263.5948486328125\r",
      "INFO - Step 4738, rl-loss: 1.0356544256210327\r",
      "INFO - Step 4739, rl-loss: 1.2984520196914673\r",
      "INFO - Step 4740, rl-loss: 183.9191131591797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4741, rl-loss: 429.63140869140625\r",
      "INFO - Step 4742, rl-loss: 209.46499633789062\r",
      "INFO - Step 4743, rl-loss: 263.4286193847656\r",
      "INFO - Step 4744, rl-loss: 162.7327423095703\r",
      "INFO - Step 4745, rl-loss: 52.353515625\r",
      "INFO - Step 4746, rl-loss: 72.75785827636719\r",
      "INFO - Step 4747, rl-loss: 416.9339904785156\r",
      "INFO - Step 4748, rl-loss: 22.508302688598633\r",
      "INFO - Step 4749, rl-loss: 184.08692932128906\r",
      "INFO - Step 4750, rl-loss: 4.775786399841309\r",
      "INFO - Step 4751, rl-loss: 0.7106313705444336\r",
      "INFO - Step 4752, rl-loss: 97.58338165283203\r",
      "INFO - Step 4753, rl-loss: 9.90506362915039\r",
      "INFO - Step 4754, rl-loss: 1.1058650016784668\r",
      "INFO - Step 4755, rl-loss: 406.4184265136719\r",
      "INFO - Step 4756, rl-loss: 134.35159301757812\r",
      "INFO - Step 4757, rl-loss: 528.2791137695312\r",
      "INFO - Step 4758, rl-loss: 58.66763687133789\r",
      "INFO - Step 4759, rl-loss: 70.22569274902344\r",
      "INFO - Step 4760, rl-loss: 149.1166534423828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4761, rl-loss: 30.387466430664062\r",
      "INFO - Step 4762, rl-loss: 0.8894636631011963\r",
      "INFO - Step 4763, rl-loss: 165.2808380126953\r",
      "INFO - Step 4764, rl-loss: 266.4835205078125\r",
      "INFO - Step 4765, rl-loss: 1.1433906555175781\r",
      "INFO - Step 4766, rl-loss: 0.5802940130233765\r",
      "INFO - Step 4767, rl-loss: 119.73700714111328\r",
      "INFO - Step 4768, rl-loss: 44.724918365478516\r",
      "INFO - Step 4769, rl-loss: 540.8202514648438\r",
      "INFO - Step 4770, rl-loss: 447.1930236816406\r",
      "INFO - Step 4771, rl-loss: 1.1288723945617676\r",
      "INFO - Step 4772, rl-loss: 207.64022827148438\r",
      "INFO - Step 4773, rl-loss: 845.6809692382812\r",
      "INFO - Step 4774, rl-loss: 205.2532958984375\r",
      "INFO - Step 4775, rl-loss: 1.3362414836883545\r",
      "INFO - Step 4776, rl-loss: 497.2679443359375\r",
      "INFO - Step 4777, rl-loss: 451.666015625\r",
      "INFO - Step 4778, rl-loss: 82.42280578613281\r",
      "INFO - Step 4779, rl-loss: 44.36807632446289\r",
      "INFO - Step 4780, rl-loss: 192.43405151367188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4781, rl-loss: 547.2615356445312\r",
      "INFO - Step 4782, rl-loss: 274.302001953125\r",
      "INFO - Step 4783, rl-loss: 91.50511169433594\r",
      "INFO - Step 4784, rl-loss: 655.84716796875\r",
      "INFO - Step 4785, rl-loss: 79.90289306640625\r",
      "INFO - Step 4786, rl-loss: 434.19580078125\r",
      "INFO - Step 4787, rl-loss: 87.89533233642578\r",
      "INFO - Step 4788, rl-loss: 427.777099609375\r",
      "INFO - Step 4789, rl-loss: 62.66719055175781\r",
      "INFO - Step 4790, rl-loss: 289.19781494140625\r",
      "INFO - Step 4791, rl-loss: 71.27836608886719\r",
      "INFO - Step 4792, rl-loss: 533.6040649414062\r",
      "INFO - Step 4793, rl-loss: 210.57872009277344\r",
      "INFO - Step 4794, rl-loss: 295.9472961425781\r",
      "INFO - Step 4795, rl-loss: 65.67676544189453\r",
      "INFO - Step 4796, rl-loss: 147.1855926513672\r",
      "INFO - Step 4797, rl-loss: 167.9936981201172\r",
      "INFO - Step 4798, rl-loss: 178.10592651367188\r",
      "INFO - Step 4799, rl-loss: 70.48857116699219\r",
      "INFO - Step 4800, rl-loss: 183.7169952392578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4801, rl-loss: 304.8844909667969\r",
      "INFO - Step 4802, rl-loss: 425.51513671875\r",
      "INFO - Step 4803, rl-loss: 65.94805908203125\r",
      "INFO - Step 4804, rl-loss: 7.93852424621582\r",
      "INFO - Step 4805, rl-loss: 271.7408752441406\r",
      "INFO - Step 4806, rl-loss: 286.2154846191406\r",
      "INFO - Step 4807, rl-loss: 258.2851867675781\r",
      "INFO - Step 4808, rl-loss: 134.08731079101562\r",
      "INFO - Step 4809, rl-loss: 1.0600337982177734\r",
      "INFO - Step 4810, rl-loss: 81.52423858642578\r",
      "INFO - Step 4811, rl-loss: 118.42835235595703\r",
      "INFO - Step 4812, rl-loss: 0.698846697807312\r",
      "INFO - Step 4813, rl-loss: 253.8413848876953\r",
      "INFO - Step 4814, rl-loss: 0.6922565698623657\r",
      "INFO - Step 4815, rl-loss: 72.15022277832031\r",
      "INFO - Step 4816, rl-loss: 91.59063720703125\r",
      "INFO - Step 4817, rl-loss: 207.13406372070312\r",
      "INFO - Step 4818, rl-loss: 177.4962158203125\r",
      "INFO - Step 4819, rl-loss: 123.00694274902344\r",
      "INFO - Step 4820, rl-loss: 0.722231924533844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4821, rl-loss: 1.24371337890625\r",
      "INFO - Step 4822, rl-loss: 1.0283360481262207\r",
      "INFO - Step 4823, rl-loss: 216.4248046875\r",
      "INFO - Step 4824, rl-loss: 770.7835083007812\r",
      "INFO - Step 4825, rl-loss: 961.048583984375\r",
      "INFO - Step 4826, rl-loss: 0.7112425565719604\r",
      "INFO - Step 4827, rl-loss: 216.52737426757812\r",
      "INFO - Step 4828, rl-loss: 0.9624539017677307\r",
      "INFO - Step 4829, rl-loss: 143.68582153320312\r",
      "INFO - Step 4830, rl-loss: 857.1409301757812\r",
      "INFO - Step 4831, rl-loss: 0.9616414308547974\r",
      "INFO - Step 4832, rl-loss: 391.1202392578125\r",
      "INFO - Step 4833, rl-loss: 35.61003494262695\r",
      "INFO - Step 4834, rl-loss: 75.20585632324219\r",
      "INFO - Step 4835, rl-loss: 400.31622314453125\r",
      "INFO - Step 4836, rl-loss: 458.6571044921875\r",
      "INFO - Step 4837, rl-loss: 211.98606872558594\r",
      "INFO - Step 4838, rl-loss: 67.55171966552734\r",
      "INFO - Step 4839, rl-loss: 1.136428713798523\r",
      "INFO - Step 4840, rl-loss: 0.531805157661438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4841, rl-loss: 0.5862459540367126\r",
      "INFO - Step 4842, rl-loss: 6.1817851066589355\r",
      "INFO - Step 4843, rl-loss: 67.25166320800781\r",
      "INFO - Step 4844, rl-loss: 1.2190302610397339\r",
      "INFO - Step 4845, rl-loss: 371.8619079589844\r",
      "INFO - Step 4846, rl-loss: 218.3050537109375\r",
      "INFO - Step 4847, rl-loss: 274.5421447753906\r",
      "INFO - Step 4848, rl-loss: 0.7054911851882935\r",
      "INFO - Step 4849, rl-loss: 365.96563720703125\r",
      "INFO - Step 4850, rl-loss: 7.838594913482666\r",
      "INFO - Step 4851, rl-loss: 348.535888671875\r",
      "INFO - Step 4852, rl-loss: 97.55594635009766\r",
      "INFO - Step 4853, rl-loss: 145.2742462158203\r",
      "INFO - Step 4854, rl-loss: 0.9676263332366943\r",
      "INFO - Step 4855, rl-loss: 23.42877197265625\r",
      "INFO - Step 4856, rl-loss: 448.9823913574219\r",
      "INFO - Step 4857, rl-loss: 114.2889404296875\r",
      "INFO - Step 4858, rl-loss: 227.9976806640625\r",
      "INFO - Step 4859, rl-loss: 0.5418748259544373\r",
      "INFO - Step 4860, rl-loss: 293.8182373046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4861, rl-loss: 213.4950408935547\r",
      "INFO - Step 4862, rl-loss: 65.72853088378906\r",
      "INFO - Step 4863, rl-loss: 282.7808837890625\r",
      "INFO - Step 4864, rl-loss: 181.76455688476562\r",
      "INFO - Step 4865, rl-loss: 799.4998779296875\r",
      "INFO - Step 4866, rl-loss: 342.598876953125\r",
      "INFO - Step 4867, rl-loss: 119.3464584350586\r",
      "INFO - Step 4868, rl-loss: 151.70266723632812\r",
      "INFO - Step 4869, rl-loss: 374.0638122558594\r",
      "INFO - Step 4870, rl-loss: 1.3465776443481445\r",
      "INFO - Step 4871, rl-loss: 839.689453125\r",
      "INFO - Step 4872, rl-loss: 357.8757019042969\r",
      "INFO - Step 4873, rl-loss: 190.51068115234375\r",
      "INFO - Step 4874, rl-loss: 0.8364589214324951\r",
      "INFO - Step 4875, rl-loss: 147.1118621826172\r",
      "INFO - Step 4876, rl-loss: 188.0130615234375\r",
      "INFO - Step 4877, rl-loss: 1.2602972984313965\r",
      "INFO - Step 4878, rl-loss: 113.38714599609375\r",
      "INFO - Step 4879, rl-loss: 84.57691192626953\r",
      "INFO - Step 4880, rl-loss: 1.731834053993225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4881, rl-loss: 181.43338012695312\r",
      "INFO - Step 4882, rl-loss: 681.059326171875\r",
      "INFO - Step 4883, rl-loss: 499.6360778808594\r",
      "INFO - Step 4884, rl-loss: 1.1888182163238525\r",
      "INFO - Step 4885, rl-loss: 66.37844848632812\r",
      "INFO - Step 4886, rl-loss: 354.3388977050781\r",
      "INFO - Step 4887, rl-loss: 152.88743591308594\r",
      "INFO - Step 4888, rl-loss: 594.581298828125\r",
      "INFO - Step 4889, rl-loss: 0.7666768431663513\r",
      "INFO - Step 4890, rl-loss: 315.5046691894531\r",
      "INFO - Step 4891, rl-loss: 164.53958129882812\r",
      "INFO - Step 4892, rl-loss: 69.70259857177734\r",
      "INFO - Step 4893, rl-loss: 5.856888294219971\r",
      "INFO - Step 4894, rl-loss: 1.1596968173980713\r",
      "INFO - Step 4895, rl-loss: 156.84249877929688\r",
      "INFO - Step 4896, rl-loss: 475.8396911621094\r",
      "INFO - Step 4897, rl-loss: 183.83067321777344\r",
      "INFO - Step 4898, rl-loss: 193.72216796875\r",
      "INFO - Step 4899, rl-loss: 253.4779052734375\r",
      "INFO - Step 4900, rl-loss: 432.81427001953125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4901, rl-loss: 19.866252899169922\r",
      "INFO - Step 4902, rl-loss: 2.1446645259857178\r",
      "INFO - Step 4903, rl-loss: 145.90225219726562\r",
      "INFO - Step 4904, rl-loss: 723.07080078125\r",
      "INFO - Step 4905, rl-loss: 289.7818298339844\r",
      "INFO - Step 4906, rl-loss: 432.285400390625\r",
      "INFO - Step 4907, rl-loss: 338.6049499511719\r",
      "INFO - Step 4908, rl-loss: 118.92835998535156\r",
      "INFO - Step 4909, rl-loss: 597.1800537109375\r",
      "INFO - Step 4910, rl-loss: 401.9914855957031\r",
      "INFO - Step 4911, rl-loss: 635.4381103515625\r",
      "INFO - Step 4912, rl-loss: 531.0867919921875\r",
      "INFO - Step 4913, rl-loss: 0.8232331275939941\r",
      "INFO - Step 4914, rl-loss: 369.9283752441406\r",
      "INFO - Step 4915, rl-loss: 0.8141167163848877\r",
      "INFO - Step 4916, rl-loss: 284.9278564453125\r",
      "INFO - Step 4917, rl-loss: 1.0622260570526123\r",
      "INFO - Step 4918, rl-loss: 175.32989501953125\r",
      "INFO - Step 4919, rl-loss: 177.97657775878906\r",
      "INFO - Step 4920, rl-loss: 178.37574768066406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4921, rl-loss: 187.189208984375\r",
      "INFO - Step 4922, rl-loss: 1.0957921743392944\r",
      "INFO - Step 4923, rl-loss: 68.39498138427734\r",
      "INFO - Step 4924, rl-loss: 573.6671752929688\r",
      "INFO - Step 4925, rl-loss: 116.76685333251953\r",
      "INFO - Step 4926, rl-loss: 1.0476222038269043\r",
      "INFO - Step 4927, rl-loss: 0.9459273219108582\r",
      "INFO - Step 4928, rl-loss: 290.15081787109375\r",
      "INFO - Step 4929, rl-loss: 158.6168212890625\r",
      "INFO - Step 4930, rl-loss: 0.7110927104949951\r",
      "INFO - Step 4931, rl-loss: 299.38055419921875\r",
      "INFO - Step 4932, rl-loss: 115.60997009277344\r",
      "INFO - Step 4933, rl-loss: 109.19696044921875\r",
      "INFO - Step 4934, rl-loss: 135.90377807617188\r",
      "INFO - Step 4935, rl-loss: 402.507568359375\r",
      "INFO - Step 4936, rl-loss: 533.3330688476562\r",
      "INFO - Step 4937, rl-loss: 394.69482421875\r",
      "INFO - Step 4938, rl-loss: 106.12073516845703\r",
      "INFO - Step 4939, rl-loss: 166.62547302246094\r",
      "INFO - Step 4940, rl-loss: 135.67974853515625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4941, rl-loss: 508.2178955078125\r",
      "INFO - Step 4942, rl-loss: 110.88484954833984\r",
      "INFO - Step 4943, rl-loss: 539.4722290039062\r",
      "INFO - Step 4944, rl-loss: 0.8219164609909058\r",
      "INFO - Step 4945, rl-loss: 0.6938095092773438\r",
      "INFO - Step 4946, rl-loss: 375.3188171386719\r",
      "INFO - Step 4947, rl-loss: 236.0601806640625\r",
      "INFO - Step 4948, rl-loss: 275.9130554199219\r",
      "INFO - Step 4949, rl-loss: 98.0457763671875\r",
      "INFO - Step 4950, rl-loss: 645.1453857421875\r",
      "INFO - Step 4951, rl-loss: 993.0484619140625\r",
      "INFO - Step 4952, rl-loss: 76.96133422851562\r",
      "INFO - Step 4953, rl-loss: 147.1984405517578\r",
      "INFO - Step 4954, rl-loss: 111.65879821777344\r",
      "INFO - Step 4955, rl-loss: 141.68649291992188\r",
      "INFO - Step 4956, rl-loss: 66.21805572509766\r",
      "INFO - Step 4957, rl-loss: 28.740266799926758\r",
      "INFO - Step 4958, rl-loss: 38.58925247192383\r",
      "INFO - Step 4959, rl-loss: 450.8921813964844\r",
      "INFO - Step 4960, rl-loss: 87.51510620117188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4961, rl-loss: 1.4367674589157104\r",
      "INFO - Step 4962, rl-loss: 19.767494201660156\r",
      "INFO - Step 4963, rl-loss: 85.09933471679688\r",
      "INFO - Step 4964, rl-loss: 1.594353199005127\r",
      "INFO - Step 4965, rl-loss: 372.23150634765625\r",
      "INFO - Step 4966, rl-loss: 236.6585235595703\r",
      "INFO - Step 4967, rl-loss: 66.11058044433594\r",
      "INFO - Step 4968, rl-loss: 1.3322808742523193\r",
      "INFO - Step 4969, rl-loss: 61.58857345581055\r",
      "INFO - Step 4970, rl-loss: 1.654380202293396\r",
      "INFO - Step 4971, rl-loss: 379.11444091796875\r",
      "INFO - Step 4972, rl-loss: 399.8091735839844\r",
      "INFO - Step 4973, rl-loss: 38.83684539794922\r",
      "INFO - Step 4974, rl-loss: 67.17726135253906\r",
      "INFO - Step 4975, rl-loss: 109.50751495361328\r",
      "INFO - Step 4976, rl-loss: 233.50946044921875\r",
      "INFO - Step 4977, rl-loss: 235.05743408203125\r",
      "INFO - Step 4978, rl-loss: 267.03857421875\r",
      "INFO - Step 4979, rl-loss: 1.3362922668457031\r",
      "INFO - Step 4980, rl-loss: 229.7625274658203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 4981, rl-loss: 157.59176635742188\r",
      "INFO - Step 4982, rl-loss: 7.586991310119629\r",
      "INFO - Step 4983, rl-loss: 1.1928832530975342\r",
      "INFO - Step 4984, rl-loss: 478.8336181640625\r",
      "INFO - Step 4985, rl-loss: 88.42683410644531\r",
      "INFO - Step 4986, rl-loss: 75.86293029785156\r",
      "INFO - Step 4987, rl-loss: 29.20281982421875\r",
      "INFO - Step 4988, rl-loss: 622.0797729492188\r",
      "INFO - Step 4989, rl-loss: 96.95518493652344\r",
      "INFO - Step 4990, rl-loss: 61.49003219604492\r",
      "INFO - Step 4991, rl-loss: 1.0396361351013184\r",
      "INFO - Step 4992, rl-loss: 131.5220489501953\r",
      "INFO - Step 4993, rl-loss: 135.13296508789062\r",
      "INFO - Step 4994, rl-loss: 311.2408752441406\r",
      "INFO - Step 4995, rl-loss: 63.14564895629883\r",
      "INFO - Step 4996, rl-loss: 447.3863220214844\r",
      "INFO - Step 4997, rl-loss: 419.3324890136719\r",
      "INFO - Step 4998, rl-loss: 45.485443115234375\r",
      "INFO - Step 4999, rl-loss: 397.63604736328125\r",
      "INFO - Step 5000, rl-loss: 109.27088928222656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 5020, rl-loss: 284.74182128906257\n",
      "----------------------------------------\n",
      "  timestep     |  543321\n",
      "  reward       |  59.04\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 5040, rl-loss: 489.68649291992192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 5060, rl-loss: 397.27490234375625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 5080, rl-loss: 88.989746093751875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5081, rl-loss: 0.9188485145568848\r",
      "INFO - Step 5082, rl-loss: 1.1715993881225586\r",
      "INFO - Step 5083, rl-loss: 81.75489807128906\r",
      "INFO - Step 5084, rl-loss: 148.04470825195312\r",
      "INFO - Step 5085, rl-loss: 34.02579879760742\r",
      "INFO - Step 5086, rl-loss: 139.28274536132812\r",
      "INFO - Step 5087, rl-loss: 173.03399658203125\r",
      "INFO - Step 5088, rl-loss: 379.8481140136719\r",
      "INFO - Step 5089, rl-loss: 281.2710876464844\r",
      "INFO - Step 5090, rl-loss: 366.8592834472656\r",
      "INFO - Step 5091, rl-loss: 74.46112060546875\r",
      "INFO - Step 5092, rl-loss: 109.4997329711914\r",
      "INFO - Step 5093, rl-loss: 7.078754901885986\r",
      "INFO - Step 5094, rl-loss: 476.6766052246094\r",
      "INFO - Step 5095, rl-loss: 604.2910766601562\r",
      "INFO - Step 5096, rl-loss: 0.6913893222808838\r",
      "INFO - Step 5097, rl-loss: 0.6308764219284058\r",
      "INFO - Step 5098, rl-loss: 1.056431770324707\r",
      "INFO - Step 5099, rl-loss: 0.9954919815063477\r",
      "INFO - Step 5100, rl-loss: 610.234619140625\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5101, rl-loss: 160.74490356445312\r",
      "INFO - Step 5102, rl-loss: 565.8172607421875\r",
      "INFO - Step 5103, rl-loss: 100.56355285644531\r",
      "INFO - Step 5104, rl-loss: 1.6700851917266846\r",
      "INFO - Step 5105, rl-loss: 1.8259053230285645\r",
      "INFO - Step 5106, rl-loss: 237.01663208007812\r",
      "INFO - Step 5107, rl-loss: 310.3375549316406\r",
      "INFO - Step 5108, rl-loss: 72.864990234375\r",
      "INFO - Step 5109, rl-loss: 1.5467000007629395\r",
      "INFO - Step 5110, rl-loss: 193.7350311279297\r",
      "INFO - Step 5111, rl-loss: 196.75299072265625\r",
      "INFO - Step 5112, rl-loss: 37.798152923583984\r",
      "INFO - Step 5113, rl-loss: 2.346435785293579\r",
      "INFO - Step 5114, rl-loss: 6.653196334838867\r",
      "INFO - Step 5115, rl-loss: 165.1796112060547\r",
      "INFO - Step 5116, rl-loss: 50.56712341308594\r",
      "INFO - Step 5117, rl-loss: 220.3458709716797\r",
      "INFO - Step 5118, rl-loss: 146.9390869140625\r",
      "INFO - Step 5119, rl-loss: 2.1528849601745605\r",
      "INFO - Step 5120, rl-loss: 328.0931701660156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 5140, rl-loss: 240.86503601074227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5141, rl-loss: 134.62860107421875\r",
      "INFO - Step 5142, rl-loss: 43.37447738647461\r",
      "INFO - Step 5143, rl-loss: 78.02128601074219\r",
      "INFO - Step 5144, rl-loss: 8.399567604064941\r",
      "INFO - Step 5145, rl-loss: 103.50714874267578\r",
      "INFO - Step 5146, rl-loss: 19.626094818115234\r",
      "INFO - Step 5147, rl-loss: 123.67919921875\r",
      "INFO - Step 5148, rl-loss: 100.789794921875\r",
      "INFO - Step 5149, rl-loss: 265.1746520996094\r",
      "INFO - Step 5150, rl-loss: 255.4476776123047\r",
      "INFO - Step 5151, rl-loss: 109.90413665771484\r",
      "INFO - Step 5152, rl-loss: 88.7000732421875\r",
      "INFO - Step 5153, rl-loss: 375.94720458984375\r",
      "INFO - Step 5154, rl-loss: 468.6317138671875\r",
      "INFO - Step 5155, rl-loss: 1.3570002317428589\r",
      "INFO - Step 5156, rl-loss: 233.81973266601562\r",
      "INFO - Step 5157, rl-loss: 130.54415893554688\r",
      "INFO - Step 5158, rl-loss: 178.08407592773438\r",
      "INFO - Step 5159, rl-loss: 95.19181823730469\r",
      "INFO - Step 5160, rl-loss: 233.17552185058594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5161, rl-loss: 1.6918106079101562\r",
      "INFO - Step 5162, rl-loss: 268.66650390625\r",
      "INFO - Step 5163, rl-loss: 36.61359786987305\r",
      "INFO - Step 5164, rl-loss: 6.991822242736816\r",
      "INFO - Step 5165, rl-loss: 256.5406188964844\r",
      "INFO - Step 5166, rl-loss: 49.956321716308594\r",
      "INFO - Step 5167, rl-loss: 234.09640502929688\r",
      "INFO - Step 5168, rl-loss: 593.555908203125\r",
      "INFO - Step 5169, rl-loss: 294.8272705078125\r",
      "INFO - Step 5170, rl-loss: 910.4842529296875\r",
      "INFO - Step 5171, rl-loss: 207.45236206054688\r",
      "INFO - Step 5172, rl-loss: 247.32681274414062\r",
      "INFO - Step 5173, rl-loss: 2.070485830307007\r",
      "INFO - Step 5174, rl-loss: 938.48779296875\r",
      "INFO - Step 5175, rl-loss: 52.51668930053711\r",
      "INFO - Step 5176, rl-loss: 83.54470825195312\r",
      "INFO - Step 5177, rl-loss: 129.2493438720703\r",
      "INFO - Step 5178, rl-loss: 475.3671875\r",
      "INFO - Step 5179, rl-loss: 1.2906913757324219\r",
      "INFO - Step 5180, rl-loss: 1.221632480621338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5181, rl-loss: 749.7913818359375\r",
      "INFO - Step 5182, rl-loss: 166.82594299316406\r",
      "INFO - Step 5183, rl-loss: 45.73122787475586\r",
      "INFO - Step 5184, rl-loss: 349.09759521484375\r",
      "INFO - Step 5185, rl-loss: 97.49667358398438\r",
      "INFO - Step 5186, rl-loss: 1.117919683456421\r",
      "INFO - Step 5187, rl-loss: 220.18980407714844\r",
      "INFO - Step 5188, rl-loss: 502.33709716796875\r",
      "INFO - Step 5189, rl-loss: 18.650894165039062\r",
      "INFO - Step 5190, rl-loss: 61.97062683105469\r",
      "INFO - Step 5191, rl-loss: 1.3713834285736084\r",
      "INFO - Step 5192, rl-loss: 79.47004699707031\r",
      "INFO - Step 5193, rl-loss: 103.90839385986328\r",
      "INFO - Step 5194, rl-loss: 181.08106994628906\r",
      "INFO - Step 5195, rl-loss: 73.90962219238281\r",
      "INFO - Step 5196, rl-loss: 756.462890625\r",
      "INFO - Step 5197, rl-loss: 202.89256286621094\r",
      "INFO - Step 5198, rl-loss: 148.49562072753906\r",
      "INFO - Step 5199, rl-loss: 2.1256442070007324\r",
      "INFO - Step 5200, rl-loss: 180.90972900390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5201, rl-loss: 212.42697143554688\r",
      "INFO - Step 5202, rl-loss: 676.7294311523438\r",
      "INFO - Step 5203, rl-loss: 298.28375244140625\r",
      "INFO - Step 5204, rl-loss: 1.3110582828521729\r",
      "INFO - Step 5205, rl-loss: 29.944087982177734\r",
      "INFO - Step 5206, rl-loss: 96.99664306640625\r",
      "INFO - Step 5207, rl-loss: 35.987186431884766\r",
      "INFO - Step 5208, rl-loss: 1.434696912765503\r",
      "INFO - Step 5209, rl-loss: 420.76837158203125\r",
      "INFO - Step 5210, rl-loss: 35.24851989746094\r",
      "INFO - Step 5211, rl-loss: 71.75418090820312\r",
      "INFO - Step 5212, rl-loss: 131.2689208984375\r",
      "INFO - Step 5213, rl-loss: 178.06112670898438\r",
      "INFO - Step 5214, rl-loss: 292.7281799316406\r",
      "INFO - Step 5215, rl-loss: 95.0545654296875\r",
      "INFO - Step 5216, rl-loss: 54.20560073852539\r",
      "INFO - Step 5217, rl-loss: 512.3726806640625\r",
      "INFO - Step 5218, rl-loss: 133.9146270751953\r",
      "INFO - Step 5219, rl-loss: 0.93171226978302\r",
      "INFO - Step 5220, rl-loss: 553.0925903320312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5221, rl-loss: 108.46595764160156\r",
      "INFO - Step 5222, rl-loss: 110.4946060180664\r",
      "INFO - Step 5223, rl-loss: 246.7406005859375\r",
      "INFO - Step 5224, rl-loss: 3.7288577556610107\r",
      "INFO - Step 5225, rl-loss: 468.19952392578125\r",
      "INFO - Step 5226, rl-loss: 75.6258544921875\r",
      "INFO - Step 5227, rl-loss: 246.37045288085938\r",
      "INFO - Step 5228, rl-loss: 241.62100219726562\r",
      "INFO - Step 5229, rl-loss: 69.12184143066406\r",
      "INFO - Step 5230, rl-loss: 35.733375549316406\r",
      "INFO - Step 5231, rl-loss: 1.3227269649505615\r",
      "INFO - Step 5232, rl-loss: 208.5032958984375\r",
      "INFO - Step 5233, rl-loss: 1.4785096645355225\r",
      "INFO - Step 5234, rl-loss: 346.3550720214844\r",
      "INFO - Step 5235, rl-loss: 298.6488952636719\r",
      "INFO - Step 5236, rl-loss: 125.5219497680664\r",
      "INFO - Step 5237, rl-loss: 84.55764770507812\r",
      "INFO - Step 5238, rl-loss: 1.4865620136260986\r",
      "INFO - Step 5239, rl-loss: 4.459868907928467\r",
      "INFO - Step 5240, rl-loss: 25.416868209838867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5241, rl-loss: 59.48603439331055\r",
      "INFO - Step 5242, rl-loss: 69.34695434570312\r",
      "INFO - Step 5243, rl-loss: 1.0412707328796387\r",
      "INFO - Step 5244, rl-loss: 71.10037231445312\r",
      "INFO - Step 5245, rl-loss: 408.7441101074219\r",
      "INFO - Step 5246, rl-loss: 35.82734680175781\r",
      "INFO - Step 5247, rl-loss: 90.4991455078125\r",
      "INFO - Step 5248, rl-loss: 134.36614990234375\r",
      "INFO - Step 5249, rl-loss: 52.63215255737305\r",
      "INFO - Step 5250, rl-loss: 1.4762184619903564\r",
      "INFO - Step 5251, rl-loss: 209.09349060058594\r",
      "INFO - Step 5252, rl-loss: 691.8834838867188\r",
      "INFO - Step 5253, rl-loss: 1.0980017185211182\r",
      "INFO - Step 5254, rl-loss: 487.992431640625\r",
      "INFO - Step 5255, rl-loss: 1.6737143993377686\r",
      "INFO - Step 5256, rl-loss: 162.60304260253906\r",
      "INFO - Step 5257, rl-loss: 383.7481689453125\r",
      "INFO - Step 5258, rl-loss: 124.12322998046875\r",
      "INFO - Step 5259, rl-loss: 229.1137237548828\r",
      "INFO - Step 5260, rl-loss: 654.5170288085938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5261, rl-loss: 53.515708923339844\r",
      "INFO - Step 5262, rl-loss: 1.2412490844726562\r",
      "INFO - Step 5263, rl-loss: 51.011009216308594\r",
      "INFO - Step 5264, rl-loss: 1.2922824621200562\r",
      "INFO - Step 5265, rl-loss: 700.2305297851562\r",
      "INFO - Step 5266, rl-loss: 393.1041259765625\r",
      "INFO - Step 5267, rl-loss: 215.1520538330078\r",
      "INFO - Step 5268, rl-loss: 1.3019577264785767\r",
      "INFO - Step 5269, rl-loss: 353.96337890625\r",
      "INFO - Step 5270, rl-loss: 452.34576416015625\r",
      "INFO - Step 5271, rl-loss: 581.77001953125\r",
      "INFO - Step 5272, rl-loss: 353.34576416015625\r",
      "INFO - Step 5273, rl-loss: 1.2445502281188965\r",
      "INFO - Step 5274, rl-loss: 1.6046295166015625\r",
      "INFO - Step 5275, rl-loss: 246.84205627441406\r",
      "INFO - Step 5276, rl-loss: 54.30327606201172\r",
      "INFO - Step 5277, rl-loss: 174.9444580078125\r",
      "INFO - Step 5278, rl-loss: 134.84901428222656\r",
      "INFO - Step 5279, rl-loss: 161.08840942382812\r",
      "INFO - Step 5280, rl-loss: 431.0002136230469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5281, rl-loss: 137.67166137695312\r",
      "INFO - Step 5282, rl-loss: 0.7046153545379639\r",
      "INFO - Step 5283, rl-loss: 1.3745806217193604\r",
      "INFO - Step 5284, rl-loss: 5.314886569976807\r",
      "INFO - Step 5285, rl-loss: 783.5369262695312\r",
      "INFO - Step 5286, rl-loss: 107.7325210571289\r",
      "INFO - Step 5287, rl-loss: 60.758941650390625\r",
      "INFO - Step 5288, rl-loss: 174.1997528076172\r",
      "INFO - Step 5289, rl-loss: 149.4400634765625\r",
      "INFO - Step 5290, rl-loss: 208.7940673828125\r",
      "INFO - Step 5291, rl-loss: 118.9394302368164\r",
      "INFO - Step 5292, rl-loss: 244.9774169921875\r",
      "INFO - Step 5293, rl-loss: 1.3624258041381836\r",
      "INFO - Step 5294, rl-loss: 6.713340759277344\r",
      "INFO - Step 5295, rl-loss: 62.84246826171875\r",
      "INFO - Step 5296, rl-loss: 1.2716079950332642\r",
      "INFO - Step 5297, rl-loss: 107.3961181640625\r",
      "INFO - Step 5298, rl-loss: 899.3467407226562\r",
      "INFO - Step 5299, rl-loss: 211.5036163330078\r",
      "INFO - Step 5300, rl-loss: 118.30645751953125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5301, rl-loss: 340.49810791015625\r",
      "INFO - Step 5302, rl-loss: 404.4256591796875\r",
      "INFO - Step 5303, rl-loss: 1.1888744831085205\r",
      "INFO - Step 5304, rl-loss: 572.1871337890625\r",
      "INFO - Step 5305, rl-loss: 146.6220703125\r",
      "INFO - Step 5306, rl-loss: 347.61248779296875\r",
      "INFO - Step 5307, rl-loss: 7.486880302429199\r",
      "INFO - Step 5308, rl-loss: 143.1527099609375\r",
      "INFO - Step 5309, rl-loss: 168.42990112304688\r",
      "INFO - Step 5310, rl-loss: 379.8207702636719\r",
      "INFO - Step 5311, rl-loss: 59.899879455566406\r",
      "INFO - Step 5312, rl-loss: 60.41530227661133\r",
      "INFO - Step 5313, rl-loss: 124.8172378540039\r",
      "INFO - Step 5314, rl-loss: 72.78622436523438\r",
      "INFO - Step 5315, rl-loss: 0.8464373350143433\r",
      "INFO - Step 5316, rl-loss: 44.709922790527344\r",
      "INFO - Step 5317, rl-loss: 720.1807861328125\r",
      "INFO - Step 5318, rl-loss: 371.38702392578125\r",
      "INFO - Step 5319, rl-loss: 754.8404541015625\r",
      "INFO - Step 5320, rl-loss: 408.6451110839844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5321, rl-loss: 268.2322692871094\r",
      "INFO - Step 5322, rl-loss: 1.0414245128631592\r",
      "INFO - Step 5323, rl-loss: 1.0902338027954102\r",
      "INFO - Step 5324, rl-loss: 90.23058319091797\r",
      "INFO - Step 5325, rl-loss: 167.33761596679688\r",
      "INFO - Step 5326, rl-loss: 310.8923034667969\r",
      "INFO - Step 5327, rl-loss: 234.20912170410156\r",
      "INFO - Step 5328, rl-loss: 44.87843704223633\r",
      "INFO - Step 5329, rl-loss: 214.38031005859375\r",
      "INFO - Step 5330, rl-loss: 388.5102233886719\r",
      "INFO - Step 5331, rl-loss: 195.46014404296875\r",
      "INFO - Step 5332, rl-loss: 223.56101989746094\r",
      "INFO - Step 5333, rl-loss: 431.4994812011719\r",
      "INFO - Step 5334, rl-loss: 326.6924133300781\r",
      "INFO - Step 5335, rl-loss: 284.1340026855469\r",
      "INFO - Step 5336, rl-loss: 636.6138305664062\r",
      "INFO - Step 5337, rl-loss: 19.125619888305664\r",
      "INFO - Step 5338, rl-loss: 37.767879486083984\r",
      "INFO - Step 5339, rl-loss: 61.021575927734375\r",
      "INFO - Step 5340, rl-loss: 361.34747314453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5341, rl-loss: 167.07322692871094\r",
      "INFO - Step 5342, rl-loss: 15.910271644592285\r",
      "INFO - Step 5343, rl-loss: 145.06578063964844\r",
      "INFO - Step 5344, rl-loss: 417.7249755859375\r",
      "INFO - Step 5345, rl-loss: 1.2647960186004639\r",
      "INFO - Step 5346, rl-loss: 182.58665466308594\r",
      "INFO - Step 5347, rl-loss: 96.74214935302734\r",
      "INFO - Step 5348, rl-loss: 215.73255920410156\r",
      "INFO - Step 5349, rl-loss: 60.55561065673828\r",
      "INFO - Step 5350, rl-loss: 133.73072814941406\r",
      "INFO - Step 5351, rl-loss: 137.00523376464844\r",
      "INFO - Step 5352, rl-loss: 481.0715637207031\r",
      "INFO - Step 5353, rl-loss: 1.095971703529358\r",
      "INFO - Step 5354, rl-loss: 450.6833190917969\r",
      "INFO - Step 5355, rl-loss: 194.033935546875\r",
      "INFO - Step 5356, rl-loss: 74.93399810791016\r",
      "INFO - Step 5357, rl-loss: 525.6719970703125\r",
      "INFO - Step 5358, rl-loss: 0.9330456852912903\r",
      "INFO - Step 5359, rl-loss: 215.49502563476562\r",
      "INFO - Step 5360, rl-loss: 340.731689453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5361, rl-loss: 330.82708740234375\r",
      "INFO - Step 5362, rl-loss: 220.99191284179688\r",
      "INFO - Step 5363, rl-loss: 705.6793823242188\r",
      "INFO - Step 5364, rl-loss: 1.1519982814788818\r",
      "INFO - Step 5365, rl-loss: 343.1837158203125\r",
      "INFO - Step 5366, rl-loss: 106.39468383789062\r",
      "INFO - Step 5367, rl-loss: 244.17332458496094\r",
      "INFO - Step 5368, rl-loss: 64.8727035522461\r",
      "INFO - Step 5369, rl-loss: 222.8651580810547\r",
      "INFO - Step 5370, rl-loss: 510.0490417480469\r",
      "INFO - Step 5371, rl-loss: 249.77886962890625\r",
      "INFO - Step 5372, rl-loss: 108.32402801513672\r",
      "INFO - Step 5373, rl-loss: 88.32573699951172\r",
      "INFO - Step 5374, rl-loss: 484.1247253417969\r",
      "INFO - Step 5375, rl-loss: 166.921142578125\r",
      "INFO - Step 5376, rl-loss: 451.9705810546875\r",
      "INFO - Step 5377, rl-loss: 345.3568420410156\r",
      "INFO - Step 5378, rl-loss: 166.78912353515625\r",
      "INFO - Step 5379, rl-loss: 184.88954162597656\r",
      "INFO - Step 5380, rl-loss: 398.2845458984375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5381, rl-loss: 0.6618002653121948\r",
      "INFO - Step 5382, rl-loss: 482.81646728515625\r",
      "INFO - Step 5383, rl-loss: 304.6396179199219\r",
      "INFO - Step 5384, rl-loss: 17.67403221130371\r",
      "INFO - Step 5385, rl-loss: 131.44615173339844\r",
      "INFO - Step 5386, rl-loss: 150.9621124267578\r",
      "INFO - Step 5387, rl-loss: 75.53878784179688\r",
      "INFO - Step 5388, rl-loss: 193.73240661621094\r",
      "INFO - Step 5389, rl-loss: 131.43736267089844\r",
      "INFO - Step 5390, rl-loss: 113.97382354736328\r",
      "INFO - Step 5391, rl-loss: 0.7782964706420898\r",
      "INFO - Step 5392, rl-loss: 1.3368431329727173\r",
      "INFO - Step 5393, rl-loss: 155.12155151367188\r",
      "INFO - Step 5394, rl-loss: 126.74922943115234\r",
      "INFO - Step 5395, rl-loss: 195.4304962158203\r",
      "INFO - Step 5396, rl-loss: 1.1452653408050537\r",
      "INFO - Step 5397, rl-loss: 12.653093338012695\r",
      "INFO - Step 5398, rl-loss: 73.02376556396484\r",
      "INFO - Step 5399, rl-loss: 558.117431640625\r",
      "INFO - Step 5400, rl-loss: 272.5555419921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5401, rl-loss: 273.43408203125\r",
      "INFO - Step 5402, rl-loss: 278.8583984375\r",
      "INFO - Step 5403, rl-loss: 0.9602997899055481\r",
      "INFO - Step 5404, rl-loss: 129.17874145507812\r",
      "INFO - Step 5405, rl-loss: 1.6957095861434937\r",
      "INFO - Step 5406, rl-loss: 1.2560241222381592\r",
      "INFO - Step 5407, rl-loss: 483.2117919921875\r",
      "INFO - Step 5408, rl-loss: 69.0198974609375\r",
      "INFO - Step 5409, rl-loss: 1.4633352756500244\r",
      "INFO - Step 5410, rl-loss: 246.19776916503906\r",
      "INFO - Step 5411, rl-loss: 426.4498596191406\r",
      "INFO - Step 5412, rl-loss: 98.425537109375\r",
      "INFO - Step 5413, rl-loss: 1.803943395614624\r",
      "INFO - Step 5414, rl-loss: 0.6497164964675903\r",
      "INFO - Step 5415, rl-loss: 1048.84619140625\r",
      "INFO - Step 5416, rl-loss: 631.33349609375\r",
      "INFO - Step 5417, rl-loss: 1.4737188816070557\r",
      "INFO - Step 5418, rl-loss: 367.5755310058594\r",
      "INFO - Step 5419, rl-loss: 113.12322998046875\r",
      "INFO - Step 5420, rl-loss: 167.89471435546875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5421, rl-loss: 1.0323669910430908\r",
      "INFO - Step 5422, rl-loss: 1.9217042922973633\r",
      "INFO - Step 5423, rl-loss: 129.65902709960938\r",
      "INFO - Step 5424, rl-loss: 272.8521728515625\r",
      "INFO - Step 5425, rl-loss: 534.477294921875\r",
      "INFO - Step 5426, rl-loss: 156.7604522705078\r",
      "INFO - Step 5427, rl-loss: 268.81256103515625\r",
      "INFO - Step 5428, rl-loss: 68.77605438232422\r",
      "INFO - Step 5429, rl-loss: 125.0234375\r",
      "INFO - Step 5430, rl-loss: 719.936767578125\r",
      "INFO - Step 5431, rl-loss: 232.81390380859375\r",
      "INFO - Step 5432, rl-loss: 335.00067138671875\r",
      "INFO - Step 5433, rl-loss: 288.5997314453125\r",
      "INFO - Step 5434, rl-loss: 107.88179779052734\r",
      "INFO - Step 5435, rl-loss: 196.436767578125\r",
      "INFO - Step 5436, rl-loss: 251.707275390625\r",
      "INFO - Step 5437, rl-loss: 1.0865118503570557\r",
      "INFO - Step 5438, rl-loss: 339.1722412109375\r",
      "INFO - Step 5439, rl-loss: 4.032871246337891\r",
      "INFO - Step 5440, rl-loss: 324.4559020996094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5441, rl-loss: 290.87298583984375\r",
      "INFO - Step 5442, rl-loss: 702.8574829101562\r",
      "INFO - Step 5443, rl-loss: 128.68687438964844\r",
      "INFO - Step 5444, rl-loss: 180.5797576904297\r",
      "INFO - Step 5445, rl-loss: 1.7479162216186523\r",
      "INFO - Step 5446, rl-loss: 0.8135339617729187\r",
      "INFO - Step 5447, rl-loss: 106.18538665771484\r",
      "INFO - Step 5448, rl-loss: 135.81307983398438\r",
      "INFO - Step 5449, rl-loss: 475.68804931640625\r",
      "INFO - Step 5450, rl-loss: 207.64300537109375\r",
      "INFO - Step 5451, rl-loss: 90.17369079589844\r",
      "INFO - Step 5452, rl-loss: 683.6950073242188\r",
      "INFO - Step 5453, rl-loss: 94.2120590209961\r",
      "INFO - Step 5454, rl-loss: 1.2588250637054443\r",
      "INFO - Step 5455, rl-loss: 801.944580078125\r",
      "INFO - Step 5456, rl-loss: 66.99992370605469\r",
      "INFO - Step 5457, rl-loss: 7.318118095397949\r",
      "INFO - Step 5458, rl-loss: 71.6427230834961\r",
      "INFO - Step 5459, rl-loss: 398.15838623046875\r",
      "INFO - Step 5460, rl-loss: 334.580322265625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5461, rl-loss: 86.34178161621094\r",
      "INFO - Step 5462, rl-loss: 265.7039489746094\r",
      "INFO - Step 5463, rl-loss: 605.3701782226562\r",
      "INFO - Step 5464, rl-loss: 320.5699157714844\r",
      "INFO - Step 5465, rl-loss: 70.91218566894531\r",
      "INFO - Step 5466, rl-loss: 459.23333740234375\r",
      "INFO - Step 5467, rl-loss: 273.38336181640625\r",
      "INFO - Step 5468, rl-loss: 12.735820770263672\r",
      "INFO - Step 5469, rl-loss: 124.3516616821289\r",
      "INFO - Step 5470, rl-loss: 74.87678527832031\r",
      "INFO - Step 5471, rl-loss: 17.467512130737305\r",
      "INFO - Step 5472, rl-loss: 184.30386352539062\r",
      "INFO - Step 5473, rl-loss: 218.44017028808594\r",
      "INFO - Step 5474, rl-loss: 1.571637749671936\r",
      "INFO - Step 5475, rl-loss: 140.97154235839844\r",
      "INFO - Step 5476, rl-loss: 0.9675466418266296\r",
      "INFO - Step 5477, rl-loss: 265.07281494140625\r",
      "INFO - Step 5478, rl-loss: 28.140220642089844\r",
      "INFO - Step 5479, rl-loss: 296.9640808105469\r",
      "INFO - Step 5480, rl-loss: 337.1709289550781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5481, rl-loss: 80.09111022949219\r",
      "INFO - Step 5482, rl-loss: 143.6598663330078\r",
      "INFO - Step 5483, rl-loss: 1.3241910934448242\r",
      "INFO - Step 5484, rl-loss: 186.00062561035156\r",
      "INFO - Step 5485, rl-loss: 145.03182983398438\r",
      "INFO - Step 5486, rl-loss: 118.92268371582031\r",
      "INFO - Step 5487, rl-loss: 224.78964233398438\r",
      "INFO - Step 5488, rl-loss: 186.99270629882812\r",
      "INFO - Step 5489, rl-loss: 343.71356201171875\r",
      "INFO - Step 5490, rl-loss: 677.7146606445312\r",
      "INFO - Step 5491, rl-loss: 13.534618377685547\r",
      "INFO - Step 5492, rl-loss: 297.6532287597656\r",
      "INFO - Step 5493, rl-loss: 96.61544799804688\r",
      "INFO - Step 5494, rl-loss: 386.80413818359375\r",
      "INFO - Step 5495, rl-loss: 423.98089599609375\r",
      "INFO - Step 5496, rl-loss: 1.2169359922409058\r",
      "INFO - Step 5497, rl-loss: 1.0142583847045898\r",
      "INFO - Step 5498, rl-loss: 0.9229727983474731\r",
      "INFO - Step 5499, rl-loss: 7.601407527923584\r",
      "INFO - Step 5500, rl-loss: 379.5078430175781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5501, rl-loss: 329.7398986816406\r",
      "INFO - Step 5502, rl-loss: 1.5549006462097168\r",
      "INFO - Step 5503, rl-loss: 694.066650390625\r",
      "INFO - Step 5504, rl-loss: 87.97848510742188\r",
      "INFO - Step 5505, rl-loss: 0.8938223123550415\r",
      "INFO - Step 5506, rl-loss: 116.27961730957031\r",
      "INFO - Step 5507, rl-loss: 87.64078521728516\r",
      "INFO - Step 5508, rl-loss: 563.8316650390625\r",
      "INFO - Step 5509, rl-loss: 310.57257080078125\r",
      "INFO - Step 5510, rl-loss: 335.3250732421875\r",
      "INFO - Step 5511, rl-loss: 68.9168472290039\r",
      "INFO - Step 5512, rl-loss: 426.8948669433594\r",
      "INFO - Step 5513, rl-loss: 443.44964599609375\r",
      "INFO - Step 5514, rl-loss: 186.62826538085938\r",
      "INFO - Step 5515, rl-loss: 533.945556640625\r",
      "INFO - Step 5516, rl-loss: 330.31646728515625\r",
      "INFO - Step 5517, rl-loss: 397.440185546875\r",
      "INFO - Step 5518, rl-loss: 620.5399169921875\r",
      "INFO - Step 5519, rl-loss: 1.600630760192871\r",
      "INFO - Step 5520, rl-loss: 54.97913360595703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5521, rl-loss: 134.95693969726562\r",
      "INFO - Step 5522, rl-loss: 1.5267372131347656\r",
      "INFO - Step 5523, rl-loss: 353.1145935058594\r",
      "INFO - Step 5524, rl-loss: 1.5328314304351807\r",
      "INFO - Step 5525, rl-loss: 244.72003173828125\r",
      "INFO - Step 5526, rl-loss: 45.41609573364258\r",
      "INFO - Step 5527, rl-loss: 310.5496520996094\r",
      "INFO - Step 5528, rl-loss: 217.61419677734375\r",
      "INFO - Step 5529, rl-loss: 1.3602287769317627\r",
      "INFO - Step 5530, rl-loss: 301.4845275878906\r",
      "INFO - Step 5531, rl-loss: 118.07475280761719\r",
      "INFO - Step 5532, rl-loss: 412.94677734375\r",
      "INFO - Step 5533, rl-loss: 1.6098225116729736\r",
      "INFO - Step 5534, rl-loss: 179.00186157226562\r",
      "INFO - Step 5535, rl-loss: 353.958251953125\r",
      "INFO - Step 5536, rl-loss: 510.6589660644531\r",
      "INFO - Step 5537, rl-loss: 111.4865493774414\r",
      "INFO - Step 5538, rl-loss: 111.86349487304688\r",
      "INFO - Step 5539, rl-loss: 218.9775390625\r",
      "INFO - Step 5540, rl-loss: 160.20257568359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5541, rl-loss: 173.77163696289062\r",
      "INFO - Step 5542, rl-loss: 65.79975891113281\r",
      "INFO - Step 5543, rl-loss: 48.79338836669922\r",
      "INFO - Step 5544, rl-loss: 1.1063179969787598\r",
      "INFO - Step 5545, rl-loss: 101.9106674194336\r",
      "INFO - Step 5546, rl-loss: 146.54031372070312\r",
      "INFO - Step 5547, rl-loss: 0.8354625701904297\r",
      "INFO - Step 5548, rl-loss: 365.755126953125\r",
      "INFO - Step 5549, rl-loss: 81.07939910888672\r",
      "INFO - Step 5550, rl-loss: 607.3480834960938\r",
      "INFO - Step 5551, rl-loss: 795.9121704101562\r",
      "INFO - Step 5552, rl-loss: 448.67230224609375\r",
      "INFO - Step 5553, rl-loss: 88.09668731689453\r",
      "INFO - Step 5554, rl-loss: 696.85986328125\r",
      "INFO - Step 5555, rl-loss: 91.80120086669922\r",
      "INFO - Step 5556, rl-loss: 1.2808424234390259\r",
      "INFO - Step 5557, rl-loss: 247.11990356445312\r",
      "INFO - Step 5558, rl-loss: 1.5507878065109253\r",
      "INFO - Step 5559, rl-loss: 147.89785766601562\r",
      "INFO - Step 5560, rl-loss: 1.4669570922851562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5561, rl-loss: 413.4323425292969\r",
      "INFO - Step 5562, rl-loss: 333.3163146972656\r",
      "INFO - Step 5563, rl-loss: 0.8250254392623901\r",
      "INFO - Step 5564, rl-loss: 101.32524108886719\r",
      "INFO - Step 5565, rl-loss: 0.9032875299453735\r",
      "INFO - Step 5566, rl-loss: 1.7232681512832642\r",
      "INFO - Step 5567, rl-loss: 68.96363830566406\r",
      "INFO - Step 5568, rl-loss: 355.7607116699219\r",
      "INFO - Step 5569, rl-loss: 1.4258232116699219\r",
      "INFO - Step 5570, rl-loss: 216.32247924804688\r",
      "INFO - Step 5571, rl-loss: 557.6580810546875\r",
      "INFO - Step 5572, rl-loss: 174.47642517089844\r",
      "INFO - Step 5573, rl-loss: 1.0268312692642212\r",
      "INFO - Step 5574, rl-loss: 168.54148864746094\r",
      "INFO - Step 5575, rl-loss: 1.0942418575286865\r",
      "INFO - Step 5576, rl-loss: 392.8990783691406\r",
      "INFO - Step 5577, rl-loss: 0.7259913086891174\r",
      "INFO - Step 5578, rl-loss: 106.19441223144531\r",
      "INFO - Step 5579, rl-loss: 290.71746826171875\r",
      "INFO - Step 5580, rl-loss: 175.97689819335938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5581, rl-loss: 265.7373962402344\r",
      "INFO - Step 5582, rl-loss: 37.87901306152344\r",
      "INFO - Step 5583, rl-loss: 1.9976567029953003\r",
      "INFO - Step 5584, rl-loss: 172.25755310058594\r",
      "INFO - Step 5585, rl-loss: 182.9722442626953\r",
      "INFO - Step 5586, rl-loss: 335.12823486328125\r",
      "INFO - Step 5587, rl-loss: 452.9969787597656\r",
      "INFO - Step 5588, rl-loss: 199.76266479492188\r",
      "INFO - Step 5589, rl-loss: 218.7638397216797\r",
      "INFO - Step 5590, rl-loss: 178.03952026367188\r",
      "INFO - Step 5591, rl-loss: 1.4586775302886963\r",
      "INFO - Step 5592, rl-loss: 27.77255630493164\r",
      "INFO - Step 5593, rl-loss: 69.34529113769531\r",
      "INFO - Step 5594, rl-loss: 245.2210693359375\r",
      "INFO - Step 5595, rl-loss: 0.7739967703819275\r",
      "INFO - Step 5596, rl-loss: 1.8918242454528809\r",
      "INFO - Step 5597, rl-loss: 318.2359619140625\r",
      "INFO - Step 5598, rl-loss: 303.2578125\r",
      "INFO - Step 5599, rl-loss: 559.2633056640625\r",
      "INFO - Step 5600, rl-loss: 43.38309860229492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5601, rl-loss: 1.0002305507659912\r",
      "INFO - Step 5602, rl-loss: 223.35008239746094\r",
      "INFO - Step 5603, rl-loss: 169.4261016845703\r",
      "INFO - Step 5604, rl-loss: 1.035601258277893\r",
      "INFO - Step 5605, rl-loss: 148.93411254882812\r",
      "INFO - Step 5606, rl-loss: 33.544315338134766\r",
      "INFO - Step 5607, rl-loss: 366.5254211425781\r",
      "INFO - Step 5608, rl-loss: 84.03007507324219\r",
      "INFO - Step 5609, rl-loss: 1.5731596946716309\r",
      "INFO - Step 5610, rl-loss: 399.7677307128906\r",
      "INFO - Step 5611, rl-loss: 107.28787994384766\r",
      "INFO - Step 5612, rl-loss: 210.244384765625\r",
      "INFO - Step 5613, rl-loss: 1.3024325370788574\r",
      "INFO - Step 5614, rl-loss: 181.64772033691406\r",
      "INFO - Step 5615, rl-loss: 1.190284252166748\r",
      "INFO - Step 5616, rl-loss: 22.06446075439453\r",
      "INFO - Step 5617, rl-loss: 143.19947814941406\r",
      "INFO - Step 5618, rl-loss: 33.86166763305664\r",
      "INFO - Step 5619, rl-loss: 279.75653076171875\r",
      "INFO - Step 5620, rl-loss: 281.10980224609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5621, rl-loss: 187.59780883789062\r",
      "INFO - Step 5622, rl-loss: 422.90753173828125\r",
      "INFO - Step 5623, rl-loss: 338.596923828125\r",
      "INFO - Step 5624, rl-loss: 396.0772705078125\r",
      "INFO - Step 5625, rl-loss: 516.9404907226562\r",
      "INFO - Step 5626, rl-loss: 263.4492492675781\r",
      "INFO - Step 5627, rl-loss: 37.31013107299805\r",
      "INFO - Step 5628, rl-loss: 21.956954956054688\r",
      "INFO - Step 5629, rl-loss: 122.57318878173828\r",
      "INFO - Step 5630, rl-loss: 0.9175443649291992\r",
      "INFO - Step 5631, rl-loss: 170.48060607910156\r",
      "INFO - Step 5632, rl-loss: 117.43708038330078\r",
      "INFO - Step 5633, rl-loss: 1258.3397216796875\r",
      "INFO - Step 5634, rl-loss: 98.49138641357422\r",
      "INFO - Step 5635, rl-loss: 52.97523880004883\r",
      "INFO - Step 5636, rl-loss: 143.3419647216797\r",
      "INFO - Step 5637, rl-loss: 197.46664428710938\r",
      "INFO - Step 5638, rl-loss: 397.8748474121094\r",
      "INFO - Step 5639, rl-loss: 178.2958526611328\r",
      "INFO - Step 5640, rl-loss: 304.6595764160156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5641, rl-loss: 196.99940490722656\r",
      "INFO - Step 5642, rl-loss: 138.32130432128906\r",
      "INFO - Step 5643, rl-loss: 1.1570734977722168\r",
      "INFO - Step 5644, rl-loss: 400.4858093261719\r",
      "INFO - Step 5645, rl-loss: 129.51864624023438\r",
      "INFO - Step 5646, rl-loss: 0.9737893342971802\r",
      "INFO - Step 5647, rl-loss: 12.048773765563965\r",
      "INFO - Step 5648, rl-loss: 185.01394653320312\r",
      "INFO - Step 5649, rl-loss: 217.01565551757812\r",
      "INFO - Step 5650, rl-loss: 181.0655975341797\r",
      "INFO - Step 5651, rl-loss: 135.7787628173828\r",
      "INFO - Step 5652, rl-loss: 177.8678436279297\r",
      "INFO - Step 5653, rl-loss: 393.65655517578125\r",
      "INFO - Step 5654, rl-loss: 213.8249053955078\r",
      "INFO - Step 5655, rl-loss: 170.0854949951172\r",
      "INFO - Step 5656, rl-loss: 511.09912109375\r",
      "INFO - Step 5657, rl-loss: 70.08235931396484\r",
      "INFO - Step 5658, rl-loss: 531.8219604492188\r",
      "INFO - Step 5659, rl-loss: 101.37433624267578\r",
      "INFO - Step 5660, rl-loss: 0.7539496421813965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5661, rl-loss: 98.52979278564453\r",
      "INFO - Step 5662, rl-loss: 83.88435363769531\r",
      "INFO - Step 5663, rl-loss: 85.96295928955078\r",
      "INFO - Step 5664, rl-loss: 1.4373698234558105\r",
      "INFO - Step 5665, rl-loss: 158.651611328125\r",
      "INFO - Step 5666, rl-loss: 1.6068902015686035\r",
      "INFO - Step 5667, rl-loss: 189.43943786621094\r",
      "INFO - Step 5668, rl-loss: 160.2779541015625\r",
      "INFO - Step 5669, rl-loss: 160.8754425048828\r",
      "INFO - Step 5670, rl-loss: 62.81940460205078\r",
      "INFO - Step 5671, rl-loss: 156.44754028320312\r",
      "INFO - Step 5672, rl-loss: 331.8110046386719\r",
      "INFO - Step 5673, rl-loss: 597.5286865234375\r",
      "INFO - Step 5674, rl-loss: 223.6968536376953\r",
      "INFO - Step 5675, rl-loss: 2.0186808109283447\r",
      "INFO - Step 5676, rl-loss: 54.31167221069336\r",
      "INFO - Step 5677, rl-loss: 404.771240234375\r",
      "INFO - Step 5678, rl-loss: 1.1361355781555176\r",
      "INFO - Step 5679, rl-loss: 170.2506103515625\r",
      "INFO - Step 5680, rl-loss: 78.84378051757812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5681, rl-loss: 71.14439392089844\r",
      "INFO - Step 5682, rl-loss: 104.19537353515625\r",
      "INFO - Step 5683, rl-loss: 178.1228790283203\r",
      "INFO - Step 5684, rl-loss: 156.56883239746094\r",
      "INFO - Step 5685, rl-loss: 397.85394287109375\r",
      "INFO - Step 5686, rl-loss: 66.85260009765625\r",
      "INFO - Step 5687, rl-loss: 286.9228515625\r",
      "INFO - Step 5688, rl-loss: 1.0046358108520508\r",
      "INFO - Step 5689, rl-loss: 135.38243103027344\r",
      "INFO - Step 5690, rl-loss: 356.4283142089844\r",
      "INFO - Step 5691, rl-loss: 0.8324206471443176\r",
      "INFO - Step 5692, rl-loss: 66.15169525146484\r",
      "INFO - Step 5693, rl-loss: 358.1783142089844\r",
      "INFO - Step 5694, rl-loss: 70.8025894165039\r",
      "INFO - Step 5695, rl-loss: 70.45397186279297\r",
      "INFO - Step 5696, rl-loss: 519.8652954101562\r",
      "INFO - Step 5697, rl-loss: 107.25006103515625\r",
      "INFO - Step 5698, rl-loss: 61.1730842590332\r",
      "INFO - Step 5699, rl-loss: 0.7835325002670288\r",
      "INFO - Step 5700, rl-loss: 457.1210632324219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5701, rl-loss: 14.541138648986816\r",
      "INFO - Step 5702, rl-loss: 203.99215698242188\r",
      "INFO - Step 5703, rl-loss: 302.11126708984375\r",
      "INFO - Step 5704, rl-loss: 305.5850830078125\r",
      "INFO - Step 5705, rl-loss: 356.6940002441406\r",
      "INFO - Step 5706, rl-loss: 318.5439147949219\r",
      "INFO - Step 5707, rl-loss: 1.0095763206481934\r",
      "INFO - Step 5708, rl-loss: 52.659332275390625\r",
      "INFO - Step 5709, rl-loss: 4.6295061111450195\r",
      "INFO - Step 5710, rl-loss: 287.57415771484375\r",
      "INFO - Step 5711, rl-loss: 91.4318618774414\r",
      "INFO - Step 5712, rl-loss: 1.3505195379257202\r",
      "INFO - Step 5713, rl-loss: 43.53799057006836\r",
      "INFO - Step 5714, rl-loss: 1.219620704650879\r",
      "INFO - Step 5715, rl-loss: 0.6726484298706055\r",
      "INFO - Step 5716, rl-loss: 481.24462890625\r",
      "INFO - Step 5717, rl-loss: 1139.2547607421875\r",
      "INFO - Step 5718, rl-loss: 275.098388671875\r",
      "INFO - Step 5719, rl-loss: 135.81689453125\r",
      "INFO - Step 5720, rl-loss: 287.625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5721, rl-loss: 499.7559814453125\r",
      "INFO - Step 5722, rl-loss: 6.485098361968994\r",
      "INFO - Step 5723, rl-loss: 385.4490051269531\r",
      "INFO - Step 5724, rl-loss: 48.844791412353516\r",
      "INFO - Step 5725, rl-loss: 200.45089721679688\r",
      "INFO - Step 5726, rl-loss: 1.101841926574707\r",
      "INFO - Step 5727, rl-loss: 367.8585205078125\r",
      "INFO - Step 5728, rl-loss: 499.8636169433594\r",
      "INFO - Step 5729, rl-loss: 0.8351186513900757\r",
      "INFO - Step 5730, rl-loss: 217.42759704589844\r",
      "INFO - Step 5731, rl-loss: 316.7367248535156\r",
      "INFO - Step 5732, rl-loss: 1.0715469121932983\r",
      "INFO - Step 5733, rl-loss: 68.25855255126953\r",
      "INFO - Step 5734, rl-loss: 195.93409729003906\r",
      "INFO - Step 5735, rl-loss: 294.882080078125\r",
      "INFO - Step 5736, rl-loss: 357.6419982910156\r",
      "INFO - Step 5737, rl-loss: 307.8745422363281\r",
      "INFO - Step 5738, rl-loss: 54.37931823730469\r",
      "INFO - Step 5739, rl-loss: 262.7248229980469\r",
      "INFO - Step 5740, rl-loss: 168.2420196533203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5741, rl-loss: 125.458251953125\r",
      "INFO - Step 5742, rl-loss: 168.63925170898438\r",
      "INFO - Step 5743, rl-loss: 38.88209533691406\r",
      "INFO - Step 5744, rl-loss: 57.03875732421875\r",
      "INFO - Step 5745, rl-loss: 7.447805881500244\r",
      "INFO - Step 5746, rl-loss: 210.22335815429688\r",
      "INFO - Step 5747, rl-loss: 275.1949462890625\r",
      "INFO - Step 5748, rl-loss: 335.71514892578125\r",
      "INFO - Step 5749, rl-loss: 1.0184657573699951\r",
      "INFO - Step 5750, rl-loss: 123.8045425415039\r",
      "INFO - Step 5751, rl-loss: 1.4583346843719482\r",
      "INFO - Step 5752, rl-loss: 1.1171127557754517\r",
      "INFO - Step 5753, rl-loss: 297.8887634277344\r",
      "INFO - Step 5754, rl-loss: 275.4444580078125\r",
      "INFO - Step 5755, rl-loss: 1.3582115173339844\r",
      "INFO - Step 5756, rl-loss: 1.2193567752838135\r",
      "INFO - Step 5757, rl-loss: 83.51931762695312\r",
      "INFO - Step 5758, rl-loss: 1.0836396217346191\r",
      "INFO - Step 5759, rl-loss: 270.9717712402344\r",
      "INFO - Step 5760, rl-loss: 213.13673400878906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5761, rl-loss: 27.948705673217773\r",
      "INFO - Step 5762, rl-loss: 1.3647549152374268\r",
      "INFO - Step 5763, rl-loss: 78.78955841064453\r",
      "INFO - Step 5764, rl-loss: 81.0769271850586\r",
      "INFO - Step 5765, rl-loss: 1.318589448928833\r",
      "INFO - Step 5766, rl-loss: 0.7443835139274597\r",
      "INFO - Step 5767, rl-loss: 180.10121154785156\r",
      "INFO - Step 5768, rl-loss: 140.71713256835938\r",
      "INFO - Step 5769, rl-loss: 613.59765625\r",
      "INFO - Step 5770, rl-loss: 278.4856872558594\r",
      "INFO - Step 5771, rl-loss: 327.64312744140625\r",
      "INFO - Step 5772, rl-loss: 223.01223754882812\r",
      "INFO - Step 5773, rl-loss: 543.2940063476562\r",
      "INFO - Step 5774, rl-loss: 44.75920486450195\r",
      "INFO - Step 5775, rl-loss: 186.6725616455078\r",
      "INFO - Step 5776, rl-loss: 1.1091866493225098\r",
      "INFO - Step 5777, rl-loss: 12.233725547790527\r",
      "INFO - Step 5778, rl-loss: 1.1847590208053589\r",
      "INFO - Step 5779, rl-loss: 124.32677459716797\r",
      "INFO - Step 5780, rl-loss: 106.78849792480469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5781, rl-loss: 110.07707214355469\r",
      "INFO - Step 5782, rl-loss: 41.836341857910156\r",
      "INFO - Step 5783, rl-loss: 238.4310760498047\r",
      "INFO - Step 5784, rl-loss: 1.1886972188949585\r",
      "INFO - Step 5785, rl-loss: 0.966585636138916\r",
      "INFO - Step 5786, rl-loss: 395.84521484375\r",
      "INFO - Step 5787, rl-loss: 153.38548278808594\r",
      "INFO - Step 5788, rl-loss: 355.51934814453125\r",
      "INFO - Step 5789, rl-loss: 320.4140319824219\r",
      "INFO - Step 5790, rl-loss: 1.083907127380371\r",
      "INFO - Step 5791, rl-loss: 7.368797779083252\r",
      "INFO - Step 5792, rl-loss: 323.9849853515625\r",
      "INFO - Step 5793, rl-loss: 386.4213562011719\r",
      "INFO - Step 5794, rl-loss: 106.1054916381836\r",
      "INFO - Step 5795, rl-loss: 107.76094055175781\r",
      "INFO - Step 5796, rl-loss: 318.9543151855469\r",
      "INFO - Step 5797, rl-loss: 1.1761621236801147\r",
      "INFO - Step 5798, rl-loss: 1.7244377136230469\r",
      "INFO - Step 5799, rl-loss: 87.62586975097656\r",
      "INFO - Step 5800, rl-loss: 1.2074387073516846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5801, rl-loss: 840.1931762695312\r",
      "INFO - Step 5802, rl-loss: 317.9151306152344\r",
      "INFO - Step 5803, rl-loss: 349.35748291015625\r",
      "INFO - Step 5804, rl-loss: 315.58123779296875\r",
      "INFO - Step 5805, rl-loss: 287.0914306640625\r",
      "INFO - Step 5806, rl-loss: 426.2605895996094\r",
      "INFO - Step 5807, rl-loss: 20.220932006835938\r",
      "INFO - Step 5808, rl-loss: 232.94400024414062\r",
      "INFO - Step 5809, rl-loss: 146.47532653808594\r",
      "INFO - Step 5810, rl-loss: 409.696044921875\r",
      "INFO - Step 5811, rl-loss: 113.1100845336914\r",
      "INFO - Step 5812, rl-loss: 212.9053497314453\r",
      "INFO - Step 5813, rl-loss: 230.3901824951172\r",
      "INFO - Step 5814, rl-loss: 77.765380859375\r",
      "INFO - Step 5815, rl-loss: 270.39447021484375\r",
      "INFO - Step 5816, rl-loss: 83.80238342285156\r",
      "INFO - Step 5817, rl-loss: 196.40985107421875\r",
      "INFO - Step 5818, rl-loss: 134.27035522460938\r",
      "INFO - Step 5819, rl-loss: 62.992889404296875\r",
      "INFO - Step 5820, rl-loss: 231.06312561035156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5821, rl-loss: 137.78909301757812\r",
      "INFO - Step 5822, rl-loss: 298.7950744628906\r",
      "INFO - Step 5823, rl-loss: 139.4969940185547\r",
      "INFO - Step 5824, rl-loss: 449.2342834472656\r",
      "INFO - Step 5825, rl-loss: 439.57452392578125\r",
      "INFO - Step 5826, rl-loss: 1.1935902833938599\r",
      "INFO - Step 5827, rl-loss: 254.40562438964844\r",
      "INFO - Step 5828, rl-loss: 108.45021057128906\r",
      "INFO - Step 5829, rl-loss: 405.1827392578125\r",
      "INFO - Step 5830, rl-loss: 0.9191108345985413\r",
      "INFO - Step 5831, rl-loss: 44.31806945800781\r",
      "INFO - Step 5832, rl-loss: 449.0267639160156\r",
      "INFO - Step 5833, rl-loss: 0.7778788805007935\r",
      "INFO - Step 5834, rl-loss: 522.5291748046875\r",
      "INFO - Step 5835, rl-loss: 112.55292510986328\r",
      "INFO - Step 5836, rl-loss: 140.45436096191406\r",
      "INFO - Step 5837, rl-loss: 66.60138702392578\r",
      "INFO - Step 5838, rl-loss: 462.225830078125\r",
      "INFO - Step 5839, rl-loss: 12.27794075012207\r",
      "INFO - Step 5840, rl-loss: 15.93986988067627"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5841, rl-loss: 107.68115234375\r",
      "INFO - Step 5842, rl-loss: 52.93745803833008\r",
      "INFO - Step 5843, rl-loss: 1.5664613246917725\r",
      "INFO - Step 5844, rl-loss: 124.0546646118164\r",
      "INFO - Step 5845, rl-loss: 395.82733154296875\r",
      "INFO - Step 5846, rl-loss: 105.11697387695312\r",
      "INFO - Step 5847, rl-loss: 1.0013374090194702\r",
      "INFO - Step 5848, rl-loss: 23.85138702392578\r",
      "INFO - Step 5849, rl-loss: 308.8416748046875\r",
      "INFO - Step 5850, rl-loss: 23.112102508544922\r",
      "INFO - Step 5851, rl-loss: 308.166259765625\r",
      "INFO - Step 5852, rl-loss: 80.71598815917969\r",
      "INFO - Step 5853, rl-loss: 371.337890625\r",
      "INFO - Step 5854, rl-loss: 129.6900177001953\r",
      "INFO - Step 5855, rl-loss: 149.79795837402344\r",
      "INFO - Step 5856, rl-loss: 172.62448120117188\r",
      "INFO - Step 5857, rl-loss: 1.3418841361999512\r",
      "INFO - Step 5858, rl-loss: 274.01727294921875\r",
      "INFO - Step 5859, rl-loss: 0.9763815402984619\r",
      "INFO - Step 5860, rl-loss: 91.20542907714844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5861, rl-loss: 0.5346361398696899\r",
      "INFO - Step 5862, rl-loss: 63.918880462646484\r",
      "INFO - Step 5863, rl-loss: 509.2912292480469\r",
      "INFO - Step 5864, rl-loss: 52.39228439331055\r",
      "INFO - Step 5865, rl-loss: 234.33775329589844\r",
      "INFO - Step 5866, rl-loss: 731.0994873046875\r",
      "INFO - Step 5867, rl-loss: 179.42877197265625\r",
      "INFO - Step 5868, rl-loss: 89.39545440673828\r",
      "INFO - Step 5869, rl-loss: 444.000732421875\r",
      "INFO - Step 5870, rl-loss: 150.52490234375\r",
      "INFO - Step 5871, rl-loss: 12.813138961791992\r",
      "INFO - Step 5872, rl-loss: 31.697425842285156\r",
      "INFO - Step 5873, rl-loss: 51.989994049072266\r",
      "INFO - Step 5874, rl-loss: 165.9247283935547\r",
      "INFO - Step 5875, rl-loss: 117.32127380371094\r",
      "INFO - Step 5876, rl-loss: 1.376940131187439\r",
      "INFO - Step 5877, rl-loss: 1.5175790786743164\r",
      "INFO - Step 5878, rl-loss: 358.7225341796875\r",
      "INFO - Step 5879, rl-loss: 368.82244873046875\r",
      "INFO - Step 5880, rl-loss: 73.74169158935547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5881, rl-loss: 310.8193359375\r",
      "INFO - Step 5882, rl-loss: 142.40267944335938\r",
      "INFO - Step 5883, rl-loss: 7.581306457519531\r",
      "INFO - Step 5884, rl-loss: 465.18157958984375\r",
      "INFO - Step 5885, rl-loss: 421.4087219238281\r",
      "INFO - Step 5886, rl-loss: 68.0350112915039\r",
      "INFO - Step 5887, rl-loss: 1.3591201305389404\r",
      "INFO - Step 5888, rl-loss: 1.575341820716858\r",
      "INFO - Step 5889, rl-loss: 88.84468078613281\r",
      "INFO - Step 5890, rl-loss: 501.9158630371094\r",
      "INFO - Step 5891, rl-loss: 49.7127799987793\r",
      "INFO - Step 5892, rl-loss: 112.17000579833984\r",
      "INFO - Step 5893, rl-loss: 107.43568420410156\r",
      "INFO - Step 5894, rl-loss: 129.57444763183594\r",
      "INFO - Step 5895, rl-loss: 0.7718938589096069\r",
      "INFO - Step 5896, rl-loss: 115.67977905273438\r",
      "INFO - Step 5897, rl-loss: 268.9937744140625\r",
      "INFO - Step 5898, rl-loss: 542.4541015625\r",
      "INFO - Step 5899, rl-loss: 725.4166870117188\r",
      "INFO - Step 5900, rl-loss: 161.98468017578125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5901, rl-loss: 437.2181396484375\r",
      "INFO - Step 5902, rl-loss: 12.120978355407715\r",
      "INFO - Step 5903, rl-loss: 109.88920593261719\r",
      "INFO - Step 5904, rl-loss: 0.9000046253204346\r",
      "INFO - Step 5905, rl-loss: 115.01048278808594\r",
      "INFO - Step 5906, rl-loss: 326.7547607421875\r",
      "INFO - Step 5907, rl-loss: 1.4200599193572998\r",
      "INFO - Step 5908, rl-loss: 1.4860398769378662\r",
      "INFO - Step 5909, rl-loss: 226.49185180664062\r",
      "INFO - Step 5910, rl-loss: 140.95823669433594\r",
      "INFO - Step 5911, rl-loss: 616.4361572265625\r",
      "INFO - Step 5912, rl-loss: 839.0778198242188\r",
      "INFO - Step 5913, rl-loss: 112.68746948242188\r",
      "INFO - Step 5914, rl-loss: 93.75856018066406\r",
      "INFO - Step 5915, rl-loss: 209.39456176757812\r",
      "INFO - Step 5916, rl-loss: 54.11058044433594\r",
      "INFO - Step 5917, rl-loss: 298.0054626464844\r",
      "INFO - Step 5918, rl-loss: 70.50806427001953\r",
      "INFO - Step 5919, rl-loss: 61.57200241088867\r",
      "INFO - Step 5920, rl-loss: 1098.2978515625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5921, rl-loss: 380.5166015625\r",
      "INFO - Step 5922, rl-loss: 1.476386547088623\r",
      "INFO - Step 5923, rl-loss: 88.83638000488281\r",
      "INFO - Step 5924, rl-loss: 97.84664916992188\r",
      "INFO - Step 5925, rl-loss: 401.3461608886719\r",
      "INFO - Step 5926, rl-loss: 0.8586683869361877\r",
      "INFO - Step 5927, rl-loss: 309.13214111328125\r",
      "INFO - Step 5928, rl-loss: 0.6956585049629211\r",
      "INFO - Step 5929, rl-loss: 43.378639221191406\r",
      "INFO - Step 5930, rl-loss: 70.2418212890625\r",
      "INFO - Step 5931, rl-loss: 268.8153381347656\r",
      "INFO - Step 5932, rl-loss: 1.0007102489471436\r",
      "INFO - Step 5933, rl-loss: 143.53759765625\r",
      "INFO - Step 5934, rl-loss: 188.22264099121094\r",
      "INFO - Step 5935, rl-loss: 226.9483184814453\r",
      "INFO - Step 5936, rl-loss: 0.8443110585212708\r",
      "INFO - Step 5937, rl-loss: 13.613011360168457\r",
      "INFO - Step 5938, rl-loss: 331.7033386230469\r",
      "INFO - Step 5939, rl-loss: 109.75053405761719\r",
      "INFO - Step 5940, rl-loss: 1.1517727375030518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5941, rl-loss: 98.06900787353516\r",
      "INFO - Step 5942, rl-loss: 1.2217274904251099\r",
      "INFO - Step 5943, rl-loss: 105.21220397949219\r",
      "INFO - Step 5944, rl-loss: 1.6000196933746338\r",
      "INFO - Step 5945, rl-loss: 1.8753154277801514\r",
      "INFO - Step 5946, rl-loss: 435.57159423828125\r",
      "INFO - Step 5947, rl-loss: 245.0261688232422\r",
      "INFO - Step 5948, rl-loss: 259.4169006347656\r",
      "INFO - Step 5949, rl-loss: 218.83663940429688\r",
      "INFO - Step 5950, rl-loss: 232.600341796875\r",
      "INFO - Step 5951, rl-loss: 156.20819091796875\r",
      "INFO - Step 5952, rl-loss: 15.56795883178711\r",
      "INFO - Step 5953, rl-loss: 155.79234313964844\r",
      "INFO - Step 5954, rl-loss: 176.18746948242188\r",
      "INFO - Step 5955, rl-loss: 311.98321533203125\r",
      "INFO - Step 5956, rl-loss: 298.8418884277344\r",
      "INFO - Step 5957, rl-loss: 227.58506774902344\r",
      "INFO - Step 5958, rl-loss: 138.42471313476562\r",
      "INFO - Step 5959, rl-loss: 385.0806579589844\r",
      "INFO - Step 5960, rl-loss: 43.75581359863281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5961, rl-loss: 213.44058227539062\r",
      "INFO - Step 5962, rl-loss: 135.63272094726562\r",
      "INFO - Step 5963, rl-loss: 128.01914978027344\r",
      "INFO - Step 5964, rl-loss: 0.7890430688858032\r",
      "INFO - Step 5965, rl-loss: 36.034481048583984\r",
      "INFO - Step 5966, rl-loss: 307.5438232421875\r",
      "INFO - Step 5967, rl-loss: 36.91071319580078\r",
      "INFO - Step 5968, rl-loss: 118.61174011230469\r",
      "INFO - Step 5969, rl-loss: 154.4759979248047\r",
      "INFO - Step 5970, rl-loss: 381.6070556640625\r",
      "INFO - Step 5971, rl-loss: 129.38375854492188\r",
      "INFO - Step 5972, rl-loss: 1.5045733451843262\r",
      "INFO - Step 5973, rl-loss: 475.24993896484375\r",
      "INFO - Step 5974, rl-loss: 349.5039367675781\r",
      "INFO - Step 5975, rl-loss: 18.910188674926758\r",
      "INFO - Step 5976, rl-loss: 187.78890991210938\r",
      "INFO - Step 5977, rl-loss: 513.5137329101562\r",
      "INFO - Step 5978, rl-loss: 109.45046997070312\r",
      "INFO - Step 5979, rl-loss: 1.1084853410720825\r",
      "INFO - Step 5980, rl-loss: 354.7681884765625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 5981, rl-loss: 335.23248291015625\r",
      "INFO - Step 5982, rl-loss: 0.6324702501296997\r",
      "INFO - Step 5983, rl-loss: 1.257277250289917\r",
      "INFO - Step 5984, rl-loss: 274.76361083984375\r",
      "INFO - Step 5985, rl-loss: 283.2360534667969\r",
      "INFO - Step 5986, rl-loss: 81.66117858886719\r",
      "INFO - Step 5987, rl-loss: 5.8778557777404785\r",
      "INFO - Step 5988, rl-loss: 369.20849609375\r",
      "INFO - Step 5989, rl-loss: 60.12907028198242\r",
      "INFO - Step 5990, rl-loss: 0.8284212350845337\r",
      "INFO - Step 5991, rl-loss: 1.3313151597976685\r",
      "INFO - Step 5992, rl-loss: 325.4640808105469\r",
      "INFO - Step 5993, rl-loss: 53.989112854003906\r",
      "INFO - Step 5994, rl-loss: 214.60362243652344\r",
      "INFO - Step 5995, rl-loss: 326.6790771484375\r",
      "INFO - Step 5996, rl-loss: 89.81632995605469\r",
      "INFO - Step 5997, rl-loss: 243.7446746826172\r",
      "INFO - Step 5998, rl-loss: 737.9561767578125\r",
      "INFO - Step 5999, rl-loss: 212.31875610351562\r",
      "INFO - Step 6000, rl-loss: 177.37152099609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 6020, rl-loss: 171.85964965820312\n",
      "----------------------------------------\n",
      "  timestep     |  549253\n",
      "  reward       |  61.7\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 6040, rl-loss: 495.57135009765625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6041, rl-loss: 126.18644714355469\r",
      "INFO - Step 6042, rl-loss: 307.47161865234375\r",
      "INFO - Step 6043, rl-loss: 50.680023193359375\r",
      "INFO - Step 6044, rl-loss: 305.2879333496094\r",
      "INFO - Step 6045, rl-loss: 1.156212568283081\r",
      "INFO - Step 6046, rl-loss: 143.486328125\r",
      "INFO - Step 6047, rl-loss: 219.7499542236328\r",
      "INFO - Step 6048, rl-loss: 1.3974182605743408\r",
      "INFO - Step 6049, rl-loss: 135.69058227539062\r",
      "INFO - Step 6050, rl-loss: 293.8050231933594\r",
      "INFO - Step 6051, rl-loss: 6.78402853012085\r",
      "INFO - Step 6052, rl-loss: 655.1748046875\r",
      "INFO - Step 6053, rl-loss: 80.70433807373047\r",
      "INFO - Step 6054, rl-loss: 513.27392578125\r",
      "INFO - Step 6055, rl-loss: 541.8514404296875\r",
      "INFO - Step 6056, rl-loss: 1.1148180961608887\r",
      "INFO - Step 6057, rl-loss: 133.5904998779297\r",
      "INFO - Step 6058, rl-loss: 436.9005126953125\r",
      "INFO - Step 6059, rl-loss: 59.05200958251953\r",
      "INFO - Step 6060, rl-loss: 1.1270688772201538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6061, rl-loss: 273.3171081542969\r",
      "INFO - Step 6062, rl-loss: 594.9821166992188\r",
      "INFO - Step 6063, rl-loss: 79.01449584960938\r",
      "INFO - Step 6064, rl-loss: 155.1657257080078\r",
      "INFO - Step 6065, rl-loss: 473.7210388183594\r",
      "INFO - Step 6066, rl-loss: 106.02473449707031\r",
      "INFO - Step 6067, rl-loss: 359.4654541015625\r",
      "INFO - Step 6068, rl-loss: 124.25888061523438\r",
      "INFO - Step 6069, rl-loss: 180.0808563232422\r",
      "INFO - Step 6070, rl-loss: 269.15118408203125\r",
      "INFO - Step 6071, rl-loss: 301.986572265625\r",
      "INFO - Step 6072, rl-loss: 1.0777359008789062\r",
      "INFO - Step 6073, rl-loss: 545.7421875\r",
      "INFO - Step 6074, rl-loss: 222.27081298828125\r",
      "INFO - Step 6075, rl-loss: 530.8211059570312\r",
      "INFO - Step 6076, rl-loss: 1.1721832752227783\r",
      "INFO - Step 6077, rl-loss: 307.06683349609375\r",
      "INFO - Step 6078, rl-loss: 113.46975708007812\r",
      "INFO - Step 6079, rl-loss: 0.9430645704269409\r",
      "INFO - Step 6080, rl-loss: 388.3428955078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6081, rl-loss: 146.15635681152344\r",
      "INFO - Step 6082, rl-loss: 131.73080444335938\r",
      "INFO - Step 6083, rl-loss: 1.0155060291290283\r",
      "INFO - Step 6084, rl-loss: 1.4054088592529297\r",
      "INFO - Step 6085, rl-loss: 63.09251403808594\r",
      "INFO - Step 6086, rl-loss: 1.3019040822982788\r",
      "INFO - Step 6087, rl-loss: 402.456787109375\r",
      "INFO - Step 6088, rl-loss: 153.87586975097656\r",
      "INFO - Step 6089, rl-loss: 1.5622379779815674\r",
      "INFO - Step 6090, rl-loss: 66.94912719726562\r",
      "INFO - Step 6091, rl-loss: 288.92864990234375\r",
      "INFO - Step 6092, rl-loss: 116.77839660644531\r",
      "INFO - Step 6093, rl-loss: 0.7199428081512451\r",
      "INFO - Step 6094, rl-loss: 524.6682739257812\r",
      "INFO - Step 6095, rl-loss: 0.7522727847099304\r",
      "INFO - Step 6096, rl-loss: 489.80145263671875\r",
      "INFO - Step 6097, rl-loss: 101.8897705078125\r",
      "INFO - Step 6098, rl-loss: 208.2074737548828\r",
      "INFO - Step 6099, rl-loss: 1.2878135442733765\r",
      "INFO - Step 6100, rl-loss: 704.490478515625\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6101, rl-loss: 215.19667053222656\r",
      "INFO - Step 6102, rl-loss: 35.70902633666992\r",
      "INFO - Step 6103, rl-loss: 282.6845703125\r",
      "INFO - Step 6104, rl-loss: 110.78211212158203\r",
      "INFO - Step 6105, rl-loss: 247.7571258544922\r",
      "INFO - Step 6106, rl-loss: 87.38172912597656\r",
      "INFO - Step 6107, rl-loss: 210.28570556640625\r",
      "INFO - Step 6108, rl-loss: 369.4517517089844\r",
      "INFO - Step 6109, rl-loss: 227.92425537109375\r",
      "INFO - Step 6110, rl-loss: 79.65025329589844\r",
      "INFO - Step 6111, rl-loss: 236.2299346923828\r",
      "INFO - Step 6112, rl-loss: 2.00252628326416\r",
      "INFO - Step 6113, rl-loss: 332.851806640625\r",
      "INFO - Step 6114, rl-loss: 1.668086051940918\r",
      "INFO - Step 6115, rl-loss: 229.7484130859375\r",
      "INFO - Step 6116, rl-loss: 85.49832916259766\r",
      "INFO - Step 6117, rl-loss: 143.0937957763672\r",
      "INFO - Step 6118, rl-loss: 82.7814712524414\r",
      "INFO - Step 6119, rl-loss: 82.23393249511719\r",
      "INFO - Step 6120, rl-loss: 375.75921630859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6121, rl-loss: 1.496390700340271\r",
      "INFO - Step 6122, rl-loss: 366.1863708496094\r",
      "INFO - Step 6123, rl-loss: 287.5335693359375\r",
      "INFO - Step 6124, rl-loss: 36.25110626220703\r",
      "INFO - Step 6125, rl-loss: 359.49871826171875\r",
      "INFO - Step 6126, rl-loss: 430.2379150390625\r",
      "INFO - Step 6127, rl-loss: 5.773454189300537\r",
      "INFO - Step 6128, rl-loss: 350.3516845703125\r",
      "INFO - Step 6129, rl-loss: 435.73187255859375\r",
      "INFO - Step 6130, rl-loss: 63.44482421875\r",
      "INFO - Step 6131, rl-loss: 181.37562561035156\r",
      "INFO - Step 6132, rl-loss: 513.3496704101562\r",
      "INFO - Step 6133, rl-loss: 1.399792194366455\r",
      "INFO - Step 6134, rl-loss: 66.89608001708984\r",
      "INFO - Step 6135, rl-loss: 251.54017639160156\r",
      "INFO - Step 6136, rl-loss: 418.5543518066406\r",
      "INFO - Step 6137, rl-loss: 473.9265441894531\r",
      "INFO - Step 6138, rl-loss: 2.151662826538086\r",
      "INFO - Step 6139, rl-loss: 0.6729755401611328\r",
      "INFO - Step 6140, rl-loss: 82.25444793701172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6141, rl-loss: 52.51298522949219\r",
      "INFO - Step 6142, rl-loss: 219.99078369140625\r",
      "INFO - Step 6143, rl-loss: 121.1893310546875\r",
      "INFO - Step 6144, rl-loss: 35.521759033203125\r",
      "INFO - Step 6145, rl-loss: 1.6462516784667969\r",
      "INFO - Step 6146, rl-loss: 688.3194580078125\r",
      "INFO - Step 6147, rl-loss: 1.4153341054916382\r",
      "INFO - Step 6148, rl-loss: 106.10649108886719\r",
      "INFO - Step 6149, rl-loss: 1.2278188467025757\r",
      "INFO - Step 6150, rl-loss: 1.7436975240707397\r",
      "INFO - Step 6151, rl-loss: 174.50636291503906\r",
      "INFO - Step 6152, rl-loss: 377.4041748046875\r",
      "INFO - Step 6153, rl-loss: 1.631001591682434\r",
      "INFO - Step 6154, rl-loss: 116.71022033691406\r",
      "INFO - Step 6155, rl-loss: 1.9163844585418701\r",
      "INFO - Step 6156, rl-loss: 163.11773681640625\r",
      "INFO - Step 6157, rl-loss: 2.1372790336608887\r",
      "INFO - Step 6158, rl-loss: 238.5599365234375\r",
      "INFO - Step 6159, rl-loss: 309.15673828125\r",
      "INFO - Step 6160, rl-loss: 1.0998401641845703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6161, rl-loss: 1.6984972953796387\r",
      "INFO - Step 6162, rl-loss: 600.903076171875\r",
      "INFO - Step 6163, rl-loss: 131.13839721679688\r",
      "INFO - Step 6164, rl-loss: 94.10822296142578\r",
      "INFO - Step 6165, rl-loss: 739.2256469726562\r",
      "INFO - Step 6166, rl-loss: 65.70565795898438\r",
      "INFO - Step 6167, rl-loss: 49.6185302734375\r",
      "INFO - Step 6168, rl-loss: 1.1798367500305176\r",
      "INFO - Step 6169, rl-loss: 79.69419860839844\r",
      "INFO - Step 6170, rl-loss: 317.6763916015625\r",
      "INFO - Step 6171, rl-loss: 132.3695831298828\r",
      "INFO - Step 6172, rl-loss: 410.733154296875\r",
      "INFO - Step 6173, rl-loss: 1.3160756826400757\r",
      "INFO - Step 6174, rl-loss: 474.9462890625\r",
      "INFO - Step 6175, rl-loss: 90.98126983642578\r",
      "INFO - Step 6176, rl-loss: 1.3895065784454346\r",
      "INFO - Step 6177, rl-loss: 501.9186706542969\r",
      "INFO - Step 6178, rl-loss: 212.8122100830078\r",
      "INFO - Step 6179, rl-loss: 139.9576416015625\r",
      "INFO - Step 6180, rl-loss: 0.9784805774688721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6181, rl-loss: 176.2832794189453\r",
      "INFO - Step 6182, rl-loss: 69.88443756103516\r",
      "INFO - Step 6183, rl-loss: 304.17181396484375\r",
      "INFO - Step 6184, rl-loss: 183.0001678466797\r",
      "INFO - Step 6185, rl-loss: 296.61444091796875\r",
      "INFO - Step 6186, rl-loss: 110.23965454101562\r",
      "INFO - Step 6187, rl-loss: 111.4367904663086\r",
      "INFO - Step 6188, rl-loss: 53.99193572998047\r",
      "INFO - Step 6189, rl-loss: 1.617269515991211\r",
      "INFO - Step 6190, rl-loss: 3.2463629245758057\r",
      "INFO - Step 6191, rl-loss: 1.270886778831482\r",
      "INFO - Step 6192, rl-loss: 616.91796875\r",
      "INFO - Step 6193, rl-loss: 84.18512725830078\r",
      "INFO - Step 6194, rl-loss: 151.90069580078125\r",
      "INFO - Step 6195, rl-loss: 1.4254872798919678\r",
      "INFO - Step 6196, rl-loss: 485.3118896484375\r",
      "INFO - Step 6197, rl-loss: 128.9941864013672\r",
      "INFO - Step 6198, rl-loss: 717.8699951171875\r",
      "INFO - Step 6199, rl-loss: 1.669007658958435\r",
      "INFO - Step 6200, rl-loss: 21.498584747314453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6201, rl-loss: 280.366455078125\r",
      "INFO - Step 6202, rl-loss: 225.3345947265625\r",
      "INFO - Step 6203, rl-loss: 44.03874588012695\r",
      "INFO - Step 6204, rl-loss: 108.18795776367188\r",
      "INFO - Step 6205, rl-loss: 207.23019409179688\r",
      "INFO - Step 6206, rl-loss: 191.28305053710938\r",
      "INFO - Step 6207, rl-loss: 101.8935775756836\r",
      "INFO - Step 6208, rl-loss: 491.7988586425781\r",
      "INFO - Step 6209, rl-loss: 248.3634490966797\r",
      "INFO - Step 6210, rl-loss: 150.236328125\r",
      "INFO - Step 6211, rl-loss: 500.2958068847656\r",
      "INFO - Step 6212, rl-loss: 1.0448415279388428\r",
      "INFO - Step 6213, rl-loss: 67.47136688232422\r",
      "INFO - Step 6214, rl-loss: 218.65676879882812\r",
      "INFO - Step 6215, rl-loss: 677.0125122070312\r",
      "INFO - Step 6216, rl-loss: 1.3487099409103394\r",
      "INFO - Step 6217, rl-loss: 1.8118677139282227\r",
      "INFO - Step 6218, rl-loss: 90.27000427246094\r",
      "INFO - Step 6219, rl-loss: 266.60302734375\r",
      "INFO - Step 6220, rl-loss: 450.29815673828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6221, rl-loss: 435.2726745605469\r",
      "INFO - Step 6222, rl-loss: 79.43240356445312\r",
      "INFO - Step 6223, rl-loss: 216.3163299560547\r",
      "INFO - Step 6224, rl-loss: 167.00144958496094\r",
      "INFO - Step 6225, rl-loss: 43.96931838989258\r",
      "INFO - Step 6226, rl-loss: 163.75111389160156\r",
      "INFO - Step 6227, rl-loss: 81.29824829101562\r",
      "INFO - Step 6228, rl-loss: 76.28950500488281\r",
      "INFO - Step 6229, rl-loss: 244.9921875\r",
      "INFO - Step 6230, rl-loss: 212.32679748535156\r",
      "INFO - Step 6231, rl-loss: 345.7415771484375\r",
      "INFO - Step 6232, rl-loss: 597.454833984375\r",
      "INFO - Step 6233, rl-loss: 131.42962646484375\r",
      "INFO - Step 6234, rl-loss: 1.2336938381195068\r",
      "INFO - Step 6235, rl-loss: 316.9707336425781\r",
      "INFO - Step 6236, rl-loss: 512.4539794921875\r",
      "INFO - Step 6237, rl-loss: 1.1471651792526245\r",
      "INFO - Step 6238, rl-loss: 181.48411560058594\r",
      "INFO - Step 6239, rl-loss: 96.17280578613281\r",
      "INFO - Step 6240, rl-loss: 7.328786849975586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6241, rl-loss: 1.3138794898986816\r",
      "INFO - Step 6242, rl-loss: 1.3814637660980225\r",
      "INFO - Step 6243, rl-loss: 180.5164337158203\r",
      "INFO - Step 6244, rl-loss: 398.92047119140625\r",
      "INFO - Step 6245, rl-loss: 33.40156555175781\r",
      "INFO - Step 6246, rl-loss: 308.29669189453125\r",
      "INFO - Step 6247, rl-loss: 421.94146728515625\r",
      "INFO - Step 6248, rl-loss: 182.7717742919922\r",
      "INFO - Step 6249, rl-loss: 84.7968521118164\r",
      "INFO - Step 6250, rl-loss: 121.10554504394531\r",
      "INFO - Step 6251, rl-loss: 164.4132843017578\r",
      "INFO - Step 6252, rl-loss: 0.6378985643386841\r",
      "INFO - Step 6253, rl-loss: 162.48265075683594\r",
      "INFO - Step 6254, rl-loss: 52.84772491455078\r",
      "INFO - Step 6255, rl-loss: 310.56695556640625\r",
      "INFO - Step 6256, rl-loss: 131.36434936523438\r",
      "INFO - Step 6257, rl-loss: 217.98593139648438\r",
      "INFO - Step 6258, rl-loss: 308.2282409667969\r",
      "INFO - Step 6259, rl-loss: 83.83255767822266\r",
      "INFO - Step 6260, rl-loss: 108.33637237548828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6261, rl-loss: 364.08087158203125\r",
      "INFO - Step 6262, rl-loss: 184.64755249023438\r",
      "INFO - Step 6263, rl-loss: 262.6553955078125\r",
      "INFO - Step 6264, rl-loss: 134.93162536621094\r",
      "INFO - Step 6265, rl-loss: 389.0792541503906\r",
      "INFO - Step 6266, rl-loss: 988.949951171875\r",
      "INFO - Step 6267, rl-loss: 544.4269409179688\r",
      "INFO - Step 6268, rl-loss: 66.77892303466797\r",
      "INFO - Step 6269, rl-loss: 104.51828002929688\r",
      "INFO - Step 6270, rl-loss: 255.84042358398438\r",
      "INFO - Step 6271, rl-loss: 135.32359313964844\r",
      "INFO - Step 6272, rl-loss: 417.65869140625\r",
      "INFO - Step 6273, rl-loss: 108.52278900146484\r",
      "INFO - Step 6274, rl-loss: 166.7011260986328\r",
      "INFO - Step 6275, rl-loss: 586.7180786132812\r",
      "INFO - Step 6276, rl-loss: 554.2886352539062\r",
      "INFO - Step 6277, rl-loss: 61.51520919799805\r",
      "INFO - Step 6278, rl-loss: 114.24398803710938\r",
      "INFO - Step 6279, rl-loss: 43.030067443847656\r",
      "INFO - Step 6280, rl-loss: 427.5386962890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6281, rl-loss: 652.2907104492188\r",
      "INFO - Step 6282, rl-loss: 36.482643127441406\r",
      "INFO - Step 6283, rl-loss: 175.9264678955078\r",
      "INFO - Step 6284, rl-loss: 1.4610328674316406\r",
      "INFO - Step 6285, rl-loss: 201.35586547851562\r",
      "INFO - Step 6286, rl-loss: 147.70333862304688\r",
      "INFO - Step 6287, rl-loss: 691.3724365234375\r",
      "INFO - Step 6288, rl-loss: 133.09437561035156\r",
      "INFO - Step 6289, rl-loss: 341.23345947265625\r",
      "INFO - Step 6290, rl-loss: 1.0599949359893799\r",
      "INFO - Step 6291, rl-loss: 42.999366760253906\r",
      "INFO - Step 6292, rl-loss: 0.5936859250068665\r",
      "INFO - Step 6293, rl-loss: 453.4122619628906\r",
      "INFO - Step 6294, rl-loss: 120.10787963867188\r",
      "INFO - Step 6295, rl-loss: 49.900634765625\r",
      "INFO - Step 6296, rl-loss: 120.47869110107422\r",
      "INFO - Step 6297, rl-loss: 90.53971862792969\r",
      "INFO - Step 6298, rl-loss: 96.9544677734375\r",
      "INFO - Step 6299, rl-loss: 548.9527587890625\r",
      "INFO - Step 6300, rl-loss: 258.3216247558594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6301, rl-loss: 52.1260871887207\r",
      "INFO - Step 6302, rl-loss: 632.7526245117188\r",
      "INFO - Step 6303, rl-loss: 1.2285237312316895\r",
      "INFO - Step 6304, rl-loss: 279.4907531738281\r",
      "INFO - Step 6305, rl-loss: 651.073486328125\r",
      "INFO - Step 6306, rl-loss: 98.94029235839844\r",
      "INFO - Step 6307, rl-loss: 1.070059895515442\r",
      "INFO - Step 6308, rl-loss: 237.49554443359375\r",
      "INFO - Step 6309, rl-loss: 219.8770751953125\r",
      "INFO - Step 6310, rl-loss: 106.15586853027344\r",
      "INFO - Step 6311, rl-loss: 462.6311340332031\r",
      "INFO - Step 6312, rl-loss: 578.81591796875\r",
      "INFO - Step 6313, rl-loss: 1.5017203092575073\r",
      "INFO - Step 6314, rl-loss: 427.32159423828125\r",
      "INFO - Step 6315, rl-loss: 161.48098754882812\r",
      "INFO - Step 6316, rl-loss: 1.2516193389892578\r",
      "INFO - Step 6317, rl-loss: 1.2041689157485962\r",
      "INFO - Step 6318, rl-loss: 147.28179931640625\r",
      "INFO - Step 6319, rl-loss: 0.7397804856300354\r",
      "INFO - Step 6320, rl-loss: 131.3885498046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6321, rl-loss: 217.91757202148438\r",
      "INFO - Step 6322, rl-loss: 48.09733581542969\r",
      "INFO - Step 6323, rl-loss: 176.48208618164062\r",
      "INFO - Step 6324, rl-loss: 21.628856658935547\r",
      "INFO - Step 6325, rl-loss: 533.7898559570312\r",
      "INFO - Step 6326, rl-loss: 1.7718052864074707\r",
      "INFO - Step 6327, rl-loss: 757.0415649414062\r",
      "INFO - Step 6328, rl-loss: 0.9506483674049377\r",
      "INFO - Step 6329, rl-loss: 1.3772001266479492\r",
      "INFO - Step 6330, rl-loss: 295.39617919921875\r",
      "INFO - Step 6331, rl-loss: 306.8236999511719\r",
      "INFO - Step 6332, rl-loss: 73.69236755371094\r",
      "INFO - Step 6333, rl-loss: 1.0294773578643799\r",
      "INFO - Step 6334, rl-loss: 216.56285095214844\r",
      "INFO - Step 6335, rl-loss: 182.0114288330078\r",
      "INFO - Step 6336, rl-loss: 305.1831359863281\r",
      "INFO - Step 6337, rl-loss: 1.4798840284347534\r",
      "INFO - Step 6338, rl-loss: 102.1006088256836\r",
      "INFO - Step 6339, rl-loss: 1.2316315174102783\r",
      "INFO - Step 6340, rl-loss: 71.0218276977539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6341, rl-loss: 106.89936065673828\r",
      "INFO - Step 6342, rl-loss: 172.05714416503906\r",
      "INFO - Step 6343, rl-loss: 450.43719482421875\r",
      "INFO - Step 6344, rl-loss: 36.75165557861328\r",
      "INFO - Step 6345, rl-loss: 12.094857215881348\r",
      "INFO - Step 6346, rl-loss: 1.8089015483856201\r",
      "INFO - Step 6347, rl-loss: 75.24279022216797\r",
      "INFO - Step 6348, rl-loss: 1.1586918830871582\r",
      "INFO - Step 6349, rl-loss: 307.31866455078125\r",
      "INFO - Step 6350, rl-loss: 475.76409912109375\r",
      "INFO - Step 6351, rl-loss: 321.4146423339844\r",
      "INFO - Step 6352, rl-loss: 593.3544921875\r",
      "INFO - Step 6353, rl-loss: 131.32460021972656\r",
      "INFO - Step 6354, rl-loss: 186.12026977539062\r",
      "INFO - Step 6355, rl-loss: 368.6605224609375\r",
      "INFO - Step 6356, rl-loss: 101.3183822631836\r",
      "INFO - Step 6357, rl-loss: 96.87374114990234\r",
      "INFO - Step 6358, rl-loss: 727.604248046875\r",
      "INFO - Step 6359, rl-loss: 6.019214153289795\r",
      "INFO - Step 6360, rl-loss: 152.51571655273438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6361, rl-loss: 1.0831036567687988\r",
      "INFO - Step 6362, rl-loss: 361.20452880859375\r",
      "INFO - Step 6363, rl-loss: 306.35321044921875\r",
      "INFO - Step 6364, rl-loss: 271.77398681640625\r",
      "INFO - Step 6365, rl-loss: 1.2639142274856567\r",
      "INFO - Step 6366, rl-loss: 137.28323364257812\r",
      "INFO - Step 6367, rl-loss: 290.8741455078125\r",
      "INFO - Step 6368, rl-loss: 339.8432312011719\r",
      "INFO - Step 6369, rl-loss: 0.9409114122390747\r",
      "INFO - Step 6370, rl-loss: 300.825927734375\r",
      "INFO - Step 6371, rl-loss: 1.5157084465026855\r",
      "INFO - Step 6372, rl-loss: 91.08439636230469\r",
      "INFO - Step 6373, rl-loss: 162.18495178222656\r",
      "INFO - Step 6374, rl-loss: 175.1430206298828\r",
      "INFO - Step 6375, rl-loss: 1.1554579734802246\r",
      "INFO - Step 6376, rl-loss: 457.0755310058594\r",
      "INFO - Step 6377, rl-loss: 66.44943237304688\r",
      "INFO - Step 6378, rl-loss: 550.8463745117188\r",
      "INFO - Step 6379, rl-loss: 1.0176422595977783\r",
      "INFO - Step 6380, rl-loss: 134.4752960205078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6381, rl-loss: 1.2757518291473389\r",
      "INFO - Step 6382, rl-loss: 1.4295293092727661\r",
      "INFO - Step 6383, rl-loss: 336.7262268066406\r",
      "INFO - Step 6384, rl-loss: 726.9364013671875\r",
      "INFO - Step 6385, rl-loss: 1.456451654434204\r",
      "INFO - Step 6386, rl-loss: 4.363257884979248\r",
      "INFO - Step 6387, rl-loss: 1.4836468696594238\r",
      "INFO - Step 6388, rl-loss: 1.579106330871582\r",
      "INFO - Step 6389, rl-loss: 457.385498046875\r",
      "INFO - Step 6390, rl-loss: 436.69903564453125\r",
      "INFO - Step 6391, rl-loss: 449.6005859375\r",
      "INFO - Step 6392, rl-loss: 93.01107788085938\r",
      "INFO - Step 6393, rl-loss: 49.20057678222656\r",
      "INFO - Step 6394, rl-loss: 60.25947570800781\r",
      "INFO - Step 6395, rl-loss: 93.06301879882812\r",
      "INFO - Step 6396, rl-loss: 6.406260013580322\r",
      "INFO - Step 6397, rl-loss: 437.42413330078125\r",
      "INFO - Step 6398, rl-loss: 153.75537109375\r",
      "INFO - Step 6399, rl-loss: 861.6441650390625\r",
      "INFO - Step 6400, rl-loss: 0.9295198917388916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6401, rl-loss: 192.43463134765625\r",
      "INFO - Step 6402, rl-loss: 256.6582946777344\r",
      "INFO - Step 6403, rl-loss: 52.39053726196289\r",
      "INFO - Step 6404, rl-loss: 245.25582885742188\r",
      "INFO - Step 6405, rl-loss: 216.4085693359375\r",
      "INFO - Step 6406, rl-loss: 0.9335654377937317\r",
      "INFO - Step 6407, rl-loss: 67.0457992553711\r",
      "INFO - Step 6408, rl-loss: 62.497474670410156\r",
      "INFO - Step 6409, rl-loss: 1.106952428817749\r",
      "INFO - Step 6410, rl-loss: 104.4066162109375\r",
      "INFO - Step 6411, rl-loss: 220.5325469970703\r",
      "INFO - Step 6412, rl-loss: 1.0563756227493286\r",
      "INFO - Step 6413, rl-loss: 215.01402282714844\r",
      "INFO - Step 6414, rl-loss: 284.8690490722656\r",
      "INFO - Step 6415, rl-loss: 208.36219787597656\r",
      "INFO - Step 6416, rl-loss: 1.1940701007843018\r",
      "INFO - Step 6417, rl-loss: 162.88133239746094\r",
      "INFO - Step 6418, rl-loss: 0.690933883190155\r",
      "INFO - Step 6419, rl-loss: 0.953745424747467\r",
      "INFO - Step 6420, rl-loss: 888.0491333007812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6421, rl-loss: 118.67839050292969\r",
      "INFO - Step 6422, rl-loss: 195.75363159179688\r",
      "INFO - Step 6423, rl-loss: 0.874691367149353\r",
      "INFO - Step 6424, rl-loss: 584.5333251953125\r",
      "INFO - Step 6425, rl-loss: 155.7750244140625\r",
      "INFO - Step 6426, rl-loss: 175.746826171875\r",
      "INFO - Step 6427, rl-loss: 221.33609008789062\r",
      "INFO - Step 6428, rl-loss: 304.74322509765625\r",
      "INFO - Step 6429, rl-loss: 59.22531509399414\r",
      "INFO - Step 6430, rl-loss: 383.9084167480469\r",
      "INFO - Step 6431, rl-loss: 165.46163940429688\r",
      "INFO - Step 6432, rl-loss: 344.57672119140625\r",
      "INFO - Step 6433, rl-loss: 1.438166856765747\r",
      "INFO - Step 6434, rl-loss: 1.5823101997375488\r",
      "INFO - Step 6435, rl-loss: 71.703857421875\r",
      "INFO - Step 6436, rl-loss: 443.2422790527344\r",
      "INFO - Step 6437, rl-loss: 1.71330726146698\r",
      "INFO - Step 6438, rl-loss: 469.998046875\r",
      "INFO - Step 6439, rl-loss: 1.077453374862671\r",
      "INFO - Step 6440, rl-loss: 141.6561737060547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6441, rl-loss: 1.4379494190216064\r",
      "INFO - Step 6442, rl-loss: 1.2611881494522095\r",
      "INFO - Step 6443, rl-loss: 228.98199462890625\r",
      "INFO - Step 6444, rl-loss: 90.18307495117188\r",
      "INFO - Step 6445, rl-loss: 111.9472885131836\r",
      "INFO - Step 6446, rl-loss: 402.0528869628906\r",
      "INFO - Step 6447, rl-loss: 917.7978515625\r",
      "INFO - Step 6448, rl-loss: 100.87115478515625\r",
      "INFO - Step 6449, rl-loss: 486.15338134765625\r",
      "INFO - Step 6450, rl-loss: 228.23350524902344\r",
      "INFO - Step 6451, rl-loss: 483.61932373046875\r",
      "INFO - Step 6452, rl-loss: 142.16897583007812\r",
      "INFO - Step 6453, rl-loss: 1.4366464614868164\r",
      "INFO - Step 6454, rl-loss: 149.08135986328125\r",
      "INFO - Step 6455, rl-loss: 278.9571838378906\r",
      "INFO - Step 6456, rl-loss: 267.6195373535156\r",
      "INFO - Step 6457, rl-loss: 408.29547119140625\r",
      "INFO - Step 6458, rl-loss: 115.17300415039062\r",
      "INFO - Step 6459, rl-loss: 94.69902801513672\r",
      "INFO - Step 6460, rl-loss: 2.96659779548645"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6461, rl-loss: 311.9813232421875\r",
      "INFO - Step 6462, rl-loss: 115.27229309082031\r",
      "INFO - Step 6463, rl-loss: 111.55337524414062\r",
      "INFO - Step 6464, rl-loss: 0.9522966742515564\r",
      "INFO - Step 6465, rl-loss: 280.68865966796875\r",
      "INFO - Step 6466, rl-loss: 623.956787109375\r",
      "INFO - Step 6467, rl-loss: 65.8924560546875\r",
      "INFO - Step 6468, rl-loss: 1.389682412147522\r",
      "INFO - Step 6469, rl-loss: 94.93456268310547\r",
      "INFO - Step 6470, rl-loss: 65.58050537109375\r",
      "INFO - Step 6471, rl-loss: 674.178466796875\r",
      "INFO - Step 6472, rl-loss: 202.332275390625\r",
      "INFO - Step 6473, rl-loss: 190.75802612304688\r",
      "INFO - Step 6474, rl-loss: 84.91094207763672\r",
      "INFO - Step 6475, rl-loss: 161.2915802001953\r",
      "INFO - Step 6476, rl-loss: 178.8326873779297\r",
      "INFO - Step 6477, rl-loss: 1.3792879581451416\r",
      "INFO - Step 6478, rl-loss: 1.9786789417266846\r",
      "INFO - Step 6479, rl-loss: 1.8320109844207764\r",
      "INFO - Step 6480, rl-loss: 138.93890380859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6481, rl-loss: 148.65509033203125\r",
      "INFO - Step 6482, rl-loss: 151.068359375\r",
      "INFO - Step 6483, rl-loss: 140.965576171875\r",
      "INFO - Step 6484, rl-loss: 2.1425647735595703\r",
      "INFO - Step 6485, rl-loss: 26.1552734375\r",
      "INFO - Step 6486, rl-loss: 105.11579895019531\r",
      "INFO - Step 6487, rl-loss: 326.3564453125\r",
      "INFO - Step 6488, rl-loss: 418.52154541015625\r",
      "INFO - Step 6489, rl-loss: 488.2825012207031\r",
      "INFO - Step 6490, rl-loss: 1.7195093631744385\r",
      "INFO - Step 6491, rl-loss: 1.0153472423553467\r",
      "INFO - Step 6492, rl-loss: 106.68243408203125\r",
      "INFO - Step 6493, rl-loss: 368.680419921875\r",
      "INFO - Step 6494, rl-loss: 205.77877807617188\r",
      "INFO - Step 6495, rl-loss: 258.44842529296875\r",
      "INFO - Step 6496, rl-loss: 1.4092119932174683\r",
      "INFO - Step 6497, rl-loss: 6.875339031219482\r",
      "INFO - Step 6498, rl-loss: 1.5362911224365234\r",
      "INFO - Step 6499, rl-loss: 137.6716766357422\r",
      "INFO - Step 6500, rl-loss: 171.010498046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6501, rl-loss: 318.077880859375\r",
      "INFO - Step 6502, rl-loss: 242.55117797851562\r",
      "INFO - Step 6503, rl-loss: 457.37890625\r",
      "INFO - Step 6504, rl-loss: 1.3079584836959839\r",
      "INFO - Step 6505, rl-loss: 36.79235076904297\r",
      "INFO - Step 6506, rl-loss: 1.2069751024246216\r",
      "INFO - Step 6507, rl-loss: 1.1716731786727905\r",
      "INFO - Step 6508, rl-loss: 1.1351865530014038\r",
      "INFO - Step 6509, rl-loss: 195.53582763671875\r",
      "INFO - Step 6510, rl-loss: 265.57440185546875\r",
      "INFO - Step 6511, rl-loss: 1.0424097776412964\r",
      "INFO - Step 6512, rl-loss: 1.169935703277588\r",
      "INFO - Step 6513, rl-loss: 1.6122090816497803\r",
      "INFO - Step 6514, rl-loss: 338.6245422363281\r",
      "INFO - Step 6515, rl-loss: 206.8834991455078\r",
      "INFO - Step 6516, rl-loss: 349.08673095703125\r",
      "INFO - Step 6517, rl-loss: 44.80424880981445\r",
      "INFO - Step 6518, rl-loss: 1.1968932151794434\r",
      "INFO - Step 6519, rl-loss: 119.8351821899414\r",
      "INFO - Step 6520, rl-loss: 80.36067962646484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6521, rl-loss: 550.564453125\r",
      "INFO - Step 6522, rl-loss: 270.91644287109375\r",
      "INFO - Step 6523, rl-loss: 213.46200561523438\r",
      "INFO - Step 6524, rl-loss: 52.390865325927734\r",
      "INFO - Step 6525, rl-loss: 398.8905029296875\r",
      "INFO - Step 6526, rl-loss: 91.63201904296875\r",
      "INFO - Step 6527, rl-loss: 104.4645004272461\r",
      "INFO - Step 6528, rl-loss: 488.08074951171875\r",
      "INFO - Step 6529, rl-loss: 312.8555603027344\r",
      "INFO - Step 6530, rl-loss: 0.9985624551773071\r",
      "INFO - Step 6531, rl-loss: 403.46087646484375\r",
      "INFO - Step 6532, rl-loss: 81.09595489501953\r",
      "INFO - Step 6533, rl-loss: 69.69283294677734\r",
      "INFO - Step 6534, rl-loss: 56.4307975769043\r",
      "INFO - Step 6535, rl-loss: 0.8557175397872925\r",
      "INFO - Step 6536, rl-loss: 320.4864196777344\r",
      "INFO - Step 6537, rl-loss: 1.622928261756897\r",
      "INFO - Step 6538, rl-loss: 0.9480476379394531\r",
      "INFO - Step 6539, rl-loss: 254.56207275390625\r",
      "INFO - Step 6540, rl-loss: 0.6313679218292236"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6541, rl-loss: 134.05438232421875\r",
      "INFO - Step 6542, rl-loss: 1.3253543376922607\r",
      "INFO - Step 6543, rl-loss: 169.2559051513672\r",
      "INFO - Step 6544, rl-loss: 106.8512954711914\r",
      "INFO - Step 6545, rl-loss: 44.41542053222656\r",
      "INFO - Step 6546, rl-loss: 780.1771240234375\r",
      "INFO - Step 6547, rl-loss: 57.753265380859375\r",
      "INFO - Step 6548, rl-loss: 241.15695190429688\r",
      "INFO - Step 6549, rl-loss: 1.6553728580474854\r",
      "INFO - Step 6550, rl-loss: 1.1213886737823486\r",
      "INFO - Step 6551, rl-loss: 265.50823974609375\r",
      "INFO - Step 6552, rl-loss: 191.1037139892578\r",
      "INFO - Step 6553, rl-loss: 0.8584624528884888\r",
      "INFO - Step 6554, rl-loss: 1.205748438835144\r",
      "INFO - Step 6555, rl-loss: 1.0471014976501465\r",
      "INFO - Step 6556, rl-loss: 134.39935302734375\r",
      "INFO - Step 6557, rl-loss: 219.11093139648438\r",
      "INFO - Step 6558, rl-loss: 257.0738220214844\r",
      "INFO - Step 6559, rl-loss: 174.22152709960938\r",
      "INFO - Step 6560, rl-loss: 151.4046630859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6561, rl-loss: 730.055419921875\r",
      "INFO - Step 6562, rl-loss: 168.75502014160156\r",
      "INFO - Step 6563, rl-loss: 3.227020740509033\r",
      "INFO - Step 6564, rl-loss: 430.54449462890625\r",
      "INFO - Step 6565, rl-loss: 36.877708435058594\r",
      "INFO - Step 6566, rl-loss: 1.0772333145141602\r",
      "INFO - Step 6567, rl-loss: 234.47802734375\r",
      "INFO - Step 6568, rl-loss: 35.71053695678711\r",
      "INFO - Step 6569, rl-loss: 70.38030242919922\r",
      "INFO - Step 6570, rl-loss: 515.3029174804688\r",
      "INFO - Step 6571, rl-loss: 300.2109069824219\r",
      "INFO - Step 6572, rl-loss: 69.31950378417969\r",
      "INFO - Step 6573, rl-loss: 305.07012939453125\r",
      "INFO - Step 6574, rl-loss: 175.1976776123047\r",
      "INFO - Step 6575, rl-loss: 127.71892547607422\r",
      "INFO - Step 6576, rl-loss: 0.8191468119621277\r",
      "INFO - Step 6577, rl-loss: 0.9586228132247925\r",
      "INFO - Step 6578, rl-loss: 26.80353546142578\r",
      "INFO - Step 6579, rl-loss: 302.3127136230469\r",
      "INFO - Step 6580, rl-loss: 1.362185001373291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6581, rl-loss: 49.527191162109375\r",
      "INFO - Step 6582, rl-loss: 129.841552734375\r",
      "INFO - Step 6583, rl-loss: 185.40167236328125\r",
      "INFO - Step 6584, rl-loss: 32.42957305908203\r",
      "INFO - Step 6585, rl-loss: 0.8831850290298462\r",
      "INFO - Step 6586, rl-loss: 167.49002075195312\r",
      "INFO - Step 6587, rl-loss: 304.57666015625\r",
      "INFO - Step 6588, rl-loss: 100.49761962890625\r",
      "INFO - Step 6589, rl-loss: 455.4352722167969\r",
      "INFO - Step 6590, rl-loss: 0.9237833023071289\r",
      "INFO - Step 6591, rl-loss: 403.003173828125\r",
      "INFO - Step 6592, rl-loss: 64.58899688720703\r",
      "INFO - Step 6593, rl-loss: 149.78591918945312\r",
      "INFO - Step 6594, rl-loss: 62.07561111450195\r",
      "INFO - Step 6595, rl-loss: 287.4796142578125\r",
      "INFO - Step 6596, rl-loss: 165.17584228515625\r",
      "INFO - Step 6597, rl-loss: 555.7821655273438\r",
      "INFO - Step 6598, rl-loss: 100.5741958618164\r",
      "INFO - Step 6599, rl-loss: 92.52706909179688\r",
      "INFO - Step 6600, rl-loss: 89.53327941894531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6601, rl-loss: 298.02508544921875\r",
      "INFO - Step 6602, rl-loss: 404.0646057128906\r",
      "INFO - Step 6603, rl-loss: 0.5418280363082886\r",
      "INFO - Step 6604, rl-loss: 1.5358266830444336\r",
      "INFO - Step 6605, rl-loss: 1.33316969871521\r",
      "INFO - Step 6606, rl-loss: 395.66656494140625\r",
      "INFO - Step 6607, rl-loss: 0.70708167552948\r",
      "INFO - Step 6608, rl-loss: 111.88860321044922\r",
      "INFO - Step 6609, rl-loss: 30.56641387939453\r",
      "INFO - Step 6610, rl-loss: 1.4470915794372559\r",
      "INFO - Step 6611, rl-loss: 220.8821258544922\r",
      "INFO - Step 6612, rl-loss: 49.61531448364258\r",
      "INFO - Step 6613, rl-loss: 41.99420928955078\r",
      "INFO - Step 6614, rl-loss: 0.9168941974639893\r",
      "INFO - Step 6615, rl-loss: 194.57635498046875\r",
      "INFO - Step 6616, rl-loss: 111.19230651855469\r",
      "INFO - Step 6617, rl-loss: 157.83538818359375\r",
      "INFO - Step 6618, rl-loss: 0.9045306444168091\r",
      "INFO - Step 6619, rl-loss: 0.8540488481521606\r",
      "INFO - Step 6620, rl-loss: 114.07624816894531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6621, rl-loss: 337.62689208984375\r",
      "INFO - Step 6622, rl-loss: 325.7678527832031\r",
      "INFO - Step 6623, rl-loss: 532.8146362304688\r",
      "INFO - Step 6624, rl-loss: 2.1866743564605713\r",
      "INFO - Step 6625, rl-loss: 50.0716552734375\r",
      "INFO - Step 6626, rl-loss: 45.40019226074219\r",
      "INFO - Step 6627, rl-loss: 1.7099015712738037\r",
      "INFO - Step 6628, rl-loss: 599.2938232421875\r",
      "INFO - Step 6629, rl-loss: 693.4878540039062\r",
      "INFO - Step 6630, rl-loss: 1.7483357191085815\r",
      "INFO - Step 6631, rl-loss: 1.1835168600082397\r",
      "INFO - Step 6632, rl-loss: 132.00001525878906\r",
      "INFO - Step 6633, rl-loss: 69.42926788330078\r",
      "INFO - Step 6634, rl-loss: 65.23916625976562\r",
      "INFO - Step 6635, rl-loss: 1.3463895320892334\r",
      "INFO - Step 6636, rl-loss: 308.2508239746094\r",
      "INFO - Step 6637, rl-loss: 694.9210205078125\r",
      "INFO - Step 6638, rl-loss: 227.1944122314453\r",
      "INFO - Step 6639, rl-loss: 47.42687225341797\r",
      "INFO - Step 6640, rl-loss: 1.4477450847625732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6641, rl-loss: 1.1079823970794678\r",
      "INFO - Step 6642, rl-loss: 707.9232177734375\r",
      "INFO - Step 6643, rl-loss: 1.2910192012786865\r",
      "INFO - Step 6644, rl-loss: 100.26911163330078\r",
      "INFO - Step 6645, rl-loss: 475.94793701171875\r",
      "INFO - Step 6646, rl-loss: 72.062744140625\r",
      "INFO - Step 6647, rl-loss: 230.9051055908203\r",
      "INFO - Step 6648, rl-loss: 677.7135009765625\r",
      "INFO - Step 6649, rl-loss: 480.12066650390625\r",
      "INFO - Step 6650, rl-loss: 66.23490905761719\r",
      "INFO - Step 6651, rl-loss: 10.89702320098877\r",
      "INFO - Step 6652, rl-loss: 303.7974548339844\r",
      "INFO - Step 6653, rl-loss: 397.98211669921875\r",
      "INFO - Step 6654, rl-loss: 49.24429702758789\r",
      "INFO - Step 6655, rl-loss: 211.14801025390625\r",
      "INFO - Step 6656, rl-loss: 419.5461730957031\r",
      "INFO - Step 6657, rl-loss: 143.96905517578125\r",
      "INFO - Step 6658, rl-loss: 21.630613327026367\r",
      "INFO - Step 6659, rl-loss: 274.41119384765625\r",
      "INFO - Step 6660, rl-loss: 621.7640991210938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6661, rl-loss: 108.68644714355469\r",
      "INFO - Step 6662, rl-loss: 1.134443998336792\r",
      "INFO - Step 6663, rl-loss: 377.7656555175781\r",
      "INFO - Step 6664, rl-loss: 62.128578186035156\r",
      "INFO - Step 6665, rl-loss: 57.885169982910156\r",
      "INFO - Step 6666, rl-loss: 136.05982971191406\r",
      "INFO - Step 6667, rl-loss: 253.24551391601562\r",
      "INFO - Step 6668, rl-loss: 382.8813171386719\r",
      "INFO - Step 6669, rl-loss: 156.78477478027344\r",
      "INFO - Step 6670, rl-loss: 1.1147189140319824\r",
      "INFO - Step 6671, rl-loss: 1.35648775100708\r",
      "INFO - Step 6672, rl-loss: 507.5150451660156\r",
      "INFO - Step 6673, rl-loss: 122.52047729492188\r",
      "INFO - Step 6674, rl-loss: 113.11914825439453\r",
      "INFO - Step 6675, rl-loss: 0.6940619945526123\r",
      "INFO - Step 6676, rl-loss: 35.98638153076172\r",
      "INFO - Step 6677, rl-loss: 199.57876586914062\r",
      "INFO - Step 6678, rl-loss: 81.62472534179688\r",
      "INFO - Step 6679, rl-loss: 125.27859497070312\r",
      "INFO - Step 6680, rl-loss: 1.1067020893096924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6681, rl-loss: 165.71119689941406\r",
      "INFO - Step 6682, rl-loss: 187.99075317382812\r",
      "INFO - Step 6683, rl-loss: 144.07212829589844\r",
      "INFO - Step 6684, rl-loss: 320.962646484375\r",
      "INFO - Step 6685, rl-loss: 1.6633411645889282\r",
      "INFO - Step 6686, rl-loss: 151.3816680908203\r",
      "INFO - Step 6687, rl-loss: 373.3616027832031\r",
      "INFO - Step 6688, rl-loss: 243.9534912109375\r",
      "INFO - Step 6689, rl-loss: 485.4847106933594\r",
      "INFO - Step 6690, rl-loss: 473.9417724609375\r",
      "INFO - Step 6691, rl-loss: 384.63360595703125\r",
      "INFO - Step 6692, rl-loss: 271.3375244140625\r",
      "INFO - Step 6693, rl-loss: 0.6988282799720764\r",
      "INFO - Step 6694, rl-loss: 279.70526123046875\r",
      "INFO - Step 6695, rl-loss: 726.3768310546875\r",
      "INFO - Step 6696, rl-loss: 269.888671875\r",
      "INFO - Step 6697, rl-loss: 270.0306396484375\r",
      "INFO - Step 6698, rl-loss: 266.0308837890625\r",
      "INFO - Step 6699, rl-loss: 237.93804931640625\r",
      "INFO - Step 6700, rl-loss: 145.84654235839844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6701, rl-loss: 721.182861328125\r",
      "INFO - Step 6702, rl-loss: 34.49081802368164\r",
      "INFO - Step 6703, rl-loss: 141.8963165283203\r",
      "INFO - Step 6704, rl-loss: 68.12161254882812\r",
      "INFO - Step 6705, rl-loss: 896.353515625\r",
      "INFO - Step 6706, rl-loss: 295.91192626953125\r",
      "INFO - Step 6707, rl-loss: 1.0288071632385254\r",
      "INFO - Step 6708, rl-loss: 167.59532165527344\r",
      "INFO - Step 6709, rl-loss: 1.4139809608459473\r",
      "INFO - Step 6710, rl-loss: 0.975700855255127\r",
      "INFO - Step 6711, rl-loss: 460.00750732421875\r",
      "INFO - Step 6712, rl-loss: 66.07980346679688\r",
      "INFO - Step 6713, rl-loss: 345.7794189453125\r",
      "INFO - Step 6714, rl-loss: 617.1278686523438\r",
      "INFO - Step 6715, rl-loss: 0.9690266847610474\r",
      "INFO - Step 6716, rl-loss: 151.8939208984375\r",
      "INFO - Step 6717, rl-loss: 59.016357421875\r",
      "INFO - Step 6718, rl-loss: 1.188309907913208\r",
      "INFO - Step 6719, rl-loss: 350.99725341796875\r",
      "INFO - Step 6720, rl-loss: 514.4609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6721, rl-loss: 65.81720733642578\r",
      "INFO - Step 6722, rl-loss: 198.21871948242188\r",
      "INFO - Step 6723, rl-loss: 32.978919982910156\r",
      "INFO - Step 6724, rl-loss: 268.7252197265625\r",
      "INFO - Step 6725, rl-loss: 1.2626564502716064\r",
      "INFO - Step 6726, rl-loss: 402.73992919921875\r",
      "INFO - Step 6727, rl-loss: 188.48007202148438\r",
      "INFO - Step 6728, rl-loss: 286.8671569824219\r",
      "INFO - Step 6729, rl-loss: 330.26934814453125\r",
      "INFO - Step 6730, rl-loss: 427.724609375\r",
      "INFO - Step 6731, rl-loss: 1.388053297996521\r",
      "INFO - Step 6732, rl-loss: 51.31745910644531\r",
      "INFO - Step 6733, rl-loss: 129.10601806640625\r",
      "INFO - Step 6734, rl-loss: 22.168733596801758\r",
      "INFO - Step 6735, rl-loss: 1.5064826011657715\r",
      "INFO - Step 6736, rl-loss: 1.5629373788833618\r",
      "INFO - Step 6737, rl-loss: 321.04632568359375\r",
      "INFO - Step 6738, rl-loss: 78.04383087158203\r",
      "INFO - Step 6739, rl-loss: 638.5353393554688\r",
      "INFO - Step 6740, rl-loss: 229.17218017578125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6741, rl-loss: 333.3053894042969\r",
      "INFO - Step 6742, rl-loss: 107.57366943359375\r",
      "INFO - Step 6743, rl-loss: 21.840538024902344\r",
      "INFO - Step 6744, rl-loss: 269.71624755859375\r",
      "INFO - Step 6745, rl-loss: 543.8369750976562\r",
      "INFO - Step 6746, rl-loss: 0.9793976545333862\r",
      "INFO - Step 6747, rl-loss: 0.9991190433502197\r",
      "INFO - Step 6748, rl-loss: 42.09626007080078\r",
      "INFO - Step 6749, rl-loss: 15.18986988067627\r",
      "INFO - Step 6750, rl-loss: 23.3690185546875\r",
      "INFO - Step 6751, rl-loss: 761.4086303710938\r",
      "INFO - Step 6752, rl-loss: 128.1855926513672\r",
      "INFO - Step 6753, rl-loss: 61.1795654296875\r",
      "INFO - Step 6754, rl-loss: 0.8685119152069092\r",
      "INFO - Step 6755, rl-loss: 405.0756530761719\r",
      "INFO - Step 6756, rl-loss: 238.16757202148438\r",
      "INFO - Step 6757, rl-loss: 242.32362365722656\r",
      "INFO - Step 6758, rl-loss: 206.92111206054688\r",
      "INFO - Step 6759, rl-loss: 193.37953186035156\r",
      "INFO - Step 6760, rl-loss: 161.51409912109375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6761, rl-loss: 404.9328308105469\r",
      "INFO - Step 6762, rl-loss: 255.32650756835938\r",
      "INFO - Step 6763, rl-loss: 279.2799072265625\r",
      "INFO - Step 6764, rl-loss: 1.6448537111282349\r",
      "INFO - Step 6765, rl-loss: 239.65151977539062\r",
      "INFO - Step 6766, rl-loss: 141.25241088867188\r",
      "INFO - Step 6767, rl-loss: 236.15896606445312\r",
      "INFO - Step 6768, rl-loss: 144.21661376953125\r",
      "INFO - Step 6769, rl-loss: 278.26275634765625\r",
      "INFO - Step 6770, rl-loss: 0.9925034642219543\r",
      "INFO - Step 6771, rl-loss: 85.94135284423828\r",
      "INFO - Step 6772, rl-loss: 0.9918527007102966\r",
      "INFO - Step 6773, rl-loss: 303.46563720703125\r",
      "INFO - Step 6774, rl-loss: 137.03671264648438\r",
      "INFO - Step 6775, rl-loss: 103.74475860595703\r",
      "INFO - Step 6776, rl-loss: 44.9803352355957\r",
      "INFO - Step 6777, rl-loss: 359.2182312011719\r",
      "INFO - Step 6778, rl-loss: 439.71441650390625\r",
      "INFO - Step 6779, rl-loss: 102.73345947265625\r",
      "INFO - Step 6780, rl-loss: 48.61122131347656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6781, rl-loss: 106.8881607055664\r",
      "INFO - Step 6782, rl-loss: 43.46873474121094\r",
      "INFO - Step 6783, rl-loss: 51.642879486083984\r",
      "INFO - Step 6784, rl-loss: 205.0698699951172\r",
      "INFO - Step 6785, rl-loss: 165.37210083007812\r",
      "INFO - Step 6786, rl-loss: 1.3165600299835205\r",
      "INFO - Step 6787, rl-loss: 1.3976528644561768\r",
      "INFO - Step 6788, rl-loss: 135.6219940185547\r",
      "INFO - Step 6789, rl-loss: 147.99664306640625\r",
      "INFO - Step 6790, rl-loss: 220.6606903076172\r",
      "INFO - Step 6791, rl-loss: 301.7767333984375\r",
      "INFO - Step 6792, rl-loss: 90.4791030883789\r",
      "INFO - Step 6793, rl-loss: 1.3516892194747925\r",
      "INFO - Step 6794, rl-loss: 165.9973602294922\r",
      "INFO - Step 6795, rl-loss: 10.198260307312012\r",
      "INFO - Step 6796, rl-loss: 0.7190437316894531\r",
      "INFO - Step 6797, rl-loss: 300.91827392578125\r",
      "INFO - Step 6798, rl-loss: 1.4985920190811157\r",
      "INFO - Step 6799, rl-loss: 1.0010404586791992\r",
      "INFO - Step 6800, rl-loss: 239.2951202392578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6801, rl-loss: 1.1228435039520264\r",
      "INFO - Step 6802, rl-loss: 408.37109375\r",
      "INFO - Step 6803, rl-loss: 436.45062255859375\r",
      "INFO - Step 6804, rl-loss: 35.79791259765625\r",
      "INFO - Step 6805, rl-loss: 1.5242347717285156\r",
      "INFO - Step 6806, rl-loss: 240.60101318359375\r",
      "INFO - Step 6807, rl-loss: 1.5749671459197998\r",
      "INFO - Step 6808, rl-loss: 42.58120346069336\r",
      "INFO - Step 6809, rl-loss: 594.34912109375\r",
      "INFO - Step 6810, rl-loss: 522.9942626953125\r",
      "INFO - Step 6811, rl-loss: 149.25010681152344\r",
      "INFO - Step 6812, rl-loss: 939.6181640625\r",
      "INFO - Step 6813, rl-loss: 0.8396400213241577\r",
      "INFO - Step 6814, rl-loss: 0.9920660257339478\r",
      "INFO - Step 6815, rl-loss: 299.6077880859375\r",
      "INFO - Step 6816, rl-loss: 68.88814544677734\r",
      "INFO - Step 6817, rl-loss: 74.54624938964844\r",
      "INFO - Step 6818, rl-loss: 20.55670738220215\r",
      "INFO - Step 6819, rl-loss: 342.15399169921875\r",
      "INFO - Step 6820, rl-loss: 106.6438980102539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6821, rl-loss: 199.71282958984375\r",
      "INFO - Step 6822, rl-loss: 138.88075256347656\r",
      "INFO - Step 6823, rl-loss: 206.54359436035156\r",
      "INFO - Step 6824, rl-loss: 1.3487590551376343\r",
      "INFO - Step 6825, rl-loss: 131.68661499023438\r",
      "INFO - Step 6826, rl-loss: 206.08885192871094\r",
      "INFO - Step 6827, rl-loss: 21.6691951751709\r",
      "INFO - Step 6828, rl-loss: 61.554954528808594\r",
      "INFO - Step 6829, rl-loss: 0.739769697189331\r",
      "INFO - Step 6830, rl-loss: 376.8071594238281\r",
      "INFO - Step 6831, rl-loss: 339.27301025390625\r",
      "INFO - Step 6832, rl-loss: 306.44183349609375\r",
      "INFO - Step 6833, rl-loss: 521.092529296875\r",
      "INFO - Step 6834, rl-loss: 148.15773010253906\r",
      "INFO - Step 6835, rl-loss: 149.70860290527344\r",
      "INFO - Step 6836, rl-loss: 160.63375854492188\r",
      "INFO - Step 6837, rl-loss: 413.26239013671875\r",
      "INFO - Step 6838, rl-loss: 1.257851481437683\r",
      "INFO - Step 6839, rl-loss: 267.0833740234375\r",
      "INFO - Step 6840, rl-loss: 1.6726198196411133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6841, rl-loss: 267.1681823730469\r",
      "INFO - Step 6842, rl-loss: 218.662841796875\r",
      "INFO - Step 6843, rl-loss: 376.03485107421875\r",
      "INFO - Step 6844, rl-loss: 9.547945976257324\r",
      "INFO - Step 6845, rl-loss: 387.9646301269531\r",
      "INFO - Step 6846, rl-loss: 116.75277709960938\r",
      "INFO - Step 6847, rl-loss: 162.08738708496094\r",
      "INFO - Step 6848, rl-loss: 145.5393524169922\r",
      "INFO - Step 6849, rl-loss: 79.17994689941406\r",
      "INFO - Step 6850, rl-loss: 128.361572265625\r",
      "INFO - Step 6851, rl-loss: 1.591676115989685\r",
      "INFO - Step 6852, rl-loss: 460.4443664550781\r",
      "INFO - Step 6853, rl-loss: 362.7867431640625\r",
      "INFO - Step 6854, rl-loss: 282.91827392578125\r",
      "INFO - Step 6855, rl-loss: 31.12579917907715\r",
      "INFO - Step 6856, rl-loss: 354.5418701171875\r",
      "INFO - Step 6857, rl-loss: 1.836625337600708\r",
      "INFO - Step 6858, rl-loss: 197.939208984375\r",
      "INFO - Step 6859, rl-loss: 1.2169456481933594\r",
      "INFO - Step 6860, rl-loss: 50.37966537475586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6861, rl-loss: 1.4295567274093628\r",
      "INFO - Step 6862, rl-loss: 120.13861083984375\r",
      "INFO - Step 6863, rl-loss: 0.8674799203872681\r",
      "INFO - Step 6864, rl-loss: 113.4572525024414\r",
      "INFO - Step 6865, rl-loss: 0.784349262714386\r",
      "INFO - Step 6866, rl-loss: 1.2139335870742798\r",
      "INFO - Step 6867, rl-loss: 162.75389099121094\r",
      "INFO - Step 6868, rl-loss: 88.06166076660156\r",
      "INFO - Step 6869, rl-loss: 174.04185485839844\r",
      "INFO - Step 6870, rl-loss: 199.76022338867188\r",
      "INFO - Step 6871, rl-loss: 25.57175064086914\r",
      "INFO - Step 6872, rl-loss: 27.317386627197266\r",
      "INFO - Step 6873, rl-loss: 143.07249450683594\r",
      "INFO - Step 6874, rl-loss: 81.74746704101562\r",
      "INFO - Step 6875, rl-loss: 210.08169555664062\r",
      "INFO - Step 6876, rl-loss: 480.4576416015625\r",
      "INFO - Step 6877, rl-loss: 351.4924621582031\r",
      "INFO - Step 6878, rl-loss: 225.76246643066406\r",
      "INFO - Step 6879, rl-loss: 29.116724014282227\r",
      "INFO - Step 6880, rl-loss: 1.371207356452942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6881, rl-loss: 1.6161950826644897\r",
      "INFO - Step 6882, rl-loss: 203.6255340576172\r",
      "INFO - Step 6883, rl-loss: 127.3560562133789\r",
      "INFO - Step 6884, rl-loss: 0.7397634983062744\r",
      "INFO - Step 6885, rl-loss: 29.32259178161621\r",
      "INFO - Step 6886, rl-loss: 411.0489501953125\r",
      "INFO - Step 6887, rl-loss: 183.2792510986328\r",
      "INFO - Step 6888, rl-loss: 156.50241088867188\r",
      "INFO - Step 6889, rl-loss: 5.756906509399414\r",
      "INFO - Step 6890, rl-loss: 121.00965881347656\r",
      "INFO - Step 6891, rl-loss: 267.83282470703125\r",
      "INFO - Step 6892, rl-loss: 393.8935546875\r",
      "INFO - Step 6893, rl-loss: 1.7084946632385254\r",
      "INFO - Step 6894, rl-loss: 51.33844757080078\r",
      "INFO - Step 6895, rl-loss: 668.6779174804688\r",
      "INFO - Step 6896, rl-loss: 198.83047485351562\r",
      "INFO - Step 6897, rl-loss: 162.19696044921875\r",
      "INFO - Step 6898, rl-loss: 117.89291381835938\r",
      "INFO - Step 6899, rl-loss: 429.6169128417969\r",
      "INFO - Step 6900, rl-loss: 118.7294692993164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6901, rl-loss: 0.9157456159591675\r",
      "INFO - Step 6902, rl-loss: 126.83971405029297\r",
      "INFO - Step 6903, rl-loss: 384.9590148925781\r",
      "INFO - Step 6904, rl-loss: 1.7002880573272705\r",
      "INFO - Step 6905, rl-loss: 1.5149034261703491\r",
      "INFO - Step 6906, rl-loss: 51.5142822265625\r",
      "INFO - Step 6907, rl-loss: 445.6602783203125\r",
      "INFO - Step 6908, rl-loss: 148.86968994140625\r",
      "INFO - Step 6909, rl-loss: 305.8212585449219\r",
      "INFO - Step 6910, rl-loss: 10.448163986206055\r",
      "INFO - Step 6911, rl-loss: 190.97604370117188\r",
      "INFO - Step 6912, rl-loss: 48.1916618347168\r",
      "INFO - Step 6913, rl-loss: 180.32601928710938\r",
      "INFO - Step 6914, rl-loss: 68.48124694824219\r",
      "INFO - Step 6915, rl-loss: 43.810184478759766\r",
      "INFO - Step 6916, rl-loss: 148.72402954101562\r",
      "INFO - Step 6917, rl-loss: 1.1041944026947021\r",
      "INFO - Step 6918, rl-loss: 1.211434006690979\r",
      "INFO - Step 6919, rl-loss: 106.11637878417969\r",
      "INFO - Step 6920, rl-loss: 717.201416015625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6921, rl-loss: 363.0677185058594\r",
      "INFO - Step 6922, rl-loss: 197.4080352783203\r",
      "INFO - Step 6923, rl-loss: 624.5670776367188\r",
      "INFO - Step 6924, rl-loss: 2.005708932876587\r",
      "INFO - Step 6925, rl-loss: 22.075510025024414\r",
      "INFO - Step 6926, rl-loss: 364.6065368652344\r",
      "INFO - Step 6927, rl-loss: 98.28471374511719\r",
      "INFO - Step 6928, rl-loss: 43.23263931274414\r",
      "INFO - Step 6929, rl-loss: 1.538643717765808\r",
      "INFO - Step 6930, rl-loss: 1.4030778408050537\r",
      "INFO - Step 6931, rl-loss: 95.24056243896484\r",
      "INFO - Step 6932, rl-loss: 480.1405029296875\r",
      "INFO - Step 6933, rl-loss: 124.53297424316406\r",
      "INFO - Step 6934, rl-loss: 386.1034240722656\r",
      "INFO - Step 6935, rl-loss: 75.0896987915039\r",
      "INFO - Step 6936, rl-loss: 59.888450622558594\r",
      "INFO - Step 6937, rl-loss: 1.9780495166778564\r",
      "INFO - Step 6938, rl-loss: 189.49636840820312\r",
      "INFO - Step 6939, rl-loss: 321.1479187011719\r",
      "INFO - Step 6940, rl-loss: 108.32730102539062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6941, rl-loss: 2.510500431060791\r",
      "INFO - Step 6942, rl-loss: 85.63178253173828\r",
      "INFO - Step 6943, rl-loss: 321.3733825683594\r",
      "INFO - Step 6944, rl-loss: 128.02377319335938\r",
      "INFO - Step 6945, rl-loss: 853.3924560546875\r",
      "INFO - Step 6946, rl-loss: 203.9827880859375\r",
      "INFO - Step 6947, rl-loss: 64.19441223144531\r",
      "INFO - Step 6948, rl-loss: 371.173583984375\r",
      "INFO - Step 6949, rl-loss: 362.3569641113281\r",
      "INFO - Step 6950, rl-loss: 377.143798828125\r",
      "INFO - Step 6951, rl-loss: 98.76512145996094\r",
      "INFO - Step 6952, rl-loss: 25.28827476501465\r",
      "INFO - Step 6953, rl-loss: 333.36199951171875\r",
      "INFO - Step 6954, rl-loss: 593.9813842773438\r",
      "INFO - Step 6955, rl-loss: 262.0797424316406\r",
      "INFO - Step 6956, rl-loss: 177.66397094726562\r",
      "INFO - Step 6957, rl-loss: 408.6637878417969\r",
      "INFO - Step 6958, rl-loss: 234.69815063476562\r",
      "INFO - Step 6959, rl-loss: 399.956298828125\r",
      "INFO - Step 6960, rl-loss: 148.21981811523438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6961, rl-loss: 64.75072479248047\r",
      "INFO - Step 6962, rl-loss: 161.62548828125\r",
      "INFO - Step 6963, rl-loss: 0.9298354387283325\r",
      "INFO - Step 6964, rl-loss: 410.4028015136719\r",
      "INFO - Step 6965, rl-loss: 78.01829528808594\r",
      "INFO - Step 6966, rl-loss: 98.72538757324219\r",
      "INFO - Step 6967, rl-loss: 1.076218843460083\r",
      "INFO - Step 6968, rl-loss: 130.70875549316406\r",
      "INFO - Step 6969, rl-loss: 1.321042537689209\r",
      "INFO - Step 6970, rl-loss: 594.5940551757812\r",
      "INFO - Step 6971, rl-loss: 61.96037673950195\r",
      "INFO - Step 6972, rl-loss: 273.879638671875\r",
      "INFO - Step 6973, rl-loss: 414.9971923828125\r",
      "INFO - Step 6974, rl-loss: 178.5287628173828\r",
      "INFO - Step 6975, rl-loss: 1.0723941326141357\r",
      "INFO - Step 6976, rl-loss: 174.4767303466797\r",
      "INFO - Step 6977, rl-loss: 396.84307861328125\r",
      "INFO - Step 6978, rl-loss: 71.2350082397461\r",
      "INFO - Step 6979, rl-loss: 116.32707214355469\r",
      "INFO - Step 6980, rl-loss: 192.95034790039062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 6981, rl-loss: 126.7627944946289\r",
      "INFO - Step 6982, rl-loss: 109.69001007080078\r",
      "INFO - Step 6983, rl-loss: 7.544240951538086\r",
      "INFO - Step 6984, rl-loss: 244.02252197265625\r",
      "INFO - Step 6985, rl-loss: 106.50236511230469\r",
      "INFO - Step 6986, rl-loss: 133.7406463623047\r",
      "INFO - Step 6987, rl-loss: 74.62420654296875\r",
      "INFO - Step 6988, rl-loss: 150.11785888671875\r",
      "INFO - Step 6989, rl-loss: 1.1162792444229126\r",
      "INFO - Step 6990, rl-loss: 81.8284683227539\r",
      "INFO - Step 6991, rl-loss: 1.2965760231018066\r",
      "INFO - Step 6992, rl-loss: 310.80914306640625\r",
      "INFO - Step 6993, rl-loss: 1.186309576034546\r",
      "INFO - Step 6994, rl-loss: 221.60073852539062\r",
      "INFO - Step 6995, rl-loss: 0.8952323198318481\r",
      "INFO - Step 6996, rl-loss: 142.99395751953125\r",
      "INFO - Step 6997, rl-loss: 963.3685913085938\r",
      "INFO - Step 6998, rl-loss: 305.2204895019531\r",
      "INFO - Step 6999, rl-loss: 816.4227294921875\r",
      "INFO - Step 7000, rl-loss: 0.7740038633346558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 7020, rl-loss: 1.0519444942474365\n",
      "----------------------------------------\n",
      "  timestep     |  555183\n",
      "  reward       |  56.24\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7021, rl-loss: 5.621275424957275\r",
      "INFO - Step 7022, rl-loss: 66.72335815429688\r",
      "INFO - Step 7023, rl-loss: 1.980591058731079\r",
      "INFO - Step 7024, rl-loss: 541.50830078125\r",
      "INFO - Step 7025, rl-loss: 252.68270874023438\r",
      "INFO - Step 7026, rl-loss: 614.1843872070312\r",
      "INFO - Step 7027, rl-loss: 156.4884796142578\r",
      "INFO - Step 7028, rl-loss: 73.81927490234375\r",
      "INFO - Step 7029, rl-loss: 1.7571969032287598\r",
      "INFO - Step 7030, rl-loss: 5.693602085113525\r",
      "INFO - Step 7031, rl-loss: 1441.112060546875\r",
      "INFO - Step 7032, rl-loss: 65.04051208496094\r",
      "INFO - Step 7033, rl-loss: 252.93936157226562\r",
      "INFO - Step 7034, rl-loss: 121.14869689941406\r",
      "INFO - Step 7035, rl-loss: 77.89215850830078\r",
      "INFO - Step 7036, rl-loss: 190.7487030029297\r",
      "INFO - Step 7037, rl-loss: 226.6259765625\r",
      "INFO - Step 7038, rl-loss: 380.768798828125\r",
      "INFO - Step 7039, rl-loss: 234.87982177734375\r",
      "INFO - Step 7040, rl-loss: 877.6808471679688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7041, rl-loss: 96.48828887939453\r",
      "INFO - Step 7042, rl-loss: 154.6832275390625\r",
      "INFO - Step 7043, rl-loss: 1.0555506944656372\r",
      "INFO - Step 7044, rl-loss: 1.3471580743789673\r",
      "INFO - Step 7045, rl-loss: 174.10496520996094\r",
      "INFO - Step 7046, rl-loss: 136.82699584960938\r",
      "INFO - Step 7047, rl-loss: 253.6334686279297\r",
      "INFO - Step 7048, rl-loss: 178.62037658691406\r",
      "INFO - Step 7049, rl-loss: 1.3234108686447144\r",
      "INFO - Step 7050, rl-loss: 544.4030151367188\r",
      "INFO - Step 7051, rl-loss: 529.22314453125\r",
      "INFO - Step 7052, rl-loss: 425.97808837890625\r",
      "INFO - Step 7053, rl-loss: 793.8834838867188\r",
      "INFO - Step 7054, rl-loss: 50.177772521972656\r",
      "INFO - Step 7055, rl-loss: 212.44049072265625\r",
      "INFO - Step 7056, rl-loss: 134.5538787841797\r",
      "INFO - Step 7057, rl-loss: 129.96414184570312\r",
      "INFO - Step 7058, rl-loss: 298.5585021972656\r",
      "INFO - Step 7059, rl-loss: 239.9730987548828\r",
      "INFO - Step 7060, rl-loss: 146.5699920654297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7061, rl-loss: 21.10835838317871\r",
      "INFO - Step 7062, rl-loss: 209.79095458984375\r",
      "INFO - Step 7063, rl-loss: 197.37767028808594\r",
      "INFO - Step 7064, rl-loss: 414.6159973144531\r",
      "INFO - Step 7065, rl-loss: 192.71119689941406\r",
      "INFO - Step 7066, rl-loss: 271.8255310058594\r",
      "INFO - Step 7067, rl-loss: 171.99525451660156\r",
      "INFO - Step 7068, rl-loss: 51.93992614746094\r",
      "INFO - Step 7069, rl-loss: 360.0621337890625\r",
      "INFO - Step 7070, rl-loss: 1.1555638313293457\r",
      "INFO - Step 7071, rl-loss: 201.55772399902344\r",
      "INFO - Step 7072, rl-loss: 537.8784790039062\r",
      "INFO - Step 7073, rl-loss: 637.7393188476562\r",
      "INFO - Step 7074, rl-loss: 148.5305938720703\r",
      "INFO - Step 7075, rl-loss: 74.53361511230469\r",
      "INFO - Step 7076, rl-loss: 275.7738952636719\r",
      "INFO - Step 7077, rl-loss: 1.6800973415374756\r",
      "INFO - Step 7078, rl-loss: 1.8442565202713013\r",
      "INFO - Step 7079, rl-loss: 216.69937133789062\r",
      "INFO - Step 7080, rl-loss: 179.30282592773438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7081, rl-loss: 1.093273639678955\r",
      "INFO - Step 7082, rl-loss: 305.076416015625\r",
      "INFO - Step 7083, rl-loss: 4.3548688888549805\r",
      "INFO - Step 7084, rl-loss: 0.7738486528396606\r",
      "INFO - Step 7085, rl-loss: 490.92755126953125\r",
      "INFO - Step 7086, rl-loss: 63.021671295166016\r",
      "INFO - Step 7087, rl-loss: 65.4415283203125\r",
      "INFO - Step 7088, rl-loss: 191.90338134765625\r",
      "INFO - Step 7089, rl-loss: 88.05783081054688\r",
      "INFO - Step 7090, rl-loss: 156.1968231201172\r",
      "INFO - Step 7091, rl-loss: 348.89483642578125\r",
      "INFO - Step 7092, rl-loss: 1.700248122215271\r",
      "INFO - Step 7093, rl-loss: 0.8332691788673401\r",
      "INFO - Step 7094, rl-loss: 141.95880126953125\r",
      "INFO - Step 7095, rl-loss: 302.0350036621094\r",
      "INFO - Step 7096, rl-loss: 25.47321128845215\r",
      "INFO - Step 7097, rl-loss: 0.9265360236167908\r",
      "INFO - Step 7098, rl-loss: 776.7609252929688\r",
      "INFO - Step 7099, rl-loss: 341.99188232421875\r",
      "INFO - Step 7100, rl-loss: 1.6197152137756348\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7101, rl-loss: 1.383925437927246\r",
      "INFO - Step 7102, rl-loss: 1.0641065835952759\r",
      "INFO - Step 7103, rl-loss: 35.75932312011719\r",
      "INFO - Step 7104, rl-loss: 174.74588012695312\r",
      "INFO - Step 7105, rl-loss: 190.99778747558594\r",
      "INFO - Step 7106, rl-loss: 1.1264463663101196\r",
      "INFO - Step 7107, rl-loss: 392.31585693359375\r",
      "INFO - Step 7108, rl-loss: 74.935302734375\r",
      "INFO - Step 7109, rl-loss: 701.5241088867188\r",
      "INFO - Step 7110, rl-loss: 406.21185302734375\r",
      "INFO - Step 7111, rl-loss: 474.868408203125\r",
      "INFO - Step 7112, rl-loss: 5.535757064819336\r",
      "INFO - Step 7113, rl-loss: 442.7854309082031\r",
      "INFO - Step 7114, rl-loss: 44.22889709472656\r",
      "INFO - Step 7115, rl-loss: 298.9217529296875\r",
      "INFO - Step 7116, rl-loss: 72.8302001953125\r",
      "INFO - Step 7117, rl-loss: 397.043212890625\r",
      "INFO - Step 7118, rl-loss: 65.64055633544922\r",
      "INFO - Step 7119, rl-loss: 141.4028778076172\r",
      "INFO - Step 7120, rl-loss: 631.0655517578125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7121, rl-loss: 33.204803466796875\r",
      "INFO - Step 7122, rl-loss: 546.4884643554688\r",
      "INFO - Step 7123, rl-loss: 151.52272033691406\r",
      "INFO - Step 7124, rl-loss: 1.4378033876419067\r",
      "INFO - Step 7125, rl-loss: 302.98626708984375\r",
      "INFO - Step 7126, rl-loss: 553.1246337890625\r",
      "INFO - Step 7127, rl-loss: 58.2069091796875\r",
      "INFO - Step 7128, rl-loss: 334.65057373046875\r",
      "INFO - Step 7129, rl-loss: 2.2054824829101562\r",
      "INFO - Step 7130, rl-loss: 1.8689543008804321\r",
      "INFO - Step 7131, rl-loss: 1.8759933710098267\r",
      "INFO - Step 7132, rl-loss: 359.3633728027344\r",
      "INFO - Step 7133, rl-loss: 1080.4544677734375\r",
      "INFO - Step 7134, rl-loss: 150.0322723388672\r",
      "INFO - Step 7135, rl-loss: 80.82714080810547\r",
      "INFO - Step 7136, rl-loss: 234.1776885986328\r",
      "INFO - Step 7137, rl-loss: 930.51171875\r",
      "INFO - Step 7138, rl-loss: 540.7678833007812\r",
      "INFO - Step 7139, rl-loss: 384.09332275390625\r",
      "INFO - Step 7140, rl-loss: 344.9367370605469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7141, rl-loss: 216.24636840820312\r",
      "INFO - Step 7142, rl-loss: 292.4141540527344\r",
      "INFO - Step 7143, rl-loss: 254.68789672851562\r",
      "INFO - Step 7144, rl-loss: 270.99139404296875\r",
      "INFO - Step 7145, rl-loss: 497.8913269042969\r",
      "INFO - Step 7146, rl-loss: 75.02040100097656\r",
      "INFO - Step 7147, rl-loss: 700.7310791015625\r",
      "INFO - Step 7148, rl-loss: 112.15745544433594\r",
      "INFO - Step 7149, rl-loss: 543.366943359375\r",
      "INFO - Step 7150, rl-loss: 119.88888549804688\r",
      "INFO - Step 7151, rl-loss: 524.4871215820312\r",
      "INFO - Step 7152, rl-loss: 566.9110107421875\r",
      "INFO - Step 7153, rl-loss: 397.013671875\r",
      "INFO - Step 7154, rl-loss: 136.46682739257812\r",
      "INFO - Step 7155, rl-loss: 0.958498477935791\r",
      "INFO - Step 7156, rl-loss: 147.14926147460938\r",
      "INFO - Step 7157, rl-loss: 222.22093200683594\r",
      "INFO - Step 7158, rl-loss: 2.8691720962524414\r",
      "INFO - Step 7159, rl-loss: 54.24707794189453\r",
      "INFO - Step 7160, rl-loss: 329.8645324707031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7161, rl-loss: 86.66497802734375\r",
      "INFO - Step 7162, rl-loss: 19.786251068115234\r",
      "INFO - Step 7163, rl-loss: 132.4703369140625\r",
      "INFO - Step 7164, rl-loss: 104.61505889892578\r",
      "INFO - Step 7165, rl-loss: 560.05419921875\r",
      "INFO - Step 7166, rl-loss: 321.02294921875\r",
      "INFO - Step 7167, rl-loss: 188.06295776367188\r",
      "INFO - Step 7168, rl-loss: 363.9742431640625\r",
      "INFO - Step 7169, rl-loss: 208.26895141601562\r",
      "INFO - Step 7170, rl-loss: 190.531494140625\r",
      "INFO - Step 7171, rl-loss: 382.9508972167969\r",
      "INFO - Step 7172, rl-loss: 476.1365966796875\r",
      "INFO - Step 7173, rl-loss: 119.57250213623047\r",
      "INFO - Step 7174, rl-loss: 578.0790405273438\r",
      "INFO - Step 7175, rl-loss: 1.2176084518432617\r",
      "INFO - Step 7176, rl-loss: 209.99440002441406\r",
      "INFO - Step 7177, rl-loss: 0.8170221447944641\r",
      "INFO - Step 7178, rl-loss: 261.22027587890625\r",
      "INFO - Step 7179, rl-loss: 319.62969970703125\r",
      "INFO - Step 7180, rl-loss: 590.177734375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7181, rl-loss: 680.4833984375\r",
      "INFO - Step 7182, rl-loss: 48.74397277832031\r",
      "INFO - Step 7183, rl-loss: 275.9737854003906\r",
      "INFO - Step 7184, rl-loss: 265.5614929199219\r",
      "INFO - Step 7185, rl-loss: 224.65142822265625\r",
      "INFO - Step 7186, rl-loss: 255.80458068847656\r",
      "INFO - Step 7187, rl-loss: 27.54559898376465\r",
      "INFO - Step 7188, rl-loss: 335.3634033203125\r",
      "INFO - Step 7189, rl-loss: 170.2164306640625\r",
      "INFO - Step 7190, rl-loss: 182.78729248046875\r",
      "INFO - Step 7191, rl-loss: 405.1368713378906\r",
      "INFO - Step 7192, rl-loss: 277.60028076171875\r",
      "INFO - Step 7193, rl-loss: 1.7078921794891357\r",
      "INFO - Step 7194, rl-loss: 191.40960693359375\r",
      "INFO - Step 7195, rl-loss: 301.6933288574219\r",
      "INFO - Step 7196, rl-loss: 218.9617462158203\r",
      "INFO - Step 7197, rl-loss: 99.70321655273438\r",
      "INFO - Step 7198, rl-loss: 43.00712585449219\r",
      "INFO - Step 7199, rl-loss: 1.380000114440918\r",
      "INFO - Step 7200, rl-loss: 407.91217041015625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7201, rl-loss: 209.33489990234375\r",
      "INFO - Step 7202, rl-loss: 41.960289001464844\r",
      "INFO - Step 7203, rl-loss: 92.8007583618164\r",
      "INFO - Step 7204, rl-loss: 79.08212280273438\r",
      "INFO - Step 7205, rl-loss: 266.3019104003906\r",
      "INFO - Step 7206, rl-loss: 175.4086151123047\r",
      "INFO - Step 7207, rl-loss: 367.43548583984375\r",
      "INFO - Step 7208, rl-loss: 314.462646484375\r",
      "INFO - Step 7209, rl-loss: 158.20272827148438\r",
      "INFO - Step 7210, rl-loss: 259.6578674316406\r",
      "INFO - Step 7211, rl-loss: 104.33171844482422\r",
      "INFO - Step 7212, rl-loss: 469.036865234375\r",
      "INFO - Step 7213, rl-loss: 631.400390625\r",
      "INFO - Step 7214, rl-loss: 43.70619201660156\r",
      "INFO - Step 7215, rl-loss: 581.4578247070312\r",
      "INFO - Step 7216, rl-loss: 10.661239624023438\r",
      "INFO - Step 7217, rl-loss: 406.54412841796875\r",
      "INFO - Step 7218, rl-loss: 1.8362462520599365\r",
      "INFO - Step 7219, rl-loss: 1.4017746448516846\r",
      "INFO - Step 7220, rl-loss: 66.46163177490234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7221, rl-loss: 44.779212951660156\r",
      "INFO - Step 7222, rl-loss: 272.496337890625\r",
      "INFO - Step 7223, rl-loss: 371.65960693359375\r",
      "INFO - Step 7224, rl-loss: 250.75848388671875\r",
      "INFO - Step 7225, rl-loss: 127.07917022705078\r",
      "INFO - Step 7226, rl-loss: 1.8786085844039917\r",
      "INFO - Step 7227, rl-loss: 181.4103546142578\r",
      "INFO - Step 7228, rl-loss: 29.18369483947754\r",
      "INFO - Step 7229, rl-loss: 1.5919084548950195\r",
      "INFO - Step 7230, rl-loss: 573.3695068359375\r",
      "INFO - Step 7231, rl-loss: 1.387250542640686\r",
      "INFO - Step 7232, rl-loss: 594.5123291015625\r",
      "INFO - Step 7233, rl-loss: 228.1141815185547\r",
      "INFO - Step 7234, rl-loss: 249.4490966796875\r",
      "INFO - Step 7235, rl-loss: 1.6281765699386597\r",
      "INFO - Step 7236, rl-loss: 1.1734564304351807\r",
      "INFO - Step 7237, rl-loss: 1.990422248840332\r",
      "INFO - Step 7238, rl-loss: 1.4331003427505493\r",
      "INFO - Step 7239, rl-loss: 129.20089721679688\r",
      "INFO - Step 7240, rl-loss: 349.947998046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7241, rl-loss: 241.37838745117188\r",
      "INFO - Step 7242, rl-loss: 1.5998615026474\r",
      "INFO - Step 7243, rl-loss: 0.8494384288787842\r",
      "INFO - Step 7244, rl-loss: 1.8162635564804077\r",
      "INFO - Step 7245, rl-loss: 1.9008381366729736\r",
      "INFO - Step 7246, rl-loss: 110.10113525390625\r",
      "INFO - Step 7247, rl-loss: 1.4848747253417969\r",
      "INFO - Step 7248, rl-loss: 1.1230913400650024\r",
      "INFO - Step 7249, rl-loss: 201.0839385986328\r",
      "INFO - Step 7250, rl-loss: 424.419677734375\r",
      "INFO - Step 7251, rl-loss: 141.7044677734375\r",
      "INFO - Step 7252, rl-loss: 145.84146118164062\r",
      "INFO - Step 7253, rl-loss: 87.78744506835938\r",
      "INFO - Step 7254, rl-loss: 5.232447147369385\r",
      "INFO - Step 7255, rl-loss: 208.41082763671875\r",
      "INFO - Step 7256, rl-loss: 38.37579345703125\r",
      "INFO - Step 7257, rl-loss: 61.55172348022461\r",
      "INFO - Step 7258, rl-loss: 710.701416015625\r",
      "INFO - Step 7259, rl-loss: 123.29778289794922\r",
      "INFO - Step 7260, rl-loss: 374.19683837890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7261, rl-loss: 99.14659118652344\r",
      "INFO - Step 7262, rl-loss: 361.2093505859375\r",
      "INFO - Step 7263, rl-loss: 95.82759857177734\r",
      "INFO - Step 7264, rl-loss: 557.763427734375\r",
      "INFO - Step 7265, rl-loss: 128.98487854003906\r",
      "INFO - Step 7266, rl-loss: 1.119563102722168\r",
      "INFO - Step 7267, rl-loss: 286.98675537109375\r",
      "INFO - Step 7268, rl-loss: 134.3249053955078\r",
      "INFO - Step 7269, rl-loss: 1.3567651510238647\r",
      "INFO - Step 7270, rl-loss: 279.8157653808594\r",
      "INFO - Step 7271, rl-loss: 1.2435595989227295\r",
      "INFO - Step 7272, rl-loss: 334.3268127441406\r",
      "INFO - Step 7273, rl-loss: 240.412109375\r",
      "INFO - Step 7274, rl-loss: 251.86407470703125\r",
      "INFO - Step 7275, rl-loss: 136.92196655273438\r",
      "INFO - Step 7276, rl-loss: 222.45242309570312\r",
      "INFO - Step 7277, rl-loss: 284.6473388671875\r",
      "INFO - Step 7278, rl-loss: 211.98416137695312\r",
      "INFO - Step 7279, rl-loss: 205.8373565673828\r",
      "INFO - Step 7280, rl-loss: 59.421634674072266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7281, rl-loss: 6.635574817657471\r",
      "INFO - Step 7282, rl-loss: 380.845947265625\r",
      "INFO - Step 7283, rl-loss: 274.89599609375\r",
      "INFO - Step 7284, rl-loss: 39.990726470947266\r",
      "INFO - Step 7285, rl-loss: 678.8587036132812\r",
      "INFO - Step 7286, rl-loss: 239.02801513671875\r",
      "INFO - Step 7287, rl-loss: 614.6835327148438\r",
      "INFO - Step 7288, rl-loss: 64.82402038574219\r",
      "INFO - Step 7289, rl-loss: 1.1124958992004395\r",
      "INFO - Step 7290, rl-loss: 264.612060546875\r",
      "INFO - Step 7291, rl-loss: 315.7541809082031\r",
      "INFO - Step 7292, rl-loss: 71.10208892822266\r",
      "INFO - Step 7293, rl-loss: 139.75852966308594\r",
      "INFO - Step 7294, rl-loss: 1.7477900981903076\r",
      "INFO - Step 7295, rl-loss: 1.8444571495056152\r",
      "INFO - Step 7296, rl-loss: 129.17919921875\r",
      "INFO - Step 7297, rl-loss: 210.53831481933594\r",
      "INFO - Step 7298, rl-loss: 238.79971313476562\r",
      "INFO - Step 7299, rl-loss: 60.64356231689453\r",
      "INFO - Step 7300, rl-loss: 375.66583251953125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7301, rl-loss: 41.2092170715332\r",
      "INFO - Step 7302, rl-loss: 192.26478576660156\r",
      "INFO - Step 7303, rl-loss: 61.60126876831055\r",
      "INFO - Step 7304, rl-loss: 252.71237182617188\r",
      "INFO - Step 7305, rl-loss: 206.04837036132812\r",
      "INFO - Step 7306, rl-loss: 799.2783203125\r",
      "INFO - Step 7307, rl-loss: 18.18499183654785\r",
      "INFO - Step 7308, rl-loss: 135.83665466308594\r",
      "INFO - Step 7309, rl-loss: 207.0144500732422\r",
      "INFO - Step 7310, rl-loss: 0.9600430727005005\r",
      "INFO - Step 7311, rl-loss: 1.6014995574951172\r",
      "INFO - Step 7312, rl-loss: 505.1454772949219\r",
      "INFO - Step 7313, rl-loss: 172.81288146972656\r",
      "INFO - Step 7314, rl-loss: 173.11126708984375\r",
      "INFO - Step 7315, rl-loss: 162.31251525878906\r",
      "INFO - Step 7316, rl-loss: 239.6262969970703\r",
      "INFO - Step 7317, rl-loss: 243.4929656982422\r",
      "INFO - Step 7318, rl-loss: 215.33705139160156\r",
      "INFO - Step 7319, rl-loss: 1.8354663848876953\r",
      "INFO - Step 7320, rl-loss: 227.9295196533203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7321, rl-loss: 127.25563049316406\r",
      "INFO - Step 7322, rl-loss: 1.655759334564209\r",
      "INFO - Step 7323, rl-loss: 63.498416900634766\r",
      "INFO - Step 7324, rl-loss: 0.912375807762146\r",
      "INFO - Step 7325, rl-loss: 1.9620307683944702\r",
      "INFO - Step 7326, rl-loss: 388.6612548828125\r",
      "INFO - Step 7327, rl-loss: 240.24745178222656\r",
      "INFO - Step 7328, rl-loss: 173.0323944091797\r",
      "INFO - Step 7329, rl-loss: 217.5864715576172\r",
      "INFO - Step 7330, rl-loss: 75.96658325195312\r",
      "INFO - Step 7331, rl-loss: 140.00445556640625\r",
      "INFO - Step 7332, rl-loss: 430.27301025390625\r",
      "INFO - Step 7333, rl-loss: 107.3672866821289\r",
      "INFO - Step 7334, rl-loss: 1.748693823814392\r",
      "INFO - Step 7335, rl-loss: 6.94480037689209\r",
      "INFO - Step 7336, rl-loss: 37.87156295776367\r",
      "INFO - Step 7337, rl-loss: 80.08142852783203\r",
      "INFO - Step 7338, rl-loss: 136.5695343017578\r",
      "INFO - Step 7339, rl-loss: 304.9712829589844\r",
      "INFO - Step 7340, rl-loss: 1.3336429595947266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7341, rl-loss: 148.32357788085938\r",
      "INFO - Step 7342, rl-loss: 60.838356018066406\r",
      "INFO - Step 7343, rl-loss: 1.0851588249206543\r",
      "INFO - Step 7344, rl-loss: 228.07330322265625\r",
      "INFO - Step 7345, rl-loss: 545.765625\r",
      "INFO - Step 7346, rl-loss: 283.6017761230469\r",
      "INFO - Step 7347, rl-loss: 414.47113037109375\r",
      "INFO - Step 7348, rl-loss: 50.4041862487793\r",
      "INFO - Step 7349, rl-loss: 304.3046569824219\r",
      "INFO - Step 7350, rl-loss: 918.9457397460938\r",
      "INFO - Step 7351, rl-loss: 1.4860479831695557\r",
      "INFO - Step 7352, rl-loss: 58.15947341918945\r",
      "INFO - Step 7353, rl-loss: 11.011175155639648\r",
      "INFO - Step 7354, rl-loss: 213.9456024169922\r",
      "INFO - Step 7355, rl-loss: 146.99990844726562\r",
      "INFO - Step 7356, rl-loss: 5.80800724029541\r",
      "INFO - Step 7357, rl-loss: 3.083874225616455\r",
      "INFO - Step 7358, rl-loss: 1.777753472328186\r",
      "INFO - Step 7359, rl-loss: 1.1461677551269531\r",
      "INFO - Step 7360, rl-loss: 1.4096490144729614"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7361, rl-loss: 26.993240356445312\r",
      "INFO - Step 7362, rl-loss: 198.48416137695312\r",
      "INFO - Step 7363, rl-loss: 643.1808471679688\r",
      "INFO - Step 7364, rl-loss: 224.53335571289062\r",
      "INFO - Step 7365, rl-loss: 479.55706787109375\r",
      "INFO - Step 7366, rl-loss: 638.5526123046875\r",
      "INFO - Step 7367, rl-loss: 1.798719882965088\r",
      "INFO - Step 7368, rl-loss: 28.592315673828125\r",
      "INFO - Step 7369, rl-loss: 0.6972808837890625\r",
      "INFO - Step 7370, rl-loss: 222.45587158203125\r",
      "INFO - Step 7371, rl-loss: 312.91876220703125\r",
      "INFO - Step 7372, rl-loss: 671.2590942382812\r",
      "INFO - Step 7373, rl-loss: 171.03982543945312\r",
      "INFO - Step 7374, rl-loss: 33.16801452636719\r",
      "INFO - Step 7375, rl-loss: 73.24940490722656\r",
      "INFO - Step 7376, rl-loss: 500.8439025878906\r",
      "INFO - Step 7377, rl-loss: 181.52511596679688\r",
      "INFO - Step 7378, rl-loss: 119.98451232910156\r",
      "INFO - Step 7379, rl-loss: 174.67259216308594\r",
      "INFO - Step 7380, rl-loss: 11.237248420715332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7381, rl-loss: 111.74813842773438\r",
      "INFO - Step 7382, rl-loss: 592.8419189453125\r",
      "INFO - Step 7383, rl-loss: 1.0985753536224365\r",
      "INFO - Step 7384, rl-loss: 265.4056091308594\r",
      "INFO - Step 7385, rl-loss: 94.26005554199219\r",
      "INFO - Step 7386, rl-loss: 8.846505165100098\r",
      "INFO - Step 7387, rl-loss: 1.5141470432281494\r",
      "INFO - Step 7388, rl-loss: 772.1831665039062\r",
      "INFO - Step 7389, rl-loss: 170.96719360351562\r",
      "INFO - Step 7390, rl-loss: 115.72526550292969\r",
      "INFO - Step 7391, rl-loss: 251.21424865722656\r",
      "INFO - Step 7392, rl-loss: 740.005126953125\r",
      "INFO - Step 7393, rl-loss: 106.01264190673828\r",
      "INFO - Step 7394, rl-loss: 1.8172625303268433\r",
      "INFO - Step 7395, rl-loss: 122.37185668945312\r",
      "INFO - Step 7396, rl-loss: 702.3414306640625\r",
      "INFO - Step 7397, rl-loss: 308.5536804199219\r",
      "INFO - Step 7398, rl-loss: 1.336445927619934\r",
      "INFO - Step 7399, rl-loss: 1.0433944463729858\r",
      "INFO - Step 7400, rl-loss: 176.0903778076172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7401, rl-loss: 320.99273681640625\r",
      "INFO - Step 7402, rl-loss: 216.13340759277344\r",
      "INFO - Step 7403, rl-loss: 103.5350341796875\r",
      "INFO - Step 7404, rl-loss: 548.7327880859375\r",
      "INFO - Step 7405, rl-loss: 1.5215662717819214\r",
      "INFO - Step 7406, rl-loss: 71.30577087402344\r",
      "INFO - Step 7407, rl-loss: 244.5274200439453\r",
      "INFO - Step 7408, rl-loss: 211.386962890625\r",
      "INFO - Step 7409, rl-loss: 376.3733215332031\r",
      "INFO - Step 7410, rl-loss: 6.408915996551514\r",
      "INFO - Step 7411, rl-loss: 1.2369554042816162\r",
      "INFO - Step 7412, rl-loss: 364.12030029296875\r",
      "INFO - Step 7413, rl-loss: 317.7779541015625\r",
      "INFO - Step 7414, rl-loss: 136.78468322753906\r",
      "INFO - Step 7415, rl-loss: 0.9196592569351196\r",
      "INFO - Step 7416, rl-loss: 1.667057991027832\r",
      "INFO - Step 7417, rl-loss: 6.705081462860107\r",
      "INFO - Step 7418, rl-loss: 2.144745349884033\r",
      "INFO - Step 7419, rl-loss: 0.753079354763031\r",
      "INFO - Step 7420, rl-loss: 535.4542236328125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7421, rl-loss: 25.025833129882812\r",
      "INFO - Step 7422, rl-loss: 1.7650771141052246\r",
      "INFO - Step 7423, rl-loss: 513.0619506835938\r",
      "INFO - Step 7424, rl-loss: 450.89288330078125\r",
      "INFO - Step 7425, rl-loss: 171.2518768310547\r",
      "INFO - Step 7426, rl-loss: 0.729640007019043\r",
      "INFO - Step 7427, rl-loss: 1.0665626525878906\r",
      "INFO - Step 7428, rl-loss: 302.6790771484375\r",
      "INFO - Step 7429, rl-loss: 80.24735260009766\r",
      "INFO - Step 7430, rl-loss: 1.3341923952102661\r",
      "INFO - Step 7431, rl-loss: 178.60292053222656\r",
      "INFO - Step 7432, rl-loss: 1.0459123849868774\r",
      "INFO - Step 7433, rl-loss: 625.0517578125\r",
      "INFO - Step 7434, rl-loss: 294.041015625\r",
      "INFO - Step 7435, rl-loss: 225.65098571777344\r",
      "INFO - Step 7436, rl-loss: 152.58370971679688\r",
      "INFO - Step 7437, rl-loss: 215.62759399414062\r",
      "INFO - Step 7438, rl-loss: 20.989578247070312\r",
      "INFO - Step 7439, rl-loss: 314.80548095703125\r",
      "INFO - Step 7440, rl-loss: 14.34189510345459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7441, rl-loss: 0.8731271028518677\r",
      "INFO - Step 7442, rl-loss: 218.5020294189453\r",
      "INFO - Step 7443, rl-loss: 78.32109832763672\r",
      "INFO - Step 7444, rl-loss: 146.37985229492188\r",
      "INFO - Step 7445, rl-loss: 135.1245880126953\r",
      "INFO - Step 7446, rl-loss: 260.2470703125\r",
      "INFO - Step 7447, rl-loss: 1.5859553813934326\r",
      "INFO - Step 7448, rl-loss: 1.2115161418914795\r",
      "INFO - Step 7449, rl-loss: 47.162635803222656\r",
      "INFO - Step 7450, rl-loss: 179.27891540527344\r",
      "INFO - Step 7451, rl-loss: 0.8076545000076294\r",
      "INFO - Step 7452, rl-loss: 293.06707763671875\r",
      "INFO - Step 7453, rl-loss: 0.8586208820343018\r",
      "INFO - Step 7454, rl-loss: 55.22031021118164\r",
      "INFO - Step 7455, rl-loss: 318.3138122558594\r",
      "INFO - Step 7456, rl-loss: 959.8810424804688\r",
      "INFO - Step 7457, rl-loss: 580.697265625\r",
      "INFO - Step 7458, rl-loss: 309.11700439453125\r",
      "INFO - Step 7459, rl-loss: 501.7592468261719\r",
      "INFO - Step 7460, rl-loss: 66.10420989990234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7461, rl-loss: 94.56194305419922\r",
      "INFO - Step 7462, rl-loss: 489.2860107421875\r",
      "INFO - Step 7463, rl-loss: 251.0917510986328\r",
      "INFO - Step 7464, rl-loss: 120.68817901611328\r",
      "INFO - Step 7465, rl-loss: 0.9293580651283264\r",
      "INFO - Step 7466, rl-loss: 223.21902465820312\r",
      "INFO - Step 7467, rl-loss: 1.2239196300506592\r",
      "INFO - Step 7468, rl-loss: 304.2742004394531\r",
      "INFO - Step 7469, rl-loss: 510.95172119140625\r",
      "INFO - Step 7470, rl-loss: 1.7575218677520752\r",
      "INFO - Step 7471, rl-loss: 126.1325912475586\r",
      "INFO - Step 7472, rl-loss: 61.76019287109375\r",
      "INFO - Step 7473, rl-loss: 1.1333588361740112\r",
      "INFO - Step 7474, rl-loss: 103.49552917480469\r",
      "INFO - Step 7475, rl-loss: 239.13682556152344\r",
      "INFO - Step 7476, rl-loss: 315.87725830078125\r",
      "INFO - Step 7477, rl-loss: 320.2742919921875\r",
      "INFO - Step 7478, rl-loss: 243.26185607910156\r",
      "INFO - Step 7479, rl-loss: 411.4999084472656\r",
      "INFO - Step 7480, rl-loss: 257.22247314453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7481, rl-loss: 172.8292999267578\r",
      "INFO - Step 7482, rl-loss: 398.92626953125\r",
      "INFO - Step 7483, rl-loss: 1.4272758960723877\r",
      "INFO - Step 7484, rl-loss: 753.4161987304688\r",
      "INFO - Step 7485, rl-loss: 559.7403564453125\r",
      "INFO - Step 7486, rl-loss: 192.25706481933594\r",
      "INFO - Step 7487, rl-loss: 1.0564444065093994\r",
      "INFO - Step 7488, rl-loss: 0.9825870394706726\r",
      "INFO - Step 7489, rl-loss: 1.7243492603302002\r",
      "INFO - Step 7490, rl-loss: 153.7071533203125\r",
      "INFO - Step 7491, rl-loss: 638.9259033203125\r",
      "INFO - Step 7492, rl-loss: 1.3368463516235352\r",
      "INFO - Step 7493, rl-loss: 406.8304138183594\r",
      "INFO - Step 7494, rl-loss: 1.7713369131088257\r",
      "INFO - Step 7495, rl-loss: 87.10404968261719\r",
      "INFO - Step 7496, rl-loss: 98.92964935302734\r",
      "INFO - Step 7497, rl-loss: 132.59140014648438\r",
      "INFO - Step 7498, rl-loss: 488.90496826171875\r",
      "INFO - Step 7499, rl-loss: 176.0821075439453\r",
      "INFO - Step 7500, rl-loss: 151.06109619140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7501, rl-loss: 123.3936996459961\r",
      "INFO - Step 7502, rl-loss: 172.3459014892578\r",
      "INFO - Step 7503, rl-loss: 63.52838134765625\r",
      "INFO - Step 7504, rl-loss: 146.1607666015625\r",
      "INFO - Step 7505, rl-loss: 603.83447265625\r",
      "INFO - Step 7506, rl-loss: 774.9265747070312\r",
      "INFO - Step 7507, rl-loss: 220.30088806152344\r",
      "INFO - Step 7508, rl-loss: 97.99188995361328\r",
      "INFO - Step 7509, rl-loss: 371.21630859375\r",
      "INFO - Step 7510, rl-loss: 78.38170623779297\r",
      "INFO - Step 7511, rl-loss: 118.15234375\r",
      "INFO - Step 7512, rl-loss: 229.61062622070312\r",
      "INFO - Step 7513, rl-loss: 28.156742095947266\r",
      "INFO - Step 7514, rl-loss: 178.9408721923828\r",
      "INFO - Step 7515, rl-loss: 1.0075362920761108\r",
      "INFO - Step 7516, rl-loss: 523.338134765625\r",
      "INFO - Step 7517, rl-loss: 623.3536987304688\r",
      "INFO - Step 7518, rl-loss: 62.43059539794922\r",
      "INFO - Step 7519, rl-loss: 0.760154128074646\r",
      "INFO - Step 7520, rl-loss: 76.37073516845703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7521, rl-loss: 186.1552734375\r",
      "INFO - Step 7522, rl-loss: 117.18023681640625\r",
      "INFO - Step 7523, rl-loss: 355.5376892089844\r",
      "INFO - Step 7524, rl-loss: 96.1353530883789\r",
      "INFO - Step 7525, rl-loss: 249.79562377929688\r",
      "INFO - Step 7526, rl-loss: 1.7429091930389404\r",
      "INFO - Step 7527, rl-loss: 250.93173217773438\r",
      "INFO - Step 7528, rl-loss: 110.11800384521484\r",
      "INFO - Step 7529, rl-loss: 296.4176025390625\r",
      "INFO - Step 7530, rl-loss: 9.827192306518555\r",
      "INFO - Step 7531, rl-loss: 1.5552724599838257\r",
      "INFO - Step 7532, rl-loss: 1.8827040195465088\r",
      "INFO - Step 7533, rl-loss: 1.5150468349456787\r",
      "INFO - Step 7534, rl-loss: 89.62995910644531\r",
      "INFO - Step 7535, rl-loss: 1.603537678718567\r",
      "INFO - Step 7536, rl-loss: 111.47518920898438\r",
      "INFO - Step 7537, rl-loss: 1.9083172082901\r",
      "INFO - Step 7538, rl-loss: 208.56561279296875\r",
      "INFO - Step 7539, rl-loss: 211.64871215820312\r",
      "INFO - Step 7540, rl-loss: 345.19964599609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7541, rl-loss: 62.97529602050781\r",
      "INFO - Step 7542, rl-loss: 2.424537181854248\r",
      "INFO - Step 7543, rl-loss: 343.71905517578125\r",
      "INFO - Step 7544, rl-loss: 25.699295043945312\r",
      "INFO - Step 7545, rl-loss: 266.44488525390625\r",
      "INFO - Step 7546, rl-loss: 90.33229064941406\r",
      "INFO - Step 7547, rl-loss: 31.832735061645508\r",
      "INFO - Step 7548, rl-loss: 165.93026733398438\r",
      "INFO - Step 7549, rl-loss: 274.5089111328125\r",
      "INFO - Step 7550, rl-loss: 228.10162353515625\r",
      "INFO - Step 7551, rl-loss: 149.49903869628906\r",
      "INFO - Step 7552, rl-loss: 41.13947296142578\r",
      "INFO - Step 7553, rl-loss: 31.59542465209961\r",
      "INFO - Step 7554, rl-loss: 314.9380187988281\r",
      "INFO - Step 7555, rl-loss: 116.9441146850586\r",
      "INFO - Step 7556, rl-loss: 2.90710186958313\r",
      "INFO - Step 7557, rl-loss: 678.2921142578125\r",
      "INFO - Step 7558, rl-loss: 197.70965576171875\r",
      "INFO - Step 7559, rl-loss: 108.67662048339844\r",
      "INFO - Step 7560, rl-loss: 115.01039123535156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7561, rl-loss: 47.300537109375\r",
      "INFO - Step 7562, rl-loss: 168.33053588867188\r",
      "INFO - Step 7563, rl-loss: 194.12989807128906\r",
      "INFO - Step 7564, rl-loss: 0.8992301225662231\r",
      "INFO - Step 7565, rl-loss: 140.87635803222656\r",
      "INFO - Step 7566, rl-loss: 348.0206298828125\r",
      "INFO - Step 7567, rl-loss: 581.8111572265625\r",
      "INFO - Step 7568, rl-loss: 18.900583267211914\r",
      "INFO - Step 7569, rl-loss: 377.23309326171875\r",
      "INFO - Step 7570, rl-loss: 1.1943124532699585\r",
      "INFO - Step 7571, rl-loss: 2.439452886581421\r",
      "INFO - Step 7572, rl-loss: 10.738870620727539\r",
      "INFO - Step 7573, rl-loss: 803.1951904296875\r",
      "INFO - Step 7574, rl-loss: 360.31890869140625\r",
      "INFO - Step 7575, rl-loss: 406.65203857421875\r",
      "INFO - Step 7576, rl-loss: 191.0028533935547\r",
      "INFO - Step 7577, rl-loss: 132.07138061523438\r",
      "INFO - Step 7578, rl-loss: 724.1241455078125\r",
      "INFO - Step 7579, rl-loss: 548.392578125\r",
      "INFO - Step 7580, rl-loss: 102.72882080078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7581, rl-loss: 4.159087657928467\r",
      "INFO - Step 7582, rl-loss: 446.50665283203125\r",
      "INFO - Step 7583, rl-loss: 111.03311920166016\r",
      "INFO - Step 7584, rl-loss: 1.2960628271102905\r",
      "INFO - Step 7585, rl-loss: 227.21710205078125\r",
      "INFO - Step 7586, rl-loss: 128.64097595214844\r",
      "INFO - Step 7587, rl-loss: 59.65522384643555\r",
      "INFO - Step 7588, rl-loss: 115.13484954833984\r",
      "INFO - Step 7589, rl-loss: 0.9063137173652649\r",
      "INFO - Step 7590, rl-loss: 32.093597412109375\r",
      "INFO - Step 7591, rl-loss: 159.58189392089844\r",
      "INFO - Step 7592, rl-loss: 86.79646301269531\r",
      "INFO - Step 7593, rl-loss: 259.1319580078125\r",
      "INFO - Step 7594, rl-loss: 205.89743041992188\r",
      "INFO - Step 7595, rl-loss: 202.88145446777344\r",
      "INFO - Step 7596, rl-loss: 64.9057846069336\r",
      "INFO - Step 7597, rl-loss: 360.6078186035156\r",
      "INFO - Step 7598, rl-loss: 56.51471710205078\r",
      "INFO - Step 7599, rl-loss: 178.14413452148438\r",
      "INFO - Step 7600, rl-loss: 5.943536758422852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7601, rl-loss: 479.02294921875\r",
      "INFO - Step 7602, rl-loss: 1.2992732524871826\r",
      "INFO - Step 7603, rl-loss: 236.4644317626953\r",
      "INFO - Step 7604, rl-loss: 0.7726419568061829\r",
      "INFO - Step 7605, rl-loss: 196.66297912597656\r",
      "INFO - Step 7606, rl-loss: 165.84915161132812\r",
      "INFO - Step 7607, rl-loss: 223.87606811523438\r",
      "INFO - Step 7608, rl-loss: 1.1447107791900635\r",
      "INFO - Step 7609, rl-loss: 236.78953552246094\r",
      "INFO - Step 7610, rl-loss: 533.7107543945312\r",
      "INFO - Step 7611, rl-loss: 49.55583953857422\r",
      "INFO - Step 7612, rl-loss: 109.03740692138672\r",
      "INFO - Step 7613, rl-loss: 468.254150390625\r",
      "INFO - Step 7614, rl-loss: 1.2852367162704468\r",
      "INFO - Step 7615, rl-loss: 264.46844482421875\r",
      "INFO - Step 7616, rl-loss: 181.2208251953125\r",
      "INFO - Step 7617, rl-loss: 85.479248046875\r",
      "INFO - Step 7618, rl-loss: 304.15472412109375\r",
      "INFO - Step 7619, rl-loss: 198.4085693359375\r",
      "INFO - Step 7620, rl-loss: 139.4378204345703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7621, rl-loss: 271.39703369140625\r",
      "INFO - Step 7622, rl-loss: 10.987529754638672\r",
      "INFO - Step 7623, rl-loss: 701.0394287109375\r",
      "INFO - Step 7624, rl-loss: 61.250431060791016\r",
      "INFO - Step 7625, rl-loss: 161.86813354492188\r",
      "INFO - Step 7626, rl-loss: 1.0510432720184326\r",
      "INFO - Step 7627, rl-loss: 1.778841495513916\r",
      "INFO - Step 7628, rl-loss: 2.044428586959839\r",
      "INFO - Step 7629, rl-loss: 1.86434805393219\r",
      "INFO - Step 7630, rl-loss: 121.27659606933594\r",
      "INFO - Step 7631, rl-loss: 198.72457885742188\r",
      "INFO - Step 7632, rl-loss: 2.2241251468658447\r",
      "INFO - Step 7633, rl-loss: 1.2818078994750977\r",
      "INFO - Step 7634, rl-loss: 97.521484375\r",
      "INFO - Step 7635, rl-loss: 171.25204467773438\r",
      "INFO - Step 7636, rl-loss: 287.68878173828125\r",
      "INFO - Step 7637, rl-loss: 151.2865753173828\r",
      "INFO - Step 7638, rl-loss: 472.70513916015625\r",
      "INFO - Step 7639, rl-loss: 1.1888937950134277\r",
      "INFO - Step 7640, rl-loss: 130.44097900390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7641, rl-loss: 1.1019673347473145\r",
      "INFO - Step 7642, rl-loss: 216.85226440429688\r",
      "INFO - Step 7643, rl-loss: 76.35042572021484\r",
      "INFO - Step 7644, rl-loss: 255.5039825439453\r",
      "INFO - Step 7645, rl-loss: 0.5545979142189026\r",
      "INFO - Step 7646, rl-loss: 358.8471374511719\r",
      "INFO - Step 7647, rl-loss: 36.37099838256836\r",
      "INFO - Step 7648, rl-loss: 138.7304229736328\r",
      "INFO - Step 7649, rl-loss: 0.9776890277862549\r",
      "INFO - Step 7650, rl-loss: 64.53305053710938\r",
      "INFO - Step 7651, rl-loss: 143.5941162109375\r",
      "INFO - Step 7652, rl-loss: 529.5721435546875\r",
      "INFO - Step 7653, rl-loss: 170.23829650878906\r",
      "INFO - Step 7654, rl-loss: 191.97569274902344\r",
      "INFO - Step 7655, rl-loss: 136.8004608154297\r",
      "INFO - Step 7656, rl-loss: 157.89581298828125\r",
      "INFO - Step 7657, rl-loss: 387.6790771484375\r",
      "INFO - Step 7658, rl-loss: 1.1502846479415894\r",
      "INFO - Step 7659, rl-loss: 2.351229667663574\r",
      "INFO - Step 7660, rl-loss: 240.8004913330078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7661, rl-loss: 159.48507690429688\r",
      "INFO - Step 7662, rl-loss: 134.70518493652344\r",
      "INFO - Step 7663, rl-loss: 246.53411865234375\r",
      "INFO - Step 7664, rl-loss: 213.30435180664062\r",
      "INFO - Step 7665, rl-loss: 63.38639831542969\r",
      "INFO - Step 7666, rl-loss: 149.44760131835938\r",
      "INFO - Step 7667, rl-loss: 209.72320556640625\r",
      "INFO - Step 7668, rl-loss: 395.6652526855469\r",
      "INFO - Step 7669, rl-loss: 148.8882293701172\r",
      "INFO - Step 7670, rl-loss: 264.6678771972656\r",
      "INFO - Step 7671, rl-loss: 801.1607666015625\r",
      "INFO - Step 7672, rl-loss: 511.58758544921875\r",
      "INFO - Step 7673, rl-loss: 34.09680938720703\r",
      "INFO - Step 7674, rl-loss: 1.4256830215454102\r",
      "INFO - Step 7675, rl-loss: 248.2784423828125\r",
      "INFO - Step 7676, rl-loss: 66.8350601196289\r",
      "INFO - Step 7677, rl-loss: 162.62991333007812\r",
      "INFO - Step 7678, rl-loss: 0.9859548807144165\r",
      "INFO - Step 7679, rl-loss: 251.20623779296875\r",
      "INFO - Step 7680, rl-loss: 64.59107971191406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7681, rl-loss: 1.7632747888565063\r",
      "INFO - Step 7682, rl-loss: 209.99127197265625\r",
      "INFO - Step 7683, rl-loss: 467.75872802734375\r",
      "INFO - Step 7684, rl-loss: 1.1240665912628174\r",
      "INFO - Step 7685, rl-loss: 312.3988952636719\r",
      "INFO - Step 7686, rl-loss: 112.89265441894531\r",
      "INFO - Step 7687, rl-loss: 558.0872192382812\r",
      "INFO - Step 7688, rl-loss: 105.11144256591797\r",
      "INFO - Step 7689, rl-loss: 140.88951110839844\r",
      "INFO - Step 7690, rl-loss: 579.76953125\r",
      "INFO - Step 7691, rl-loss: 245.27920532226562\r",
      "INFO - Step 7692, rl-loss: 418.18353271484375\r",
      "INFO - Step 7693, rl-loss: 66.40929412841797\r",
      "INFO - Step 7694, rl-loss: 165.07553100585938\r",
      "INFO - Step 7695, rl-loss: 399.7542419433594\r",
      "INFO - Step 7696, rl-loss: 93.07144927978516\r",
      "INFO - Step 7697, rl-loss: 35.23389434814453\r",
      "INFO - Step 7698, rl-loss: 621.1826171875\r",
      "INFO - Step 7699, rl-loss: 0.9424941539764404\r",
      "INFO - Step 7700, rl-loss: 63.436378479003906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7701, rl-loss: 260.981201171875\r",
      "INFO - Step 7702, rl-loss: 225.2708740234375\r",
      "INFO - Step 7703, rl-loss: 139.43150329589844\r",
      "INFO - Step 7704, rl-loss: 59.53068923950195\r",
      "INFO - Step 7705, rl-loss: 59.5518913269043\r",
      "INFO - Step 7706, rl-loss: 1.6435972452163696\r",
      "INFO - Step 7707, rl-loss: 1.6502501964569092\r",
      "INFO - Step 7708, rl-loss: 143.17086791992188\r",
      "INFO - Step 7709, rl-loss: 224.16867065429688\r",
      "INFO - Step 7710, rl-loss: 374.7879333496094\r",
      "INFO - Step 7711, rl-loss: 384.18438720703125\r",
      "INFO - Step 7712, rl-loss: 311.34930419921875\r",
      "INFO - Step 7713, rl-loss: 173.3629150390625\r",
      "INFO - Step 7714, rl-loss: 616.4664306640625\r",
      "INFO - Step 7715, rl-loss: 247.5576629638672\r",
      "INFO - Step 7716, rl-loss: 1.609169602394104\r",
      "INFO - Step 7717, rl-loss: 229.73553466796875\r",
      "INFO - Step 7718, rl-loss: 65.44506072998047\r",
      "INFO - Step 7719, rl-loss: 274.4278564453125\r",
      "INFO - Step 7720, rl-loss: 319.50677490234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7721, rl-loss: 34.66487121582031\r",
      "INFO - Step 7722, rl-loss: 10.808539390563965\r",
      "INFO - Step 7723, rl-loss: 126.44055938720703\r",
      "INFO - Step 7724, rl-loss: 86.44060516357422\r",
      "INFO - Step 7725, rl-loss: 166.14784240722656\r",
      "INFO - Step 7726, rl-loss: 45.68721008300781\r",
      "INFO - Step 7727, rl-loss: 34.39946746826172\r",
      "INFO - Step 7728, rl-loss: 23.10555648803711\r",
      "INFO - Step 7729, rl-loss: 489.51629638671875\r",
      "INFO - Step 7730, rl-loss: 1.865002989768982\r",
      "INFO - Step 7731, rl-loss: 4.942975044250488\r",
      "INFO - Step 7732, rl-loss: 101.84551239013672\r",
      "INFO - Step 7733, rl-loss: 636.1448364257812\r",
      "INFO - Step 7734, rl-loss: 1.7134517431259155\r",
      "INFO - Step 7735, rl-loss: 97.07626342773438\r",
      "INFO - Step 7736, rl-loss: 585.2289428710938\r",
      "INFO - Step 7737, rl-loss: 1.771024465560913\r",
      "INFO - Step 7738, rl-loss: 795.562744140625\r",
      "INFO - Step 7739, rl-loss: 126.64734649658203\r",
      "INFO - Step 7740, rl-loss: 275.84686279296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7741, rl-loss: 1.0314993858337402\r",
      "INFO - Step 7742, rl-loss: 1.074500560760498\r",
      "INFO - Step 7743, rl-loss: 683.4887084960938\r",
      "INFO - Step 7744, rl-loss: 434.021484375\r",
      "INFO - Step 7745, rl-loss: 351.73651123046875\r",
      "INFO - Step 7746, rl-loss: 97.06824493408203\r",
      "INFO - Step 7747, rl-loss: 163.97763061523438\r",
      "INFO - Step 7748, rl-loss: 0.882902979850769\r",
      "INFO - Step 7749, rl-loss: 1.767085075378418\r",
      "INFO - Step 7750, rl-loss: 120.75181579589844\r",
      "INFO - Step 7751, rl-loss: 468.4181823730469\r",
      "INFO - Step 7752, rl-loss: 178.46600341796875\r",
      "INFO - Step 7753, rl-loss: 467.69342041015625\r",
      "INFO - Step 7754, rl-loss: 151.19326782226562\r",
      "INFO - Step 7755, rl-loss: 268.15380859375\r",
      "INFO - Step 7756, rl-loss: 304.6119689941406\r",
      "INFO - Step 7757, rl-loss: 118.07067108154297\r",
      "INFO - Step 7758, rl-loss: 297.1470642089844\r",
      "INFO - Step 7759, rl-loss: 136.30628967285156\r",
      "INFO - Step 7760, rl-loss: 183.37933349609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7761, rl-loss: 64.62127685546875\r",
      "INFO - Step 7762, rl-loss: 54.7984619140625\r",
      "INFO - Step 7763, rl-loss: 452.05889892578125\r",
      "INFO - Step 7764, rl-loss: 104.92838287353516\r",
      "INFO - Step 7765, rl-loss: 1.393463373184204\r",
      "INFO - Step 7766, rl-loss: 126.27299499511719\r",
      "INFO - Step 7767, rl-loss: 641.67529296875\r",
      "INFO - Step 7768, rl-loss: 762.5587768554688\r",
      "INFO - Step 7769, rl-loss: 342.003173828125\r",
      "INFO - Step 7770, rl-loss: 271.9246520996094\r",
      "INFO - Step 7771, rl-loss: 1.5286716222763062\r",
      "INFO - Step 7772, rl-loss: 0.8610092997550964\r",
      "INFO - Step 7773, rl-loss: 46.01143264770508\r",
      "INFO - Step 7774, rl-loss: 500.2388610839844\r",
      "INFO - Step 7775, rl-loss: 51.35581588745117\r",
      "INFO - Step 7776, rl-loss: 23.65308952331543\r",
      "INFO - Step 7777, rl-loss: 87.72218322753906\r",
      "INFO - Step 7778, rl-loss: 236.13522338867188\r",
      "INFO - Step 7779, rl-loss: 153.4239959716797\r",
      "INFO - Step 7780, rl-loss: 121.77418518066406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7781, rl-loss: 103.39689636230469\r",
      "INFO - Step 7782, rl-loss: 46.950111389160156\r",
      "INFO - Step 7783, rl-loss: 305.3611145019531\r",
      "INFO - Step 7784, rl-loss: 117.17412567138672\r",
      "INFO - Step 7785, rl-loss: 109.78897094726562\r",
      "INFO - Step 7786, rl-loss: 172.53472900390625\r",
      "INFO - Step 7787, rl-loss: 1.177628517150879\r",
      "INFO - Step 7788, rl-loss: 707.1221313476562\r",
      "INFO - Step 7789, rl-loss: 2.5957107543945312\r",
      "INFO - Step 7790, rl-loss: 238.46737670898438\r",
      "INFO - Step 7791, rl-loss: 141.7269744873047\r",
      "INFO - Step 7792, rl-loss: 1.168419599533081\r",
      "INFO - Step 7793, rl-loss: 93.78225708007812\r",
      "INFO - Step 7794, rl-loss: 182.66978454589844\r",
      "INFO - Step 7795, rl-loss: 2.2786948680877686\r",
      "INFO - Step 7796, rl-loss: 1.0213736295700073\r",
      "INFO - Step 7797, rl-loss: 111.65475463867188\r",
      "INFO - Step 7798, rl-loss: 63.51371383666992\r",
      "INFO - Step 7799, rl-loss: 102.02836608886719\r",
      "INFO - Step 7800, rl-loss: 294.5121765136719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7801, rl-loss: 17.405691146850586\r",
      "INFO - Step 7802, rl-loss: 21.1533203125\r",
      "INFO - Step 7803, rl-loss: 148.66439819335938\r",
      "INFO - Step 7804, rl-loss: 207.4794921875\r",
      "INFO - Step 7805, rl-loss: 78.30853271484375\r",
      "INFO - Step 7806, rl-loss: 47.28382873535156\r",
      "INFO - Step 7807, rl-loss: 185.45135498046875\r",
      "INFO - Step 7808, rl-loss: 220.06637573242188\r",
      "INFO - Step 7809, rl-loss: 104.29545593261719\r",
      "INFO - Step 7810, rl-loss: 1.3409035205841064\r",
      "INFO - Step 7811, rl-loss: 1.8547792434692383\r",
      "INFO - Step 7812, rl-loss: 154.92877197265625\r",
      "INFO - Step 7813, rl-loss: 41.81570816040039\r",
      "INFO - Step 7814, rl-loss: 101.52030944824219\r",
      "INFO - Step 7815, rl-loss: 1.1690523624420166\r",
      "INFO - Step 7816, rl-loss: 0.6736272573471069\r",
      "INFO - Step 7817, rl-loss: 29.031099319458008\r",
      "INFO - Step 7818, rl-loss: 3.053811550140381\r",
      "INFO - Step 7819, rl-loss: 212.69227600097656\r",
      "INFO - Step 7820, rl-loss: 318.40283203125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7821, rl-loss: 113.96601104736328\r",
      "INFO - Step 7822, rl-loss: 63.991146087646484\r",
      "INFO - Step 7823, rl-loss: 152.95086669921875\r",
      "INFO - Step 7824, rl-loss: 98.92434692382812\r",
      "INFO - Step 7825, rl-loss: 197.07254028320312\r",
      "INFO - Step 7826, rl-loss: 50.43640899658203\r",
      "INFO - Step 7827, rl-loss: 0.9033946394920349\r",
      "INFO - Step 7828, rl-loss: 1.5896967649459839\r",
      "INFO - Step 7829, rl-loss: 225.9220428466797\r",
      "INFO - Step 7830, rl-loss: 257.3786315917969\r",
      "INFO - Step 7831, rl-loss: 104.31774139404297\r",
      "INFO - Step 7832, rl-loss: 237.14993286132812\r",
      "INFO - Step 7833, rl-loss: 65.69723510742188\r",
      "INFO - Step 7834, rl-loss: 231.895751953125\r",
      "INFO - Step 7835, rl-loss: 187.83489990234375\r",
      "INFO - Step 7836, rl-loss: 48.08271789550781\r",
      "INFO - Step 7837, rl-loss: 201.0955047607422\r",
      "INFO - Step 7838, rl-loss: 322.6596374511719\r",
      "INFO - Step 7839, rl-loss: 200.98509216308594\r",
      "INFO - Step 7840, rl-loss: 118.77239227294922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7841, rl-loss: 493.3404235839844\r",
      "INFO - Step 7842, rl-loss: 1.7794198989868164\r",
      "INFO - Step 7843, rl-loss: 225.36300659179688\r",
      "INFO - Step 7844, rl-loss: 1.4222333431243896\r",
      "INFO - Step 7845, rl-loss: 266.9906005859375\r",
      "INFO - Step 7846, rl-loss: 48.49208450317383\r",
      "INFO - Step 7847, rl-loss: 122.58027648925781\r",
      "INFO - Step 7848, rl-loss: 119.73247528076172\r",
      "INFO - Step 7849, rl-loss: 263.9437255859375\r",
      "INFO - Step 7850, rl-loss: 399.1542663574219\r",
      "INFO - Step 7851, rl-loss: 111.02056121826172\r",
      "INFO - Step 7852, rl-loss: 214.30763244628906\r",
      "INFO - Step 7853, rl-loss: 127.78958892822266\r",
      "INFO - Step 7854, rl-loss: 111.64067840576172\r",
      "INFO - Step 7855, rl-loss: 76.76311492919922\r",
      "INFO - Step 7856, rl-loss: 0.9826965928077698\r",
      "INFO - Step 7857, rl-loss: 76.64138793945312\r",
      "INFO - Step 7858, rl-loss: 0.8990870118141174\r",
      "INFO - Step 7859, rl-loss: 382.620849609375\r",
      "INFO - Step 7860, rl-loss: 102.1207504272461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7861, rl-loss: 477.88970947265625\r",
      "INFO - Step 7862, rl-loss: 202.09791564941406\r",
      "INFO - Step 7863, rl-loss: 1.464629888534546\r",
      "INFO - Step 7864, rl-loss: 97.89238739013672\r",
      "INFO - Step 7865, rl-loss: 401.28546142578125\r",
      "INFO - Step 7866, rl-loss: 210.17726135253906\r",
      "INFO - Step 7867, rl-loss: 91.74784088134766\r",
      "INFO - Step 7868, rl-loss: 0.6178880333900452\r",
      "INFO - Step 7869, rl-loss: 582.9083251953125\r",
      "INFO - Step 7870, rl-loss: 106.89445495605469\r",
      "INFO - Step 7871, rl-loss: 526.5652465820312\r",
      "INFO - Step 7872, rl-loss: 138.34947204589844\r",
      "INFO - Step 7873, rl-loss: 401.6217956542969\r",
      "INFO - Step 7874, rl-loss: 311.36309814453125\r",
      "INFO - Step 7875, rl-loss: 57.265811920166016\r",
      "INFO - Step 7876, rl-loss: 63.762203216552734\r",
      "INFO - Step 7877, rl-loss: 45.96440124511719\r",
      "INFO - Step 7878, rl-loss: 71.990234375\r",
      "INFO - Step 7879, rl-loss: 124.76417541503906\r",
      "INFO - Step 7880, rl-loss: 1.1439626216888428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7881, rl-loss: 103.35535430908203\r",
      "INFO - Step 7882, rl-loss: 0.454229474067688\r",
      "INFO - Step 7883, rl-loss: 1.6162047386169434\r",
      "INFO - Step 7884, rl-loss: 86.27403259277344\r",
      "INFO - Step 7885, rl-loss: 1.5196150541305542\r",
      "INFO - Step 7886, rl-loss: 399.78326416015625\r",
      "INFO - Step 7887, rl-loss: 351.98748779296875\r",
      "INFO - Step 7888, rl-loss: 114.5553970336914\r",
      "INFO - Step 7889, rl-loss: 76.51348114013672\r",
      "INFO - Step 7890, rl-loss: 107.23601531982422\r",
      "INFO - Step 7891, rl-loss: 487.54473876953125\r",
      "INFO - Step 7892, rl-loss: 174.65655517578125\r",
      "INFO - Step 7893, rl-loss: 246.86851501464844\r",
      "INFO - Step 7894, rl-loss: 0.7697056531906128\r",
      "INFO - Step 7895, rl-loss: 103.11888122558594\r",
      "INFO - Step 7896, rl-loss: 364.41851806640625\r",
      "INFO - Step 7897, rl-loss: 111.13807678222656\r",
      "INFO - Step 7898, rl-loss: 1.5199689865112305\r",
      "INFO - Step 7899, rl-loss: 240.51776123046875\r",
      "INFO - Step 7900, rl-loss: 5.415806770324707"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7901, rl-loss: 1.211991548538208\r",
      "INFO - Step 7902, rl-loss: 103.1167221069336\r",
      "INFO - Step 7903, rl-loss: 81.38712310791016\r",
      "INFO - Step 7904, rl-loss: 0.8842846751213074\r",
      "INFO - Step 7905, rl-loss: 102.60952758789062\r",
      "INFO - Step 7906, rl-loss: 9.051204681396484\r",
      "INFO - Step 7907, rl-loss: 131.64540100097656\r",
      "INFO - Step 7908, rl-loss: 2.1733345985412598\r",
      "INFO - Step 7909, rl-loss: 315.6329040527344\r",
      "INFO - Step 7910, rl-loss: 1.2490912675857544\r",
      "INFO - Step 7911, rl-loss: 98.97425842285156\r",
      "INFO - Step 7912, rl-loss: 166.61654663085938\r",
      "INFO - Step 7913, rl-loss: 1.4399690628051758\r",
      "INFO - Step 7914, rl-loss: 1.603826642036438\r",
      "INFO - Step 7915, rl-loss: 648.43310546875\r",
      "INFO - Step 7916, rl-loss: 2.6363906860351562\r",
      "INFO - Step 7917, rl-loss: 304.4344177246094\r",
      "INFO - Step 7918, rl-loss: 407.4329833984375\r",
      "INFO - Step 7919, rl-loss: 225.8049774169922\r",
      "INFO - Step 7920, rl-loss: 1.1316616535186768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7921, rl-loss: 293.4031982421875\r",
      "INFO - Step 7922, rl-loss: 163.9407501220703\r",
      "INFO - Step 7923, rl-loss: 1.744488000869751\r",
      "INFO - Step 7924, rl-loss: 178.27210998535156\r",
      "INFO - Step 7925, rl-loss: 91.4903793334961\r",
      "INFO - Step 7926, rl-loss: 248.8907928466797\r",
      "INFO - Step 7927, rl-loss: 492.33648681640625\r",
      "INFO - Step 7928, rl-loss: 147.52655029296875\r",
      "INFO - Step 7929, rl-loss: 117.666015625\r",
      "INFO - Step 7930, rl-loss: 0.8699268102645874\r",
      "INFO - Step 7931, rl-loss: 2.809734582901001\r",
      "INFO - Step 7932, rl-loss: 151.83595275878906\r",
      "INFO - Step 7933, rl-loss: 239.8328857421875\r",
      "INFO - Step 7934, rl-loss: 137.30386352539062\r",
      "INFO - Step 7935, rl-loss: 181.00460815429688\r",
      "INFO - Step 7936, rl-loss: 194.52847290039062\r",
      "INFO - Step 7937, rl-loss: 1.204270362854004\r",
      "INFO - Step 7938, rl-loss: 285.583251953125\r",
      "INFO - Step 7939, rl-loss: 240.39732360839844\r",
      "INFO - Step 7940, rl-loss: 42.68512725830078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7941, rl-loss: 191.34815979003906\r",
      "INFO - Step 7942, rl-loss: 5.690440654754639\r",
      "INFO - Step 7943, rl-loss: 558.9605712890625\r",
      "INFO - Step 7944, rl-loss: 187.6243133544922\r",
      "INFO - Step 7945, rl-loss: 45.944217681884766\r",
      "INFO - Step 7946, rl-loss: 0.6935110092163086\r",
      "INFO - Step 7947, rl-loss: 119.77120208740234\r",
      "INFO - Step 7948, rl-loss: 11.578529357910156\r",
      "INFO - Step 7949, rl-loss: 119.44178771972656\r",
      "INFO - Step 7950, rl-loss: 220.96713256835938\r",
      "INFO - Step 7951, rl-loss: 252.40293884277344\r",
      "INFO - Step 7952, rl-loss: 217.82757568359375\r",
      "INFO - Step 7953, rl-loss: 0.9924061298370361\r",
      "INFO - Step 7954, rl-loss: 469.5826416015625\r",
      "INFO - Step 7955, rl-loss: 712.1820678710938\r",
      "INFO - Step 7956, rl-loss: 0.8941164016723633\r",
      "INFO - Step 7957, rl-loss: 1.8206292390823364\r",
      "INFO - Step 7958, rl-loss: 1.4133487939834595\r",
      "INFO - Step 7959, rl-loss: 382.183349609375\r",
      "INFO - Step 7960, rl-loss: 514.2543334960938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7961, rl-loss: 1.2285590171813965\r",
      "INFO - Step 7962, rl-loss: 143.1822052001953\r",
      "INFO - Step 7963, rl-loss: 325.2041015625\r",
      "INFO - Step 7964, rl-loss: 2.5387380123138428\r",
      "INFO - Step 7965, rl-loss: 302.4195251464844\r",
      "INFO - Step 7966, rl-loss: 84.93531799316406\r",
      "INFO - Step 7967, rl-loss: 194.54537963867188\r",
      "INFO - Step 7968, rl-loss: 282.12213134765625\r",
      "INFO - Step 7969, rl-loss: 104.32366943359375\r",
      "INFO - Step 7970, rl-loss: 245.80987548828125\r",
      "INFO - Step 7971, rl-loss: 131.44944763183594\r",
      "INFO - Step 7972, rl-loss: 480.0142822265625\r",
      "INFO - Step 7973, rl-loss: 42.52946090698242\r",
      "INFO - Step 7974, rl-loss: 32.878387451171875\r",
      "INFO - Step 7975, rl-loss: 100.9293441772461\r",
      "INFO - Step 7976, rl-loss: 181.7477569580078\r",
      "INFO - Step 7977, rl-loss: 113.23268127441406\r",
      "INFO - Step 7978, rl-loss: 181.00518798828125\r",
      "INFO - Step 7979, rl-loss: 542.371337890625\r",
      "INFO - Step 7980, rl-loss: 362.91876220703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 7981, rl-loss: 259.7801818847656\r",
      "INFO - Step 7982, rl-loss: 1070.29052734375\r",
      "INFO - Step 7983, rl-loss: 71.55843353271484\r",
      "INFO - Step 7984, rl-loss: 1.1970336437225342\r",
      "INFO - Step 7985, rl-loss: 855.48193359375\r",
      "INFO - Step 7986, rl-loss: 0.9262092113494873\r",
      "INFO - Step 7987, rl-loss: 345.3266296386719\r",
      "INFO - Step 7988, rl-loss: 294.1698913574219\r",
      "INFO - Step 7989, rl-loss: 123.9301986694336\r",
      "INFO - Step 7990, rl-loss: 40.41567611694336\r",
      "INFO - Step 7991, rl-loss: 47.834049224853516\r",
      "INFO - Step 7992, rl-loss: 149.99951171875\r",
      "INFO - Step 7993, rl-loss: 106.27642822265625\r",
      "INFO - Step 7994, rl-loss: 2.474691867828369\r",
      "INFO - Step 7995, rl-loss: 198.41842651367188\r",
      "INFO - Step 7996, rl-loss: 1.4772595167160034\r",
      "INFO - Step 7997, rl-loss: 679.09912109375\r",
      "INFO - Step 7998, rl-loss: 262.1248779296875\r",
      "INFO - Step 7999, rl-loss: 7.356368064880371\r",
      "INFO - Step 8000, rl-loss: 60.9761848449707"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 8020, rl-loss: 104.49468231201172\n",
      "----------------------------------------\n",
      "  timestep     |  561107\n",
      "  reward       |  62.89\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8021, rl-loss: 93.43696594238281\r",
      "INFO - Step 8022, rl-loss: 134.097412109375\r",
      "INFO - Step 8023, rl-loss: 1.5620296001434326\r",
      "INFO - Step 8024, rl-loss: 472.5370788574219\r",
      "INFO - Step 8025, rl-loss: 212.5345458984375\r",
      "INFO - Step 8026, rl-loss: 337.8059387207031\r",
      "INFO - Step 8027, rl-loss: 227.3171844482422\r",
      "INFO - Step 8028, rl-loss: 323.81768798828125\r",
      "INFO - Step 8029, rl-loss: 385.2628479003906\r",
      "INFO - Step 8030, rl-loss: 379.28106689453125\r",
      "INFO - Step 8031, rl-loss: 2.091069459915161\r",
      "INFO - Step 8032, rl-loss: 157.6846923828125\r",
      "INFO - Step 8033, rl-loss: 40.40055847167969\r",
      "INFO - Step 8034, rl-loss: 1.4983124732971191\r",
      "INFO - Step 8035, rl-loss: 206.36141967773438\r",
      "INFO - Step 8036, rl-loss: 310.3843078613281\r",
      "INFO - Step 8037, rl-loss: 58.264381408691406\r",
      "INFO - Step 8038, rl-loss: 332.2927551269531\r",
      "INFO - Step 8039, rl-loss: 374.2473449707031\r",
      "INFO - Step 8040, rl-loss: 108.1015396118164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8041, rl-loss: 397.82098388671875\r",
      "INFO - Step 8042, rl-loss: 400.3790588378906\r",
      "INFO - Step 8043, rl-loss: 351.1939697265625\r",
      "INFO - Step 8044, rl-loss: 683.693603515625\r",
      "INFO - Step 8045, rl-loss: 190.8538360595703\r",
      "INFO - Step 8046, rl-loss: 146.74957275390625\r",
      "INFO - Step 8047, rl-loss: 213.6289825439453\r",
      "INFO - Step 8048, rl-loss: 323.07196044921875\r",
      "INFO - Step 8049, rl-loss: 1.4504693746566772\r",
      "INFO - Step 8050, rl-loss: 79.87792205810547\r",
      "INFO - Step 8051, rl-loss: 205.96353149414062\r",
      "INFO - Step 8052, rl-loss: 59.741397857666016\r",
      "INFO - Step 8053, rl-loss: 1.1897591352462769\r",
      "INFO - Step 8054, rl-loss: 198.72242736816406\r",
      "INFO - Step 8055, rl-loss: 404.1222229003906\r",
      "INFO - Step 8056, rl-loss: 387.46929931640625\r",
      "INFO - Step 8057, rl-loss: 49.85039520263672\r",
      "INFO - Step 8058, rl-loss: 232.8520965576172\r",
      "INFO - Step 8059, rl-loss: 59.6627197265625\r",
      "INFO - Step 8060, rl-loss: 402.2359619140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8061, rl-loss: 201.75033569335938\r",
      "INFO - Step 8062, rl-loss: 319.744873046875\r",
      "INFO - Step 8063, rl-loss: 179.29054260253906\r",
      "INFO - Step 8064, rl-loss: 75.78546905517578\r",
      "INFO - Step 8065, rl-loss: 224.91262817382812\r",
      "INFO - Step 8066, rl-loss: 201.23699951171875\r",
      "INFO - Step 8067, rl-loss: 1.3724991083145142\r",
      "INFO - Step 8068, rl-loss: 178.0184783935547\r",
      "INFO - Step 8069, rl-loss: 76.7290267944336\r",
      "INFO - Step 8070, rl-loss: 0.879725456237793\r",
      "INFO - Step 8071, rl-loss: 418.1116943359375\r",
      "INFO - Step 8072, rl-loss: 216.59576416015625\r",
      "INFO - Step 8073, rl-loss: 271.8625793457031\r",
      "INFO - Step 8074, rl-loss: 26.133813858032227\r",
      "INFO - Step 8075, rl-loss: 39.62841033935547\r",
      "INFO - Step 8076, rl-loss: 103.01439666748047\r",
      "INFO - Step 8077, rl-loss: 51.54456329345703\r",
      "INFO - Step 8078, rl-loss: 242.8893280029297\r",
      "INFO - Step 8079, rl-loss: 1.441171407699585\r",
      "INFO - Step 8080, rl-loss: 92.29367065429688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8081, rl-loss: 311.69659423828125\r",
      "INFO - Step 8082, rl-loss: 186.55499267578125\r",
      "INFO - Step 8083, rl-loss: 8.199405670166016\r",
      "INFO - Step 8084, rl-loss: 1.872298002243042\r",
      "INFO - Step 8085, rl-loss: 19.621070861816406\r",
      "INFO - Step 8086, rl-loss: 414.6139221191406\r",
      "INFO - Step 8087, rl-loss: 434.7239074707031\r",
      "INFO - Step 8088, rl-loss: 247.2666015625\r",
      "INFO - Step 8089, rl-loss: 339.8187255859375\r",
      "INFO - Step 8090, rl-loss: 200.2352294921875\r",
      "INFO - Step 8091, rl-loss: 40.47208786010742\r",
      "INFO - Step 8092, rl-loss: 76.53956604003906\r",
      "INFO - Step 8093, rl-loss: 533.542724609375\r",
      "INFO - Step 8094, rl-loss: 0.8552171587944031\r",
      "INFO - Step 8095, rl-loss: 165.6083221435547\r",
      "INFO - Step 8096, rl-loss: 54.504005432128906\r",
      "INFO - Step 8097, rl-loss: 50.94972229003906\r",
      "INFO - Step 8098, rl-loss: 293.6683654785156\r",
      "INFO - Step 8099, rl-loss: 45.72857666015625\r",
      "INFO - Step 8100, rl-loss: 532.051513671875\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8101, rl-loss: 328.8710632324219\r",
      "INFO - Step 8102, rl-loss: 132.12136840820312\r",
      "INFO - Step 8103, rl-loss: 399.568603515625\r",
      "INFO - Step 8104, rl-loss: 132.20050048828125\r",
      "INFO - Step 8105, rl-loss: 32.47356033325195\r",
      "INFO - Step 8106, rl-loss: 105.67992401123047\r",
      "INFO - Step 8107, rl-loss: 334.7012939453125\r",
      "INFO - Step 8108, rl-loss: 393.73846435546875\r",
      "INFO - Step 8109, rl-loss: 1.429997205734253\r",
      "INFO - Step 8110, rl-loss: 101.98461151123047\r",
      "INFO - Step 8111, rl-loss: 41.64482879638672\r",
      "INFO - Step 8112, rl-loss: 958.07861328125\r",
      "INFO - Step 8113, rl-loss: 1.8356691598892212\r",
      "INFO - Step 8114, rl-loss: 174.51759338378906\r",
      "INFO - Step 8115, rl-loss: 540.333984375\r",
      "INFO - Step 8116, rl-loss: 755.382080078125\r",
      "INFO - Step 8117, rl-loss: 158.5447235107422\r",
      "INFO - Step 8118, rl-loss: 443.47882080078125\r",
      "INFO - Step 8119, rl-loss: 224.5521697998047\r",
      "INFO - Step 8120, rl-loss: 2.4475209712982178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8121, rl-loss: 117.02991485595703\r",
      "INFO - Step 8122, rl-loss: 111.48025512695312\r",
      "INFO - Step 8123, rl-loss: 207.54501342773438\r",
      "INFO - Step 8124, rl-loss: 340.879150390625\r",
      "INFO - Step 8125, rl-loss: 1.1260898113250732\r",
      "INFO - Step 8126, rl-loss: 290.479248046875\r",
      "INFO - Step 8127, rl-loss: 319.17901611328125\r",
      "INFO - Step 8128, rl-loss: 144.8136444091797\r",
      "INFO - Step 8129, rl-loss: 226.75778198242188\r",
      "INFO - Step 8130, rl-loss: 1.8417487144470215\r",
      "INFO - Step 8131, rl-loss: 191.20571899414062\r",
      "INFO - Step 8132, rl-loss: 298.7308654785156\r",
      "INFO - Step 8133, rl-loss: 67.64933013916016\r",
      "INFO - Step 8134, rl-loss: 0.9636606574058533\r",
      "INFO - Step 8135, rl-loss: 116.54478454589844\r",
      "INFO - Step 8136, rl-loss: 1.2696630954742432\r",
      "INFO - Step 8137, rl-loss: 430.5744323730469\r",
      "INFO - Step 8138, rl-loss: 17.968414306640625\r",
      "INFO - Step 8139, rl-loss: 110.36737060546875\r",
      "INFO - Step 8140, rl-loss: 284.761474609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8141, rl-loss: 491.53302001953125\r",
      "INFO - Step 8142, rl-loss: 0.593466579914093\r",
      "INFO - Step 8143, rl-loss: 0.9806274175643921\r",
      "INFO - Step 8144, rl-loss: 342.3185729980469\r",
      "INFO - Step 8145, rl-loss: 302.194091796875\r",
      "INFO - Step 8146, rl-loss: 185.55596923828125\r",
      "INFO - Step 8147, rl-loss: 87.75849151611328\r",
      "INFO - Step 8148, rl-loss: 102.91302490234375\r",
      "INFO - Step 8149, rl-loss: 73.38493347167969\r",
      "INFO - Step 8150, rl-loss: 210.3894500732422\r",
      "INFO - Step 8151, rl-loss: 270.7152404785156\r",
      "INFO - Step 8152, rl-loss: 646.9998779296875\r",
      "INFO - Step 8153, rl-loss: 0.944037914276123\r",
      "INFO - Step 8154, rl-loss: 0.7936439514160156\r",
      "INFO - Step 8155, rl-loss: 84.08490753173828\r",
      "INFO - Step 8156, rl-loss: 240.3925323486328\r",
      "INFO - Step 8157, rl-loss: 48.25053405761719\r",
      "INFO - Step 8158, rl-loss: 154.91598510742188\r",
      "INFO - Step 8159, rl-loss: 175.2848663330078\r",
      "INFO - Step 8160, rl-loss: 0.8324112892150879"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8161, rl-loss: 558.851806640625\r",
      "INFO - Step 8162, rl-loss: 34.57066345214844\r",
      "INFO - Step 8163, rl-loss: 62.79801940917969\r",
      "INFO - Step 8164, rl-loss: 124.03650665283203\r",
      "INFO - Step 8165, rl-loss: 351.0023498535156\r",
      "INFO - Step 8166, rl-loss: 59.7974853515625\r",
      "INFO - Step 8167, rl-loss: 108.33966827392578\r",
      "INFO - Step 8168, rl-loss: 437.7200927734375\r",
      "INFO - Step 8169, rl-loss: 240.44912719726562\r",
      "INFO - Step 8170, rl-loss: 122.20821380615234\r",
      "INFO - Step 8171, rl-loss: 249.7322998046875\r",
      "INFO - Step 8172, rl-loss: 1.3391480445861816\r",
      "INFO - Step 8173, rl-loss: 177.16061401367188\r",
      "INFO - Step 8174, rl-loss: 249.97113037109375\r",
      "INFO - Step 8175, rl-loss: 127.50052642822266\r",
      "INFO - Step 8176, rl-loss: 1.1070842742919922\r",
      "INFO - Step 8177, rl-loss: 2.025397300720215\r",
      "INFO - Step 8178, rl-loss: 187.72874450683594\r",
      "INFO - Step 8179, rl-loss: 462.1622314453125\r",
      "INFO - Step 8180, rl-loss: 126.24639129638672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8181, rl-loss: 211.72119140625\r",
      "INFO - Step 8182, rl-loss: 0.8068670034408569\r",
      "INFO - Step 8183, rl-loss: 474.68829345703125\r",
      "INFO - Step 8184, rl-loss: 76.22669982910156\r",
      "INFO - Step 8185, rl-loss: 1.9373196363449097\r",
      "INFO - Step 8186, rl-loss: 311.6517639160156\r",
      "INFO - Step 8187, rl-loss: 124.949951171875\r",
      "INFO - Step 8188, rl-loss: 138.55911254882812\r",
      "INFO - Step 8189, rl-loss: 486.1173095703125\r",
      "INFO - Step 8190, rl-loss: 294.77923583984375\r",
      "INFO - Step 8191, rl-loss: 60.84832763671875\r",
      "INFO - Step 8192, rl-loss: 102.45519256591797\r",
      "INFO - Step 8193, rl-loss: 385.1139221191406\r",
      "INFO - Step 8194, rl-loss: 1.8807101249694824\r",
      "INFO - Step 8195, rl-loss: 653.5925903320312\r",
      "INFO - Step 8196, rl-loss: 749.3069458007812\r",
      "INFO - Step 8197, rl-loss: 363.2464294433594\r",
      "INFO - Step 8198, rl-loss: 301.1499328613281\r",
      "INFO - Step 8199, rl-loss: 61.71229934692383\r",
      "INFO - Step 8200, rl-loss: 1.6770527362823486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8201, rl-loss: 249.95008850097656\r",
      "INFO - Step 8202, rl-loss: 1.6613750457763672\r",
      "INFO - Step 8203, rl-loss: 298.60784912109375\r",
      "INFO - Step 8204, rl-loss: 52.4077033996582\r",
      "INFO - Step 8205, rl-loss: 200.50701904296875\r",
      "INFO - Step 8206, rl-loss: 0.8881257176399231\r",
      "INFO - Step 8207, rl-loss: 857.9920654296875\r",
      "INFO - Step 8208, rl-loss: 122.04136657714844\r",
      "INFO - Step 8209, rl-loss: 47.77854537963867\r",
      "INFO - Step 8210, rl-loss: 59.72256088256836\r",
      "INFO - Step 8211, rl-loss: 1.5160621404647827\r",
      "INFO - Step 8212, rl-loss: 441.5816955566406\r",
      "INFO - Step 8213, rl-loss: 171.5842742919922\r",
      "INFO - Step 8214, rl-loss: 249.80621337890625\r",
      "INFO - Step 8215, rl-loss: 91.22886657714844\r",
      "INFO - Step 8216, rl-loss: 431.5335998535156\r",
      "INFO - Step 8217, rl-loss: 1.1190669536590576\r",
      "INFO - Step 8218, rl-loss: 3.700038433074951\r",
      "INFO - Step 8219, rl-loss: 197.711669921875\r",
      "INFO - Step 8220, rl-loss: 324.7496337890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8221, rl-loss: 261.1142578125\r",
      "INFO - Step 8222, rl-loss: 277.0265808105469\r",
      "INFO - Step 8223, rl-loss: 0.9790714979171753\r",
      "INFO - Step 8224, rl-loss: 99.27349090576172\r",
      "INFO - Step 8225, rl-loss: 126.45356750488281\r",
      "INFO - Step 8226, rl-loss: 48.7882194519043\r",
      "INFO - Step 8227, rl-loss: 259.20135498046875\r",
      "INFO - Step 8228, rl-loss: 90.22789764404297\r",
      "INFO - Step 8229, rl-loss: 461.4866943359375\r",
      "INFO - Step 8230, rl-loss: 228.4594268798828\r",
      "INFO - Step 8231, rl-loss: 373.18408203125\r",
      "INFO - Step 8232, rl-loss: 114.44242858886719\r",
      "INFO - Step 8233, rl-loss: 102.67322540283203\r",
      "INFO - Step 8234, rl-loss: 221.40579223632812\r",
      "INFO - Step 8235, rl-loss: 165.7701416015625\r",
      "INFO - Step 8236, rl-loss: 77.29850769042969\r",
      "INFO - Step 8237, rl-loss: 384.4664306640625\r",
      "INFO - Step 8238, rl-loss: 1.4661080837249756\r",
      "INFO - Step 8239, rl-loss: 1.9762091636657715\r",
      "INFO - Step 8240, rl-loss: 1.3953049182891846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8241, rl-loss: 1.5659260749816895\r",
      "INFO - Step 8242, rl-loss: 1.7814046144485474\r",
      "INFO - Step 8243, rl-loss: 452.257080078125\r",
      "INFO - Step 8244, rl-loss: 230.22914123535156\r",
      "INFO - Step 8245, rl-loss: 144.646484375\r",
      "INFO - Step 8246, rl-loss: 549.7354125976562\r",
      "INFO - Step 8247, rl-loss: 248.2688446044922\r",
      "INFO - Step 8248, rl-loss: 181.78897094726562\r",
      "INFO - Step 8249, rl-loss: 22.592552185058594\r",
      "INFO - Step 8250, rl-loss: 468.08013916015625\r",
      "INFO - Step 8251, rl-loss: 324.1092224121094\r",
      "INFO - Step 8252, rl-loss: 66.157470703125\r",
      "INFO - Step 8253, rl-loss: 46.74207305908203\r",
      "INFO - Step 8254, rl-loss: 47.55010986328125\r",
      "INFO - Step 8255, rl-loss: 1.4723782539367676\r",
      "INFO - Step 8256, rl-loss: 476.61419677734375\r",
      "INFO - Step 8257, rl-loss: 304.3451843261719\r",
      "INFO - Step 8258, rl-loss: 188.60696411132812\r",
      "INFO - Step 8259, rl-loss: 185.32089233398438\r",
      "INFO - Step 8260, rl-loss: 120.78831481933594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8261, rl-loss: 158.19654846191406\r",
      "INFO - Step 8262, rl-loss: 163.6409454345703\r",
      "INFO - Step 8263, rl-loss: 428.21917724609375\r",
      "INFO - Step 8264, rl-loss: 451.4322204589844\r",
      "INFO - Step 8265, rl-loss: 46.35688018798828\r",
      "INFO - Step 8266, rl-loss: 121.377197265625\r",
      "INFO - Step 8267, rl-loss: 489.9397888183594\r",
      "INFO - Step 8268, rl-loss: 2.129939556121826\r",
      "INFO - Step 8269, rl-loss: 1.8285164833068848\r",
      "INFO - Step 8270, rl-loss: 105.29230499267578\r",
      "INFO - Step 8271, rl-loss: 48.52360153198242\r",
      "INFO - Step 8272, rl-loss: 2.067166566848755\r",
      "INFO - Step 8273, rl-loss: 233.49606323242188\r",
      "INFO - Step 8274, rl-loss: 89.25472259521484\r",
      "INFO - Step 8275, rl-loss: 331.8954162597656\r",
      "INFO - Step 8276, rl-loss: 178.39097595214844\r",
      "INFO - Step 8277, rl-loss: 203.83209228515625\r",
      "INFO - Step 8278, rl-loss: 238.99392700195312\r",
      "INFO - Step 8279, rl-loss: 203.45497131347656\r",
      "INFO - Step 8280, rl-loss: 1.6084625720977783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8281, rl-loss: 159.43394470214844\r",
      "INFO - Step 8282, rl-loss: 141.5189971923828\r",
      "INFO - Step 8283, rl-loss: 272.5289001464844\r",
      "INFO - Step 8284, rl-loss: 366.43145751953125\r",
      "INFO - Step 8285, rl-loss: 588.259765625\r",
      "INFO - Step 8286, rl-loss: 104.22577667236328\r",
      "INFO - Step 8287, rl-loss: 147.7163543701172\r",
      "INFO - Step 8288, rl-loss: 456.3328857421875\r",
      "INFO - Step 8289, rl-loss: 276.86273193359375\r",
      "INFO - Step 8290, rl-loss: 2.2393088340759277\r",
      "INFO - Step 8291, rl-loss: 44.940486907958984\r",
      "INFO - Step 8292, rl-loss: 513.3627319335938\r",
      "INFO - Step 8293, rl-loss: 206.18069458007812\r",
      "INFO - Step 8294, rl-loss: 535.10107421875\r",
      "INFO - Step 8295, rl-loss: 160.0812225341797\r",
      "INFO - Step 8296, rl-loss: 77.02198028564453\r",
      "INFO - Step 8297, rl-loss: 302.1881408691406\r",
      "INFO - Step 8298, rl-loss: 80.83656311035156\r",
      "INFO - Step 8299, rl-loss: 144.80496215820312\r",
      "INFO - Step 8300, rl-loss: 1.2818106412887573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8301, rl-loss: 1.5227158069610596\r",
      "INFO - Step 8302, rl-loss: 55.23648452758789\r",
      "INFO - Step 8303, rl-loss: 543.9338989257812\r",
      "INFO - Step 8304, rl-loss: 2.258903741836548\r",
      "INFO - Step 8305, rl-loss: 1.4503018856048584\r",
      "INFO - Step 8306, rl-loss: 209.00697326660156\r",
      "INFO - Step 8307, rl-loss: 215.7340545654297\r",
      "INFO - Step 8308, rl-loss: 0.875673234462738\r",
      "INFO - Step 8309, rl-loss: 208.74240112304688\r",
      "INFO - Step 8310, rl-loss: 2.704993963241577\r",
      "INFO - Step 8311, rl-loss: 199.26486206054688\r",
      "INFO - Step 8312, rl-loss: 1.8563191890716553\r",
      "INFO - Step 8313, rl-loss: 102.88164520263672\r",
      "INFO - Step 8314, rl-loss: 75.28764343261719\r",
      "INFO - Step 8315, rl-loss: 282.0672607421875\r",
      "INFO - Step 8316, rl-loss: 274.7236022949219\r",
      "INFO - Step 8317, rl-loss: 1.0607249736785889\r",
      "INFO - Step 8318, rl-loss: 939.8014526367188\r",
      "INFO - Step 8319, rl-loss: 143.1796875\r",
      "INFO - Step 8320, rl-loss: 280.6318664550781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8321, rl-loss: 54.996604919433594\r",
      "INFO - Step 8322, rl-loss: 254.9434356689453\r",
      "INFO - Step 8323, rl-loss: 108.29449462890625\r",
      "INFO - Step 8324, rl-loss: 100.26953125\r",
      "INFO - Step 8325, rl-loss: 142.82040405273438\r",
      "INFO - Step 8326, rl-loss: 175.2821044921875\r",
      "INFO - Step 8327, rl-loss: 508.6432189941406\r",
      "INFO - Step 8328, rl-loss: 149.22691345214844\r",
      "INFO - Step 8329, rl-loss: 175.33676147460938\r",
      "INFO - Step 8330, rl-loss: 763.369384765625\r",
      "INFO - Step 8331, rl-loss: 138.02557373046875\r",
      "INFO - Step 8332, rl-loss: 5.170590400695801\r",
      "INFO - Step 8333, rl-loss: 288.96356201171875\r",
      "INFO - Step 8334, rl-loss: 256.60247802734375\r",
      "INFO - Step 8335, rl-loss: 796.006103515625\r",
      "INFO - Step 8336, rl-loss: 480.44879150390625\r",
      "INFO - Step 8337, rl-loss: 335.109375\r",
      "INFO - Step 8338, rl-loss: 662.3231811523438\r",
      "INFO - Step 8339, rl-loss: 1.8951054811477661\r",
      "INFO - Step 8340, rl-loss: 83.8079833984375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8341, rl-loss: 256.48919677734375\r",
      "INFO - Step 8342, rl-loss: 2.235607385635376\r",
      "INFO - Step 8343, rl-loss: 130.5165557861328\r",
      "INFO - Step 8344, rl-loss: 0.859522819519043\r",
      "INFO - Step 8345, rl-loss: 93.64434814453125\r",
      "INFO - Step 8346, rl-loss: 1.3146920204162598\r",
      "INFO - Step 8347, rl-loss: 77.05007934570312\r",
      "INFO - Step 8348, rl-loss: 92.30823516845703\r",
      "INFO - Step 8349, rl-loss: 547.5963134765625\r",
      "INFO - Step 8350, rl-loss: 328.4941101074219\r",
      "INFO - Step 8351, rl-loss: 319.95501708984375\r",
      "INFO - Step 8352, rl-loss: 1.0541977882385254\r",
      "INFO - Step 8353, rl-loss: 1.389066219329834\r",
      "INFO - Step 8354, rl-loss: 288.16241455078125\r",
      "INFO - Step 8355, rl-loss: 65.91381072998047\r",
      "INFO - Step 8356, rl-loss: 228.10601806640625\r",
      "INFO - Step 8357, rl-loss: 426.60552978515625\r",
      "INFO - Step 8358, rl-loss: 225.2689666748047\r",
      "INFO - Step 8359, rl-loss: 309.6390380859375\r",
      "INFO - Step 8360, rl-loss: 115.04682159423828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8361, rl-loss: 497.652099609375\r",
      "INFO - Step 8362, rl-loss: 189.0912322998047\r",
      "INFO - Step 8363, rl-loss: 133.29144287109375\r",
      "INFO - Step 8364, rl-loss: 1.082533836364746\r",
      "INFO - Step 8365, rl-loss: 529.1636352539062\r",
      "INFO - Step 8366, rl-loss: 182.75357055664062\r",
      "INFO - Step 8367, rl-loss: 232.02102661132812\r",
      "INFO - Step 8368, rl-loss: 223.22923278808594\r",
      "INFO - Step 8369, rl-loss: 144.6687774658203\r",
      "INFO - Step 8370, rl-loss: 257.4409484863281\r",
      "INFO - Step 8371, rl-loss: 269.06146240234375\r",
      "INFO - Step 8372, rl-loss: 188.76702880859375\r",
      "INFO - Step 8373, rl-loss: 115.1377944946289\r",
      "INFO - Step 8374, rl-loss: 354.65167236328125\r",
      "INFO - Step 8375, rl-loss: 83.4076156616211\r",
      "INFO - Step 8376, rl-loss: 496.5633239746094\r",
      "INFO - Step 8377, rl-loss: 182.16757202148438\r",
      "INFO - Step 8378, rl-loss: 143.25892639160156\r",
      "INFO - Step 8379, rl-loss: 666.8646850585938\r",
      "INFO - Step 8380, rl-loss: 250.2316436767578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8381, rl-loss: 1.181774616241455\r",
      "INFO - Step 8382, rl-loss: 330.3734130859375\r",
      "INFO - Step 8383, rl-loss: 102.28948211669922\r",
      "INFO - Step 8384, rl-loss: 104.13536071777344\r",
      "INFO - Step 8385, rl-loss: 378.64398193359375\r",
      "INFO - Step 8386, rl-loss: 228.2299041748047\r",
      "INFO - Step 8387, rl-loss: 1.3513458967208862\r",
      "INFO - Step 8388, rl-loss: 123.01712799072266\r",
      "INFO - Step 8389, rl-loss: 2.3064398765563965\r",
      "INFO - Step 8390, rl-loss: 430.3912658691406\r",
      "INFO - Step 8391, rl-loss: 1.372241497039795\r",
      "INFO - Step 8392, rl-loss: 203.72842407226562\r",
      "INFO - Step 8393, rl-loss: 96.33795928955078\r",
      "INFO - Step 8394, rl-loss: 186.99258422851562\r",
      "INFO - Step 8395, rl-loss: 1.2847106456756592\r",
      "INFO - Step 8396, rl-loss: 157.1826171875\r",
      "INFO - Step 8397, rl-loss: 180.5268096923828\r",
      "INFO - Step 8398, rl-loss: 45.08153533935547\r",
      "INFO - Step 8399, rl-loss: 44.85285186767578\r",
      "INFO - Step 8400, rl-loss: 256.8639221191406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8401, rl-loss: 131.82513427734375\r",
      "INFO - Step 8402, rl-loss: 70.13719177246094\r",
      "INFO - Step 8403, rl-loss: 14.136855125427246\r",
      "INFO - Step 8404, rl-loss: 128.74453735351562\r",
      "INFO - Step 8405, rl-loss: 888.092041015625\r",
      "INFO - Step 8406, rl-loss: 0.9676609039306641\r",
      "INFO - Step 8407, rl-loss: 278.4194030761719\r",
      "INFO - Step 8408, rl-loss: 1.7789812088012695\r",
      "INFO - Step 8409, rl-loss: 1.3118253946304321\r",
      "INFO - Step 8410, rl-loss: 0.8831020593643188\r",
      "INFO - Step 8411, rl-loss: 147.29161071777344\r",
      "INFO - Step 8412, rl-loss: 95.2409439086914\r",
      "INFO - Step 8413, rl-loss: 341.4727478027344\r",
      "INFO - Step 8414, rl-loss: 171.05206298828125\r",
      "INFO - Step 8415, rl-loss: 41.378929138183594\r",
      "INFO - Step 8416, rl-loss: 232.40206909179688\r",
      "INFO - Step 8417, rl-loss: 2.126849889755249\r",
      "INFO - Step 8418, rl-loss: 306.4061584472656\r",
      "INFO - Step 8419, rl-loss: 1.272414207458496\r",
      "INFO - Step 8420, rl-loss: 74.96822357177734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8421, rl-loss: 27.534690856933594\r",
      "INFO - Step 8422, rl-loss: 155.17062377929688\r",
      "INFO - Step 8423, rl-loss: 159.7107391357422\r",
      "INFO - Step 8424, rl-loss: 0.5994815826416016\r",
      "INFO - Step 8425, rl-loss: 30.750951766967773\r",
      "INFO - Step 8426, rl-loss: 99.20463562011719\r",
      "INFO - Step 8427, rl-loss: 125.5564193725586\r",
      "INFO - Step 8428, rl-loss: 383.9390563964844\r",
      "INFO - Step 8429, rl-loss: 97.07791137695312\r",
      "INFO - Step 8430, rl-loss: 115.0555191040039\r",
      "INFO - Step 8431, rl-loss: 114.91571044921875\r",
      "INFO - Step 8432, rl-loss: 286.87457275390625\r",
      "INFO - Step 8433, rl-loss: 325.6195068359375\r",
      "INFO - Step 8434, rl-loss: 147.48985290527344\r",
      "INFO - Step 8435, rl-loss: 46.152976989746094\r",
      "INFO - Step 8436, rl-loss: 137.3318634033203\r",
      "INFO - Step 8437, rl-loss: 157.8222198486328\r",
      "INFO - Step 8438, rl-loss: 60.73264694213867\r",
      "INFO - Step 8439, rl-loss: 346.20941162109375\r",
      "INFO - Step 8440, rl-loss: 211.99887084960938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8441, rl-loss: 313.8892822265625\r",
      "INFO - Step 8442, rl-loss: 2.015176296234131\r",
      "INFO - Step 8443, rl-loss: 1.645868182182312\r",
      "INFO - Step 8444, rl-loss: 103.69548034667969\r",
      "INFO - Step 8445, rl-loss: 2.354823112487793\r",
      "INFO - Step 8446, rl-loss: 145.2602996826172\r",
      "INFO - Step 8447, rl-loss: 430.3206481933594\r",
      "INFO - Step 8448, rl-loss: 1.6546235084533691\r",
      "INFO - Step 8449, rl-loss: 41.69057846069336\r",
      "INFO - Step 8450, rl-loss: 494.6275329589844\r",
      "INFO - Step 8451, rl-loss: 500.8941345214844\r",
      "INFO - Step 8452, rl-loss: 287.207275390625\r",
      "INFO - Step 8453, rl-loss: 154.47055053710938\r",
      "INFO - Step 8454, rl-loss: 1.4954017400741577\r",
      "INFO - Step 8455, rl-loss: 537.861328125\r",
      "INFO - Step 8456, rl-loss: 174.44908142089844\r",
      "INFO - Step 8457, rl-loss: 62.148048400878906\r",
      "INFO - Step 8458, rl-loss: 714.2682495117188\r",
      "INFO - Step 8459, rl-loss: 33.69022750854492\r",
      "INFO - Step 8460, rl-loss: 520.4859619140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8461, rl-loss: 1.0419726371765137\r",
      "INFO - Step 8462, rl-loss: 537.3421020507812\r",
      "INFO - Step 8463, rl-loss: 1.790632963180542\r",
      "INFO - Step 8464, rl-loss: 217.0473175048828\r",
      "INFO - Step 8465, rl-loss: 93.2696762084961\r",
      "INFO - Step 8466, rl-loss: 726.59130859375\r",
      "INFO - Step 8467, rl-loss: 339.93695068359375\r",
      "INFO - Step 8468, rl-loss: 372.08441162109375\r",
      "INFO - Step 8469, rl-loss: 107.07752227783203\r",
      "INFO - Step 8470, rl-loss: 282.4921569824219\r",
      "INFO - Step 8471, rl-loss: 1184.9261474609375\r",
      "INFO - Step 8472, rl-loss: 0.9059288501739502\r",
      "INFO - Step 8473, rl-loss: 92.36650085449219\r",
      "INFO - Step 8474, rl-loss: 127.53691864013672\r",
      "INFO - Step 8475, rl-loss: 436.0137634277344\r",
      "INFO - Step 8476, rl-loss: 216.54159545898438\r",
      "INFO - Step 8477, rl-loss: 80.61995697021484\r",
      "INFO - Step 8478, rl-loss: 1.2115135192871094\r",
      "INFO - Step 8479, rl-loss: 98.82335662841797\r",
      "INFO - Step 8480, rl-loss: 307.77691650390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8481, rl-loss: 217.28749084472656\r",
      "INFO - Step 8482, rl-loss: 369.91229248046875\r",
      "INFO - Step 8483, rl-loss: 1.1185109615325928\r",
      "INFO - Step 8484, rl-loss: 204.40467834472656\r",
      "INFO - Step 8485, rl-loss: 315.48541259765625\r",
      "INFO - Step 8486, rl-loss: 1.8820184469223022\r",
      "INFO - Step 8487, rl-loss: 2.942408561706543\r",
      "INFO - Step 8488, rl-loss: 2.068372964859009\r",
      "INFO - Step 8489, rl-loss: 357.3488464355469\r",
      "INFO - Step 8490, rl-loss: 1.4700536727905273\r",
      "INFO - Step 8491, rl-loss: 59.459190368652344\r",
      "INFO - Step 8492, rl-loss: 36.2220458984375\r",
      "INFO - Step 8493, rl-loss: 57.22420120239258\r",
      "INFO - Step 8494, rl-loss: 277.6080017089844\r",
      "INFO - Step 8495, rl-loss: 0.8326606154441833\r",
      "INFO - Step 8496, rl-loss: 62.72078323364258\r",
      "INFO - Step 8497, rl-loss: 153.88446044921875\r",
      "INFO - Step 8498, rl-loss: 23.659706115722656\r",
      "INFO - Step 8499, rl-loss: 0.5278201103210449\r",
      "INFO - Step 8500, rl-loss: 177.41812133789062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8501, rl-loss: 41.42193603515625\r",
      "INFO - Step 8502, rl-loss: 419.5882873535156\r",
      "INFO - Step 8503, rl-loss: 73.79566192626953\r",
      "INFO - Step 8504, rl-loss: 93.39323425292969\r",
      "INFO - Step 8505, rl-loss: 49.863094329833984\r",
      "INFO - Step 8506, rl-loss: 433.4349670410156\r",
      "INFO - Step 8507, rl-loss: 317.22509765625\r",
      "INFO - Step 8508, rl-loss: 136.25323486328125\r",
      "INFO - Step 8509, rl-loss: 311.477294921875\r",
      "INFO - Step 8510, rl-loss: 125.28905487060547\r",
      "INFO - Step 8511, rl-loss: 520.6200561523438\r",
      "INFO - Step 8512, rl-loss: 550.2098388671875\r",
      "INFO - Step 8513, rl-loss: 142.83639526367188\r",
      "INFO - Step 8514, rl-loss: 1.6524856090545654\r",
      "INFO - Step 8515, rl-loss: 491.90386962890625\r",
      "INFO - Step 8516, rl-loss: 51.33787155151367\r",
      "INFO - Step 8517, rl-loss: 161.09226989746094\r",
      "INFO - Step 8518, rl-loss: 169.3854217529297\r",
      "INFO - Step 8519, rl-loss: 0.8923830389976501\r",
      "INFO - Step 8520, rl-loss: 671.7304077148438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8521, rl-loss: 72.400146484375\r",
      "INFO - Step 8522, rl-loss: 2.5207393169403076\r",
      "INFO - Step 8523, rl-loss: 125.11492919921875\r",
      "INFO - Step 8524, rl-loss: 1.0206894874572754\r",
      "INFO - Step 8525, rl-loss: 10.457200050354004\r",
      "INFO - Step 8526, rl-loss: 41.986473083496094\r",
      "INFO - Step 8527, rl-loss: 221.68307495117188\r",
      "INFO - Step 8528, rl-loss: 98.42871856689453\r",
      "INFO - Step 8529, rl-loss: 187.7142333984375\r",
      "INFO - Step 8530, rl-loss: 329.5938720703125\r",
      "INFO - Step 8531, rl-loss: 156.54641723632812\r",
      "INFO - Step 8532, rl-loss: 168.1089630126953\r",
      "INFO - Step 8533, rl-loss: 293.1990051269531\r",
      "INFO - Step 8534, rl-loss: 416.38446044921875\r",
      "INFO - Step 8535, rl-loss: 2.5039963722229004\r",
      "INFO - Step 8536, rl-loss: 227.87615966796875\r",
      "INFO - Step 8537, rl-loss: 0.762397050857544\r",
      "INFO - Step 8538, rl-loss: 2.458399772644043\r",
      "INFO - Step 8539, rl-loss: 205.57420349121094\r",
      "INFO - Step 8540, rl-loss: 363.83563232421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8541, rl-loss: 1.032815933227539\r",
      "INFO - Step 8542, rl-loss: 114.73030090332031\r",
      "INFO - Step 8543, rl-loss: 313.6864013671875\r",
      "INFO - Step 8544, rl-loss: 414.6226806640625\r",
      "INFO - Step 8545, rl-loss: 2.253871440887451\r",
      "INFO - Step 8546, rl-loss: 46.614524841308594\r",
      "INFO - Step 8547, rl-loss: 120.7739486694336\r",
      "INFO - Step 8548, rl-loss: 0.7534030079841614\r",
      "INFO - Step 8549, rl-loss: 160.6955108642578\r",
      "INFO - Step 8550, rl-loss: 149.98468017578125\r",
      "INFO - Step 8551, rl-loss: 66.5625\r",
      "INFO - Step 8552, rl-loss: 205.2615203857422\r",
      "INFO - Step 8553, rl-loss: 45.24412536621094\r",
      "INFO - Step 8554, rl-loss: 76.6933364868164\r",
      "INFO - Step 8555, rl-loss: 208.36375427246094\r",
      "INFO - Step 8556, rl-loss: 265.59814453125\r",
      "INFO - Step 8557, rl-loss: 236.52491760253906\r",
      "INFO - Step 8558, rl-loss: 2.6839776039123535\r",
      "INFO - Step 8559, rl-loss: 193.55162048339844\r",
      "INFO - Step 8560, rl-loss: 211.6941680908203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8561, rl-loss: 96.59928131103516\r",
      "INFO - Step 8562, rl-loss: 292.8816833496094\r",
      "INFO - Step 8563, rl-loss: 0.9158614873886108\r",
      "INFO - Step 8564, rl-loss: 12.245174407958984\r",
      "INFO - Step 8565, rl-loss: 0.9232553243637085\r",
      "INFO - Step 8566, rl-loss: 144.41371154785156\r",
      "INFO - Step 8567, rl-loss: 76.0218734741211\r",
      "INFO - Step 8568, rl-loss: 209.84046936035156\r",
      "INFO - Step 8569, rl-loss: 125.63865661621094\r",
      "INFO - Step 8570, rl-loss: 168.66905212402344\r",
      "INFO - Step 8571, rl-loss: 404.8532409667969\r",
      "INFO - Step 8572, rl-loss: 1.5207056999206543\r",
      "INFO - Step 8573, rl-loss: 1.1443252563476562\r",
      "INFO - Step 8574, rl-loss: 47.14849090576172\r",
      "INFO - Step 8575, rl-loss: 329.7039794921875\r",
      "INFO - Step 8576, rl-loss: 121.68406677246094\r",
      "INFO - Step 8577, rl-loss: 947.800537109375\r",
      "INFO - Step 8578, rl-loss: 1.1259840726852417\r",
      "INFO - Step 8579, rl-loss: 41.52024841308594\r",
      "INFO - Step 8580, rl-loss: 85.90251159667969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8581, rl-loss: 58.50938415527344\r",
      "INFO - Step 8582, rl-loss: 8.554544448852539\r",
      "INFO - Step 8583, rl-loss: 170.1606903076172\r",
      "INFO - Step 8584, rl-loss: 282.355712890625\r",
      "INFO - Step 8585, rl-loss: 311.0361328125\r",
      "INFO - Step 8586, rl-loss: 237.04322814941406\r",
      "INFO - Step 8587, rl-loss: 73.22550201416016\r",
      "INFO - Step 8588, rl-loss: 104.82047271728516\r",
      "INFO - Step 8589, rl-loss: 371.556884765625\r",
      "INFO - Step 8590, rl-loss: 1.7376413345336914\r",
      "INFO - Step 8591, rl-loss: 62.669921875\r",
      "INFO - Step 8592, rl-loss: 743.7047119140625\r",
      "INFO - Step 8593, rl-loss: 189.96983337402344\r",
      "INFO - Step 8594, rl-loss: 83.58097076416016\r",
      "INFO - Step 8595, rl-loss: 93.31504821777344\r",
      "INFO - Step 8596, rl-loss: 593.5171508789062\r",
      "INFO - Step 8597, rl-loss: 162.01974487304688\r",
      "INFO - Step 8598, rl-loss: 2.023632049560547\r",
      "INFO - Step 8599, rl-loss: 304.28826904296875\r",
      "INFO - Step 8600, rl-loss: 41.34698486328125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8601, rl-loss: 366.0705871582031\r",
      "INFO - Step 8602, rl-loss: 61.16416549682617\r",
      "INFO - Step 8603, rl-loss: 148.05628967285156\r",
      "INFO - Step 8604, rl-loss: 167.87046813964844\r",
      "INFO - Step 8605, rl-loss: 234.450927734375\r",
      "INFO - Step 8606, rl-loss: 1.4430725574493408\r",
      "INFO - Step 8607, rl-loss: 252.068115234375\r",
      "INFO - Step 8608, rl-loss: 1.1637988090515137\r",
      "INFO - Step 8609, rl-loss: 116.3558578491211\r",
      "INFO - Step 8610, rl-loss: 94.18116760253906\r",
      "INFO - Step 8611, rl-loss: 2.263507843017578\r",
      "INFO - Step 8612, rl-loss: 70.24666595458984\r",
      "INFO - Step 8613, rl-loss: 103.1853256225586\r",
      "INFO - Step 8614, rl-loss: 0.8185560703277588\r",
      "INFO - Step 8615, rl-loss: 271.5994567871094\r",
      "INFO - Step 8616, rl-loss: 62.94647979736328\r",
      "INFO - Step 8617, rl-loss: 80.100341796875\r",
      "INFO - Step 8618, rl-loss: 0.6640235781669617\r",
      "INFO - Step 8619, rl-loss: 124.46949005126953\r",
      "INFO - Step 8620, rl-loss: 61.68892288208008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8621, rl-loss: 132.9788360595703\r",
      "INFO - Step 8622, rl-loss: 261.07037353515625\r",
      "INFO - Step 8623, rl-loss: 106.58124542236328\r",
      "INFO - Step 8624, rl-loss: 312.017333984375\r",
      "INFO - Step 8625, rl-loss: 293.492431640625\r",
      "INFO - Step 8626, rl-loss: 0.9696520566940308\r",
      "INFO - Step 8627, rl-loss: 154.93980407714844\r",
      "INFO - Step 8628, rl-loss: 788.55029296875\r",
      "INFO - Step 8629, rl-loss: 258.2160949707031\r",
      "INFO - Step 8630, rl-loss: 401.6445007324219\r",
      "INFO - Step 8631, rl-loss: 88.41580963134766\r",
      "INFO - Step 8632, rl-loss: 133.45147705078125\r",
      "INFO - Step 8633, rl-loss: 224.57276916503906\r",
      "INFO - Step 8634, rl-loss: 179.36544799804688\r",
      "INFO - Step 8635, rl-loss: 1.3224893808364868\r",
      "INFO - Step 8636, rl-loss: 174.8705291748047\r",
      "INFO - Step 8637, rl-loss: 117.50535583496094\r",
      "INFO - Step 8638, rl-loss: 248.2318878173828\r",
      "INFO - Step 8639, rl-loss: 60.36537551879883\r",
      "INFO - Step 8640, rl-loss: 286.84423828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8641, rl-loss: 368.9339294433594\r",
      "INFO - Step 8642, rl-loss: 163.4606170654297\r",
      "INFO - Step 8643, rl-loss: 288.2937927246094\r",
      "INFO - Step 8644, rl-loss: 1.1109298467636108\r",
      "INFO - Step 8645, rl-loss: 130.5972900390625\r",
      "INFO - Step 8646, rl-loss: 1.3883123397827148\r",
      "INFO - Step 8647, rl-loss: 101.66532897949219\r",
      "INFO - Step 8648, rl-loss: 79.1810073852539\r",
      "INFO - Step 8649, rl-loss: 372.1873779296875\r",
      "INFO - Step 8650, rl-loss: 117.77277374267578\r",
      "INFO - Step 8651, rl-loss: 348.5781555175781\r",
      "INFO - Step 8652, rl-loss: 126.92655944824219\r",
      "INFO - Step 8653, rl-loss: 37.872955322265625\r",
      "INFO - Step 8654, rl-loss: 141.1088409423828\r",
      "INFO - Step 8655, rl-loss: 183.03292846679688\r",
      "INFO - Step 8656, rl-loss: 2.550487518310547\r",
      "INFO - Step 8657, rl-loss: 424.3616027832031\r",
      "INFO - Step 8658, rl-loss: 75.73014068603516\r",
      "INFO - Step 8659, rl-loss: 639.861083984375\r",
      "INFO - Step 8660, rl-loss: 264.4733581542969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8661, rl-loss: 1.4860931634902954\r",
      "INFO - Step 8662, rl-loss: 114.23219299316406\r",
      "INFO - Step 8663, rl-loss: 115.0675277709961\r",
      "INFO - Step 8664, rl-loss: 559.1995849609375\r",
      "INFO - Step 8665, rl-loss: 1.2370359897613525\r",
      "INFO - Step 8666, rl-loss: 645.3453979492188\r",
      "INFO - Step 8667, rl-loss: 0.878224790096283\r",
      "INFO - Step 8668, rl-loss: 135.13214111328125\r",
      "INFO - Step 8669, rl-loss: 61.35731506347656\r",
      "INFO - Step 8670, rl-loss: 217.41506958007812\r",
      "INFO - Step 8671, rl-loss: 68.39167785644531\r",
      "INFO - Step 8672, rl-loss: 148.30087280273438\r",
      "INFO - Step 8673, rl-loss: 273.2527160644531\r",
      "INFO - Step 8674, rl-loss: 67.33478546142578\r",
      "INFO - Step 8675, rl-loss: 141.42308044433594\r",
      "INFO - Step 8676, rl-loss: 354.9725646972656\r",
      "INFO - Step 8677, rl-loss: 261.1307373046875\r",
      "INFO - Step 8678, rl-loss: 48.460174560546875\r",
      "INFO - Step 8679, rl-loss: 2.0181243419647217\r",
      "INFO - Step 8680, rl-loss: 197.89044189453125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8681, rl-loss: 57.06330108642578\r",
      "INFO - Step 8682, rl-loss: 111.29536437988281\r",
      "INFO - Step 8683, rl-loss: 12.375134468078613\r",
      "INFO - Step 8684, rl-loss: 188.00588989257812\r",
      "INFO - Step 8685, rl-loss: 2.218413829803467\r",
      "INFO - Step 8686, rl-loss: 174.99185180664062\r",
      "INFO - Step 8687, rl-loss: 256.5739440917969\r",
      "INFO - Step 8688, rl-loss: 434.65411376953125\r",
      "INFO - Step 8689, rl-loss: 234.5956268310547\r",
      "INFO - Step 8690, rl-loss: 528.5926513671875\r",
      "INFO - Step 8691, rl-loss: 24.34772491455078\r",
      "INFO - Step 8692, rl-loss: 197.1382293701172\r",
      "INFO - Step 8693, rl-loss: 110.78990173339844\r",
      "INFO - Step 8694, rl-loss: 102.72021484375\r",
      "INFO - Step 8695, rl-loss: 108.4097671508789\r",
      "INFO - Step 8696, rl-loss: 503.00872802734375\r",
      "INFO - Step 8697, rl-loss: 146.00938415527344\r",
      "INFO - Step 8698, rl-loss: 1.5902152061462402\r",
      "INFO - Step 8699, rl-loss: 333.2156982421875\r",
      "INFO - Step 8700, rl-loss: 55.409812927246094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8701, rl-loss: 120.43124389648438\r",
      "INFO - Step 8702, rl-loss: 324.1798095703125\r",
      "INFO - Step 8703, rl-loss: 73.9150161743164\r",
      "INFO - Step 8704, rl-loss: 206.76565551757812\r",
      "INFO - Step 8705, rl-loss: 2.7965242862701416\r",
      "INFO - Step 8706, rl-loss: 180.35415649414062\r",
      "INFO - Step 8707, rl-loss: 208.542236328125\r",
      "INFO - Step 8708, rl-loss: 130.5464630126953\r",
      "INFO - Step 8709, rl-loss: 224.7102813720703\r",
      "INFO - Step 8710, rl-loss: 1.7102572917938232\r",
      "INFO - Step 8711, rl-loss: 0.6752773523330688\r",
      "INFO - Step 8712, rl-loss: 297.85626220703125\r",
      "INFO - Step 8713, rl-loss: 296.8282775878906\r",
      "INFO - Step 8714, rl-loss: 61.85639953613281\r",
      "INFO - Step 8715, rl-loss: 54.23063278198242\r",
      "INFO - Step 8716, rl-loss: 558.569580078125\r",
      "INFO - Step 8717, rl-loss: 151.9817657470703\r",
      "INFO - Step 8718, rl-loss: 41.635921478271484\r",
      "INFO - Step 8719, rl-loss: 93.55888366699219\r",
      "INFO - Step 8720, rl-loss: 55.656978607177734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8721, rl-loss: 170.55946350097656\r",
      "INFO - Step 8722, rl-loss: 40.809852600097656\r",
      "INFO - Step 8723, rl-loss: 1.20046067237854\r",
      "INFO - Step 8724, rl-loss: 1.1553304195404053\r",
      "INFO - Step 8725, rl-loss: 319.4715881347656\r",
      "INFO - Step 8726, rl-loss: 354.62158203125\r",
      "INFO - Step 8727, rl-loss: 39.3458137512207\r",
      "INFO - Step 8728, rl-loss: 78.68624114990234\r",
      "INFO - Step 8729, rl-loss: 83.97291564941406\r",
      "INFO - Step 8730, rl-loss: 103.57942199707031\r",
      "INFO - Step 8731, rl-loss: 481.1344909667969\r",
      "INFO - Step 8732, rl-loss: 764.5185546875\r",
      "INFO - Step 8733, rl-loss: 128.07310485839844\r",
      "INFO - Step 8734, rl-loss: 2.9661922454833984\r",
      "INFO - Step 8735, rl-loss: 679.41015625\r",
      "INFO - Step 8736, rl-loss: 265.39398193359375\r",
      "INFO - Step 8737, rl-loss: 211.57122802734375\r",
      "INFO - Step 8738, rl-loss: 0.8734240531921387\r",
      "INFO - Step 8739, rl-loss: 83.74583435058594\r",
      "INFO - Step 8740, rl-loss: 128.3286895751953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8741, rl-loss: 310.144287109375\r",
      "INFO - Step 8742, rl-loss: 247.8501434326172\r",
      "INFO - Step 8743, rl-loss: 1.5032129287719727\r",
      "INFO - Step 8744, rl-loss: 236.5936737060547\r",
      "INFO - Step 8745, rl-loss: 285.66510009765625\r",
      "INFO - Step 8746, rl-loss: 60.266143798828125\r",
      "INFO - Step 8747, rl-loss: 77.63689422607422\r",
      "INFO - Step 8748, rl-loss: 275.1861572265625\r",
      "INFO - Step 8749, rl-loss: 0.6523882150650024\r",
      "INFO - Step 8750, rl-loss: 63.514217376708984\r",
      "INFO - Step 8751, rl-loss: 377.2699279785156\r",
      "INFO - Step 8752, rl-loss: 1.264262318611145\r",
      "INFO - Step 8753, rl-loss: 200.6592559814453\r",
      "INFO - Step 8754, rl-loss: 534.1619873046875\r",
      "INFO - Step 8755, rl-loss: 2.184353828430176\r",
      "INFO - Step 8756, rl-loss: 213.93505859375\r",
      "INFO - Step 8757, rl-loss: 1.1880953311920166\r",
      "INFO - Step 8758, rl-loss: 75.73197174072266\r",
      "INFO - Step 8759, rl-loss: 203.43038940429688\r",
      "INFO - Step 8760, rl-loss: 82.59031677246094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8761, rl-loss: 1.0426493883132935\r",
      "INFO - Step 8762, rl-loss: 405.9066162109375\r",
      "INFO - Step 8763, rl-loss: 114.66536712646484\r",
      "INFO - Step 8764, rl-loss: 425.15753173828125\r",
      "INFO - Step 8765, rl-loss: 63.4381217956543\r",
      "INFO - Step 8766, rl-loss: 618.4468994140625\r",
      "INFO - Step 8767, rl-loss: 466.21478271484375\r",
      "INFO - Step 8768, rl-loss: 48.30349349975586\r",
      "INFO - Step 8769, rl-loss: 46.92448425292969\r",
      "INFO - Step 8770, rl-loss: 32.17012023925781\r",
      "INFO - Step 8771, rl-loss: 70.77899169921875\r",
      "INFO - Step 8772, rl-loss: 1.8602559566497803\r",
      "INFO - Step 8773, rl-loss: 27.9475154876709\r",
      "INFO - Step 8774, rl-loss: 361.7131652832031\r",
      "INFO - Step 8775, rl-loss: 104.8630142211914\r",
      "INFO - Step 8776, rl-loss: 0.8822661638259888\r",
      "INFO - Step 8777, rl-loss: 73.88729095458984\r",
      "INFO - Step 8778, rl-loss: 186.8258514404297\r",
      "INFO - Step 8779, rl-loss: 161.22303771972656\r",
      "INFO - Step 8780, rl-loss: 132.97650146484375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8781, rl-loss: 1.827965497970581\r",
      "INFO - Step 8782, rl-loss: 247.77395629882812\r",
      "INFO - Step 8783, rl-loss: 148.27268981933594\r",
      "INFO - Step 8784, rl-loss: 152.88119506835938\r",
      "INFO - Step 8785, rl-loss: 2.0035247802734375\r",
      "INFO - Step 8786, rl-loss: 163.3434295654297\r",
      "INFO - Step 8787, rl-loss: 615.4165649414062\r",
      "INFO - Step 8788, rl-loss: 788.0621948242188\r",
      "INFO - Step 8789, rl-loss: 1.9160195589065552\r",
      "INFO - Step 8790, rl-loss: 25.313657760620117\r",
      "INFO - Step 8791, rl-loss: 1.9124809503555298\r",
      "INFO - Step 8792, rl-loss: 253.9720458984375\r",
      "INFO - Step 8793, rl-loss: 119.26244354248047\r",
      "INFO - Step 8794, rl-loss: 2.031371593475342\r",
      "INFO - Step 8795, rl-loss: 322.86822509765625\r",
      "INFO - Step 8796, rl-loss: 2.785090684890747\r",
      "INFO - Step 8797, rl-loss: 528.675048828125\r",
      "INFO - Step 8798, rl-loss: 108.74554443359375\r",
      "INFO - Step 8799, rl-loss: 123.91783905029297\r",
      "INFO - Step 8800, rl-loss: 0.811963677406311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8801, rl-loss: 58.15865707397461\r",
      "INFO - Step 8802, rl-loss: 432.93359375\r",
      "INFO - Step 8803, rl-loss: 494.817138671875\r",
      "INFO - Step 8804, rl-loss: 57.81814956665039\r",
      "INFO - Step 8805, rl-loss: 167.01148986816406\r",
      "INFO - Step 8806, rl-loss: 202.39129638671875\r",
      "INFO - Step 8807, rl-loss: 513.5877685546875\r",
      "INFO - Step 8808, rl-loss: 202.3389129638672\r",
      "INFO - Step 8809, rl-loss: 347.07012939453125\r",
      "INFO - Step 8810, rl-loss: 36.54409408569336\r",
      "INFO - Step 8811, rl-loss: 388.0550231933594\r",
      "INFO - Step 8812, rl-loss: 139.63238525390625\r",
      "INFO - Step 8813, rl-loss: 205.16641235351562\r",
      "INFO - Step 8814, rl-loss: 160.91549682617188\r",
      "INFO - Step 8815, rl-loss: 39.37643814086914\r",
      "INFO - Step 8816, rl-loss: 374.7796936035156\r",
      "INFO - Step 8817, rl-loss: 163.8090362548828\r",
      "INFO - Step 8818, rl-loss: 528.082763671875\r",
      "INFO - Step 8819, rl-loss: 213.1240692138672\r",
      "INFO - Step 8820, rl-loss: 494.5210876464844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8821, rl-loss: 389.27496337890625\r",
      "INFO - Step 8822, rl-loss: 83.12060546875\r",
      "INFO - Step 8823, rl-loss: 48.95353698730469\r",
      "INFO - Step 8824, rl-loss: 139.85592651367188\r",
      "INFO - Step 8825, rl-loss: 200.85939025878906\r",
      "INFO - Step 8826, rl-loss: 1.382664442062378\r",
      "INFO - Step 8827, rl-loss: 218.90097045898438\r",
      "INFO - Step 8828, rl-loss: 96.49240112304688\r",
      "INFO - Step 8829, rl-loss: 282.48028564453125\r",
      "INFO - Step 8830, rl-loss: 3.27797794342041\r",
      "INFO - Step 8831, rl-loss: 0.8792957067489624\r",
      "INFO - Step 8832, rl-loss: 37.1347541809082\r",
      "INFO - Step 8833, rl-loss: 243.80426025390625\r",
      "INFO - Step 8834, rl-loss: 965.89990234375\r",
      "INFO - Step 8835, rl-loss: 286.6954650878906\r",
      "INFO - Step 8836, rl-loss: 159.92474365234375\r",
      "INFO - Step 8837, rl-loss: 328.796630859375\r",
      "INFO - Step 8838, rl-loss: 239.63490295410156\r",
      "INFO - Step 8839, rl-loss: 347.98370361328125\r",
      "INFO - Step 8840, rl-loss: 54.34572982788086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8841, rl-loss: 405.0813293457031\r",
      "INFO - Step 8842, rl-loss: 348.27960205078125\r",
      "INFO - Step 8843, rl-loss: 43.705474853515625\r",
      "INFO - Step 8844, rl-loss: 133.56753540039062\r",
      "INFO - Step 8845, rl-loss: 108.88502502441406\r",
      "INFO - Step 8846, rl-loss: 202.2689666748047\r",
      "INFO - Step 8847, rl-loss: 405.2022705078125\r",
      "INFO - Step 8848, rl-loss: 685.8507690429688\r",
      "INFO - Step 8849, rl-loss: 174.31138610839844\r",
      "INFO - Step 8850, rl-loss: 314.2243957519531\r",
      "INFO - Step 8851, rl-loss: 1.0706220865249634\r",
      "INFO - Step 8852, rl-loss: 413.8243713378906\r",
      "INFO - Step 8853, rl-loss: 298.3235168457031\r",
      "INFO - Step 8854, rl-loss: 355.4967346191406\r",
      "INFO - Step 8855, rl-loss: 1.0327551364898682\r",
      "INFO - Step 8856, rl-loss: 561.5604858398438\r",
      "INFO - Step 8857, rl-loss: 48.44983673095703\r",
      "INFO - Step 8858, rl-loss: 542.8206787109375\r",
      "INFO - Step 8859, rl-loss: 100.42046356201172\r",
      "INFO - Step 8860, rl-loss: 411.76617431640625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8861, rl-loss: 127.70909881591797\r",
      "INFO - Step 8862, rl-loss: 183.89801025390625\r",
      "INFO - Step 8863, rl-loss: 2.993419647216797\r",
      "INFO - Step 8864, rl-loss: 26.998777389526367\r",
      "INFO - Step 8865, rl-loss: 3.251056432723999\r",
      "INFO - Step 8866, rl-loss: 242.414794921875\r",
      "INFO - Step 8867, rl-loss: 133.51527404785156\r",
      "INFO - Step 8868, rl-loss: 137.35496520996094\r",
      "INFO - Step 8869, rl-loss: 2.242122173309326\r",
      "INFO - Step 8870, rl-loss: 0.7673730850219727\r",
      "INFO - Step 8871, rl-loss: 1.72543203830719\r",
      "INFO - Step 8872, rl-loss: 183.56460571289062\r",
      "INFO - Step 8873, rl-loss: 134.97665405273438\r",
      "INFO - Step 8874, rl-loss: 174.97811889648438\r",
      "INFO - Step 8875, rl-loss: 222.59536743164062\r",
      "INFO - Step 8876, rl-loss: 220.24098205566406\r",
      "INFO - Step 8877, rl-loss: 42.7976188659668\r",
      "INFO - Step 8878, rl-loss: 310.42889404296875\r",
      "INFO - Step 8879, rl-loss: 499.63726806640625\r",
      "INFO - Step 8880, rl-loss: 184.37892150878906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8881, rl-loss: 0.9589742422103882\r",
      "INFO - Step 8882, rl-loss: 98.56574249267578\r",
      "INFO - Step 8883, rl-loss: 369.68756103515625\r",
      "INFO - Step 8884, rl-loss: 89.28705596923828\r",
      "INFO - Step 8885, rl-loss: 349.609375\r",
      "INFO - Step 8886, rl-loss: 113.56584167480469\r",
      "INFO - Step 8887, rl-loss: 458.3155517578125\r",
      "INFO - Step 8888, rl-loss: 234.115966796875\r",
      "INFO - Step 8889, rl-loss: 234.1581573486328\r",
      "INFO - Step 8890, rl-loss: 0.9375795125961304\r",
      "INFO - Step 8891, rl-loss: 0.844957172870636\r",
      "INFO - Step 8892, rl-loss: 635.4527587890625\r",
      "INFO - Step 8893, rl-loss: 345.88775634765625\r",
      "INFO - Step 8894, rl-loss: 338.57952880859375\r",
      "INFO - Step 8895, rl-loss: 64.90345764160156\r",
      "INFO - Step 8896, rl-loss: 488.6103820800781\r",
      "INFO - Step 8897, rl-loss: 1.3092303276062012\r",
      "INFO - Step 8898, rl-loss: 137.16799926757812\r",
      "INFO - Step 8899, rl-loss: 1.0174384117126465\r",
      "INFO - Step 8900, rl-loss: 1.2661547660827637"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8901, rl-loss: 1.285061240196228\r",
      "INFO - Step 8902, rl-loss: 155.2395477294922\r",
      "INFO - Step 8903, rl-loss: 167.4167938232422\r",
      "INFO - Step 8904, rl-loss: 0.8223472833633423\r",
      "INFO - Step 8905, rl-loss: 0.9063901901245117\r",
      "INFO - Step 8906, rl-loss: 206.617919921875\r",
      "INFO - Step 8907, rl-loss: 305.3081359863281\r",
      "INFO - Step 8908, rl-loss: 260.8596496582031\r",
      "INFO - Step 8909, rl-loss: 1.2739934921264648\r",
      "INFO - Step 8910, rl-loss: 216.44830322265625\r",
      "INFO - Step 8911, rl-loss: 339.54052734375\r",
      "INFO - Step 8912, rl-loss: 180.85043334960938\r",
      "INFO - Step 8913, rl-loss: 731.6422729492188\r",
      "INFO - Step 8914, rl-loss: 75.29600524902344\r",
      "INFO - Step 8915, rl-loss: 72.69324493408203\r",
      "INFO - Step 8916, rl-loss: 613.611572265625\r",
      "INFO - Step 8917, rl-loss: 202.3313446044922\r",
      "INFO - Step 8918, rl-loss: 1.199884057044983\r",
      "INFO - Step 8919, rl-loss: 70.95126342773438\r",
      "INFO - Step 8920, rl-loss: 68.69268798828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8921, rl-loss: 60.59371566772461\r",
      "INFO - Step 8922, rl-loss: 52.92786407470703\r",
      "INFO - Step 8923, rl-loss: 547.311279296875\r",
      "INFO - Step 8924, rl-loss: 61.937137603759766\r",
      "INFO - Step 8925, rl-loss: 180.95339965820312\r",
      "INFO - Step 8926, rl-loss: 248.53143310546875\r",
      "INFO - Step 8927, rl-loss: 1.0970652103424072\r",
      "INFO - Step 8928, rl-loss: 163.22193908691406\r",
      "INFO - Step 8929, rl-loss: 101.6815414428711\r",
      "INFO - Step 8930, rl-loss: 58.3076286315918\r",
      "INFO - Step 8931, rl-loss: 522.2048950195312\r",
      "INFO - Step 8932, rl-loss: 117.16278839111328\r",
      "INFO - Step 8933, rl-loss: 63.4665641784668\r",
      "INFO - Step 8934, rl-loss: 1.2895698547363281\r",
      "INFO - Step 8935, rl-loss: 212.36544799804688\r",
      "INFO - Step 8936, rl-loss: 143.9487762451172\r",
      "INFO - Step 8937, rl-loss: 671.9487915039062\r",
      "INFO - Step 8938, rl-loss: 4.47214937210083\r",
      "INFO - Step 8939, rl-loss: 63.205780029296875\r",
      "INFO - Step 8940, rl-loss: 65.66300201416016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8941, rl-loss: 0.8429264426231384\r",
      "INFO - Step 8942, rl-loss: 145.61514282226562\r",
      "INFO - Step 8943, rl-loss: 43.63519287109375\r",
      "INFO - Step 8944, rl-loss: 1.019630789756775\r",
      "INFO - Step 8945, rl-loss: 1.9317562580108643\r",
      "INFO - Step 8946, rl-loss: 354.6896057128906\r",
      "INFO - Step 8947, rl-loss: 614.9104614257812\r",
      "INFO - Step 8948, rl-loss: 1.2941243648529053\r",
      "INFO - Step 8949, rl-loss: 113.23553466796875\r",
      "INFO - Step 8950, rl-loss: 90.08416748046875\r",
      "INFO - Step 8951, rl-loss: 321.1617126464844\r",
      "INFO - Step 8952, rl-loss: 1.4315532445907593\r",
      "INFO - Step 8953, rl-loss: 124.20079040527344\r",
      "INFO - Step 8954, rl-loss: 46.943355560302734\r",
      "INFO - Step 8955, rl-loss: 296.0437927246094\r",
      "INFO - Step 8956, rl-loss: 680.6976318359375\r",
      "INFO - Step 8957, rl-loss: 86.79814147949219\r",
      "INFO - Step 8958, rl-loss: 399.16888427734375\r",
      "INFO - Step 8959, rl-loss: 262.5423583984375\r",
      "INFO - Step 8960, rl-loss: 65.9476318359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8961, rl-loss: 40.785316467285156\r",
      "INFO - Step 8962, rl-loss: 22.098634719848633\r",
      "INFO - Step 8963, rl-loss: 95.58975982666016\r",
      "INFO - Step 8964, rl-loss: 271.4574279785156\r",
      "INFO - Step 8965, rl-loss: 0.6692714691162109\r",
      "INFO - Step 8966, rl-loss: 236.6429443359375\r",
      "INFO - Step 8967, rl-loss: 213.6966552734375\r",
      "INFO - Step 8968, rl-loss: 182.61111450195312\r",
      "INFO - Step 8969, rl-loss: 262.9954528808594\r",
      "INFO - Step 8970, rl-loss: 1.8882238864898682\r",
      "INFO - Step 8971, rl-loss: 174.58148193359375\r",
      "INFO - Step 8972, rl-loss: 194.84571838378906\r",
      "INFO - Step 8973, rl-loss: 105.51776885986328\r",
      "INFO - Step 8974, rl-loss: 7.5781569480896\r",
      "INFO - Step 8975, rl-loss: 45.62232971191406\r",
      "INFO - Step 8976, rl-loss: 71.47616577148438\r",
      "INFO - Step 8977, rl-loss: 206.78182983398438\r",
      "INFO - Step 8978, rl-loss: 175.83261108398438\r",
      "INFO - Step 8979, rl-loss: 294.0914306640625\r",
      "INFO - Step 8980, rl-loss: 353.2109069824219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 8981, rl-loss: 87.73839569091797\r",
      "INFO - Step 8982, rl-loss: 1.4935975074768066\r",
      "INFO - Step 8983, rl-loss: 50.4865608215332\r",
      "INFO - Step 8984, rl-loss: 30.092775344848633\r",
      "INFO - Step 8985, rl-loss: 402.86944580078125\r",
      "INFO - Step 8986, rl-loss: 1.1563931703567505\r",
      "INFO - Step 8987, rl-loss: 1.5433268547058105\r",
      "INFO - Step 8988, rl-loss: 57.370391845703125\r",
      "INFO - Step 8989, rl-loss: 377.6421813964844\r",
      "INFO - Step 8990, rl-loss: 1.2451167106628418\r",
      "INFO - Step 8991, rl-loss: 2.5413496494293213\r",
      "INFO - Step 8992, rl-loss: 188.34732055664062\r",
      "INFO - Step 8993, rl-loss: 1.1243972778320312\r",
      "INFO - Step 8994, rl-loss: 61.73781204223633\r",
      "INFO - Step 8995, rl-loss: 0.9201344847679138\r",
      "INFO - Step 8996, rl-loss: 378.75531005859375\r",
      "INFO - Step 8997, rl-loss: 324.2816162109375\r",
      "INFO - Step 8998, rl-loss: 537.0413208007812\r",
      "INFO - Step 8999, rl-loss: 240.69090270996094\r",
      "INFO - Step 9000, rl-loss: 104.36724090576172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 9020, rl-loss: 3.0975108146667488\n",
      "----------------------------------------\n",
      "  timestep     |  567026\n",
      "  reward       |  56.78\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 9040, rl-loss: 103.11643981933594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9041, rl-loss: 112.25860595703125\r",
      "INFO - Step 9042, rl-loss: 521.5418701171875\r",
      "INFO - Step 9043, rl-loss: 293.5057067871094\r",
      "INFO - Step 9044, rl-loss: 111.89061737060547\r",
      "INFO - Step 9045, rl-loss: 115.72796630859375\r",
      "INFO - Step 9046, rl-loss: 241.6741485595703\r",
      "INFO - Step 9047, rl-loss: 1.7483232021331787\r",
      "INFO - Step 9048, rl-loss: 3.470280885696411\r",
      "INFO - Step 9049, rl-loss: 171.88119506835938\r",
      "INFO - Step 9050, rl-loss: 452.5492858886719\r",
      "INFO - Step 9051, rl-loss: 264.0565490722656\r",
      "INFO - Step 9052, rl-loss: 83.1557388305664\r",
      "INFO - Step 9053, rl-loss: 1.7212704420089722\r",
      "INFO - Step 9054, rl-loss: 129.96585083007812\r",
      "INFO - Step 9055, rl-loss: 51.71711349487305\r",
      "INFO - Step 9056, rl-loss: 0.9913938045501709\r",
      "INFO - Step 9057, rl-loss: 122.10169219970703\r",
      "INFO - Step 9058, rl-loss: 84.66729736328125\r",
      "INFO - Step 9059, rl-loss: 343.9419250488281\r",
      "INFO - Step 9060, rl-loss: 0.6774119138717651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9061, rl-loss: 252.79725646972656\r",
      "INFO - Step 9062, rl-loss: 599.808349609375\r",
      "INFO - Step 9063, rl-loss: 40.905181884765625\r",
      "INFO - Step 9064, rl-loss: 373.4605407714844\r",
      "INFO - Step 9065, rl-loss: 162.29905700683594\r",
      "INFO - Step 9066, rl-loss: 610.5108642578125\r",
      "INFO - Step 9067, rl-loss: 344.38726806640625\r",
      "INFO - Step 9068, rl-loss: 105.87775421142578\r",
      "INFO - Step 9069, rl-loss: 444.6132507324219\r",
      "INFO - Step 9070, rl-loss: 350.5004577636719\r",
      "INFO - Step 9071, rl-loss: 192.29208374023438\r",
      "INFO - Step 9072, rl-loss: 1.11883544921875\r",
      "INFO - Step 9073, rl-loss: 47.34499740600586\r",
      "INFO - Step 9074, rl-loss: 116.24577331542969\r",
      "INFO - Step 9075, rl-loss: 2.1984453201293945\r",
      "INFO - Step 9076, rl-loss: 156.7019805908203\r",
      "INFO - Step 9077, rl-loss: 118.44502258300781\r",
      "INFO - Step 9078, rl-loss: 709.1094970703125\r",
      "INFO - Step 9079, rl-loss: 170.7129364013672\r",
      "INFO - Step 9080, rl-loss: 103.8783950805664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9081, rl-loss: 218.24310302734375\r",
      "INFO - Step 9082, rl-loss: 223.3136444091797\r",
      "INFO - Step 9083, rl-loss: 1.5585931539535522\r",
      "INFO - Step 9084, rl-loss: 422.812744140625\r",
      "INFO - Step 9085, rl-loss: 0.7853449583053589\r",
      "INFO - Step 9086, rl-loss: 331.69854736328125\r",
      "INFO - Step 9087, rl-loss: 76.11919403076172\r",
      "INFO - Step 9088, rl-loss: 0.7180582284927368\r",
      "INFO - Step 9089, rl-loss: 286.4164123535156\r",
      "INFO - Step 9090, rl-loss: 122.84452056884766\r",
      "INFO - Step 9091, rl-loss: 230.3712921142578\r",
      "INFO - Step 9092, rl-loss: 140.51380920410156\r",
      "INFO - Step 9093, rl-loss: 587.0856323242188\r",
      "INFO - Step 9094, rl-loss: 35.51423263549805\r",
      "INFO - Step 9095, rl-loss: 255.978515625\r",
      "INFO - Step 9096, rl-loss: 42.34123229980469\r",
      "INFO - Step 9097, rl-loss: 646.9255981445312\r",
      "INFO - Step 9098, rl-loss: 33.058349609375\r",
      "INFO - Step 9099, rl-loss: 385.04541015625\r",
      "INFO - Step 9100, rl-loss: 223.43991088867188\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9101, rl-loss: 375.4072265625\r",
      "INFO - Step 9102, rl-loss: 3.787797451019287\r",
      "INFO - Step 9103, rl-loss: 63.27078628540039\r",
      "INFO - Step 9104, rl-loss: 1.068843126296997\r",
      "INFO - Step 9105, rl-loss: 78.80604553222656\r",
      "INFO - Step 9106, rl-loss: 42.79555892944336\r",
      "INFO - Step 9107, rl-loss: 850.021728515625\r",
      "INFO - Step 9108, rl-loss: 54.21219253540039\r",
      "INFO - Step 9109, rl-loss: 290.3436279296875\r",
      "INFO - Step 9110, rl-loss: 2.436767101287842\r",
      "INFO - Step 9111, rl-loss: 93.08816528320312\r",
      "INFO - Step 9112, rl-loss: 317.0314025878906\r",
      "INFO - Step 9113, rl-loss: 87.35567474365234\r",
      "INFO - Step 9114, rl-loss: 279.62359619140625\r",
      "INFO - Step 9115, rl-loss: 204.6680145263672\r",
      "INFO - Step 9116, rl-loss: 111.44782257080078\r",
      "INFO - Step 9117, rl-loss: 3.1034655570983887\r",
      "INFO - Step 9118, rl-loss: 447.85504150390625\r",
      "INFO - Step 9119, rl-loss: 1093.2764892578125\r",
      "INFO - Step 9120, rl-loss: 2.553227424621582"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9121, rl-loss: 125.7445068359375\r",
      "INFO - Step 9122, rl-loss: 58.837005615234375\r",
      "INFO - Step 9123, rl-loss: 567.8333129882812\r",
      "INFO - Step 9124, rl-loss: 536.5443115234375\r",
      "INFO - Step 9125, rl-loss: 62.64593505859375\r",
      "INFO - Step 9126, rl-loss: 200.415283203125\r",
      "INFO - Step 9127, rl-loss: 484.9736022949219\r",
      "INFO - Step 9128, rl-loss: 135.7950897216797\r",
      "INFO - Step 9129, rl-loss: 353.5381164550781\r",
      "INFO - Step 9130, rl-loss: 187.2247314453125\r",
      "INFO - Step 9131, rl-loss: 211.98153686523438\r",
      "INFO - Step 9132, rl-loss: 673.20458984375\r",
      "INFO - Step 9133, rl-loss: 5.422122955322266\r",
      "INFO - Step 9134, rl-loss: 87.981201171875\r",
      "INFO - Step 9135, rl-loss: 410.9212646484375\r",
      "INFO - Step 9136, rl-loss: 226.5310516357422\r",
      "INFO - Step 9137, rl-loss: 1.9090726375579834\r",
      "INFO - Step 9138, rl-loss: 909.1741943359375\r",
      "INFO - Step 9139, rl-loss: 175.12803649902344\r",
      "INFO - Step 9140, rl-loss: 1.1569881439208984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9141, rl-loss: 291.04547119140625\r",
      "INFO - Step 9142, rl-loss: 1.1596834659576416\r",
      "INFO - Step 9143, rl-loss: 104.42902374267578\r",
      "INFO - Step 9144, rl-loss: 396.37506103515625\r",
      "INFO - Step 9145, rl-loss: 1.9979526996612549\r",
      "INFO - Step 9146, rl-loss: 31.78447151184082\r",
      "INFO - Step 9147, rl-loss: 93.7010269165039\r",
      "INFO - Step 9148, rl-loss: 2.2202463150024414\r",
      "INFO - Step 9149, rl-loss: 1.574902057647705\r",
      "INFO - Step 9150, rl-loss: 533.6356201171875\r",
      "INFO - Step 9151, rl-loss: 225.93531799316406\r",
      "INFO - Step 9152, rl-loss: 138.39132690429688\r",
      "INFO - Step 9153, rl-loss: 59.47484588623047\r",
      "INFO - Step 9154, rl-loss: 113.52216339111328\r",
      "INFO - Step 9155, rl-loss: 525.0296630859375\r",
      "INFO - Step 9156, rl-loss: 227.376708984375\r",
      "INFO - Step 9157, rl-loss: 135.5248565673828\r",
      "INFO - Step 9158, rl-loss: 518.5265502929688\r",
      "INFO - Step 9159, rl-loss: 122.82418823242188\r",
      "INFO - Step 9160, rl-loss: 79.58781433105469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9161, rl-loss: 21.186464309692383\r",
      "INFO - Step 9162, rl-loss: 154.92568969726562\r",
      "INFO - Step 9163, rl-loss: 1.413185715675354\r",
      "INFO - Step 9164, rl-loss: 1.5202291011810303\r",
      "INFO - Step 9165, rl-loss: 2.6486668586730957\r",
      "INFO - Step 9166, rl-loss: 29.41345977783203\r",
      "INFO - Step 9167, rl-loss: 310.540283203125\r",
      "INFO - Step 9168, rl-loss: 1.0506982803344727\r",
      "INFO - Step 9169, rl-loss: 1.8820850849151611\r",
      "INFO - Step 9170, rl-loss: 254.19686889648438\r",
      "INFO - Step 9171, rl-loss: 1.9016474485397339\r",
      "INFO - Step 9172, rl-loss: 27.438430786132812\r",
      "INFO - Step 9173, rl-loss: 337.1527404785156\r",
      "INFO - Step 9174, rl-loss: 74.71591186523438\r",
      "INFO - Step 9175, rl-loss: 80.38497924804688\r",
      "INFO - Step 9176, rl-loss: 213.2426300048828\r",
      "INFO - Step 9177, rl-loss: 131.3121795654297\r",
      "INFO - Step 9178, rl-loss: 103.91555786132812\r",
      "INFO - Step 9179, rl-loss: 160.3356170654297\r",
      "INFO - Step 9180, rl-loss: 399.3874206542969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9181, rl-loss: 1.7090884447097778\r",
      "INFO - Step 9182, rl-loss: 1.125119686126709\r",
      "INFO - Step 9183, rl-loss: 626.2459106445312\r",
      "INFO - Step 9184, rl-loss: 1.2476966381072998\r",
      "INFO - Step 9185, rl-loss: 99.9935531616211\r",
      "INFO - Step 9186, rl-loss: 91.58061218261719\r",
      "INFO - Step 9187, rl-loss: 456.2303771972656\r",
      "INFO - Step 9188, rl-loss: 369.933349609375\r",
      "INFO - Step 9189, rl-loss: 130.39979553222656\r",
      "INFO - Step 9190, rl-loss: 533.910400390625\r",
      "INFO - Step 9191, rl-loss: 549.155029296875\r",
      "INFO - Step 9192, rl-loss: 296.689697265625\r",
      "INFO - Step 9193, rl-loss: 101.15047454833984\r",
      "INFO - Step 9194, rl-loss: 272.8688659667969\r",
      "INFO - Step 9195, rl-loss: 820.84619140625\r",
      "INFO - Step 9196, rl-loss: 105.44893646240234\r",
      "INFO - Step 9197, rl-loss: 235.67294311523438\r",
      "INFO - Step 9198, rl-loss: 294.07501220703125\r",
      "INFO - Step 9199, rl-loss: 38.95838928222656\r",
      "INFO - Step 9200, rl-loss: 234.02674865722656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9201, rl-loss: 46.601036071777344\r",
      "INFO - Step 9202, rl-loss: 1.3939672708511353\r",
      "INFO - Step 9203, rl-loss: 260.89776611328125\r",
      "INFO - Step 9204, rl-loss: 243.90206909179688\r",
      "INFO - Step 9205, rl-loss: 142.41981506347656\r",
      "INFO - Step 9206, rl-loss: 281.4182434082031\r",
      "INFO - Step 9207, rl-loss: 156.0389862060547\r",
      "INFO - Step 9208, rl-loss: 167.2254180908203\r",
      "INFO - Step 9209, rl-loss: 706.27099609375\r",
      "INFO - Step 9210, rl-loss: 334.1435546875\r",
      "INFO - Step 9211, rl-loss: 1.058617353439331\r",
      "INFO - Step 9212, rl-loss: 218.98187255859375\r",
      "INFO - Step 9213, rl-loss: 822.9822387695312\r",
      "INFO - Step 9214, rl-loss: 85.16317749023438\r",
      "INFO - Step 9215, rl-loss: 256.7364807128906\r",
      "INFO - Step 9216, rl-loss: 305.5635986328125\r",
      "INFO - Step 9217, rl-loss: 128.89749145507812\r",
      "INFO - Step 9218, rl-loss: 277.4044189453125\r",
      "INFO - Step 9219, rl-loss: 286.4125671386719\r",
      "INFO - Step 9220, rl-loss: 293.5132141113281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9221, rl-loss: 2.4056005477905273\r",
      "INFO - Step 9222, rl-loss: 47.17074203491211\r",
      "INFO - Step 9223, rl-loss: 72.68990325927734\r",
      "INFO - Step 9224, rl-loss: 53.30662155151367\r",
      "INFO - Step 9225, rl-loss: 50.31940460205078\r",
      "INFO - Step 9226, rl-loss: 2.824948310852051\r",
      "INFO - Step 9227, rl-loss: 25.690689086914062\r",
      "INFO - Step 9228, rl-loss: 176.6851348876953\r",
      "INFO - Step 9229, rl-loss: 2.8441386222839355\r",
      "INFO - Step 9230, rl-loss: 181.69961547851562\r",
      "INFO - Step 9231, rl-loss: 265.84783935546875\r",
      "INFO - Step 9232, rl-loss: 103.24366760253906\r",
      "INFO - Step 9233, rl-loss: 215.601806640625\r",
      "INFO - Step 9234, rl-loss: 45.26564407348633\r",
      "INFO - Step 9235, rl-loss: 172.00778198242188\r",
      "INFO - Step 9236, rl-loss: 404.9321594238281\r",
      "INFO - Step 9237, rl-loss: 1.2436754703521729\r",
      "INFO - Step 9238, rl-loss: 325.83319091796875\r",
      "INFO - Step 9239, rl-loss: 571.61865234375\r",
      "INFO - Step 9240, rl-loss: 288.8720703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9241, rl-loss: 345.2308044433594\r",
      "INFO - Step 9242, rl-loss: 1.4775913953781128\r",
      "INFO - Step 9243, rl-loss: 96.48201751708984\r",
      "INFO - Step 9244, rl-loss: 8.656729698181152\r",
      "INFO - Step 9245, rl-loss: 1.617355227470398\r",
      "INFO - Step 9246, rl-loss: 8.925741195678711\r",
      "INFO - Step 9247, rl-loss: 1.6434283256530762\r",
      "INFO - Step 9248, rl-loss: 235.9283447265625\r",
      "INFO - Step 9249, rl-loss: 1.3939149379730225\r",
      "INFO - Step 9250, rl-loss: 46.31346130371094\r",
      "INFO - Step 9251, rl-loss: 324.1557922363281\r",
      "INFO - Step 9252, rl-loss: 143.7230682373047\r",
      "INFO - Step 9253, rl-loss: 401.32220458984375\r",
      "INFO - Step 9254, rl-loss: 203.05039978027344\r",
      "INFO - Step 9255, rl-loss: 44.2983512878418\r",
      "INFO - Step 9256, rl-loss: 220.68162536621094\r",
      "INFO - Step 9257, rl-loss: 2.9354848861694336\r",
      "INFO - Step 9258, rl-loss: 3.618309497833252\r",
      "INFO - Step 9259, rl-loss: 486.983154296875\r",
      "INFO - Step 9260, rl-loss: 1.284508228302002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9261, rl-loss: 51.46284484863281\r",
      "INFO - Step 9262, rl-loss: 150.16531372070312\r",
      "INFO - Step 9263, rl-loss: 1.4577739238739014\r",
      "INFO - Step 9264, rl-loss: 1.5436445474624634\r",
      "INFO - Step 9265, rl-loss: 443.64703369140625\r",
      "INFO - Step 9266, rl-loss: 443.9646301269531\r",
      "INFO - Step 9267, rl-loss: 102.53497314453125\r",
      "INFO - Step 9268, rl-loss: 348.97271728515625\r",
      "INFO - Step 9269, rl-loss: 59.37398147583008\r",
      "INFO - Step 9270, rl-loss: 111.44514465332031\r",
      "INFO - Step 9271, rl-loss: 193.47421264648438\r",
      "INFO - Step 9272, rl-loss: 281.5670471191406\r",
      "INFO - Step 9273, rl-loss: 1.856498122215271\r",
      "INFO - Step 9274, rl-loss: 100.61812591552734\r",
      "INFO - Step 9275, rl-loss: 29.325260162353516\r",
      "INFO - Step 9276, rl-loss: 138.74765014648438\r",
      "INFO - Step 9277, rl-loss: 413.5292663574219\r",
      "INFO - Step 9278, rl-loss: 90.14676666259766\r",
      "INFO - Step 9279, rl-loss: 335.9022216796875\r",
      "INFO - Step 9280, rl-loss: 92.07547760009766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9281, rl-loss: 829.421875\r",
      "INFO - Step 9282, rl-loss: 1.9461917877197266\r",
      "INFO - Step 9283, rl-loss: 62.6019401550293\r",
      "INFO - Step 9284, rl-loss: 235.46588134765625\r",
      "INFO - Step 9285, rl-loss: 403.19378662109375\r",
      "INFO - Step 9286, rl-loss: 379.5691223144531\r",
      "INFO - Step 9287, rl-loss: 223.951416015625\r",
      "INFO - Step 9288, rl-loss: 182.2386932373047\r",
      "INFO - Step 9289, rl-loss: 1.915777325630188\r",
      "INFO - Step 9290, rl-loss: 69.38165283203125\r",
      "INFO - Step 9291, rl-loss: 594.4525146484375\r",
      "INFO - Step 9292, rl-loss: 138.65545654296875\r",
      "INFO - Step 9293, rl-loss: 17.80536651611328\r",
      "INFO - Step 9294, rl-loss: 60.22233963012695\r",
      "INFO - Step 9295, rl-loss: 151.7894744873047\r",
      "INFO - Step 9296, rl-loss: 2.9803261756896973\r",
      "INFO - Step 9297, rl-loss: 171.96563720703125\r",
      "INFO - Step 9298, rl-loss: 101.87313842773438\r",
      "INFO - Step 9299, rl-loss: 1.0481019020080566\r",
      "INFO - Step 9300, rl-loss: 369.1410217285156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9301, rl-loss: 44.933509826660156\r",
      "INFO - Step 9302, rl-loss: 195.86572265625\r",
      "INFO - Step 9303, rl-loss: 61.357391357421875\r",
      "INFO - Step 9304, rl-loss: 141.59933471679688\r",
      "INFO - Step 9305, rl-loss: 232.2528533935547\r",
      "INFO - Step 9306, rl-loss: 268.5169677734375\r",
      "INFO - Step 9307, rl-loss: 188.8704071044922\r",
      "INFO - Step 9308, rl-loss: 1.5318427085876465\r",
      "INFO - Step 9309, rl-loss: 200.00775146484375\r",
      "INFO - Step 9310, rl-loss: 103.62643432617188\r",
      "INFO - Step 9311, rl-loss: 443.7843017578125\r",
      "INFO - Step 9312, rl-loss: 733.019287109375\r",
      "INFO - Step 9313, rl-loss: 310.0299072265625\r",
      "INFO - Step 9314, rl-loss: 0.5377037525177002\r",
      "INFO - Step 9315, rl-loss: 219.18508911132812\r",
      "INFO - Step 9316, rl-loss: 1.398261308670044\r",
      "INFO - Step 9317, rl-loss: 71.46525573730469\r",
      "INFO - Step 9318, rl-loss: 5.229007720947266\r",
      "INFO - Step 9319, rl-loss: 9.664369583129883\r",
      "INFO - Step 9320, rl-loss: 1.6812586784362793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9321, rl-loss: 222.90826416015625\r",
      "INFO - Step 9322, rl-loss: 4.951517105102539\r",
      "INFO - Step 9323, rl-loss: 1.5238354206085205\r",
      "INFO - Step 9324, rl-loss: 124.98944091796875\r",
      "INFO - Step 9325, rl-loss: 375.4871826171875\r",
      "INFO - Step 9326, rl-loss: 99.99809265136719\r",
      "INFO - Step 9327, rl-loss: 50.52980041503906\r",
      "INFO - Step 9328, rl-loss: 387.06988525390625\r",
      "INFO - Step 9329, rl-loss: 171.33631896972656\r",
      "INFO - Step 9330, rl-loss: 29.277612686157227\r",
      "INFO - Step 9331, rl-loss: 0.9565294981002808\r",
      "INFO - Step 9332, rl-loss: 176.41244506835938\r",
      "INFO - Step 9333, rl-loss: 13.066180229187012\r",
      "INFO - Step 9334, rl-loss: 637.2819213867188\r",
      "INFO - Step 9335, rl-loss: 1.226438045501709\r",
      "INFO - Step 9336, rl-loss: 139.95716857910156\r",
      "INFO - Step 9337, rl-loss: 0.496432900428772\r",
      "INFO - Step 9338, rl-loss: 24.821096420288086\r",
      "INFO - Step 9339, rl-loss: 367.7650146484375\r",
      "INFO - Step 9340, rl-loss: 2.218014717102051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9341, rl-loss: 0.9748445749282837\r",
      "INFO - Step 9342, rl-loss: 37.95964050292969\r",
      "INFO - Step 9343, rl-loss: 132.85501098632812\r",
      "INFO - Step 9344, rl-loss: 209.2346954345703\r",
      "INFO - Step 9345, rl-loss: 523.8837280273438\r",
      "INFO - Step 9346, rl-loss: 259.3940124511719\r",
      "INFO - Step 9347, rl-loss: 87.89278411865234\r",
      "INFO - Step 9348, rl-loss: 394.2828063964844\r",
      "INFO - Step 9349, rl-loss: 667.5186157226562\r",
      "INFO - Step 9350, rl-loss: 234.0497283935547\r",
      "INFO - Step 9351, rl-loss: 231.57516479492188\r",
      "INFO - Step 9352, rl-loss: 2.158266544342041\r",
      "INFO - Step 9353, rl-loss: 1.1961866617202759\r",
      "INFO - Step 9354, rl-loss: 267.71673583984375\r",
      "INFO - Step 9355, rl-loss: 1.1028704643249512\r",
      "INFO - Step 9356, rl-loss: 103.43197631835938\r",
      "INFO - Step 9357, rl-loss: 257.29351806640625\r",
      "INFO - Step 9358, rl-loss: 220.23052978515625\r",
      "INFO - Step 9359, rl-loss: 878.6402587890625\r",
      "INFO - Step 9360, rl-loss: 283.422607421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9361, rl-loss: 142.9266815185547\r",
      "INFO - Step 9362, rl-loss: 641.3020629882812\r",
      "INFO - Step 9363, rl-loss: 190.61593627929688\r",
      "INFO - Step 9364, rl-loss: 271.42718505859375\r",
      "INFO - Step 9365, rl-loss: 232.6094207763672\r",
      "INFO - Step 9366, rl-loss: 42.84493637084961\r",
      "INFO - Step 9367, rl-loss: 80.22977447509766\r",
      "INFO - Step 9368, rl-loss: 286.4132995605469\r",
      "INFO - Step 9369, rl-loss: 1.4092724323272705\r",
      "INFO - Step 9370, rl-loss: 244.91909790039062\r",
      "INFO - Step 9371, rl-loss: 258.49053955078125\r",
      "INFO - Step 9372, rl-loss: 141.6214599609375\r",
      "INFO - Step 9373, rl-loss: 93.89920043945312\r",
      "INFO - Step 9374, rl-loss: 1.943457841873169\r",
      "INFO - Step 9375, rl-loss: 114.63594055175781\r",
      "INFO - Step 9376, rl-loss: 1.062453031539917\r",
      "INFO - Step 9377, rl-loss: 488.7325134277344\r",
      "INFO - Step 9378, rl-loss: 110.61956024169922\r",
      "INFO - Step 9379, rl-loss: 242.14393615722656\r",
      "INFO - Step 9380, rl-loss: 1.2587840557098389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9381, rl-loss: 247.13670349121094\r",
      "INFO - Step 9382, rl-loss: 322.71630859375\r",
      "INFO - Step 9383, rl-loss: 161.71253967285156\r",
      "INFO - Step 9384, rl-loss: 0.4248211979866028\r",
      "INFO - Step 9385, rl-loss: 0.9719716310501099\r",
      "INFO - Step 9386, rl-loss: 1.3487281799316406\r",
      "INFO - Step 9387, rl-loss: 112.20710754394531\r",
      "INFO - Step 9388, rl-loss: 2.1962966918945312\r",
      "INFO - Step 9389, rl-loss: 100.98121643066406\r",
      "INFO - Step 9390, rl-loss: 372.25677490234375\r",
      "INFO - Step 9391, rl-loss: 0.6428836584091187\r",
      "INFO - Step 9392, rl-loss: 138.3341827392578\r",
      "INFO - Step 9393, rl-loss: 378.2787170410156\r",
      "INFO - Step 9394, rl-loss: 320.5916748046875\r",
      "INFO - Step 9395, rl-loss: 129.2583770751953\r",
      "INFO - Step 9396, rl-loss: 2.460141658782959\r",
      "INFO - Step 9397, rl-loss: 167.93173217773438\r",
      "INFO - Step 9398, rl-loss: 168.4199981689453\r",
      "INFO - Step 9399, rl-loss: 730.9743041992188\r",
      "INFO - Step 9400, rl-loss: 0.7703578472137451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9401, rl-loss: 115.28669738769531\r",
      "INFO - Step 9402, rl-loss: 2.0946383476257324\r",
      "INFO - Step 9403, rl-loss: 132.4671173095703\r",
      "INFO - Step 9404, rl-loss: 286.4088439941406\r",
      "INFO - Step 9405, rl-loss: 301.0255432128906\r",
      "INFO - Step 9406, rl-loss: 216.26971435546875\r",
      "INFO - Step 9407, rl-loss: 62.60527801513672\r",
      "INFO - Step 9408, rl-loss: 0.873814582824707\r",
      "INFO - Step 9409, rl-loss: 0.9041507244110107\r",
      "INFO - Step 9410, rl-loss: 294.4520568847656\r",
      "INFO - Step 9411, rl-loss: 261.0538635253906\r",
      "INFO - Step 9412, rl-loss: 194.74024963378906\r",
      "INFO - Step 9413, rl-loss: 68.82965087890625\r",
      "INFO - Step 9414, rl-loss: 0.48399725556373596\r",
      "INFO - Step 9415, rl-loss: 0.837375283241272\r",
      "INFO - Step 9416, rl-loss: 0.9987596869468689\r",
      "INFO - Step 9417, rl-loss: 717.75927734375\r",
      "INFO - Step 9418, rl-loss: 57.00038528442383\r",
      "INFO - Step 9419, rl-loss: 298.3057861328125\r",
      "INFO - Step 9420, rl-loss: 1.0832022428512573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9421, rl-loss: 116.83055114746094\r",
      "INFO - Step 9422, rl-loss: 1.698967456817627\r",
      "INFO - Step 9423, rl-loss: 94.88555145263672\r",
      "INFO - Step 9424, rl-loss: 60.209075927734375\r",
      "INFO - Step 9425, rl-loss: 92.87379455566406\r",
      "INFO - Step 9426, rl-loss: 2.7260351181030273\r",
      "INFO - Step 9427, rl-loss: 291.80267333984375\r",
      "INFO - Step 9428, rl-loss: 99.11473083496094\r",
      "INFO - Step 9429, rl-loss: 601.9263916015625\r",
      "INFO - Step 9430, rl-loss: 390.2637939453125\r",
      "INFO - Step 9431, rl-loss: 531.4459228515625\r",
      "INFO - Step 9432, rl-loss: 91.92431640625\r",
      "INFO - Step 9433, rl-loss: 1.6771413087844849\r",
      "INFO - Step 9434, rl-loss: 144.8382568359375\r",
      "INFO - Step 9435, rl-loss: 55.397857666015625\r",
      "INFO - Step 9436, rl-loss: 129.8438720703125\r",
      "INFO - Step 9437, rl-loss: 2.157660484313965\r",
      "INFO - Step 9438, rl-loss: 298.8004455566406\r",
      "INFO - Step 9439, rl-loss: 2.2914791107177734\r",
      "INFO - Step 9440, rl-loss: 129.51161193847656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9441, rl-loss: 581.6156005859375\r",
      "INFO - Step 9442, rl-loss: 74.83184814453125\r",
      "INFO - Step 9443, rl-loss: 14.795820236206055\r",
      "INFO - Step 9444, rl-loss: 173.53921508789062\r",
      "INFO - Step 9445, rl-loss: 100.10186004638672\r",
      "INFO - Step 9446, rl-loss: 125.33148193359375\r",
      "INFO - Step 9447, rl-loss: 375.900634765625\r",
      "INFO - Step 9448, rl-loss: 393.6472473144531\r",
      "INFO - Step 9449, rl-loss: 322.7904357910156\r",
      "INFO - Step 9450, rl-loss: 328.18218994140625\r",
      "INFO - Step 9451, rl-loss: 217.30517578125\r",
      "INFO - Step 9452, rl-loss: 118.612060546875\r",
      "INFO - Step 9453, rl-loss: 386.96868896484375\r",
      "INFO - Step 9454, rl-loss: 124.78060913085938\r",
      "INFO - Step 9455, rl-loss: 1.5457801818847656\r",
      "INFO - Step 9456, rl-loss: 569.6881103515625\r",
      "INFO - Step 9457, rl-loss: 38.006683349609375\r",
      "INFO - Step 9458, rl-loss: 1.6734600067138672\r",
      "INFO - Step 9459, rl-loss: 298.67193603515625\r",
      "INFO - Step 9460, rl-loss: 34.647369384765625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9461, rl-loss: 60.51860046386719\r",
      "INFO - Step 9462, rl-loss: 110.54161834716797\r",
      "INFO - Step 9463, rl-loss: 0.9593328833580017\r",
      "INFO - Step 9464, rl-loss: 145.1978759765625\r",
      "INFO - Step 9465, rl-loss: 0.6437908411026001\r",
      "INFO - Step 9466, rl-loss: 318.38201904296875\r",
      "INFO - Step 9467, rl-loss: 585.826171875\r",
      "INFO - Step 9468, rl-loss: 201.43807983398438\r",
      "INFO - Step 9469, rl-loss: 0.7403572797775269\r",
      "INFO - Step 9470, rl-loss: 276.2248229980469\r",
      "INFO - Step 9471, rl-loss: 169.2818145751953\r",
      "INFO - Step 9472, rl-loss: 2.4159791469573975\r",
      "INFO - Step 9473, rl-loss: 52.48060989379883\r",
      "INFO - Step 9474, rl-loss: 1.2271533012390137\r",
      "INFO - Step 9475, rl-loss: 9.533021926879883\r",
      "INFO - Step 9476, rl-loss: 318.4387512207031\r",
      "INFO - Step 9477, rl-loss: 1.4236292839050293\r",
      "INFO - Step 9478, rl-loss: 427.81976318359375\r",
      "INFO - Step 9479, rl-loss: 58.81945037841797\r",
      "INFO - Step 9480, rl-loss: 386.3443298339844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9481, rl-loss: 39.786354064941406\r",
      "INFO - Step 9482, rl-loss: 273.5982360839844\r",
      "INFO - Step 9483, rl-loss: 239.48753356933594\r",
      "INFO - Step 9484, rl-loss: 356.93133544921875\r",
      "INFO - Step 9485, rl-loss: 546.644775390625\r",
      "INFO - Step 9486, rl-loss: 111.011474609375\r",
      "INFO - Step 9487, rl-loss: 192.62606811523438\r",
      "INFO - Step 9488, rl-loss: 283.13861083984375\r",
      "INFO - Step 9489, rl-loss: 231.2169189453125\r",
      "INFO - Step 9490, rl-loss: 277.0635986328125\r",
      "INFO - Step 9491, rl-loss: 106.73725891113281\r",
      "INFO - Step 9492, rl-loss: 200.2793731689453\r",
      "INFO - Step 9493, rl-loss: 1.3127944469451904\r",
      "INFO - Step 9494, rl-loss: 298.0213317871094\r",
      "INFO - Step 9495, rl-loss: 229.20086669921875\r",
      "INFO - Step 9496, rl-loss: 189.59439086914062\r",
      "INFO - Step 9497, rl-loss: 109.65019989013672\r",
      "INFO - Step 9498, rl-loss: 1.5356199741363525\r",
      "INFO - Step 9499, rl-loss: 152.7351531982422\r",
      "INFO - Step 9500, rl-loss: 155.7542724609375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9501, rl-loss: 362.193359375\r",
      "INFO - Step 9502, rl-loss: 128.6067657470703\r",
      "INFO - Step 9503, rl-loss: 0.6380531191825867\r",
      "INFO - Step 9504, rl-loss: 329.7038879394531\r",
      "INFO - Step 9505, rl-loss: 336.8272705078125\r",
      "INFO - Step 9506, rl-loss: 284.5293884277344\r",
      "INFO - Step 9507, rl-loss: 241.97982788085938\r",
      "INFO - Step 9508, rl-loss: 190.2760009765625\r",
      "INFO - Step 9509, rl-loss: 285.26336669921875\r",
      "INFO - Step 9510, rl-loss: 282.5379333496094\r",
      "INFO - Step 9511, rl-loss: 298.9446105957031\r",
      "INFO - Step 9512, rl-loss: 491.95074462890625\r",
      "INFO - Step 9513, rl-loss: 381.3228454589844\r",
      "INFO - Step 9514, rl-loss: 103.2898941040039\r",
      "INFO - Step 9515, rl-loss: 652.09765625\r",
      "INFO - Step 9516, rl-loss: 475.4820556640625\r",
      "INFO - Step 9517, rl-loss: 322.24542236328125\r",
      "INFO - Step 9518, rl-loss: 131.20938110351562\r",
      "INFO - Step 9519, rl-loss: 134.9311981201172\r",
      "INFO - Step 9520, rl-loss: 1.7079589366912842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9521, rl-loss: 144.45506286621094\r",
      "INFO - Step 9522, rl-loss: 207.1551513671875\r",
      "INFO - Step 9523, rl-loss: 44.589664459228516\r",
      "INFO - Step 9524, rl-loss: 233.3521728515625\r",
      "INFO - Step 9525, rl-loss: 60.4233512878418\r",
      "INFO - Step 9526, rl-loss: 100.87184143066406\r",
      "INFO - Step 9527, rl-loss: 94.74193572998047\r",
      "INFO - Step 9528, rl-loss: 2.367241382598877\r",
      "INFO - Step 9529, rl-loss: 149.6542510986328\r",
      "INFO - Step 9530, rl-loss: 172.43923950195312\r",
      "INFO - Step 9531, rl-loss: 136.7167510986328\r",
      "INFO - Step 9532, rl-loss: 3.177330732345581\r",
      "INFO - Step 9533, rl-loss: 59.4368896484375\r",
      "INFO - Step 9534, rl-loss: 384.11163330078125\r",
      "INFO - Step 9535, rl-loss: 156.33816528320312\r",
      "INFO - Step 9536, rl-loss: 0.9970636367797852\r",
      "INFO - Step 9537, rl-loss: 30.93976402282715\r",
      "INFO - Step 9538, rl-loss: 137.69676208496094\r",
      "INFO - Step 9539, rl-loss: 492.4919738769531\r",
      "INFO - Step 9540, rl-loss: 533.4681396484375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9541, rl-loss: 67.66110229492188\r",
      "INFO - Step 9542, rl-loss: 1.4943175315856934\r",
      "INFO - Step 9543, rl-loss: 76.16607666015625\r",
      "INFO - Step 9544, rl-loss: 103.7216567993164\r",
      "INFO - Step 9545, rl-loss: 1.2881003618240356\r",
      "INFO - Step 9546, rl-loss: 1.8037819862365723\r",
      "INFO - Step 9547, rl-loss: 325.80584716796875\r",
      "INFO - Step 9548, rl-loss: 180.2957000732422\r",
      "INFO - Step 9549, rl-loss: 2.6731324195861816\r",
      "INFO - Step 9550, rl-loss: 72.33224487304688\r",
      "INFO - Step 9551, rl-loss: 137.91714477539062\r",
      "INFO - Step 9552, rl-loss: 14.559772491455078\r",
      "INFO - Step 9553, rl-loss: 592.6928100585938\r",
      "INFO - Step 9554, rl-loss: 49.273094177246094\r",
      "INFO - Step 9555, rl-loss: 132.10887145996094\r",
      "INFO - Step 9556, rl-loss: 189.63525390625\r",
      "INFO - Step 9557, rl-loss: 32.23222351074219\r",
      "INFO - Step 9558, rl-loss: 105.1281509399414\r",
      "INFO - Step 9559, rl-loss: 766.2617797851562\r",
      "INFO - Step 9560, rl-loss: 345.21978759765625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9561, rl-loss: 27.19260025024414\r",
      "INFO - Step 9562, rl-loss: 137.4801025390625\r",
      "INFO - Step 9563, rl-loss: 24.935779571533203\r",
      "INFO - Step 9564, rl-loss: 0.7248699069023132\r",
      "INFO - Step 9565, rl-loss: 296.2101745605469\r",
      "INFO - Step 9566, rl-loss: 1.2374427318572998\r",
      "INFO - Step 9567, rl-loss: 26.300527572631836\r",
      "INFO - Step 9568, rl-loss: 91.06301879882812\r",
      "INFO - Step 9569, rl-loss: 112.16226959228516\r",
      "INFO - Step 9570, rl-loss: 146.01156616210938\r",
      "INFO - Step 9571, rl-loss: 88.27153778076172\r",
      "INFO - Step 9572, rl-loss: 2.422663927078247\r",
      "INFO - Step 9573, rl-loss: 220.66957092285156\r",
      "INFO - Step 9574, rl-loss: 1.4128836393356323\r",
      "INFO - Step 9575, rl-loss: 1.4784519672393799\r",
      "INFO - Step 9576, rl-loss: 225.4947052001953\r",
      "INFO - Step 9577, rl-loss: 199.57211303710938\r",
      "INFO - Step 9578, rl-loss: 81.1460189819336\r",
      "INFO - Step 9579, rl-loss: 1.4363124370574951\r",
      "INFO - Step 9580, rl-loss: 115.00640869140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9581, rl-loss: 119.79829406738281\r",
      "INFO - Step 9582, rl-loss: 140.30935668945312\r",
      "INFO - Step 9583, rl-loss: 421.4178466796875\r",
      "INFO - Step 9584, rl-loss: 312.3462829589844\r",
      "INFO - Step 9585, rl-loss: 0.9722719192504883\r",
      "INFO - Step 9586, rl-loss: 1.9389927387237549\r",
      "INFO - Step 9587, rl-loss: 244.40008544921875\r",
      "INFO - Step 9588, rl-loss: 2.0436127185821533\r",
      "INFO - Step 9589, rl-loss: 1.965999960899353\r",
      "INFO - Step 9590, rl-loss: 305.2398376464844\r",
      "INFO - Step 9591, rl-loss: 572.9636840820312\r",
      "INFO - Step 9592, rl-loss: 194.36863708496094\r",
      "INFO - Step 9593, rl-loss: 477.175537109375\r",
      "INFO - Step 9594, rl-loss: 342.240234375\r",
      "INFO - Step 9595, rl-loss: 361.15869140625\r",
      "INFO - Step 9596, rl-loss: 75.27171325683594\r",
      "INFO - Step 9597, rl-loss: 31.597749710083008\r",
      "INFO - Step 9598, rl-loss: 424.16192626953125\r",
      "INFO - Step 9599, rl-loss: 399.58367919921875\r",
      "INFO - Step 9600, rl-loss: 214.36111450195312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9601, rl-loss: 1.7258028984069824\r",
      "INFO - Step 9602, rl-loss: 367.779296875\r",
      "INFO - Step 9603, rl-loss: 156.43763732910156\r",
      "INFO - Step 9604, rl-loss: 126.64094543457031\r",
      "INFO - Step 9605, rl-loss: 513.17138671875\r",
      "INFO - Step 9606, rl-loss: 0.9909771680831909\r",
      "INFO - Step 9607, rl-loss: 76.35469055175781\r",
      "INFO - Step 9608, rl-loss: 73.93383026123047\r",
      "INFO - Step 9609, rl-loss: 24.14541244506836\r",
      "INFO - Step 9610, rl-loss: 176.66018676757812\r",
      "INFO - Step 9611, rl-loss: 141.70562744140625\r",
      "INFO - Step 9612, rl-loss: 201.37196350097656\r",
      "INFO - Step 9613, rl-loss: 2.5156593322753906\r",
      "INFO - Step 9614, rl-loss: 2.956228494644165\r",
      "INFO - Step 9615, rl-loss: 2.048074960708618\r",
      "INFO - Step 9616, rl-loss: 1.4624545574188232\r",
      "INFO - Step 9617, rl-loss: 328.89422607421875\r",
      "INFO - Step 9618, rl-loss: 0.751279354095459\r",
      "INFO - Step 9619, rl-loss: 112.00148010253906\r",
      "INFO - Step 9620, rl-loss: 112.23335266113281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9621, rl-loss: 40.30390167236328\r",
      "INFO - Step 9622, rl-loss: 2.3368942737579346\r",
      "INFO - Step 9623, rl-loss: 486.7513732910156\r",
      "INFO - Step 9624, rl-loss: 763.3034057617188\r",
      "INFO - Step 9625, rl-loss: 99.6142349243164\r",
      "INFO - Step 9626, rl-loss: 308.4365234375\r",
      "INFO - Step 9627, rl-loss: 195.54330444335938\r",
      "INFO - Step 9628, rl-loss: 157.68826293945312\r",
      "INFO - Step 9629, rl-loss: 112.65482330322266\r",
      "INFO - Step 9630, rl-loss: 1.1837992668151855\r",
      "INFO - Step 9631, rl-loss: 141.46014404296875\r",
      "INFO - Step 9632, rl-loss: 162.2755584716797\r",
      "INFO - Step 9633, rl-loss: 284.9403991699219\r",
      "INFO - Step 9634, rl-loss: 251.91790771484375\r",
      "INFO - Step 9635, rl-loss: 119.028564453125\r",
      "INFO - Step 9636, rl-loss: 57.31430435180664\r",
      "INFO - Step 9637, rl-loss: 256.34588623046875\r",
      "INFO - Step 9638, rl-loss: 190.77944946289062\r",
      "INFO - Step 9639, rl-loss: 268.704345703125\r",
      "INFO - Step 9640, rl-loss: 575.8901977539062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9641, rl-loss: 826.1119384765625\r",
      "INFO - Step 9642, rl-loss: 191.16989135742188\r",
      "INFO - Step 9643, rl-loss: 81.22096252441406\r",
      "INFO - Step 9644, rl-loss: 143.3532257080078\r",
      "INFO - Step 9645, rl-loss: 39.47333908081055\r",
      "INFO - Step 9646, rl-loss: 73.16356658935547\r",
      "INFO - Step 9647, rl-loss: 263.2922668457031\r",
      "INFO - Step 9648, rl-loss: 125.82205963134766\r",
      "INFO - Step 9649, rl-loss: 210.2666015625\r",
      "INFO - Step 9650, rl-loss: 93.2215347290039\r",
      "INFO - Step 9651, rl-loss: 25.491451263427734\r",
      "INFO - Step 9652, rl-loss: 116.02244567871094\r",
      "INFO - Step 9653, rl-loss: 1.161198377609253\r",
      "INFO - Step 9654, rl-loss: 210.9080352783203\r",
      "INFO - Step 9655, rl-loss: 471.86651611328125\r",
      "INFO - Step 9656, rl-loss: 1.1916732788085938\r",
      "INFO - Step 9657, rl-loss: 428.47802734375\r",
      "INFO - Step 9658, rl-loss: 1.0353686809539795\r",
      "INFO - Step 9659, rl-loss: 1.4734690189361572\r",
      "INFO - Step 9660, rl-loss: 707.985595703125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9661, rl-loss: 425.7060852050781\r",
      "INFO - Step 9662, rl-loss: 196.46746826171875\r",
      "INFO - Step 9663, rl-loss: 239.67477416992188\r",
      "INFO - Step 9664, rl-loss: 427.5599670410156\r",
      "INFO - Step 9665, rl-loss: 534.5580444335938\r",
      "INFO - Step 9666, rl-loss: 152.2642364501953\r",
      "INFO - Step 9667, rl-loss: 105.73280334472656\r",
      "INFO - Step 9668, rl-loss: 183.55880737304688\r",
      "INFO - Step 9669, rl-loss: 259.6669006347656\r",
      "INFO - Step 9670, rl-loss: 44.56985092163086\r",
      "INFO - Step 9671, rl-loss: 126.3075180053711\r",
      "INFO - Step 9672, rl-loss: 368.9117126464844\r",
      "INFO - Step 9673, rl-loss: 128.95545959472656\r",
      "INFO - Step 9674, rl-loss: 35.64595031738281\r",
      "INFO - Step 9675, rl-loss: 704.8760986328125\r",
      "INFO - Step 9676, rl-loss: 473.0419006347656\r",
      "INFO - Step 9677, rl-loss: 1.0306875705718994\r",
      "INFO - Step 9678, rl-loss: 128.74813842773438\r",
      "INFO - Step 9679, rl-loss: 327.4178466796875\r",
      "INFO - Step 9680, rl-loss: 103.30855560302734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9681, rl-loss: 402.5520324707031\r",
      "INFO - Step 9682, rl-loss: 0.9477300643920898\r",
      "INFO - Step 9683, rl-loss: 210.51846313476562\r",
      "INFO - Step 9684, rl-loss: 24.828550338745117\r",
      "INFO - Step 9685, rl-loss: 149.93801879882812\r",
      "INFO - Step 9686, rl-loss: 0.7267707586288452\r",
      "INFO - Step 9687, rl-loss: 142.5580291748047\r",
      "INFO - Step 9688, rl-loss: 239.78306579589844\r",
      "INFO - Step 9689, rl-loss: 207.76583862304688\r",
      "INFO - Step 9690, rl-loss: 13.515381813049316\r",
      "INFO - Step 9691, rl-loss: 299.46405029296875\r",
      "INFO - Step 9692, rl-loss: 2.186314105987549\r",
      "INFO - Step 9693, rl-loss: 204.12452697753906\r",
      "INFO - Step 9694, rl-loss: 126.16722106933594\r",
      "INFO - Step 9695, rl-loss: 134.37750244140625\r",
      "INFO - Step 9696, rl-loss: 224.94284057617188\r",
      "INFO - Step 9697, rl-loss: 146.971923828125\r",
      "INFO - Step 9698, rl-loss: 83.79508972167969\r",
      "INFO - Step 9699, rl-loss: 71.34292602539062\r",
      "INFO - Step 9700, rl-loss: 261.1795654296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9701, rl-loss: 1.072149634361267\r",
      "INFO - Step 9702, rl-loss: 482.9203186035156\r",
      "INFO - Step 9703, rl-loss: 158.09121704101562\r",
      "INFO - Step 9704, rl-loss: 102.04901123046875\r",
      "INFO - Step 9705, rl-loss: 216.67245483398438\r",
      "INFO - Step 9706, rl-loss: 316.6519470214844\r",
      "INFO - Step 9707, rl-loss: 457.1499938964844\r",
      "INFO - Step 9708, rl-loss: 27.04220199584961\r",
      "INFO - Step 9709, rl-loss: 1.074299931526184\r",
      "INFO - Step 9710, rl-loss: 97.2042007446289\r",
      "INFO - Step 9711, rl-loss: 124.57150268554688\r",
      "INFO - Step 9712, rl-loss: 300.4573974609375\r",
      "INFO - Step 9713, rl-loss: 8.538673400878906\r",
      "INFO - Step 9714, rl-loss: 125.31922149658203\r",
      "INFO - Step 9715, rl-loss: 345.62384033203125\r",
      "INFO - Step 9716, rl-loss: 23.68344497680664\r",
      "INFO - Step 9717, rl-loss: 92.3233871459961\r",
      "INFO - Step 9718, rl-loss: 1.035764217376709\r",
      "INFO - Step 9719, rl-loss: 145.1454620361328\r",
      "INFO - Step 9720, rl-loss: 131.33116149902344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9721, rl-loss: 40.29779052734375\r",
      "INFO - Step 9722, rl-loss: 128.75482177734375\r",
      "INFO - Step 9723, rl-loss: 399.4153747558594\r",
      "INFO - Step 9724, rl-loss: 58.741119384765625\r",
      "INFO - Step 9725, rl-loss: 422.7968444824219\r",
      "INFO - Step 9726, rl-loss: 127.02827453613281\r",
      "INFO - Step 9727, rl-loss: 0.8966056108474731\r",
      "INFO - Step 9728, rl-loss: 1.7877540588378906\r",
      "INFO - Step 9729, rl-loss: 74.31497955322266\r",
      "INFO - Step 9730, rl-loss: 1.150760531425476\r",
      "INFO - Step 9731, rl-loss: 18.367067337036133\r",
      "INFO - Step 9732, rl-loss: 436.804931640625\r",
      "INFO - Step 9733, rl-loss: 231.3173370361328\r",
      "INFO - Step 9734, rl-loss: 245.5210418701172\r",
      "INFO - Step 9735, rl-loss: 155.62515258789062\r",
      "INFO - Step 9736, rl-loss: 75.15689086914062\r",
      "INFO - Step 9737, rl-loss: 165.7365264892578\r",
      "INFO - Step 9738, rl-loss: 1.7276568412780762\r",
      "INFO - Step 9739, rl-loss: 256.2786865234375\r",
      "INFO - Step 9740, rl-loss: 58.355106353759766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9741, rl-loss: 334.35272216796875\r",
      "INFO - Step 9742, rl-loss: 256.7888488769531\r",
      "INFO - Step 9743, rl-loss: 614.7953491210938\r",
      "INFO - Step 9744, rl-loss: 0.5215768218040466\r",
      "INFO - Step 9745, rl-loss: 98.39555358886719\r",
      "INFO - Step 9746, rl-loss: 139.0895233154297\r",
      "INFO - Step 9747, rl-loss: 88.21562194824219\r",
      "INFO - Step 9748, rl-loss: 182.5648193359375\r",
      "INFO - Step 9749, rl-loss: 0.8958789110183716\r",
      "INFO - Step 9750, rl-loss: 140.01966857910156\r",
      "INFO - Step 9751, rl-loss: 57.511295318603516\r",
      "INFO - Step 9752, rl-loss: 374.3260192871094\r",
      "INFO - Step 9753, rl-loss: 444.72650146484375\r",
      "INFO - Step 9754, rl-loss: 381.8304138183594\r",
      "INFO - Step 9755, rl-loss: 141.03079223632812\r",
      "INFO - Step 9756, rl-loss: 91.3810806274414\r",
      "INFO - Step 9757, rl-loss: 483.4088134765625\r",
      "INFO - Step 9758, rl-loss: 169.70440673828125\r",
      "INFO - Step 9759, rl-loss: 1.4108953475952148\r",
      "INFO - Step 9760, rl-loss: 211.62974548339844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9761, rl-loss: 2.1217260360717773\r",
      "INFO - Step 9762, rl-loss: 26.981719970703125\r",
      "INFO - Step 9763, rl-loss: 136.06781005859375\r",
      "INFO - Step 9764, rl-loss: 1.8500696420669556\r",
      "INFO - Step 9765, rl-loss: 3.2475404739379883\r",
      "INFO - Step 9766, rl-loss: 90.25779724121094\r",
      "INFO - Step 9767, rl-loss: 48.76681137084961\r",
      "INFO - Step 9768, rl-loss: 36.08071517944336\r",
      "INFO - Step 9769, rl-loss: 157.3563995361328\r",
      "INFO - Step 9770, rl-loss: 2.4564566612243652\r",
      "INFO - Step 9771, rl-loss: 222.6784210205078\r",
      "INFO - Step 9772, rl-loss: 318.2900085449219\r",
      "INFO - Step 9773, rl-loss: 337.1361999511719\r",
      "INFO - Step 9774, rl-loss: 81.72193908691406\r",
      "INFO - Step 9775, rl-loss: 306.1730651855469\r",
      "INFO - Step 9776, rl-loss: 235.93069458007812\r",
      "INFO - Step 9777, rl-loss: 91.33831787109375\r",
      "INFO - Step 9778, rl-loss: 185.8193359375\r",
      "INFO - Step 9779, rl-loss: 1.4030864238739014\r",
      "INFO - Step 9780, rl-loss: 713.6224365234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9781, rl-loss: 1.7702821493148804\r",
      "INFO - Step 9782, rl-loss: 25.165685653686523\r",
      "INFO - Step 9783, rl-loss: 45.874664306640625\r",
      "INFO - Step 9784, rl-loss: 1.3816710710525513\r",
      "INFO - Step 9785, rl-loss: 1.471529245376587\r",
      "INFO - Step 9786, rl-loss: 241.99725341796875\r",
      "INFO - Step 9787, rl-loss: 152.14955139160156\r",
      "INFO - Step 9788, rl-loss: 164.82000732421875\r",
      "INFO - Step 9789, rl-loss: 878.7494506835938\r",
      "INFO - Step 9790, rl-loss: 457.3525085449219\r",
      "INFO - Step 9791, rl-loss: 498.371337890625\r",
      "INFO - Step 9792, rl-loss: 198.9596710205078\r",
      "INFO - Step 9793, rl-loss: 234.32460021972656\r",
      "INFO - Step 9794, rl-loss: 252.64292907714844\r",
      "INFO - Step 9795, rl-loss: 337.0265808105469\r",
      "INFO - Step 9796, rl-loss: 1.0704766511917114\r",
      "INFO - Step 9797, rl-loss: 396.9561462402344\r",
      "INFO - Step 9798, rl-loss: 52.20689392089844\r",
      "INFO - Step 9799, rl-loss: 530.1196899414062\r",
      "INFO - Step 9800, rl-loss: 80.48926544189453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9801, rl-loss: 1.7307744026184082\r",
      "INFO - Step 9802, rl-loss: 166.74562072753906\r",
      "INFO - Step 9803, rl-loss: 54.51291275024414\r",
      "INFO - Step 9804, rl-loss: 79.16663360595703\r",
      "INFO - Step 9805, rl-loss: 1.1928231716156006\r",
      "INFO - Step 9806, rl-loss: 146.37156677246094\r",
      "INFO - Step 9807, rl-loss: 644.397705078125\r",
      "INFO - Step 9808, rl-loss: 1.7529921531677246\r",
      "INFO - Step 9809, rl-loss: 28.456634521484375\r",
      "INFO - Step 9810, rl-loss: 1.0206072330474854\r",
      "INFO - Step 9811, rl-loss: 2.4557652473449707\r",
      "INFO - Step 9812, rl-loss: 259.2200927734375\r",
      "INFO - Step 9813, rl-loss: 646.634521484375\r",
      "INFO - Step 9814, rl-loss: 139.7225799560547\r",
      "INFO - Step 9815, rl-loss: 368.7367248535156\r",
      "INFO - Step 9816, rl-loss: 569.0231323242188\r",
      "INFO - Step 9817, rl-loss: 53.97187423706055\r",
      "INFO - Step 9818, rl-loss: 216.9605712890625\r",
      "INFO - Step 9819, rl-loss: 163.91964721679688\r",
      "INFO - Step 9820, rl-loss: 109.57821655273438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9821, rl-loss: 571.1161499023438\r",
      "INFO - Step 9822, rl-loss: 221.75079345703125\r",
      "INFO - Step 9823, rl-loss: 31.3902645111084\r",
      "INFO - Step 9824, rl-loss: 145.11581420898438\r",
      "INFO - Step 9825, rl-loss: 1.9204634428024292\r",
      "INFO - Step 9826, rl-loss: 198.01173400878906\r",
      "INFO - Step 9827, rl-loss: 134.48753356933594\r",
      "INFO - Step 9828, rl-loss: 90.67088317871094\r",
      "INFO - Step 9829, rl-loss: 5.900001525878906\r",
      "INFO - Step 9830, rl-loss: 1.1353263854980469\r",
      "INFO - Step 9831, rl-loss: 99.21260070800781\r",
      "INFO - Step 9832, rl-loss: 307.7496337890625\r",
      "INFO - Step 9833, rl-loss: 596.3934936523438\r",
      "INFO - Step 9834, rl-loss: 27.00907325744629\r",
      "INFO - Step 9835, rl-loss: 421.2562255859375\r",
      "INFO - Step 9836, rl-loss: 442.1258544921875\r",
      "INFO - Step 9837, rl-loss: 701.8191528320312\r",
      "INFO - Step 9838, rl-loss: 44.895477294921875\r",
      "INFO - Step 9839, rl-loss: 349.8546142578125\r",
      "INFO - Step 9840, rl-loss: 264.3755187988281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9841, rl-loss: 1.02060866355896\r",
      "INFO - Step 9842, rl-loss: 1.567781925201416\r",
      "INFO - Step 9843, rl-loss: 8.366390228271484\r",
      "INFO - Step 9844, rl-loss: 0.7794745564460754\r",
      "INFO - Step 9845, rl-loss: 149.65866088867188\r",
      "INFO - Step 9846, rl-loss: 235.7369384765625\r",
      "INFO - Step 9847, rl-loss: 71.01607513427734\r",
      "INFO - Step 9848, rl-loss: 67.5909194946289\r",
      "INFO - Step 9849, rl-loss: 0.8357815742492676\r",
      "INFO - Step 9850, rl-loss: 289.3905029296875\r",
      "INFO - Step 9851, rl-loss: 19.159578323364258\r",
      "INFO - Step 9852, rl-loss: 100.83441162109375\r",
      "INFO - Step 9853, rl-loss: 121.97870635986328\r",
      "INFO - Step 9854, rl-loss: 113.65711212158203\r",
      "INFO - Step 9855, rl-loss: 407.2926330566406\r",
      "INFO - Step 9856, rl-loss: 158.58706665039062\r",
      "INFO - Step 9857, rl-loss: 116.42466735839844\r",
      "INFO - Step 9858, rl-loss: 44.123748779296875\r",
      "INFO - Step 9859, rl-loss: 1.4169082641601562\r",
      "INFO - Step 9860, rl-loss: 188.84152221679688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9861, rl-loss: 248.7887420654297\r",
      "INFO - Step 9862, rl-loss: 54.493988037109375\r",
      "INFO - Step 9863, rl-loss: 426.8841857910156\r",
      "INFO - Step 9864, rl-loss: 259.21795654296875\r",
      "INFO - Step 9865, rl-loss: 1.170762300491333\r",
      "INFO - Step 9866, rl-loss: 45.1515007019043\r",
      "INFO - Step 9867, rl-loss: 1.3474416732788086\r",
      "INFO - Step 9868, rl-loss: 396.8487548828125\r",
      "INFO - Step 9869, rl-loss: 196.36053466796875\r",
      "INFO - Step 9870, rl-loss: 147.7414093017578\r",
      "INFO - Step 9871, rl-loss: 40.38581466674805\r",
      "INFO - Step 9872, rl-loss: 3.694319725036621\r",
      "INFO - Step 9873, rl-loss: 235.6972198486328\r",
      "INFO - Step 9874, rl-loss: 138.22323608398438\r",
      "INFO - Step 9875, rl-loss: 93.16497802734375\r",
      "INFO - Step 9876, rl-loss: 370.486328125\r",
      "INFO - Step 9877, rl-loss: 125.70724487304688\r",
      "INFO - Step 9878, rl-loss: 394.0654296875\r",
      "INFO - Step 9879, rl-loss: 47.10163116455078\r",
      "INFO - Step 9880, rl-loss: 147.92298889160156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9881, rl-loss: 1.8629021644592285\r",
      "INFO - Step 9882, rl-loss: 172.77639770507812\r",
      "INFO - Step 9883, rl-loss: 270.91766357421875\r",
      "INFO - Step 9884, rl-loss: 791.8934936523438\r",
      "INFO - Step 9885, rl-loss: 121.45341491699219\r",
      "INFO - Step 9886, rl-loss: 2.1175317764282227\r",
      "INFO - Step 9887, rl-loss: 344.5853271484375\r",
      "INFO - Step 9888, rl-loss: 17.533117294311523\r",
      "INFO - Step 9889, rl-loss: 120.68817138671875\r",
      "INFO - Step 9890, rl-loss: 299.0188293457031\r",
      "INFO - Step 9891, rl-loss: 187.85154724121094\r",
      "INFO - Step 9892, rl-loss: 265.17510986328125\r",
      "INFO - Step 9893, rl-loss: 9.404162406921387\r",
      "INFO - Step 9894, rl-loss: 99.46602630615234\r",
      "INFO - Step 9895, rl-loss: 168.73687744140625\r",
      "INFO - Step 9896, rl-loss: 1.5723330974578857\r",
      "INFO - Step 9897, rl-loss: 101.56392669677734\r",
      "INFO - Step 9898, rl-loss: 1.0572580099105835\r",
      "INFO - Step 9899, rl-loss: 0.491535484790802\r",
      "INFO - Step 9900, rl-loss: 54.57865524291992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9901, rl-loss: 280.2695617675781\r",
      "INFO - Step 9902, rl-loss: 1.2383712530136108\r",
      "INFO - Step 9903, rl-loss: 317.1366882324219\r",
      "INFO - Step 9904, rl-loss: 475.6034240722656\r",
      "INFO - Step 9905, rl-loss: 432.1409912109375\r",
      "INFO - Step 9906, rl-loss: 410.80645751953125\r",
      "INFO - Step 9907, rl-loss: 882.30224609375\r",
      "INFO - Step 9908, rl-loss: 125.8751220703125\r",
      "INFO - Step 9909, rl-loss: 136.07943725585938\r",
      "INFO - Step 9910, rl-loss: 409.1872253417969\r",
      "INFO - Step 9911, rl-loss: 1.1281051635742188\r",
      "INFO - Step 9912, rl-loss: 322.557373046875\r",
      "INFO - Step 9913, rl-loss: 297.7629699707031\r",
      "INFO - Step 9914, rl-loss: 2.0808615684509277\r",
      "INFO - Step 9915, rl-loss: 2.4565722942352295\r",
      "INFO - Step 9916, rl-loss: 304.21954345703125\r",
      "INFO - Step 9917, rl-loss: 596.0504150390625\r",
      "INFO - Step 9918, rl-loss: 179.16741943359375\r",
      "INFO - Step 9919, rl-loss: 223.3511962890625\r",
      "INFO - Step 9920, rl-loss: 167.291748046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9921, rl-loss: 55.9989128112793\r",
      "INFO - Step 9922, rl-loss: 58.37657165527344\r",
      "INFO - Step 9923, rl-loss: 26.013532638549805\r",
      "INFO - Step 9924, rl-loss: 189.9424591064453\r",
      "INFO - Step 9925, rl-loss: 4.657741546630859\r",
      "INFO - Step 9926, rl-loss: 155.9601593017578\r",
      "INFO - Step 9927, rl-loss: 444.9690856933594\r",
      "INFO - Step 9928, rl-loss: 0.8300248384475708\r",
      "INFO - Step 9929, rl-loss: 0.7573895454406738\r",
      "INFO - Step 9930, rl-loss: 552.2293090820312\r",
      "INFO - Step 9931, rl-loss: 1.7791825532913208\r",
      "INFO - Step 9932, rl-loss: 241.17242431640625\r",
      "INFO - Step 9933, rl-loss: 60.809104919433594\r",
      "INFO - Step 9934, rl-loss: 184.49610900878906\r",
      "INFO - Step 9935, rl-loss: 282.13262939453125\r",
      "INFO - Step 9936, rl-loss: 17.17912483215332\r",
      "INFO - Step 9937, rl-loss: 212.56668090820312\r",
      "INFO - Step 9938, rl-loss: 388.3304138183594\r",
      "INFO - Step 9939, rl-loss: 378.32379150390625\r",
      "INFO - Step 9940, rl-loss: 164.42759704589844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9941, rl-loss: 82.88887786865234\r",
      "INFO - Step 9942, rl-loss: 118.05257415771484\r",
      "INFO - Step 9943, rl-loss: 129.03370666503906\r",
      "INFO - Step 9944, rl-loss: 729.8822021484375\r",
      "INFO - Step 9945, rl-loss: 1.669718861579895\r",
      "INFO - Step 9946, rl-loss: 0.8793895244598389\r",
      "INFO - Step 9947, rl-loss: 0.6855739951133728\r",
      "INFO - Step 9948, rl-loss: 0.8497000336647034\r",
      "INFO - Step 9949, rl-loss: 1.7904772758483887\r",
      "INFO - Step 9950, rl-loss: 1.8261058330535889\r",
      "INFO - Step 9951, rl-loss: 24.615192413330078\r",
      "INFO - Step 9952, rl-loss: 0.9107261300086975\r",
      "INFO - Step 9953, rl-loss: 341.2396545410156\r",
      "INFO - Step 9954, rl-loss: 68.41531372070312\r",
      "INFO - Step 9955, rl-loss: 80.67800903320312\r",
      "INFO - Step 9956, rl-loss: 81.6706771850586\r",
      "INFO - Step 9957, rl-loss: 223.8558807373047\r",
      "INFO - Step 9958, rl-loss: 198.1904754638672\r",
      "INFO - Step 9959, rl-loss: 401.5405578613281\r",
      "INFO - Step 9960, rl-loss: 3.440932512283325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9961, rl-loss: 350.1327209472656\r",
      "INFO - Step 9962, rl-loss: 137.1699981689453\r",
      "INFO - Step 9963, rl-loss: 449.3847961425781\r",
      "INFO - Step 9964, rl-loss: 569.4248657226562\r",
      "INFO - Step 9965, rl-loss: 420.602294921875\r",
      "INFO - Step 9966, rl-loss: 71.86860656738281\r",
      "INFO - Step 9967, rl-loss: 450.1427307128906\r",
      "INFO - Step 9968, rl-loss: 1.235436201095581\r",
      "INFO - Step 9969, rl-loss: 288.803955078125\r",
      "INFO - Step 9970, rl-loss: 57.12910461425781\r",
      "INFO - Step 9971, rl-loss: 1.4426695108413696\r",
      "INFO - Step 9972, rl-loss: 0.8387345671653748\r",
      "INFO - Step 9973, rl-loss: 0.8333714008331299\r",
      "INFO - Step 9974, rl-loss: 224.0196075439453\r",
      "INFO - Step 9975, rl-loss: 264.0155944824219\r",
      "INFO - Step 9976, rl-loss: 328.8865966796875\r",
      "INFO - Step 9977, rl-loss: 83.005859375\r",
      "INFO - Step 9978, rl-loss: 182.4278564453125\r",
      "INFO - Step 9979, rl-loss: 304.6986389160156\r",
      "INFO - Step 9980, rl-loss: 622.5748901367188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 9981, rl-loss: 1.0651813745498657\r",
      "INFO - Step 9982, rl-loss: 311.63958740234375\r",
      "INFO - Step 9983, rl-loss: 13.998937606811523\r",
      "INFO - Step 9984, rl-loss: 118.9961166381836\r",
      "INFO - Step 9985, rl-loss: 555.0541381835938\r",
      "INFO - Step 9986, rl-loss: 189.8221435546875\r",
      "INFO - Step 9987, rl-loss: 635.18994140625\r",
      "INFO - Step 9988, rl-loss: 223.0631103515625\r",
      "INFO - Step 9989, rl-loss: 3.653275728225708\r",
      "INFO - Step 9990, rl-loss: 64.84186553955078\r",
      "INFO - Step 9991, rl-loss: 153.4466552734375\r",
      "INFO - Step 9992, rl-loss: 142.581298828125\r",
      "INFO - Step 9993, rl-loss: 200.17880249023438\r",
      "INFO - Step 9994, rl-loss: 116.5905990600586\r",
      "INFO - Step 9995, rl-loss: 320.90093994140625\r",
      "INFO - Step 9996, rl-loss: 322.12615966796875\r",
      "INFO - Step 9997, rl-loss: 325.86328125\r",
      "INFO - Step 9998, rl-loss: 587.2984008789062\r",
      "INFO - Step 9999, rl-loss: 182.6386260986328\r",
      "INFO - Step 10000, rl-loss: 60.692447662353516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 10020, rl-loss: 287.04537963867196\n",
      "----------------------------------------\n",
      "  timestep     |  572945\n",
      "  reward       |  64.5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 10040, rl-loss: 166.68524169921875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10041, rl-loss: 356.56109619140625\r",
      "INFO - Step 10042, rl-loss: 198.19007873535156\r",
      "INFO - Step 10043, rl-loss: 323.32635498046875\r",
      "INFO - Step 10044, rl-loss: 86.77959442138672\r",
      "INFO - Step 10045, rl-loss: 1.6641864776611328\r",
      "INFO - Step 10046, rl-loss: 500.9477233886719\r",
      "INFO - Step 10047, rl-loss: 60.16718673706055\r",
      "INFO - Step 10048, rl-loss: 42.90980529785156\r",
      "INFO - Step 10049, rl-loss: 25.707813262939453\r",
      "INFO - Step 10050, rl-loss: 63.7548828125\r",
      "INFO - Step 10051, rl-loss: 1.2618188858032227\r",
      "INFO - Step 10052, rl-loss: 1182.151123046875\r",
      "INFO - Step 10053, rl-loss: 0.49650946259498596\r",
      "INFO - Step 10054, rl-loss: 44.08084487915039\r",
      "INFO - Step 10055, rl-loss: 473.6968688964844\r",
      "INFO - Step 10056, rl-loss: 2.4020352363586426\r",
      "INFO - Step 10057, rl-loss: 301.75152587890625\r",
      "INFO - Step 10058, rl-loss: 1.08131742477417\r",
      "INFO - Step 10059, rl-loss: 1.5777546167373657\r",
      "INFO - Step 10060, rl-loss: 77.5452880859375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10061, rl-loss: 159.74571228027344\r",
      "INFO - Step 10062, rl-loss: 315.8809814453125\r",
      "INFO - Step 10063, rl-loss: 142.9492645263672\r",
      "INFO - Step 10064, rl-loss: 1.0008735656738281\r",
      "INFO - Step 10065, rl-loss: 243.50665283203125\r",
      "INFO - Step 10066, rl-loss: 399.3676452636719\r",
      "INFO - Step 10067, rl-loss: 191.76658630371094\r",
      "INFO - Step 10068, rl-loss: 138.07908630371094\r",
      "INFO - Step 10069, rl-loss: 123.61056518554688\r",
      "INFO - Step 10070, rl-loss: 149.94471740722656\r",
      "INFO - Step 10071, rl-loss: 116.47322082519531\r",
      "INFO - Step 10072, rl-loss: 117.72760009765625\r",
      "INFO - Step 10073, rl-loss: 171.08920288085938\r",
      "INFO - Step 10074, rl-loss: 279.95611572265625\r",
      "INFO - Step 10075, rl-loss: 1.8506662845611572\r",
      "INFO - Step 10076, rl-loss: 589.0480346679688\r",
      "INFO - Step 10077, rl-loss: 279.2857360839844\r",
      "INFO - Step 10078, rl-loss: 5.88021183013916\r",
      "INFO - Step 10079, rl-loss: 43.36285400390625\r",
      "INFO - Step 10080, rl-loss: 482.5169372558594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10081, rl-loss: 84.71285247802734\r",
      "INFO - Step 10082, rl-loss: 387.7795715332031\r",
      "INFO - Step 10083, rl-loss: 311.91064453125\r",
      "INFO - Step 10084, rl-loss: 283.5116271972656\r",
      "INFO - Step 10085, rl-loss: 0.720545768737793\r",
      "INFO - Step 10086, rl-loss: 296.71588134765625\r",
      "INFO - Step 10087, rl-loss: 132.04701232910156\r",
      "INFO - Step 10088, rl-loss: 220.30084228515625\r",
      "INFO - Step 10089, rl-loss: 140.67457580566406\r",
      "INFO - Step 10090, rl-loss: 0.9263572692871094\r",
      "INFO - Step 10091, rl-loss: 459.3345947265625\r",
      "INFO - Step 10092, rl-loss: 318.52203369140625\r",
      "INFO - Step 10093, rl-loss: 120.2520980834961\r",
      "INFO - Step 10094, rl-loss: 83.03174591064453\r",
      "INFO - Step 10095, rl-loss: 578.1317138671875\r",
      "INFO - Step 10096, rl-loss: 576.4100341796875\r",
      "INFO - Step 10097, rl-loss: 194.49041748046875\r",
      "INFO - Step 10098, rl-loss: 515.257568359375\r",
      "INFO - Step 10099, rl-loss: 41.524349212646484\r",
      "INFO - Step 10100, rl-loss: 56.32319259643555\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10101, rl-loss: 591.850830078125\r",
      "INFO - Step 10102, rl-loss: 359.8481140136719\r",
      "INFO - Step 10103, rl-loss: 151.28280639648438\r",
      "INFO - Step 10104, rl-loss: 4.6338067054748535\r",
      "INFO - Step 10105, rl-loss: 481.8556823730469\r",
      "INFO - Step 10106, rl-loss: 289.91552734375\r",
      "INFO - Step 10107, rl-loss: 436.0875244140625\r",
      "INFO - Step 10108, rl-loss: 619.09130859375\r",
      "INFO - Step 10109, rl-loss: 126.19080352783203\r",
      "INFO - Step 10110, rl-loss: 0.9957147836685181\r",
      "INFO - Step 10111, rl-loss: 570.3019409179688\r",
      "INFO - Step 10112, rl-loss: 62.30004119873047\r",
      "INFO - Step 10113, rl-loss: 204.3370819091797\r",
      "INFO - Step 10114, rl-loss: 157.17503356933594\r",
      "INFO - Step 10115, rl-loss: 2.669097661972046\r",
      "INFO - Step 10116, rl-loss: 78.7588119506836\r",
      "INFO - Step 10117, rl-loss: 110.93650817871094\r",
      "INFO - Step 10118, rl-loss: 164.3164825439453\r",
      "INFO - Step 10119, rl-loss: 83.57720947265625\r",
      "INFO - Step 10120, rl-loss: 280.53955078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10121, rl-loss: 400.7283630371094\r",
      "INFO - Step 10122, rl-loss: 217.89315795898438\r",
      "INFO - Step 10123, rl-loss: 4.897789001464844\r",
      "INFO - Step 10124, rl-loss: 3.79958438873291\r",
      "INFO - Step 10125, rl-loss: 155.97227478027344\r",
      "INFO - Step 10126, rl-loss: 414.30633544921875\r",
      "INFO - Step 10127, rl-loss: 486.89599609375\r",
      "INFO - Step 10128, rl-loss: 124.22492980957031\r",
      "INFO - Step 10129, rl-loss: 163.56912231445312\r",
      "INFO - Step 10130, rl-loss: 152.54736328125\r",
      "INFO - Step 10131, rl-loss: 2.3680331707000732\r",
      "INFO - Step 10132, rl-loss: 127.62600708007812\r",
      "INFO - Step 10133, rl-loss: 1.6785036325454712\r",
      "INFO - Step 10134, rl-loss: 115.87345886230469\r",
      "INFO - Step 10135, rl-loss: 755.6378173828125\r",
      "INFO - Step 10136, rl-loss: 19.871854782104492\r",
      "INFO - Step 10137, rl-loss: 292.9580078125\r",
      "INFO - Step 10138, rl-loss: 190.96231079101562\r",
      "INFO - Step 10139, rl-loss: 45.203922271728516\r",
      "INFO - Step 10140, rl-loss: 209.94070434570312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10141, rl-loss: 26.57259750366211\r",
      "INFO - Step 10142, rl-loss: 1.8990743160247803\r",
      "INFO - Step 10143, rl-loss: 114.55136108398438\r",
      "INFO - Step 10144, rl-loss: 124.74691772460938\r",
      "INFO - Step 10145, rl-loss: 121.46285247802734\r",
      "INFO - Step 10146, rl-loss: 218.95074462890625\r",
      "INFO - Step 10147, rl-loss: 152.37158203125\r",
      "INFO - Step 10148, rl-loss: 176.95587158203125\r",
      "INFO - Step 10149, rl-loss: 306.1024169921875\r",
      "INFO - Step 10150, rl-loss: 2.091766834259033\r",
      "INFO - Step 10151, rl-loss: 421.91943359375\r",
      "INFO - Step 10152, rl-loss: 277.466552734375\r",
      "INFO - Step 10153, rl-loss: 82.15849304199219\r",
      "INFO - Step 10154, rl-loss: 32.94270324707031\r",
      "INFO - Step 10155, rl-loss: 1.2529373168945312\r",
      "INFO - Step 10156, rl-loss: 152.5471649169922\r",
      "INFO - Step 10157, rl-loss: 1.3055834770202637\r",
      "INFO - Step 10158, rl-loss: 3.5693416595458984\r",
      "INFO - Step 10159, rl-loss: 1.3941152095794678\r",
      "INFO - Step 10160, rl-loss: 1.7126784324645996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10161, rl-loss: 329.39453125\r",
      "INFO - Step 10162, rl-loss: 128.31051635742188\r",
      "INFO - Step 10163, rl-loss: 272.0508728027344\r",
      "INFO - Step 10164, rl-loss: 508.9540100097656\r",
      "INFO - Step 10165, rl-loss: 130.251953125\r",
      "INFO - Step 10166, rl-loss: 261.5238037109375\r",
      "INFO - Step 10167, rl-loss: 3.5886714458465576\r",
      "INFO - Step 10168, rl-loss: 1.1956696510314941\r",
      "INFO - Step 10169, rl-loss: 116.63417053222656\r",
      "INFO - Step 10170, rl-loss: 101.831787109375\r",
      "INFO - Step 10171, rl-loss: 167.50205993652344\r",
      "INFO - Step 10172, rl-loss: 675.6353759765625\r",
      "INFO - Step 10173, rl-loss: 406.6175842285156\r",
      "INFO - Step 10174, rl-loss: 256.0778503417969\r",
      "INFO - Step 10175, rl-loss: 33.183372497558594\r",
      "INFO - Step 10176, rl-loss: 1.2718565464019775\r",
      "INFO - Step 10177, rl-loss: 12.49820327758789\r",
      "INFO - Step 10178, rl-loss: 642.333984375\r",
      "INFO - Step 10179, rl-loss: 4.09989070892334\r",
      "INFO - Step 10180, rl-loss: 368.14971923828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10181, rl-loss: 59.436981201171875\r",
      "INFO - Step 10182, rl-loss: 3.782196283340454\r",
      "INFO - Step 10183, rl-loss: 734.140869140625\r",
      "INFO - Step 10184, rl-loss: 250.06491088867188\r",
      "INFO - Step 10185, rl-loss: 727.04248046875\r",
      "INFO - Step 10186, rl-loss: 76.80935668945312\r",
      "INFO - Step 10187, rl-loss: 1.0661389827728271\r",
      "INFO - Step 10188, rl-loss: 143.32664489746094\r",
      "INFO - Step 10189, rl-loss: 460.17498779296875\r",
      "INFO - Step 10190, rl-loss: 176.0602264404297\r",
      "INFO - Step 10191, rl-loss: 50.24974822998047\r",
      "INFO - Step 10192, rl-loss: 488.0174865722656\r",
      "INFO - Step 10193, rl-loss: 283.1858215332031\r",
      "INFO - Step 10194, rl-loss: 155.9354248046875\r",
      "INFO - Step 10195, rl-loss: 25.59712028503418\r",
      "INFO - Step 10196, rl-loss: 1.876923680305481\r",
      "INFO - Step 10197, rl-loss: 403.67156982421875\r",
      "INFO - Step 10198, rl-loss: 2.0354113578796387\r",
      "INFO - Step 10199, rl-loss: 239.06143188476562\r",
      "INFO - Step 10200, rl-loss: 131.31068420410156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10201, rl-loss: 373.13983154296875\r",
      "INFO - Step 10202, rl-loss: 102.38487243652344\r",
      "INFO - Step 10203, rl-loss: 2.455054759979248\r",
      "INFO - Step 10204, rl-loss: 550.4833374023438\r",
      "INFO - Step 10205, rl-loss: 153.29049682617188\r",
      "INFO - Step 10206, rl-loss: 4.387302398681641\r",
      "INFO - Step 10207, rl-loss: 4.94011116027832\r",
      "INFO - Step 10208, rl-loss: 180.80345153808594\r",
      "INFO - Step 10209, rl-loss: 108.46992492675781\r",
      "INFO - Step 10210, rl-loss: 131.10743713378906\r",
      "INFO - Step 10211, rl-loss: 575.1270141601562\r",
      "INFO - Step 10212, rl-loss: 228.08363342285156\r",
      "INFO - Step 10213, rl-loss: 465.028076171875\r",
      "INFO - Step 10214, rl-loss: 101.09117889404297\r",
      "INFO - Step 10215, rl-loss: 3.9795336723327637\r",
      "INFO - Step 10216, rl-loss: 56.61661148071289\r",
      "INFO - Step 10217, rl-loss: 34.601226806640625\r",
      "INFO - Step 10218, rl-loss: 149.4564208984375\r",
      "INFO - Step 10219, rl-loss: 92.70098114013672\r",
      "INFO - Step 10220, rl-loss: 1.3451541662216187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10221, rl-loss: 391.9647521972656\r",
      "INFO - Step 10222, rl-loss: 295.157470703125\r",
      "INFO - Step 10223, rl-loss: 73.53773498535156\r",
      "INFO - Step 10224, rl-loss: 3.1352572441101074\r",
      "INFO - Step 10225, rl-loss: 3.078697919845581\r",
      "INFO - Step 10226, rl-loss: 91.69775390625\r",
      "INFO - Step 10227, rl-loss: 75.65843963623047\r",
      "INFO - Step 10228, rl-loss: 592.8434448242188\r",
      "INFO - Step 10229, rl-loss: 490.89263916015625\r",
      "INFO - Step 10230, rl-loss: 361.755615234375\r",
      "INFO - Step 10231, rl-loss: 437.7825927734375\r",
      "INFO - Step 10232, rl-loss: 0.796788215637207\r",
      "INFO - Step 10233, rl-loss: 71.8764877319336\r",
      "INFO - Step 10234, rl-loss: 296.75048828125\r",
      "INFO - Step 10235, rl-loss: 1.8734655380249023\r",
      "INFO - Step 10236, rl-loss: 225.62843322753906\r",
      "INFO - Step 10237, rl-loss: 235.6846160888672\r",
      "INFO - Step 10238, rl-loss: 66.75029754638672\r",
      "INFO - Step 10239, rl-loss: 145.1377716064453\r",
      "INFO - Step 10240, rl-loss: 1.5202381610870361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10241, rl-loss: 353.73431396484375\r",
      "INFO - Step 10242, rl-loss: 5.102461814880371\r",
      "INFO - Step 10243, rl-loss: 262.7972412109375\r",
      "INFO - Step 10244, rl-loss: 242.1799774169922\r",
      "INFO - Step 10245, rl-loss: 55.08524703979492\r",
      "INFO - Step 10246, rl-loss: 382.4151306152344\r",
      "INFO - Step 10247, rl-loss: 80.43672943115234\r",
      "INFO - Step 10248, rl-loss: 361.3837585449219\r",
      "INFO - Step 10249, rl-loss: 173.89321899414062\r",
      "INFO - Step 10250, rl-loss: 236.62307739257812\r",
      "INFO - Step 10251, rl-loss: 120.77301025390625\r",
      "INFO - Step 10252, rl-loss: 19.92862319946289\r",
      "INFO - Step 10253, rl-loss: 1.5515644550323486\r",
      "INFO - Step 10254, rl-loss: 233.9253692626953\r",
      "INFO - Step 10255, rl-loss: 120.81206512451172\r",
      "INFO - Step 10256, rl-loss: 74.01567840576172\r",
      "INFO - Step 10257, rl-loss: 778.6578369140625\r",
      "INFO - Step 10258, rl-loss: 2.372018337249756\r",
      "INFO - Step 10259, rl-loss: 96.5355224609375\r",
      "INFO - Step 10260, rl-loss: 810.9193725585938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10261, rl-loss: 423.54864501953125\r",
      "INFO - Step 10262, rl-loss: 176.65261840820312\r",
      "INFO - Step 10263, rl-loss: 304.02838134765625\r",
      "INFO - Step 10264, rl-loss: 178.0439453125\r",
      "INFO - Step 10265, rl-loss: 38.80012893676758\r",
      "INFO - Step 10266, rl-loss: 642.5821533203125\r",
      "INFO - Step 10267, rl-loss: 1.5487276315689087\r",
      "INFO - Step 10268, rl-loss: 187.85116577148438\r",
      "INFO - Step 10269, rl-loss: 466.57171630859375\r",
      "INFO - Step 10270, rl-loss: 39.73876953125\r",
      "INFO - Step 10271, rl-loss: 23.483713150024414\r",
      "INFO - Step 10272, rl-loss: 544.3582763671875\r",
      "INFO - Step 10273, rl-loss: 305.605712890625\r",
      "INFO - Step 10274, rl-loss: 316.70904541015625\r",
      "INFO - Step 10275, rl-loss: 110.02874755859375\r",
      "INFO - Step 10276, rl-loss: 691.7567749023438\r",
      "INFO - Step 10277, rl-loss: 0.6473427414894104\r",
      "INFO - Step 10278, rl-loss: 62.57697677612305\r",
      "INFO - Step 10279, rl-loss: 1.6696218252182007\r",
      "INFO - Step 10280, rl-loss: 3.325918674468994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10281, rl-loss: 143.498779296875\r",
      "INFO - Step 10282, rl-loss: 231.15011596679688\r",
      "INFO - Step 10283, rl-loss: 343.169189453125\r",
      "INFO - Step 10284, rl-loss: 1.5945721864700317\r",
      "INFO - Step 10285, rl-loss: 284.51104736328125\r",
      "INFO - Step 10286, rl-loss: 382.05511474609375\r",
      "INFO - Step 10287, rl-loss: 2.998497724533081\r",
      "INFO - Step 10288, rl-loss: 118.96268463134766\r",
      "INFO - Step 10289, rl-loss: 58.30705261230469\r",
      "INFO - Step 10290, rl-loss: 81.96625518798828\r",
      "INFO - Step 10291, rl-loss: 246.7740478515625\r",
      "INFO - Step 10292, rl-loss: 351.1034851074219\r",
      "INFO - Step 10293, rl-loss: 247.6175994873047\r",
      "INFO - Step 10294, rl-loss: 26.79831314086914\r",
      "INFO - Step 10295, rl-loss: 1.671189785003662\r",
      "INFO - Step 10296, rl-loss: 71.86083221435547\r",
      "INFO - Step 10297, rl-loss: 560.2984619140625\r",
      "INFO - Step 10298, rl-loss: 2.8150887489318848\r",
      "INFO - Step 10299, rl-loss: 153.33389282226562\r",
      "INFO - Step 10300, rl-loss: 2.2833070755004883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10301, rl-loss: 281.5226135253906\r",
      "INFO - Step 10302, rl-loss: 1.4382052421569824\r",
      "INFO - Step 10303, rl-loss: 81.7791748046875\r",
      "INFO - Step 10304, rl-loss: 293.63232421875\r",
      "INFO - Step 10305, rl-loss: 134.77389526367188\r",
      "INFO - Step 10306, rl-loss: 109.35578155517578\r",
      "INFO - Step 10307, rl-loss: 1.6177000999450684\r",
      "INFO - Step 10308, rl-loss: 210.56161499023438\r",
      "INFO - Step 10309, rl-loss: 121.72150421142578\r",
      "INFO - Step 10310, rl-loss: 2.8216629028320312\r",
      "INFO - Step 10311, rl-loss: 543.4324951171875\r",
      "INFO - Step 10312, rl-loss: 338.0871887207031\r",
      "INFO - Step 10313, rl-loss: 1.3839919567108154\r",
      "INFO - Step 10314, rl-loss: 331.9118347167969\r",
      "INFO - Step 10315, rl-loss: 705.1687622070312\r",
      "INFO - Step 10316, rl-loss: 676.0641479492188\r",
      "INFO - Step 10317, rl-loss: 1.8279517889022827\r",
      "INFO - Step 10318, rl-loss: 145.89991760253906\r",
      "INFO - Step 10319, rl-loss: 130.5603485107422\r",
      "INFO - Step 10320, rl-loss: 390.6551513671875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10321, rl-loss: 16.723909378051758\r",
      "INFO - Step 10322, rl-loss: 295.4226379394531\r",
      "INFO - Step 10323, rl-loss: 0.9733293056488037\r",
      "INFO - Step 10324, rl-loss: 389.24017333984375\r",
      "INFO - Step 10325, rl-loss: 517.6297607421875\r",
      "INFO - Step 10326, rl-loss: 3.086010694503784\r",
      "INFO - Step 10327, rl-loss: 122.8949203491211\r",
      "INFO - Step 10328, rl-loss: 480.99896240234375\r",
      "INFO - Step 10329, rl-loss: 383.33740234375\r",
      "INFO - Step 10330, rl-loss: 53.90374755859375\r",
      "INFO - Step 10331, rl-loss: 4.996676445007324\r",
      "INFO - Step 10332, rl-loss: 4.338196277618408\r",
      "INFO - Step 10333, rl-loss: 72.06752014160156\r",
      "INFO - Step 10334, rl-loss: 70.74706268310547\r",
      "INFO - Step 10335, rl-loss: 2.149984836578369\r",
      "INFO - Step 10336, rl-loss: 295.70623779296875\r",
      "INFO - Step 10337, rl-loss: 322.19671630859375\r",
      "INFO - Step 10338, rl-loss: 245.59007263183594\r",
      "INFO - Step 10339, rl-loss: 1.7008687257766724\r",
      "INFO - Step 10340, rl-loss: 1.693007469177246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10341, rl-loss: 17.164731979370117\r",
      "INFO - Step 10342, rl-loss: 820.8383178710938\r",
      "INFO - Step 10343, rl-loss: 266.11798095703125\r",
      "INFO - Step 10344, rl-loss: 104.80929565429688\r",
      "INFO - Step 10345, rl-loss: 106.85423278808594\r",
      "INFO - Step 10346, rl-loss: 198.8481903076172\r",
      "INFO - Step 10347, rl-loss: 1.88962721824646\r",
      "INFO - Step 10348, rl-loss: 301.4468078613281\r",
      "INFO - Step 10349, rl-loss: 1.0342049598693848\r",
      "INFO - Step 10350, rl-loss: 87.89994049072266\r",
      "INFO - Step 10351, rl-loss: 4.08012056350708\r",
      "INFO - Step 10352, rl-loss: 76.31033325195312\r",
      "INFO - Step 10353, rl-loss: 110.03736877441406\r",
      "INFO - Step 10354, rl-loss: 48.7625732421875\r",
      "INFO - Step 10355, rl-loss: 273.4957275390625\r",
      "INFO - Step 10356, rl-loss: 225.01998901367188\r",
      "INFO - Step 10357, rl-loss: 40.81349182128906\r",
      "INFO - Step 10358, rl-loss: 58.845306396484375\r",
      "INFO - Step 10359, rl-loss: 9.320919036865234\r",
      "INFO - Step 10360, rl-loss: 124.49784851074219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10361, rl-loss: 58.34750747680664\r",
      "INFO - Step 10362, rl-loss: 276.65972900390625\r",
      "INFO - Step 10363, rl-loss: 679.3212890625\r",
      "INFO - Step 10364, rl-loss: 131.26002502441406\r",
      "INFO - Step 10365, rl-loss: 1.6557341814041138\r",
      "INFO - Step 10366, rl-loss: 173.1714324951172\r",
      "INFO - Step 10367, rl-loss: 276.81683349609375\r",
      "INFO - Step 10368, rl-loss: 223.01150512695312\r",
      "INFO - Step 10369, rl-loss: 3.770125389099121\r",
      "INFO - Step 10370, rl-loss: 272.8865966796875\r",
      "INFO - Step 10371, rl-loss: 1.1239721775054932\r",
      "INFO - Step 10372, rl-loss: 61.168670654296875\r",
      "INFO - Step 10373, rl-loss: 0.9447322487831116\r",
      "INFO - Step 10374, rl-loss: 215.48577880859375\r",
      "INFO - Step 10375, rl-loss: 1.4872002601623535\r",
      "INFO - Step 10376, rl-loss: 290.0914306640625\r",
      "INFO - Step 10377, rl-loss: 1.443638563156128\r",
      "INFO - Step 10378, rl-loss: 137.68934631347656\r",
      "INFO - Step 10379, rl-loss: 827.5213012695312\r",
      "INFO - Step 10380, rl-loss: 2.2447805404663086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10381, rl-loss: 184.9362030029297\r",
      "INFO - Step 10382, rl-loss: 686.8197631835938\r",
      "INFO - Step 10383, rl-loss: 3.2501394748687744\r",
      "INFO - Step 10384, rl-loss: 280.852294921875\r",
      "INFO - Step 10385, rl-loss: 1.1208686828613281\r",
      "INFO - Step 10386, rl-loss: 74.87020874023438\r",
      "INFO - Step 10387, rl-loss: 219.08230590820312\r",
      "INFO - Step 10388, rl-loss: 486.37646484375\r",
      "INFO - Step 10389, rl-loss: 2.2245774269104004\r",
      "INFO - Step 10390, rl-loss: 264.200927734375\r",
      "INFO - Step 10391, rl-loss: 294.1198425292969\r",
      "INFO - Step 10392, rl-loss: 215.3822479248047\r",
      "INFO - Step 10393, rl-loss: 111.338134765625\r",
      "INFO - Step 10394, rl-loss: 416.98388671875\r",
      "INFO - Step 10395, rl-loss: 625.8333740234375\r",
      "INFO - Step 10396, rl-loss: 228.93295288085938\r",
      "INFO - Step 10397, rl-loss: 394.7684020996094\r",
      "INFO - Step 10398, rl-loss: 328.2844543457031\r",
      "INFO - Step 10399, rl-loss: 305.9234619140625\r",
      "INFO - Step 10400, rl-loss: 120.91548919677734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10401, rl-loss: 166.07533264160156\r",
      "INFO - Step 10402, rl-loss: 73.3765640258789\r",
      "INFO - Step 10403, rl-loss: 257.668701171875\r",
      "INFO - Step 10404, rl-loss: 1.7975143194198608\r",
      "INFO - Step 10405, rl-loss: 257.83013916015625\r",
      "INFO - Step 10406, rl-loss: 301.9095153808594\r",
      "INFO - Step 10407, rl-loss: 665.1944580078125\r",
      "INFO - Step 10408, rl-loss: 155.7665252685547\r",
      "INFO - Step 10409, rl-loss: 0.903416097164154\r",
      "INFO - Step 10410, rl-loss: 1.4014410972595215\r",
      "INFO - Step 10411, rl-loss: 142.65948486328125\r",
      "INFO - Step 10412, rl-loss: 31.77001953125\r",
      "INFO - Step 10413, rl-loss: 226.73660278320312\r",
      "INFO - Step 10414, rl-loss: 0.9318479299545288\r",
      "INFO - Step 10415, rl-loss: 141.27049255371094\r",
      "INFO - Step 10416, rl-loss: 301.6329345703125\r",
      "INFO - Step 10417, rl-loss: 289.08697509765625\r",
      "INFO - Step 10418, rl-loss: 608.13427734375\r",
      "INFO - Step 10419, rl-loss: 2.8164777755737305\r",
      "INFO - Step 10420, rl-loss: 63.215850830078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10421, rl-loss: 349.2484130859375\r",
      "INFO - Step 10422, rl-loss: 100.24807739257812\r",
      "INFO - Step 10423, rl-loss: 218.17141723632812\r",
      "INFO - Step 10424, rl-loss: 1.8035410642623901\r",
      "INFO - Step 10425, rl-loss: 1.404923439025879\r",
      "INFO - Step 10426, rl-loss: 387.27593994140625\r",
      "INFO - Step 10427, rl-loss: 142.16322326660156\r",
      "INFO - Step 10428, rl-loss: 136.42788696289062\r",
      "INFO - Step 10429, rl-loss: 2.5578465461730957\r",
      "INFO - Step 10430, rl-loss: 99.64019775390625\r",
      "INFO - Step 10431, rl-loss: 172.3726806640625\r",
      "INFO - Step 10432, rl-loss: 0.8152754306793213\r",
      "INFO - Step 10433, rl-loss: 1.0858476161956787\r",
      "INFO - Step 10434, rl-loss: 348.0743408203125\r",
      "INFO - Step 10435, rl-loss: 6.362461090087891\r",
      "INFO - Step 10436, rl-loss: 3.877750873565674\r",
      "INFO - Step 10437, rl-loss: 472.8719482421875\r",
      "INFO - Step 10438, rl-loss: 4.0724992752075195\r",
      "INFO - Step 10439, rl-loss: 268.1136169433594\r",
      "INFO - Step 10440, rl-loss: 155.91221618652344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10441, rl-loss: 107.49076843261719\r",
      "INFO - Step 10442, rl-loss: 0.8400764465332031\r",
      "INFO - Step 10443, rl-loss: 153.4790496826172\r",
      "INFO - Step 10444, rl-loss: 281.3850402832031\r",
      "INFO - Step 10445, rl-loss: 178.7782745361328\r",
      "INFO - Step 10446, rl-loss: 24.680938720703125\r",
      "INFO - Step 10447, rl-loss: 172.87347412109375\r",
      "INFO - Step 10448, rl-loss: 106.8941421508789\r",
      "INFO - Step 10449, rl-loss: 106.39042663574219\r",
      "INFO - Step 10450, rl-loss: 231.2401885986328\r",
      "INFO - Step 10451, rl-loss: 296.89349365234375\r",
      "INFO - Step 10452, rl-loss: 1.4328659772872925\r",
      "INFO - Step 10453, rl-loss: 220.15896606445312\r",
      "INFO - Step 10454, rl-loss: 393.9664001464844\r",
      "INFO - Step 10455, rl-loss: 185.52914428710938\r",
      "INFO - Step 10456, rl-loss: 123.06024932861328\r",
      "INFO - Step 10457, rl-loss: 201.11402893066406\r",
      "INFO - Step 10458, rl-loss: 123.52680206298828\r",
      "INFO - Step 10459, rl-loss: 1.0991219282150269\r",
      "INFO - Step 10460, rl-loss: 175.1304931640625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10461, rl-loss: 78.38858795166016\r",
      "INFO - Step 10462, rl-loss: 1.1729538440704346\r",
      "INFO - Step 10463, rl-loss: 211.32220458984375\r",
      "INFO - Step 10464, rl-loss: 60.37368392944336\r",
      "INFO - Step 10465, rl-loss: 382.6260681152344\r",
      "INFO - Step 10466, rl-loss: 100.53919982910156\r",
      "INFO - Step 10467, rl-loss: 59.219757080078125\r",
      "INFO - Step 10468, rl-loss: 304.1595458984375\r",
      "INFO - Step 10469, rl-loss: 148.40931701660156\r",
      "INFO - Step 10470, rl-loss: 9.326788902282715\r",
      "INFO - Step 10471, rl-loss: 452.6860656738281\r",
      "INFO - Step 10472, rl-loss: 197.1202850341797\r",
      "INFO - Step 10473, rl-loss: 2.5203962326049805\r",
      "INFO - Step 10474, rl-loss: 266.16656494140625\r",
      "INFO - Step 10475, rl-loss: 280.15863037109375\r",
      "INFO - Step 10476, rl-loss: 101.97518157958984\r",
      "INFO - Step 10477, rl-loss: 4.882077217102051\r",
      "INFO - Step 10478, rl-loss: 267.92047119140625\r",
      "INFO - Step 10479, rl-loss: 122.51911163330078\r",
      "INFO - Step 10480, rl-loss: 377.0552062988281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10481, rl-loss: 329.3622131347656\r",
      "INFO - Step 10482, rl-loss: 462.4988708496094\r",
      "INFO - Step 10483, rl-loss: 57.3633918762207\r",
      "INFO - Step 10484, rl-loss: 166.9593963623047\r",
      "INFO - Step 10485, rl-loss: 2.001720905303955\r",
      "INFO - Step 10486, rl-loss: 0.9057269096374512\r",
      "INFO - Step 10487, rl-loss: 856.3505249023438\r",
      "INFO - Step 10488, rl-loss: 508.818359375\r",
      "INFO - Step 10489, rl-loss: 178.10353088378906\r",
      "INFO - Step 10490, rl-loss: 355.2220764160156\r",
      "INFO - Step 10491, rl-loss: 1.3313409090042114\r",
      "INFO - Step 10492, rl-loss: 71.52367401123047\r",
      "INFO - Step 10493, rl-loss: 304.20416259765625\r",
      "INFO - Step 10494, rl-loss: 354.9351806640625\r",
      "INFO - Step 10495, rl-loss: 104.00025177001953\r",
      "INFO - Step 10496, rl-loss: 57.53573989868164\r",
      "INFO - Step 10497, rl-loss: 2.7580718994140625\r",
      "INFO - Step 10498, rl-loss: 291.6895751953125\r",
      "INFO - Step 10499, rl-loss: 56.31772232055664\r",
      "INFO - Step 10500, rl-loss: 87.25340270996094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10501, rl-loss: 2.688241720199585\r",
      "INFO - Step 10502, rl-loss: 15.3466796875\r",
      "INFO - Step 10503, rl-loss: 175.68148803710938\r",
      "INFO - Step 10504, rl-loss: 215.0706329345703\r",
      "INFO - Step 10505, rl-loss: 1.4127833843231201\r",
      "INFO - Step 10506, rl-loss: 204.89256286621094\r",
      "INFO - Step 10507, rl-loss: 159.40066528320312\r",
      "INFO - Step 10508, rl-loss: 232.11380004882812\r",
      "INFO - Step 10509, rl-loss: 159.1868896484375\r",
      "INFO - Step 10510, rl-loss: 211.3352813720703\r",
      "INFO - Step 10511, rl-loss: 509.2498474121094\r",
      "INFO - Step 10512, rl-loss: 100.29450988769531\r",
      "INFO - Step 10513, rl-loss: 97.12602233886719\r",
      "INFO - Step 10514, rl-loss: 394.03912353515625\r",
      "INFO - Step 10515, rl-loss: 1.2749412059783936\r",
      "INFO - Step 10516, rl-loss: 170.9225311279297\r",
      "INFO - Step 10517, rl-loss: 302.68231201171875\r",
      "INFO - Step 10518, rl-loss: 316.0296936035156\r",
      "INFO - Step 10519, rl-loss: 1.0610198974609375\r",
      "INFO - Step 10520, rl-loss: 72.28150939941406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10521, rl-loss: 2.168557643890381\r",
      "INFO - Step 10522, rl-loss: 343.63800048828125\r",
      "INFO - Step 10523, rl-loss: 207.3026885986328\r",
      "INFO - Step 10524, rl-loss: 364.27935791015625\r",
      "INFO - Step 10525, rl-loss: 432.7386474609375\r",
      "INFO - Step 10526, rl-loss: 321.61785888671875\r",
      "INFO - Step 10527, rl-loss: 533.9013061523438\r",
      "INFO - Step 10528, rl-loss: 60.041812896728516\r",
      "INFO - Step 10529, rl-loss: 230.439697265625\r",
      "INFO - Step 10530, rl-loss: 3.1606292724609375\r",
      "INFO - Step 10531, rl-loss: 678.316650390625\r",
      "INFO - Step 10532, rl-loss: 516.2970581054688\r",
      "INFO - Step 10533, rl-loss: 1.8380763530731201\r",
      "INFO - Step 10534, rl-loss: 0.9338451623916626\r",
      "INFO - Step 10535, rl-loss: 2.507657051086426\r",
      "INFO - Step 10536, rl-loss: 406.4354248046875\r",
      "INFO - Step 10537, rl-loss: 13.186999320983887\r",
      "INFO - Step 10538, rl-loss: 93.71131896972656\r",
      "INFO - Step 10539, rl-loss: 324.18511962890625\r",
      "INFO - Step 10540, rl-loss: 2.4919700622558594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10541, rl-loss: 220.96849060058594\r",
      "INFO - Step 10542, rl-loss: 1.212828516960144\r",
      "INFO - Step 10543, rl-loss: 250.7791748046875\r",
      "INFO - Step 10544, rl-loss: 133.17861938476562\r",
      "INFO - Step 10545, rl-loss: 178.41744995117188\r",
      "INFO - Step 10546, rl-loss: 61.084800720214844\r",
      "INFO - Step 10547, rl-loss: 251.5045928955078\r",
      "INFO - Step 10548, rl-loss: 741.6492309570312\r",
      "INFO - Step 10549, rl-loss: 300.4114990234375\r",
      "INFO - Step 10550, rl-loss: 4.037539005279541\r",
      "INFO - Step 10551, rl-loss: 2.0297610759735107\r",
      "INFO - Step 10552, rl-loss: 97.77140808105469\r",
      "INFO - Step 10553, rl-loss: 1.0104962587356567\r",
      "INFO - Step 10554, rl-loss: 107.34190368652344\r",
      "INFO - Step 10555, rl-loss: 369.552978515625\r",
      "INFO - Step 10556, rl-loss: 652.19384765625\r",
      "INFO - Step 10557, rl-loss: 89.16377258300781\r",
      "INFO - Step 10558, rl-loss: 162.6950225830078\r",
      "INFO - Step 10559, rl-loss: 0.9821171760559082\r",
      "INFO - Step 10560, rl-loss: 3.88582181930542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10561, rl-loss: 288.7488098144531\r",
      "INFO - Step 10562, rl-loss: 5.978504180908203\r",
      "INFO - Step 10563, rl-loss: 512.7369384765625\r",
      "INFO - Step 10564, rl-loss: 123.06366729736328\r",
      "INFO - Step 10565, rl-loss: 1.2992562055587769\r",
      "INFO - Step 10566, rl-loss: 330.04412841796875\r",
      "INFO - Step 10567, rl-loss: 558.5260620117188\r",
      "INFO - Step 10568, rl-loss: 229.21995544433594\r",
      "INFO - Step 10569, rl-loss: 469.09136962890625\r",
      "INFO - Step 10570, rl-loss: 492.30340576171875\r",
      "INFO - Step 10571, rl-loss: 269.3586730957031\r",
      "INFO - Step 10572, rl-loss: 1293.853271484375\r",
      "INFO - Step 10573, rl-loss: 0.6593054533004761\r",
      "INFO - Step 10574, rl-loss: 21.35065269470215\r",
      "INFO - Step 10575, rl-loss: 2.7712559700012207\r",
      "INFO - Step 10576, rl-loss: 96.08467864990234\r",
      "INFO - Step 10577, rl-loss: 1.8134078979492188\r",
      "INFO - Step 10578, rl-loss: 196.63870239257812\r",
      "INFO - Step 10579, rl-loss: 287.8305358886719\r",
      "INFO - Step 10580, rl-loss: 849.6768798828125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10581, rl-loss: 460.28302001953125\r",
      "INFO - Step 10582, rl-loss: 306.5206298828125\r",
      "INFO - Step 10583, rl-loss: 275.7665100097656\r",
      "INFO - Step 10584, rl-loss: 82.56157684326172\r",
      "INFO - Step 10585, rl-loss: 106.55147552490234\r",
      "INFO - Step 10586, rl-loss: 1.0346301794052124\r",
      "INFO - Step 10587, rl-loss: 408.4271240234375\r",
      "INFO - Step 10588, rl-loss: 165.21185302734375\r",
      "INFO - Step 10589, rl-loss: 98.80767822265625\r",
      "INFO - Step 10590, rl-loss: 58.627418518066406\r",
      "INFO - Step 10591, rl-loss: 187.793212890625\r",
      "INFO - Step 10592, rl-loss: 292.7641906738281\r",
      "INFO - Step 10593, rl-loss: 3.1800694465637207\r",
      "INFO - Step 10594, rl-loss: 326.4452209472656\r",
      "INFO - Step 10595, rl-loss: 53.50284957885742\r",
      "INFO - Step 10596, rl-loss: 9.66845989227295\r",
      "INFO - Step 10597, rl-loss: 572.0870971679688\r",
      "INFO - Step 10598, rl-loss: 378.09210205078125\r",
      "INFO - Step 10599, rl-loss: 120.26773834228516\r",
      "INFO - Step 10600, rl-loss: 179.45916748046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10601, rl-loss: 78.9841537475586\r",
      "INFO - Step 10602, rl-loss: 129.4436798095703\r",
      "INFO - Step 10603, rl-loss: 100.3004150390625\r",
      "INFO - Step 10604, rl-loss: 72.4926528930664\r",
      "INFO - Step 10605, rl-loss: 180.53582763671875\r",
      "INFO - Step 10606, rl-loss: 613.7198486328125\r",
      "INFO - Step 10607, rl-loss: 282.6515197753906\r",
      "INFO - Step 10608, rl-loss: 0.9913738965988159\r",
      "INFO - Step 10609, rl-loss: 3.1334962844848633\r",
      "INFO - Step 10610, rl-loss: 2.6128625869750977\r",
      "INFO - Step 10611, rl-loss: 376.1378479003906\r",
      "INFO - Step 10612, rl-loss: 87.03926849365234\r",
      "INFO - Step 10613, rl-loss: 3.6881327629089355\r",
      "INFO - Step 10614, rl-loss: 2.3215506076812744\r",
      "INFO - Step 10615, rl-loss: 42.856082916259766\r",
      "INFO - Step 10616, rl-loss: 253.58868408203125\r",
      "INFO - Step 10617, rl-loss: 132.52308654785156\r",
      "INFO - Step 10618, rl-loss: 193.53868103027344\r",
      "INFO - Step 10619, rl-loss: 657.58837890625\r",
      "INFO - Step 10620, rl-loss: 186.18136596679688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10621, rl-loss: 56.41043472290039\r",
      "INFO - Step 10622, rl-loss: 108.17573547363281\r",
      "INFO - Step 10623, rl-loss: 578.7843627929688\r",
      "INFO - Step 10624, rl-loss: 964.3529052734375\r",
      "INFO - Step 10625, rl-loss: 1.7467334270477295\r",
      "INFO - Step 10626, rl-loss: 1.954912781715393\r",
      "INFO - Step 10627, rl-loss: 44.78211212158203\r",
      "INFO - Step 10628, rl-loss: 466.0431823730469\r",
      "INFO - Step 10629, rl-loss: 417.946044921875\r",
      "INFO - Step 10630, rl-loss: 1.2111543416976929\r",
      "INFO - Step 10631, rl-loss: 212.8634033203125\r",
      "INFO - Step 10632, rl-loss: 102.66983032226562\r",
      "INFO - Step 10633, rl-loss: 382.39080810546875\r",
      "INFO - Step 10634, rl-loss: 25.23634910583496\r",
      "INFO - Step 10635, rl-loss: 88.24698638916016\r",
      "INFO - Step 10636, rl-loss: 355.35711669921875\r",
      "INFO - Step 10637, rl-loss: 2.7969460487365723\r",
      "INFO - Step 10638, rl-loss: 129.17286682128906\r",
      "INFO - Step 10639, rl-loss: 90.48372650146484\r",
      "INFO - Step 10640, rl-loss: 517.65087890625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10641, rl-loss: 381.0734558105469\r",
      "INFO - Step 10642, rl-loss: 198.5860595703125\r",
      "INFO - Step 10643, rl-loss: 6.844066143035889\r",
      "INFO - Step 10644, rl-loss: 222.4607391357422\r",
      "INFO - Step 10645, rl-loss: 147.06964111328125\r",
      "INFO - Step 10646, rl-loss: 5.013792037963867\r",
      "INFO - Step 10647, rl-loss: 143.35760498046875\r",
      "INFO - Step 10648, rl-loss: 2.2389209270477295\r",
      "INFO - Step 10649, rl-loss: 643.008056640625\r",
      "INFO - Step 10650, rl-loss: 417.41046142578125\r",
      "INFO - Step 10651, rl-loss: 101.57453918457031\r",
      "INFO - Step 10652, rl-loss: 0.9734030961990356\r",
      "INFO - Step 10653, rl-loss: 239.7755126953125\r",
      "INFO - Step 10654, rl-loss: 399.61907958984375\r",
      "INFO - Step 10655, rl-loss: 521.7547607421875\r",
      "INFO - Step 10656, rl-loss: 141.22320556640625\r",
      "INFO - Step 10657, rl-loss: 294.1084289550781\r",
      "INFO - Step 10658, rl-loss: 218.57276916503906\r",
      "INFO - Step 10659, rl-loss: 622.1778564453125\r",
      "INFO - Step 10660, rl-loss: 39.0446662902832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10661, rl-loss: 307.0796203613281\r",
      "INFO - Step 10662, rl-loss: 254.14303588867188\r",
      "INFO - Step 10663, rl-loss: 197.8860321044922\r",
      "INFO - Step 10664, rl-loss: 125.6514892578125\r",
      "INFO - Step 10665, rl-loss: 147.28550720214844\r",
      "INFO - Step 10666, rl-loss: 309.7249755859375\r",
      "INFO - Step 10667, rl-loss: 28.29373550415039\r",
      "INFO - Step 10668, rl-loss: 58.02821350097656\r",
      "INFO - Step 10669, rl-loss: 1.133088231086731\r",
      "INFO - Step 10670, rl-loss: 230.99839782714844\r",
      "INFO - Step 10671, rl-loss: 316.38983154296875\r",
      "INFO - Step 10672, rl-loss: 67.07106018066406\r",
      "INFO - Step 10673, rl-loss: 243.2954864501953\r",
      "INFO - Step 10674, rl-loss: 261.6195983886719\r",
      "INFO - Step 10675, rl-loss: 370.593994140625\r",
      "INFO - Step 10676, rl-loss: 185.8712158203125\r",
      "INFO - Step 10677, rl-loss: 5.423557281494141\r",
      "INFO - Step 10678, rl-loss: 58.07161331176758\r",
      "INFO - Step 10679, rl-loss: 154.13775634765625\r",
      "INFO - Step 10680, rl-loss: 265.66796875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10681, rl-loss: 1.8299834728240967\r",
      "INFO - Step 10682, rl-loss: 55.28384017944336\r",
      "INFO - Step 10683, rl-loss: 16.53326416015625\r",
      "INFO - Step 10684, rl-loss: 36.19579315185547\r",
      "INFO - Step 10685, rl-loss: 130.20997619628906\r",
      "INFO - Step 10686, rl-loss: 313.7577819824219\r",
      "INFO - Step 10687, rl-loss: 1.9338762760162354\r",
      "INFO - Step 10688, rl-loss: 0.8109742999076843\r",
      "INFO - Step 10689, rl-loss: 38.64370346069336\r",
      "INFO - Step 10690, rl-loss: 1.195019245147705\r",
      "INFO - Step 10691, rl-loss: 1.425433874130249\r",
      "INFO - Step 10692, rl-loss: 1157.255126953125\r",
      "INFO - Step 10693, rl-loss: 251.17193603515625\r",
      "INFO - Step 10694, rl-loss: 143.6928253173828\r",
      "INFO - Step 10695, rl-loss: 236.66566467285156\r",
      "INFO - Step 10696, rl-loss: 347.2003173828125\r",
      "INFO - Step 10697, rl-loss: 317.7732849121094\r",
      "INFO - Step 10698, rl-loss: 196.68231201171875\r",
      "INFO - Step 10699, rl-loss: 269.1528625488281\r",
      "INFO - Step 10700, rl-loss: 209.13314819335938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10701, rl-loss: 47.68825912475586\r",
      "INFO - Step 10702, rl-loss: 227.40261840820312\r",
      "INFO - Step 10703, rl-loss: 56.450260162353516\r",
      "INFO - Step 10704, rl-loss: 481.12481689453125\r",
      "INFO - Step 10705, rl-loss: 418.7840270996094\r",
      "INFO - Step 10706, rl-loss: 122.51628112792969\r",
      "INFO - Step 10707, rl-loss: 73.57583618164062\r",
      "INFO - Step 10708, rl-loss: 326.7879638671875\r",
      "INFO - Step 10709, rl-loss: 195.48619079589844\r",
      "INFO - Step 10710, rl-loss: 0.9667347073554993\r",
      "INFO - Step 10711, rl-loss: 204.86917114257812\r",
      "INFO - Step 10712, rl-loss: 358.7242431640625\r",
      "INFO - Step 10713, rl-loss: 29.628009796142578\r",
      "INFO - Step 10714, rl-loss: 46.4456901550293\r",
      "INFO - Step 10715, rl-loss: 1.681929349899292\r",
      "INFO - Step 10716, rl-loss: 2.1443088054656982\r",
      "INFO - Step 10717, rl-loss: 111.87538146972656\r",
      "INFO - Step 10718, rl-loss: 59.8176383972168\r",
      "INFO - Step 10719, rl-loss: 192.46585083007812\r",
      "INFO - Step 10720, rl-loss: 448.9074401855469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10721, rl-loss: 2.709440231323242\r",
      "INFO - Step 10722, rl-loss: 136.8257293701172\r",
      "INFO - Step 10723, rl-loss: 1.6034266948699951\r",
      "INFO - Step 10724, rl-loss: 124.95448303222656\r",
      "INFO - Step 10725, rl-loss: 286.7593688964844\r",
      "INFO - Step 10726, rl-loss: 41.58143615722656\r",
      "INFO - Step 10727, rl-loss: 33.93741989135742\r",
      "INFO - Step 10728, rl-loss: 106.33212280273438\r",
      "INFO - Step 10729, rl-loss: 412.6044616699219\r",
      "INFO - Step 10730, rl-loss: 3.198854923248291\r",
      "INFO - Step 10731, rl-loss: 137.83062744140625\r",
      "INFO - Step 10732, rl-loss: 353.2568664550781\r",
      "INFO - Step 10733, rl-loss: 250.48121643066406\r",
      "INFO - Step 10734, rl-loss: 81.74700164794922\r",
      "INFO - Step 10735, rl-loss: 181.8417205810547\r",
      "INFO - Step 10736, rl-loss: 766.7935791015625\r",
      "INFO - Step 10737, rl-loss: 116.42768096923828\r",
      "INFO - Step 10738, rl-loss: 435.8280334472656\r",
      "INFO - Step 10739, rl-loss: 468.3069152832031\r",
      "INFO - Step 10740, rl-loss: 185.06134033203125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10741, rl-loss: 53.9478759765625\r",
      "INFO - Step 10742, rl-loss: 316.478759765625\r",
      "INFO - Step 10743, rl-loss: 152.68341064453125\r",
      "INFO - Step 10744, rl-loss: 234.36358642578125\r",
      "INFO - Step 10745, rl-loss: 93.66175842285156\r",
      "INFO - Step 10746, rl-loss: 364.3616027832031\r",
      "INFO - Step 10747, rl-loss: 159.8943634033203\r",
      "INFO - Step 10748, rl-loss: 480.7808837890625\r",
      "INFO - Step 10749, rl-loss: 173.83807373046875\r",
      "INFO - Step 10750, rl-loss: 81.81875610351562\r",
      "INFO - Step 10751, rl-loss: 1.7919526100158691\r",
      "INFO - Step 10752, rl-loss: 2.1817216873168945\r",
      "INFO - Step 10753, rl-loss: 150.79434204101562\r",
      "INFO - Step 10754, rl-loss: 125.57308197021484\r",
      "INFO - Step 10755, rl-loss: 477.13897705078125\r",
      "INFO - Step 10756, rl-loss: 344.5423889160156\r",
      "INFO - Step 10757, rl-loss: 2.285585403442383\r",
      "INFO - Step 10758, rl-loss: 205.98611450195312\r",
      "INFO - Step 10759, rl-loss: 4.391238212585449\r",
      "INFO - Step 10760, rl-loss: 46.222862243652344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10761, rl-loss: 142.14584350585938\r",
      "INFO - Step 10762, rl-loss: 40.99972152709961\r",
      "INFO - Step 10763, rl-loss: 4.334742546081543\r",
      "INFO - Step 10764, rl-loss: 347.97216796875\r",
      "INFO - Step 10765, rl-loss: 300.76470947265625\r",
      "INFO - Step 10766, rl-loss: 4.124568462371826\r",
      "INFO - Step 10767, rl-loss: 538.4197998046875\r",
      "INFO - Step 10768, rl-loss: 581.626953125\r",
      "INFO - Step 10769, rl-loss: 34.43549728393555\r",
      "INFO - Step 10770, rl-loss: 137.95831298828125\r",
      "INFO - Step 10771, rl-loss: 343.90264892578125\r",
      "INFO - Step 10772, rl-loss: 254.8474578857422\r",
      "INFO - Step 10773, rl-loss: 305.0584411621094\r",
      "INFO - Step 10774, rl-loss: 263.8253173828125\r",
      "INFO - Step 10775, rl-loss: 767.9966430664062\r",
      "INFO - Step 10776, rl-loss: 105.16129302978516\r",
      "INFO - Step 10777, rl-loss: 82.22510528564453\r",
      "INFO - Step 10778, rl-loss: 31.261869430541992\r",
      "INFO - Step 10779, rl-loss: 32.498077392578125\r",
      "INFO - Step 10780, rl-loss: 109.82756042480469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10781, rl-loss: 1.7713788747787476\r",
      "INFO - Step 10782, rl-loss: 1.4075062274932861\r",
      "INFO - Step 10783, rl-loss: 210.9035186767578\r",
      "INFO - Step 10784, rl-loss: 151.4772491455078\r",
      "INFO - Step 10785, rl-loss: 598.3320922851562\r",
      "INFO - Step 10786, rl-loss: 1.5057942867279053\r",
      "INFO - Step 10787, rl-loss: 1.0761499404907227\r",
      "INFO - Step 10788, rl-loss: 117.26695251464844\r",
      "INFO - Step 10789, rl-loss: 213.5789031982422\r",
      "INFO - Step 10790, rl-loss: 370.2123718261719\r",
      "INFO - Step 10791, rl-loss: 1.4643402099609375\r",
      "INFO - Step 10792, rl-loss: 225.4417724609375\r",
      "INFO - Step 10793, rl-loss: 624.4006958007812\r",
      "INFO - Step 10794, rl-loss: 973.84375\r",
      "INFO - Step 10795, rl-loss: 237.0315704345703\r",
      "INFO - Step 10796, rl-loss: 0.7513557076454163\r",
      "INFO - Step 10797, rl-loss: 210.46798706054688\r",
      "INFO - Step 10798, rl-loss: 387.5646667480469\r",
      "INFO - Step 10799, rl-loss: 1.125847578048706\r",
      "INFO - Step 10800, rl-loss: 23.413501739501953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10801, rl-loss: 87.99433898925781\r",
      "INFO - Step 10802, rl-loss: 24.520370483398438\r",
      "INFO - Step 10803, rl-loss: 56.95090103149414\r",
      "INFO - Step 10804, rl-loss: 1.325140357017517\r",
      "INFO - Step 10805, rl-loss: 1.23430597782135\r",
      "INFO - Step 10806, rl-loss: 175.114013671875\r",
      "INFO - Step 10807, rl-loss: 329.11083984375\r",
      "INFO - Step 10808, rl-loss: 200.49765014648438\r",
      "INFO - Step 10809, rl-loss: 568.2208251953125\r",
      "INFO - Step 10810, rl-loss: 207.56735229492188\r",
      "INFO - Step 10811, rl-loss: 34.24739456176758\r",
      "INFO - Step 10812, rl-loss: 374.9883728027344\r",
      "INFO - Step 10813, rl-loss: 307.138427734375\r",
      "INFO - Step 10814, rl-loss: 179.08331298828125\r",
      "INFO - Step 10815, rl-loss: 1.4444427490234375\r",
      "INFO - Step 10816, rl-loss: 1.5960779190063477\r",
      "INFO - Step 10817, rl-loss: 103.57013702392578\r",
      "INFO - Step 10818, rl-loss: 194.1178436279297\r",
      "INFO - Step 10819, rl-loss: 651.4368286132812\r",
      "INFO - Step 10820, rl-loss: 78.27159118652344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10821, rl-loss: 498.6492004394531\r",
      "INFO - Step 10822, rl-loss: 52.61933135986328\r",
      "INFO - Step 10823, rl-loss: 2.539794445037842\r",
      "INFO - Step 10824, rl-loss: 130.9649658203125\r",
      "INFO - Step 10825, rl-loss: 193.83412170410156\r",
      "INFO - Step 10826, rl-loss: 404.96575927734375\r",
      "INFO - Step 10827, rl-loss: 571.7420654296875\r",
      "INFO - Step 10828, rl-loss: 259.9999694824219\r",
      "INFO - Step 10829, rl-loss: 17.382278442382812\r",
      "INFO - Step 10830, rl-loss: 273.4787902832031\r",
      "INFO - Step 10831, rl-loss: 129.9665985107422\r",
      "INFO - Step 10832, rl-loss: 4.064030647277832\r",
      "INFO - Step 10833, rl-loss: 222.0740509033203\r",
      "INFO - Step 10834, rl-loss: 37.346431732177734\r",
      "INFO - Step 10835, rl-loss: 49.0146598815918\r",
      "INFO - Step 10836, rl-loss: 38.02088928222656\r",
      "INFO - Step 10837, rl-loss: 232.91896057128906\r",
      "INFO - Step 10838, rl-loss: 390.5328063964844\r",
      "INFO - Step 10839, rl-loss: 171.1510772705078\r",
      "INFO - Step 10840, rl-loss: 143.0291748046875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10841, rl-loss: 51.88521194458008\r",
      "INFO - Step 10842, rl-loss: 265.7758483886719\r",
      "INFO - Step 10843, rl-loss: 345.7422180175781\r",
      "INFO - Step 10844, rl-loss: 243.00146484375\r",
      "INFO - Step 10845, rl-loss: 3.169532299041748\r",
      "INFO - Step 10846, rl-loss: 108.33244323730469\r",
      "INFO - Step 10847, rl-loss: 330.73681640625\r",
      "INFO - Step 10848, rl-loss: 375.52154541015625\r",
      "INFO - Step 10849, rl-loss: 192.349853515625\r",
      "INFO - Step 10850, rl-loss: 244.59689331054688\r",
      "INFO - Step 10851, rl-loss: 0.7612882852554321\r",
      "INFO - Step 10852, rl-loss: 164.2672576904297\r",
      "INFO - Step 10853, rl-loss: 23.560832977294922\r",
      "INFO - Step 10854, rl-loss: 155.66586303710938\r",
      "INFO - Step 10855, rl-loss: 168.54718017578125\r",
      "INFO - Step 10856, rl-loss: 2.4378366470336914\r",
      "INFO - Step 10857, rl-loss: 236.4510955810547\r",
      "INFO - Step 10858, rl-loss: 183.8958282470703\r",
      "INFO - Step 10859, rl-loss: 1.3123164176940918\r",
      "INFO - Step 10860, rl-loss: 89.25321197509766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10861, rl-loss: 270.662109375\r",
      "INFO - Step 10862, rl-loss: 0.6072731018066406\r",
      "INFO - Step 10863, rl-loss: 2.3315329551696777\r",
      "INFO - Step 10864, rl-loss: 28.006336212158203\r",
      "INFO - Step 10865, rl-loss: 59.719871520996094\r",
      "INFO - Step 10866, rl-loss: 342.1330871582031\r",
      "INFO - Step 10867, rl-loss: 1.158747673034668\r",
      "INFO - Step 10868, rl-loss: 1.6107659339904785\r",
      "INFO - Step 10869, rl-loss: 293.9583740234375\r",
      "INFO - Step 10870, rl-loss: 52.50535583496094\r",
      "INFO - Step 10871, rl-loss: 109.38764190673828\r",
      "INFO - Step 10872, rl-loss: 121.34852600097656\r",
      "INFO - Step 10873, rl-loss: 126.54340362548828\r",
      "INFO - Step 10874, rl-loss: 38.19582748413086\r",
      "INFO - Step 10875, rl-loss: 2.040349006652832\r",
      "INFO - Step 10876, rl-loss: 59.41086196899414\r",
      "INFO - Step 10877, rl-loss: 41.16017150878906\r",
      "INFO - Step 10878, rl-loss: 340.35980224609375\r",
      "INFO - Step 10879, rl-loss: 13.130373001098633\r",
      "INFO - Step 10880, rl-loss: 160.66139221191406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10881, rl-loss: 233.85650634765625\r",
      "INFO - Step 10882, rl-loss: 100.24244689941406\r",
      "INFO - Step 10883, rl-loss: 113.8559799194336\r",
      "INFO - Step 10884, rl-loss: 477.5552673339844\r",
      "INFO - Step 10885, rl-loss: 57.69919967651367\r",
      "INFO - Step 10886, rl-loss: 213.18069458007812\r",
      "INFO - Step 10887, rl-loss: 374.2177429199219\r",
      "INFO - Step 10888, rl-loss: 417.4404296875\r",
      "INFO - Step 10889, rl-loss: 464.631103515625\r",
      "INFO - Step 10890, rl-loss: 411.4710693359375\r",
      "INFO - Step 10891, rl-loss: 137.54928588867188\r",
      "INFO - Step 10892, rl-loss: 90.977294921875\r",
      "INFO - Step 10893, rl-loss: 295.4334411621094\r",
      "INFO - Step 10894, rl-loss: 298.6885986328125\r",
      "INFO - Step 10895, rl-loss: 204.08416748046875\r",
      "INFO - Step 10896, rl-loss: 70.19779968261719\r",
      "INFO - Step 10897, rl-loss: 76.12590789794922\r",
      "INFO - Step 10898, rl-loss: 260.0262451171875\r",
      "INFO - Step 10899, rl-loss: 242.4071807861328\r",
      "INFO - Step 10900, rl-loss: 96.43876647949219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10901, rl-loss: 138.2432861328125\r",
      "INFO - Step 10902, rl-loss: 94.15087890625\r",
      "INFO - Step 10903, rl-loss: 19.368885040283203\r",
      "INFO - Step 10904, rl-loss: 18.646974563598633\r",
      "INFO - Step 10905, rl-loss: 229.52658081054688\r",
      "INFO - Step 10906, rl-loss: 13.63488483428955\r",
      "INFO - Step 10907, rl-loss: 133.73922729492188\r",
      "INFO - Step 10908, rl-loss: 178.073974609375\r",
      "INFO - Step 10909, rl-loss: 19.50855827331543\r",
      "INFO - Step 10910, rl-loss: 687.721923828125\r",
      "INFO - Step 10911, rl-loss: 0.6286972761154175\r",
      "INFO - Step 10912, rl-loss: 329.4235534667969\r",
      "INFO - Step 10913, rl-loss: 247.2261199951172\r",
      "INFO - Step 10914, rl-loss: 318.34130859375\r",
      "INFO - Step 10915, rl-loss: 58.55204391479492\r",
      "INFO - Step 10916, rl-loss: 494.67236328125\r",
      "INFO - Step 10917, rl-loss: 401.9953918457031\r",
      "INFO - Step 10918, rl-loss: 535.3636474609375\r",
      "INFO - Step 10919, rl-loss: 71.97522735595703\r",
      "INFO - Step 10920, rl-loss: 31.977834701538086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10921, rl-loss: 162.5076141357422\r",
      "INFO - Step 10922, rl-loss: 29.466197967529297\r",
      "INFO - Step 10923, rl-loss: 578.42919921875\r",
      "INFO - Step 10924, rl-loss: 220.4345703125\r",
      "INFO - Step 10925, rl-loss: 214.80758666992188\r",
      "INFO - Step 10926, rl-loss: 181.8163299560547\r",
      "INFO - Step 10927, rl-loss: 589.130126953125\r",
      "INFO - Step 10928, rl-loss: 351.0970153808594\r",
      "INFO - Step 10929, rl-loss: 30.019176483154297\r",
      "INFO - Step 10930, rl-loss: 278.0173034667969\r",
      "INFO - Step 10931, rl-loss: 161.7101593017578\r",
      "INFO - Step 10932, rl-loss: 32.638729095458984\r",
      "INFO - Step 10933, rl-loss: 278.267822265625\r",
      "INFO - Step 10934, rl-loss: 341.0777587890625\r",
      "INFO - Step 10935, rl-loss: 135.52833557128906\r",
      "INFO - Step 10936, rl-loss: 354.07720947265625\r",
      "INFO - Step 10937, rl-loss: 146.8957977294922\r",
      "INFO - Step 10938, rl-loss: 132.21864318847656\r",
      "INFO - Step 10939, rl-loss: 3.2493362426757812\r",
      "INFO - Step 10940, rl-loss: 38.242855072021484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10941, rl-loss: 14.919689178466797\r",
      "INFO - Step 10942, rl-loss: 55.39604187011719\r",
      "INFO - Step 10943, rl-loss: 516.3306884765625\r",
      "INFO - Step 10944, rl-loss: 125.58183288574219\r",
      "INFO - Step 10945, rl-loss: 87.50696563720703\r",
      "INFO - Step 10946, rl-loss: 184.11419677734375\r",
      "INFO - Step 10947, rl-loss: 125.34963989257812\r",
      "INFO - Step 10948, rl-loss: 243.4163360595703\r",
      "INFO - Step 10949, rl-loss: 100.29358673095703\r",
      "INFO - Step 10950, rl-loss: 133.60804748535156\r",
      "INFO - Step 10951, rl-loss: 486.70654296875\r",
      "INFO - Step 10952, rl-loss: 0.7214873433113098\r",
      "INFO - Step 10953, rl-loss: 287.3936462402344\r",
      "INFO - Step 10954, rl-loss: 45.43220520019531\r",
      "INFO - Step 10955, rl-loss: 287.1702880859375\r",
      "INFO - Step 10956, rl-loss: 33.49615478515625\r",
      "INFO - Step 10957, rl-loss: 275.07696533203125\r",
      "INFO - Step 10958, rl-loss: 206.24549865722656\r",
      "INFO - Step 10959, rl-loss: 139.8637237548828\r",
      "INFO - Step 10960, rl-loss: 1.138167142868042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10961, rl-loss: 1.5816669464111328\r",
      "INFO - Step 10962, rl-loss: 2.1937549114227295\r",
      "INFO - Step 10963, rl-loss: 273.4551696777344\r",
      "INFO - Step 10964, rl-loss: 35.99528121948242\r",
      "INFO - Step 10965, rl-loss: 247.47286987304688\r",
      "INFO - Step 10966, rl-loss: 209.28329467773438\r",
      "INFO - Step 10967, rl-loss: 456.53704833984375\r",
      "INFO - Step 10968, rl-loss: 142.06234741210938\r",
      "INFO - Step 10969, rl-loss: 198.32373046875\r",
      "INFO - Step 10970, rl-loss: 262.8273010253906\r",
      "INFO - Step 10971, rl-loss: 316.629638671875\r",
      "INFO - Step 10972, rl-loss: 408.8880920410156\r",
      "INFO - Step 10973, rl-loss: 3.203329563140869\r",
      "INFO - Step 10974, rl-loss: 79.53620147705078\r",
      "INFO - Step 10975, rl-loss: 1.7915465831756592\r",
      "INFO - Step 10976, rl-loss: 219.41488647460938\r",
      "INFO - Step 10977, rl-loss: 3.6287682056427\r",
      "INFO - Step 10978, rl-loss: 210.22561645507812\r",
      "INFO - Step 10979, rl-loss: 2.314152956008911\r",
      "INFO - Step 10980, rl-loss: 180.64561462402344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 10981, rl-loss: 26.358434677124023\r",
      "INFO - Step 10982, rl-loss: 173.41616821289062\r",
      "INFO - Step 10983, rl-loss: 0.9100885391235352\r",
      "INFO - Step 10984, rl-loss: 503.97760009765625\r",
      "INFO - Step 10985, rl-loss: 1.3164337873458862\r",
      "INFO - Step 10986, rl-loss: 134.69326782226562\r",
      "INFO - Step 10987, rl-loss: 1.0759097337722778\r",
      "INFO - Step 10988, rl-loss: 700.583984375\r",
      "INFO - Step 10989, rl-loss: 531.7329711914062\r",
      "INFO - Step 10990, rl-loss: 422.6041259765625\r",
      "INFO - Step 10991, rl-loss: 244.98117065429688\r",
      "INFO - Step 10992, rl-loss: 159.4439697265625\r",
      "INFO - Step 10993, rl-loss: 48.28046798706055\r",
      "INFO - Step 10994, rl-loss: 3.262617349624634\r",
      "INFO - Step 10995, rl-loss: 0.797584056854248\r",
      "INFO - Step 10996, rl-loss: 329.7279052734375\r",
      "INFO - Step 10997, rl-loss: 221.77651977539062\r",
      "INFO - Step 10998, rl-loss: 423.96990966796875\r",
      "INFO - Step 10999, rl-loss: 424.27239990234375\r",
      "INFO - Step 11000, rl-loss: 299.00054931640625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 11020, rl-loss: 342.95574951171875\n",
      "----------------------------------------\n",
      "  timestep     |  578880\n",
      "  reward       |  62.81\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 11040, rl-loss: 2.7723000049591064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 11051, rl-loss: 1.1696784496307373\n",
      "----------------------------------------\n",
      "  timestep     |  584816\n",
      "  reward       |  64.53\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 12040, rl-loss: 160.41859436035156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 12060, rl-loss: 1.4536092281341553"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12061, rl-loss: 413.4734802246094\r",
      "INFO - Step 12062, rl-loss: 129.539306640625\r",
      "INFO - Step 12063, rl-loss: 34.83403778076172\r",
      "INFO - Step 12064, rl-loss: 1.188374400138855\r",
      "INFO - Step 12065, rl-loss: 47.310367584228516\r",
      "INFO - Step 12066, rl-loss: 280.16937255859375\r",
      "INFO - Step 12067, rl-loss: 434.1868896484375\r",
      "INFO - Step 12068, rl-loss: 364.6523132324219\r",
      "INFO - Step 12069, rl-loss: 357.112548828125\r",
      "INFO - Step 12070, rl-loss: 268.87646484375\r",
      "INFO - Step 12071, rl-loss: 122.69798278808594\r",
      "INFO - Step 12072, rl-loss: 131.6201171875\r",
      "INFO - Step 12073, rl-loss: 137.4515838623047\r",
      "INFO - Step 12074, rl-loss: 271.46417236328125\r",
      "INFO - Step 12075, rl-loss: 438.55548095703125\r",
      "INFO - Step 12076, rl-loss: 25.724895477294922\r",
      "INFO - Step 12077, rl-loss: 65.58148956298828\r",
      "INFO - Step 12078, rl-loss: 411.254150390625\r",
      "INFO - Step 12079, rl-loss: 1.73622727394104\r",
      "INFO - Step 12080, rl-loss: 215.53636169433594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12081, rl-loss: 247.4239501953125\r",
      "INFO - Step 12082, rl-loss: 211.8112030029297\r",
      "INFO - Step 12083, rl-loss: 97.89616394042969\r",
      "INFO - Step 12084, rl-loss: 217.96849060058594\r",
      "INFO - Step 12085, rl-loss: 258.06463623046875\r",
      "INFO - Step 12086, rl-loss: 206.71163940429688\r",
      "INFO - Step 12087, rl-loss: 0.47967883944511414\r",
      "INFO - Step 12088, rl-loss: 182.1143035888672\r",
      "INFO - Step 12089, rl-loss: 1.2032091617584229\r",
      "INFO - Step 12090, rl-loss: 121.69518280029297\r",
      "INFO - Step 12091, rl-loss: 210.58352661132812\r",
      "INFO - Step 12092, rl-loss: 328.5729675292969\r",
      "INFO - Step 12093, rl-loss: 119.79467010498047\r",
      "INFO - Step 12094, rl-loss: 0.3926902115345001\r",
      "INFO - Step 12095, rl-loss: 190.85543823242188\r",
      "INFO - Step 12096, rl-loss: 99.9241714477539\r",
      "INFO - Step 12097, rl-loss: 478.9159240722656\r",
      "INFO - Step 12098, rl-loss: 205.5221710205078\r",
      "INFO - Step 12099, rl-loss: 322.6736145019531\r",
      "INFO - Step 12100, rl-loss: 544.2499389648438\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12101, rl-loss: 332.1741943359375\r",
      "INFO - Step 12102, rl-loss: 280.6120300292969\r",
      "INFO - Step 12103, rl-loss: 47.8751106262207\r",
      "INFO - Step 12104, rl-loss: 227.68826293945312\r",
      "INFO - Step 12105, rl-loss: 97.42141723632812\r",
      "INFO - Step 12106, rl-loss: 25.880725860595703\r",
      "INFO - Step 12107, rl-loss: 138.8831787109375\r",
      "INFO - Step 12108, rl-loss: 231.15289306640625\r",
      "INFO - Step 12109, rl-loss: 268.3143005371094\r",
      "INFO - Step 12110, rl-loss: 58.0091438293457\r",
      "INFO - Step 12111, rl-loss: 189.1279754638672\r",
      "INFO - Step 12112, rl-loss: 1.3016859292984009\r",
      "INFO - Step 12113, rl-loss: 412.1484069824219\r",
      "INFO - Step 12114, rl-loss: 103.20185852050781\r",
      "INFO - Step 12115, rl-loss: 614.5780029296875\r",
      "INFO - Step 12116, rl-loss: 117.77445983886719\r",
      "INFO - Step 12117, rl-loss: 55.874229431152344\r",
      "INFO - Step 12118, rl-loss: 140.61581420898438\r",
      "INFO - Step 12119, rl-loss: 260.1468811035156\r",
      "INFO - Step 12120, rl-loss: 277.364013671875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12121, rl-loss: 122.1644058227539\r",
      "INFO - Step 12122, rl-loss: 4.306042671203613\r",
      "INFO - Step 12123, rl-loss: 25.545852661132812\r",
      "INFO - Step 12124, rl-loss: 1.6678285598754883\r",
      "INFO - Step 12125, rl-loss: 140.66885375976562\r",
      "INFO - Step 12126, rl-loss: 7.40669059753418\r",
      "INFO - Step 12127, rl-loss: 179.34129333496094\r",
      "INFO - Step 12128, rl-loss: 241.3000946044922\r",
      "INFO - Step 12129, rl-loss: 137.1934051513672\r",
      "INFO - Step 12130, rl-loss: 224.60157775878906\r",
      "INFO - Step 12131, rl-loss: 153.8242950439453\r",
      "INFO - Step 12132, rl-loss: 266.156982421875\r",
      "INFO - Step 12133, rl-loss: 68.16753387451172\r",
      "INFO - Step 12134, rl-loss: 0.9681584239006042\r",
      "INFO - Step 12135, rl-loss: 76.78276062011719\r",
      "INFO - Step 12136, rl-loss: 103.65730285644531\r",
      "INFO - Step 12137, rl-loss: 266.511474609375\r",
      "INFO - Step 12138, rl-loss: 118.40950775146484\r",
      "INFO - Step 12139, rl-loss: 857.75\r",
      "INFO - Step 12140, rl-loss: 2.833062171936035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12141, rl-loss: 1.1390421390533447\r",
      "INFO - Step 12142, rl-loss: 634.4479370117188\r",
      "INFO - Step 12143, rl-loss: 60.6087532043457\r",
      "INFO - Step 12144, rl-loss: 3.5911502838134766\r",
      "INFO - Step 12145, rl-loss: 83.85836029052734\r",
      "INFO - Step 12146, rl-loss: 34.235652923583984\r",
      "INFO - Step 12147, rl-loss: 51.39572525024414\r",
      "INFO - Step 12148, rl-loss: 152.10691833496094\r",
      "INFO - Step 12149, rl-loss: 229.46914672851562\r",
      "INFO - Step 12150, rl-loss: 154.64073181152344\r",
      "INFO - Step 12151, rl-loss: 116.79719543457031\r",
      "INFO - Step 12152, rl-loss: 419.6605224609375\r",
      "INFO - Step 12153, rl-loss: 294.18463134765625\r",
      "INFO - Step 12154, rl-loss: 136.0019073486328\r",
      "INFO - Step 12155, rl-loss: 657.332763671875\r",
      "INFO - Step 12156, rl-loss: 237.50538635253906\r",
      "INFO - Step 12157, rl-loss: 2.0165977478027344\r",
      "INFO - Step 12158, rl-loss: 5.492154598236084\r",
      "INFO - Step 12159, rl-loss: 53.34856414794922\r",
      "INFO - Step 12160, rl-loss: 4.40628719329834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12161, rl-loss: 303.6639709472656\r",
      "INFO - Step 12162, rl-loss: 165.2769775390625\r",
      "INFO - Step 12163, rl-loss: 58.20537185668945\r",
      "INFO - Step 12164, rl-loss: 205.89166259765625\r",
      "INFO - Step 12165, rl-loss: 455.4978332519531\r",
      "INFO - Step 12166, rl-loss: 199.78070068359375\r",
      "INFO - Step 12167, rl-loss: 456.1746826171875\r",
      "INFO - Step 12168, rl-loss: 4.466581344604492\r",
      "INFO - Step 12169, rl-loss: 1.581808090209961\r",
      "INFO - Step 12170, rl-loss: 354.0101623535156\r",
      "INFO - Step 12171, rl-loss: 2.5816104412078857\r",
      "INFO - Step 12172, rl-loss: 284.00457763671875\r",
      "INFO - Step 12173, rl-loss: 168.10296630859375\r",
      "INFO - Step 12174, rl-loss: 2.1226229667663574\r",
      "INFO - Step 12175, rl-loss: 1.8272302150726318\r",
      "INFO - Step 12176, rl-loss: 164.80470275878906\r",
      "INFO - Step 12177, rl-loss: 395.60943603515625\r",
      "INFO - Step 12178, rl-loss: 378.728515625\r",
      "INFO - Step 12179, rl-loss: 108.9526596069336\r",
      "INFO - Step 12180, rl-loss: 17.059526443481445"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12181, rl-loss: 920.9190063476562\r",
      "INFO - Step 12182, rl-loss: 78.63481903076172\r",
      "INFO - Step 12183, rl-loss: 233.21356201171875\r",
      "INFO - Step 12184, rl-loss: 600.1451416015625\r",
      "INFO - Step 12185, rl-loss: 307.8315124511719\r",
      "INFO - Step 12186, rl-loss: 30.72064208984375\r",
      "INFO - Step 12187, rl-loss: 4.036635875701904\r",
      "INFO - Step 12188, rl-loss: 29.88396453857422\r",
      "INFO - Step 12189, rl-loss: 182.44998168945312\r",
      "INFO - Step 12190, rl-loss: 513.2955322265625\r",
      "INFO - Step 12191, rl-loss: 579.6484375\r",
      "INFO - Step 12192, rl-loss: 400.32696533203125\r",
      "INFO - Step 12193, rl-loss: 54.97737121582031\r",
      "INFO - Step 12194, rl-loss: 740.3372802734375\r",
      "INFO - Step 12195, rl-loss: 134.6057586669922\r",
      "INFO - Step 12196, rl-loss: 1.8816218376159668\r",
      "INFO - Step 12197, rl-loss: 5.791210651397705\r",
      "INFO - Step 12198, rl-loss: 82.79754638671875\r",
      "INFO - Step 12199, rl-loss: 1.2273969650268555\r",
      "INFO - Step 12200, rl-loss: 357.6029052734375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12201, rl-loss: 1.404267430305481\r",
      "INFO - Step 12202, rl-loss: 94.72779846191406\r",
      "INFO - Step 12203, rl-loss: 219.6234588623047\r",
      "INFO - Step 12204, rl-loss: 417.6222839355469\r",
      "INFO - Step 12205, rl-loss: 253.43988037109375\r",
      "INFO - Step 12206, rl-loss: 2.8513662815093994\r",
      "INFO - Step 12207, rl-loss: 225.34780883789062\r",
      "INFO - Step 12208, rl-loss: 1.0843825340270996\r",
      "INFO - Step 12209, rl-loss: 154.63314819335938\r",
      "INFO - Step 12210, rl-loss: 455.1409912109375\r",
      "INFO - Step 12211, rl-loss: 184.2911834716797\r",
      "INFO - Step 12212, rl-loss: 281.1905822753906\r",
      "INFO - Step 12213, rl-loss: 285.8682861328125\r",
      "INFO - Step 12214, rl-loss: 569.62109375\r",
      "INFO - Step 12215, rl-loss: 119.64839172363281\r",
      "INFO - Step 12216, rl-loss: 162.76351928710938\r",
      "INFO - Step 12217, rl-loss: 490.2855224609375\r",
      "INFO - Step 12218, rl-loss: 130.00450134277344\r",
      "INFO - Step 12219, rl-loss: 60.6627197265625\r",
      "INFO - Step 12220, rl-loss: 1.5394222736358643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12221, rl-loss: 136.40280151367188\r",
      "INFO - Step 12222, rl-loss: 440.3208923339844\r",
      "INFO - Step 12223, rl-loss: 211.38339233398438\r",
      "INFO - Step 12224, rl-loss: 398.8310546875\r",
      "INFO - Step 12225, rl-loss: 2.408548593521118\r",
      "INFO - Step 12226, rl-loss: 279.4573059082031\r",
      "INFO - Step 12227, rl-loss: 6.264554023742676\r",
      "INFO - Step 12228, rl-loss: 126.02532958984375\r",
      "INFO - Step 12229, rl-loss: 479.5538635253906\r",
      "INFO - Step 12230, rl-loss: 135.84312438964844\r",
      "INFO - Step 12231, rl-loss: 1.8286864757537842\r",
      "INFO - Step 12232, rl-loss: 5.493329048156738\r",
      "INFO - Step 12233, rl-loss: 246.16282653808594\r",
      "INFO - Step 12234, rl-loss: 287.3750305175781\r",
      "INFO - Step 12235, rl-loss: 439.9517822265625\r",
      "INFO - Step 12236, rl-loss: 206.79736328125\r",
      "INFO - Step 12237, rl-loss: 251.0938262939453\r",
      "INFO - Step 12238, rl-loss: 720.7172241210938\r",
      "INFO - Step 12239, rl-loss: 4.362060070037842\r",
      "INFO - Step 12240, rl-loss: 15.692930221557617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 12241, rl-loss: 1.7111074924468994\r",
      "INFO - Step 12242, rl-loss: 92.54218292236328\r",
      "INFO - Step 12243, rl-loss: 656.5762939453125\r",
      "INFO - Step 12244, rl-loss: 120.17193603515625\r",
      "INFO - Step 12245, rl-loss: 277.6943359375\r",
      "INFO - Step 12246, rl-loss: 362.61419677734375\r",
      "INFO - Step 12247, rl-loss: 141.3787078857422\r",
      "INFO - Step 12248, rl-loss: 38.40959167480469\r",
      "INFO - Step 12249, rl-loss: 140.24502563476562\r",
      "INFO - Step 12250, rl-loss: 2.6709842681884766\r",
      "INFO - Step 12251, rl-loss: 288.2267761230469\r",
      "INFO - Step 12252, rl-loss: 0.8679193258285522\r",
      "INFO - Step 12253, rl-loss: 569.6082153320312\r",
      "INFO - Step 12254, rl-loss: 123.4300537109375\r",
      "INFO - Step 12255, rl-loss: 363.3482360839844\r",
      "INFO - Step 12256, rl-loss: 16.894550323486328\r",
      "INFO - Step 12257, rl-loss: 55.6496696472168\r",
      "INFO - Step 12258, rl-loss: 428.7435302734375\r",
      "INFO - Step 12259, rl-loss: 71.17938995361328\r",
      "INFO - Step 12260, rl-loss: 277.80413818359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 14020, rl-loss: 215.33749389648438\n",
      "----------------------------------------\n",
      "  timestep     |  596655\n",
      "  reward       |  64.28\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 14040, rl-loss: 102.89649200439453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 14060, rl-loss: 251.66514587402344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14061, rl-loss: 438.88311767578125\r",
      "INFO - Step 14062, rl-loss: 36.429725646972656\r",
      "INFO - Step 14063, rl-loss: 174.80233764648438\r",
      "INFO - Step 14064, rl-loss: 41.088497161865234\r",
      "INFO - Step 14065, rl-loss: 3.790325164794922\r",
      "INFO - Step 14066, rl-loss: 115.30140686035156\r",
      "INFO - Step 14067, rl-loss: 356.6162414550781\r",
      "INFO - Step 14068, rl-loss: 28.02880096435547\r",
      "INFO - Step 14069, rl-loss: 79.79690551757812\r",
      "INFO - Step 14070, rl-loss: 216.69825744628906\r",
      "INFO - Step 14071, rl-loss: 168.0072021484375\r",
      "INFO - Step 14072, rl-loss: 239.93795776367188\r",
      "INFO - Step 14073, rl-loss: 113.65144348144531\r",
      "INFO - Step 14074, rl-loss: 8.241878509521484\r",
      "INFO - Step 14075, rl-loss: 55.614723205566406\r",
      "INFO - Step 14076, rl-loss: 349.1156005859375\r",
      "INFO - Step 14077, rl-loss: 211.70118713378906\r",
      "INFO - Step 14078, rl-loss: 33.124385833740234\r",
      "INFO - Step 14079, rl-loss: 425.4393615722656\r",
      "INFO - Step 14080, rl-loss: 98.9833755493164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14081, rl-loss: 75.10626220703125\r",
      "INFO - Step 14082, rl-loss: 283.3073425292969\r",
      "INFO - Step 14083, rl-loss: 4.588680267333984\r",
      "INFO - Step 14084, rl-loss: 29.06589126586914\r",
      "INFO - Step 14085, rl-loss: 121.64312744140625\r",
      "INFO - Step 14086, rl-loss: 5.591909885406494\r",
      "INFO - Step 14087, rl-loss: 476.08587646484375\r",
      "INFO - Step 14088, rl-loss: 213.36300659179688\r",
      "INFO - Step 14089, rl-loss: 37.49321746826172\r",
      "INFO - Step 14090, rl-loss: 109.98723602294922\r",
      "INFO - Step 14091, rl-loss: 266.00543212890625\r",
      "INFO - Step 14092, rl-loss: 58.04486846923828\r",
      "INFO - Step 14093, rl-loss: 203.45664978027344\r",
      "INFO - Step 14094, rl-loss: 5.4278717041015625\r",
      "INFO - Step 14095, rl-loss: 3.42816424369812\r",
      "INFO - Step 14096, rl-loss: 97.90586853027344\r",
      "INFO - Step 14097, rl-loss: 115.27996826171875\r",
      "INFO - Step 14098, rl-loss: 72.06985473632812\r",
      "INFO - Step 14099, rl-loss: 3.4364123344421387\r",
      "INFO - Step 14100, rl-loss: 129.26109313964844\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14101, rl-loss: 340.84716796875\r",
      "INFO - Step 14102, rl-loss: 222.8760528564453\r",
      "INFO - Step 14103, rl-loss: 54.003726959228516\r",
      "INFO - Step 14104, rl-loss: 271.6142883300781\r",
      "INFO - Step 14105, rl-loss: 163.53030395507812\r",
      "INFO - Step 14106, rl-loss: 149.4329071044922\r",
      "INFO - Step 14107, rl-loss: 975.37890625\r",
      "INFO - Step 14108, rl-loss: 4.063835144042969\r",
      "INFO - Step 14109, rl-loss: 2.880054235458374\r",
      "INFO - Step 14110, rl-loss: 119.72679138183594\r",
      "INFO - Step 14111, rl-loss: 55.766937255859375\r",
      "INFO - Step 14112, rl-loss: 1.6927566528320312\r",
      "INFO - Step 14113, rl-loss: 197.39356994628906\r",
      "INFO - Step 14114, rl-loss: 66.3300552368164\r",
      "INFO - Step 14115, rl-loss: 6.691472053527832\r",
      "INFO - Step 14116, rl-loss: 62.149330139160156\r",
      "INFO - Step 14117, rl-loss: 11.832427978515625\r",
      "INFO - Step 14118, rl-loss: 226.70278930664062\r",
      "INFO - Step 14119, rl-loss: 182.40472412109375\r",
      "INFO - Step 14120, rl-loss: 51.00379180908203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14121, rl-loss: 438.8035888671875\r",
      "INFO - Step 14122, rl-loss: 338.036376953125\r",
      "INFO - Step 14123, rl-loss: 84.14488220214844\r",
      "INFO - Step 14124, rl-loss: 607.4725341796875\r",
      "INFO - Step 14125, rl-loss: 1.0830439329147339\r",
      "INFO - Step 14126, rl-loss: 73.13485717773438\r",
      "INFO - Step 14127, rl-loss: 214.36082458496094\r",
      "INFO - Step 14128, rl-loss: 1.6391890048980713\r",
      "INFO - Step 14129, rl-loss: 147.9980926513672\r",
      "INFO - Step 14130, rl-loss: 10.83925724029541\r",
      "INFO - Step 14131, rl-loss: 56.27480697631836\r",
      "INFO - Step 14132, rl-loss: 244.59359741210938\r",
      "INFO - Step 14133, rl-loss: 173.33792114257812\r",
      "INFO - Step 14134, rl-loss: 353.41473388671875\r",
      "INFO - Step 14135, rl-loss: 60.810585021972656\r",
      "INFO - Step 14136, rl-loss: 39.20197677612305\r",
      "INFO - Step 14137, rl-loss: 39.71561813354492\r",
      "INFO - Step 14138, rl-loss: 111.05350494384766\r",
      "INFO - Step 14139, rl-loss: 319.5536804199219\r",
      "INFO - Step 14140, rl-loss: 363.9308166503906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14141, rl-loss: 1.1375648975372314\r",
      "INFO - Step 14142, rl-loss: 28.368026733398438\r",
      "INFO - Step 14143, rl-loss: 4.874174118041992\r",
      "INFO - Step 14144, rl-loss: 151.97874450683594\r",
      "INFO - Step 14145, rl-loss: 153.41810607910156\r",
      "INFO - Step 14146, rl-loss: 104.41643524169922\r",
      "INFO - Step 14147, rl-loss: 2.401064395904541\r",
      "INFO - Step 14148, rl-loss: 8.496770858764648\r",
      "INFO - Step 14149, rl-loss: 25.310428619384766\r",
      "INFO - Step 14150, rl-loss: 305.7575378417969\r",
      "INFO - Step 14151, rl-loss: 41.573028564453125\r",
      "INFO - Step 14152, rl-loss: 243.02032470703125\r",
      "INFO - Step 14153, rl-loss: 326.019775390625\r",
      "INFO - Step 14154, rl-loss: 328.6999816894531\r",
      "INFO - Step 14155, rl-loss: 130.16659545898438\r",
      "INFO - Step 14156, rl-loss: 35.33087158203125\r",
      "INFO - Step 14157, rl-loss: 66.30252075195312\r",
      "INFO - Step 14158, rl-loss: 409.9199523925781\r",
      "INFO - Step 14159, rl-loss: 143.09591674804688\r",
      "INFO - Step 14160, rl-loss: 2.1853744983673096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14161, rl-loss: 293.943359375\r",
      "INFO - Step 14162, rl-loss: 5.821173667907715\r",
      "INFO - Step 14163, rl-loss: 591.8712158203125\r",
      "INFO - Step 14164, rl-loss: 311.41790771484375\r",
      "INFO - Step 14165, rl-loss: 142.0826873779297\r",
      "INFO - Step 14166, rl-loss: 13.7149076461792\r",
      "INFO - Step 14167, rl-loss: 76.85213470458984\r",
      "INFO - Step 14168, rl-loss: 212.9351806640625\r",
      "INFO - Step 14169, rl-loss: 161.18894958496094\r",
      "INFO - Step 14170, rl-loss: 1.524594783782959\r",
      "INFO - Step 14171, rl-loss: 59.0597038269043\r",
      "INFO - Step 14172, rl-loss: 3.721492290496826\r",
      "INFO - Step 14173, rl-loss: 92.40841674804688\r",
      "INFO - Step 14174, rl-loss: 75.68357849121094\r",
      "INFO - Step 14175, rl-loss: 243.3614501953125\r",
      "INFO - Step 14176, rl-loss: 563.2131958007812\r",
      "INFO - Step 14177, rl-loss: 9.958419799804688\r",
      "INFO - Step 14178, rl-loss: 65.36356353759766\r",
      "INFO - Step 14179, rl-loss: 1.4636516571044922\r",
      "INFO - Step 14180, rl-loss: 336.93646240234375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14181, rl-loss: 55.49785614013672\r",
      "INFO - Step 14182, rl-loss: 384.43267822265625\r",
      "INFO - Step 14183, rl-loss: 5.299777984619141\r",
      "INFO - Step 14184, rl-loss: 2.4602463245391846\r",
      "INFO - Step 14185, rl-loss: 78.57298278808594\r",
      "INFO - Step 14186, rl-loss: 454.159912109375\r",
      "INFO - Step 14187, rl-loss: 248.99183654785156\r",
      "INFO - Step 14188, rl-loss: 140.75424194335938\r",
      "INFO - Step 14189, rl-loss: 11.372994422912598\r",
      "INFO - Step 14190, rl-loss: 6.793107986450195\r",
      "INFO - Step 14191, rl-loss: 205.60159301757812\r",
      "INFO - Step 14192, rl-loss: 492.93121337890625\r",
      "INFO - Step 14193, rl-loss: 33.13109588623047\r",
      "INFO - Step 14194, rl-loss: 70.31367492675781\r",
      "INFO - Step 14195, rl-loss: 377.85467529296875\r",
      "INFO - Step 14196, rl-loss: 4.877419471740723\r",
      "INFO - Step 14197, rl-loss: 409.11865234375\r",
      "INFO - Step 14198, rl-loss: 229.69822692871094\r",
      "INFO - Step 14199, rl-loss: 117.88766479492188\r",
      "INFO - Step 14200, rl-loss: 4.639616966247559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14201, rl-loss: 357.46405029296875\r",
      "INFO - Step 14202, rl-loss: 121.08280944824219\r",
      "INFO - Step 14203, rl-loss: 86.77293395996094\r",
      "INFO - Step 14204, rl-loss: 26.746278762817383\r",
      "INFO - Step 14205, rl-loss: 74.42937469482422\r",
      "INFO - Step 14206, rl-loss: 248.846435546875\r",
      "INFO - Step 14207, rl-loss: 242.9676971435547\r",
      "INFO - Step 14208, rl-loss: 316.5514221191406\r",
      "INFO - Step 14209, rl-loss: 109.1778335571289\r",
      "INFO - Step 14210, rl-loss: 32.038021087646484\r",
      "INFO - Step 14211, rl-loss: 287.6118469238281\r",
      "INFO - Step 14212, rl-loss: 108.06748962402344\r",
      "INFO - Step 14213, rl-loss: 228.54107666015625\r",
      "INFO - Step 14214, rl-loss: 162.42684936523438\r",
      "INFO - Step 14215, rl-loss: 5.870944499969482\r",
      "INFO - Step 14216, rl-loss: 2.2932422161102295\r",
      "INFO - Step 14217, rl-loss: 435.9425964355469\r",
      "INFO - Step 14218, rl-loss: 108.9822998046875\r",
      "INFO - Step 14219, rl-loss: 56.73192596435547\r",
      "INFO - Step 14220, rl-loss: 25.47507667541504"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14221, rl-loss: 95.21139526367188\r",
      "INFO - Step 14222, rl-loss: 170.08544921875\r",
      "INFO - Step 14223, rl-loss: 248.4434814453125\r",
      "INFO - Step 14224, rl-loss: 239.40853881835938\r",
      "INFO - Step 14225, rl-loss: 118.00756072998047\r",
      "INFO - Step 14226, rl-loss: 0.7532991170883179\r",
      "INFO - Step 14227, rl-loss: 65.51666259765625\r",
      "INFO - Step 14228, rl-loss: 157.27444458007812\r",
      "INFO - Step 14229, rl-loss: 478.4365234375\r",
      "INFO - Step 14230, rl-loss: 66.59644317626953\r",
      "INFO - Step 14231, rl-loss: 379.9252624511719\r",
      "INFO - Step 14232, rl-loss: 143.62686157226562\r",
      "INFO - Step 14233, rl-loss: 145.5230712890625\r",
      "INFO - Step 14234, rl-loss: 72.20073699951172\r",
      "INFO - Step 14235, rl-loss: 6.750300884246826\r",
      "INFO - Step 14236, rl-loss: 478.9808654785156\r",
      "INFO - Step 14237, rl-loss: 188.60279846191406\r",
      "INFO - Step 14238, rl-loss: 310.20697021484375\r",
      "INFO - Step 14239, rl-loss: 256.5445251464844\r",
      "INFO - Step 14240, rl-loss: 334.5542907714844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14241, rl-loss: 607.5247192382812\r",
      "INFO - Step 14242, rl-loss: 94.15310668945312\r",
      "INFO - Step 14243, rl-loss: 187.12380981445312\r",
      "INFO - Step 14244, rl-loss: 170.07687377929688\r",
      "INFO - Step 14245, rl-loss: 387.5806884765625\r",
      "INFO - Step 14246, rl-loss: 605.910888671875\r",
      "INFO - Step 14247, rl-loss: 136.420166015625\r",
      "INFO - Step 14248, rl-loss: 201.1204071044922\r",
      "INFO - Step 14249, rl-loss: 0.9764232635498047\r",
      "INFO - Step 14250, rl-loss: 205.2439422607422\r",
      "INFO - Step 14251, rl-loss: 417.3766174316406\r",
      "INFO - Step 14252, rl-loss: 340.32989501953125\r",
      "INFO - Step 14253, rl-loss: 505.42578125\r",
      "INFO - Step 14254, rl-loss: 778.3955688476562\r",
      "INFO - Step 14255, rl-loss: 196.96888732910156\r",
      "INFO - Step 14256, rl-loss: 109.80879211425781\r",
      "INFO - Step 14257, rl-loss: 4.065048694610596\r",
      "INFO - Step 14258, rl-loss: 705.6250610351562\r",
      "INFO - Step 14259, rl-loss: 137.6818084716797\r",
      "INFO - Step 14260, rl-loss: 124.8563003540039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14261, rl-loss: 3.286867618560791\r",
      "INFO - Step 14262, rl-loss: 6.209583282470703\r",
      "INFO - Step 14263, rl-loss: 564.0448608398438\r",
      "INFO - Step 14264, rl-loss: 79.0831298828125\r",
      "INFO - Step 14265, rl-loss: 279.426513671875\r",
      "INFO - Step 14266, rl-loss: 103.0016098022461\r",
      "INFO - Step 14267, rl-loss: 150.9137420654297\r",
      "INFO - Step 14268, rl-loss: 71.18441772460938\r",
      "INFO - Step 14269, rl-loss: 2.4969305992126465\r",
      "INFO - Step 14270, rl-loss: 203.30482482910156\r",
      "INFO - Step 14271, rl-loss: 260.9867248535156\r",
      "INFO - Step 14272, rl-loss: 1.419006586074829\r",
      "INFO - Step 14273, rl-loss: 2.098900556564331\r",
      "INFO - Step 14274, rl-loss: 242.28793334960938\r",
      "INFO - Step 14275, rl-loss: 3.743523120880127\r",
      "INFO - Step 14276, rl-loss: 76.90079498291016\r",
      "INFO - Step 14277, rl-loss: 20.690515518188477\r",
      "INFO - Step 14278, rl-loss: 29.275630950927734\r",
      "INFO - Step 14279, rl-loss: 49.00975036621094\r",
      "INFO - Step 14280, rl-loss: 182.88021850585938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14281, rl-loss: 1.8295549154281616\r",
      "INFO - Step 14282, rl-loss: 827.6898193359375\r",
      "INFO - Step 14283, rl-loss: 1.6627429723739624\r",
      "INFO - Step 14284, rl-loss: 365.7947082519531\r",
      "INFO - Step 14285, rl-loss: 164.84620666503906\r",
      "INFO - Step 14286, rl-loss: 202.88467407226562\r",
      "INFO - Step 14287, rl-loss: 313.73675537109375\r",
      "INFO - Step 14288, rl-loss: 94.24091339111328\r",
      "INFO - Step 14289, rl-loss: 343.8647155761719\r",
      "INFO - Step 14290, rl-loss: 117.26984405517578\r",
      "INFO - Step 14291, rl-loss: 118.0749282836914\r",
      "INFO - Step 14292, rl-loss: 3.417587995529175\r",
      "INFO - Step 14293, rl-loss: 187.96499633789062\r",
      "INFO - Step 14294, rl-loss: 88.8200454711914\r",
      "INFO - Step 14295, rl-loss: 220.23809814453125\r",
      "INFO - Step 14296, rl-loss: 77.84349060058594\r",
      "INFO - Step 14297, rl-loss: 67.72797393798828\r",
      "INFO - Step 14298, rl-loss: 3.966855049133301\r",
      "INFO - Step 14299, rl-loss: 72.25662231445312\r",
      "INFO - Step 14300, rl-loss: 302.26568603515625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14301, rl-loss: 2.408325672149658\r",
      "INFO - Step 14302, rl-loss: 259.8628845214844\r",
      "INFO - Step 14303, rl-loss: 263.3047180175781\r",
      "INFO - Step 14304, rl-loss: 51.423648834228516\r",
      "INFO - Step 14305, rl-loss: 168.65443420410156\r",
      "INFO - Step 14306, rl-loss: 299.9778137207031\r",
      "INFO - Step 14307, rl-loss: 64.84236145019531\r",
      "INFO - Step 14308, rl-loss: 93.81890869140625\r",
      "INFO - Step 14309, rl-loss: 29.47107696533203\r",
      "INFO - Step 14310, rl-loss: 7.488095283508301\r",
      "INFO - Step 14311, rl-loss: 490.5889892578125\r",
      "INFO - Step 14312, rl-loss: 35.88985824584961\r",
      "INFO - Step 14313, rl-loss: 263.42022705078125\r",
      "INFO - Step 14314, rl-loss: 506.4056701660156\r",
      "INFO - Step 14315, rl-loss: 67.68568420410156\r",
      "INFO - Step 14316, rl-loss: 4.5116047859191895\r",
      "INFO - Step 14317, rl-loss: 490.6744384765625\r",
      "INFO - Step 14318, rl-loss: 59.87961196899414\r",
      "INFO - Step 14319, rl-loss: 1.1442325115203857\r",
      "INFO - Step 14320, rl-loss: 7.302694320678711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14321, rl-loss: 1.3553731441497803\r",
      "INFO - Step 14322, rl-loss: 324.83502197265625\r",
      "INFO - Step 14323, rl-loss: 87.28829956054688\r",
      "INFO - Step 14324, rl-loss: 121.6488265991211\r",
      "INFO - Step 14325, rl-loss: 4.588869571685791\r",
      "INFO - Step 14326, rl-loss: 2.317951202392578\r",
      "INFO - Step 14327, rl-loss: 172.62432861328125\r",
      "INFO - Step 14328, rl-loss: 56.6861572265625\r",
      "INFO - Step 14329, rl-loss: 152.2449188232422\r",
      "INFO - Step 14330, rl-loss: 6.586549282073975\r",
      "INFO - Step 14331, rl-loss: 339.47528076171875\r",
      "INFO - Step 14332, rl-loss: 8.442401885986328\r",
      "INFO - Step 14333, rl-loss: 42.000038146972656\r",
      "INFO - Step 14334, rl-loss: 55.16908264160156\r",
      "INFO - Step 14335, rl-loss: 266.8132019042969\r",
      "INFO - Step 14336, rl-loss: 241.395751953125\r",
      "INFO - Step 14337, rl-loss: 144.54116821289062\r",
      "INFO - Step 14338, rl-loss: 199.19789123535156\r",
      "INFO - Step 14339, rl-loss: 6.893025875091553\r",
      "INFO - Step 14340, rl-loss: 86.49788665771484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14341, rl-loss: 160.0917510986328\r",
      "INFO - Step 14342, rl-loss: 220.7026824951172\r",
      "INFO - Step 14343, rl-loss: 11.494325637817383\r",
      "INFO - Step 14344, rl-loss: 157.45816040039062\r",
      "INFO - Step 14345, rl-loss: 334.00457763671875\r",
      "INFO - Step 14346, rl-loss: 282.3653869628906\r",
      "INFO - Step 14347, rl-loss: 179.79974365234375\r",
      "INFO - Step 14348, rl-loss: 87.14736938476562\r",
      "INFO - Step 14349, rl-loss: 222.5633544921875\r",
      "INFO - Step 14350, rl-loss: 5.555976867675781\r",
      "INFO - Step 14351, rl-loss: 0.5593270063400269\r",
      "INFO - Step 14352, rl-loss: 92.41800689697266\r",
      "INFO - Step 14353, rl-loss: 379.606689453125\r",
      "INFO - Step 14354, rl-loss: 4.371034622192383\r",
      "INFO - Step 14355, rl-loss: 309.2112731933594\r",
      "INFO - Step 14356, rl-loss: 50.22187042236328\r",
      "INFO - Step 14357, rl-loss: 180.66041564941406\r",
      "INFO - Step 14358, rl-loss: 79.64207458496094\r",
      "INFO - Step 14359, rl-loss: 382.3360595703125\r",
      "INFO - Step 14360, rl-loss: 104.41214752197266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14361, rl-loss: 75.68069458007812\r",
      "INFO - Step 14362, rl-loss: 5.931941032409668\r",
      "INFO - Step 14363, rl-loss: 7.745608329772949\r",
      "INFO - Step 14364, rl-loss: 108.59619140625\r",
      "INFO - Step 14365, rl-loss: 133.31959533691406\r",
      "INFO - Step 14366, rl-loss: 184.9544677734375\r",
      "INFO - Step 14367, rl-loss: 139.8561553955078\r",
      "INFO - Step 14368, rl-loss: 37.05213928222656\r",
      "INFO - Step 14369, rl-loss: 597.739501953125\r",
      "INFO - Step 14370, rl-loss: 57.11689376831055\r",
      "INFO - Step 14371, rl-loss: 1.8699194192886353\r",
      "INFO - Step 14372, rl-loss: 144.18148803710938\r",
      "INFO - Step 14373, rl-loss: 155.64260864257812\r",
      "INFO - Step 14374, rl-loss: 97.24507141113281\r",
      "INFO - Step 14375, rl-loss: 430.62322998046875\r",
      "INFO - Step 14376, rl-loss: 6.044699192047119\r",
      "INFO - Step 14377, rl-loss: 3.5426011085510254\r",
      "INFO - Step 14378, rl-loss: 241.2322998046875\r",
      "INFO - Step 14379, rl-loss: 317.11602783203125\r",
      "INFO - Step 14380, rl-loss: 335.5835876464844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14381, rl-loss: 301.9945373535156\r",
      "INFO - Step 14382, rl-loss: 231.3397216796875\r",
      "INFO - Step 14383, rl-loss: 97.28684997558594\r",
      "INFO - Step 14384, rl-loss: 305.8284912109375\r",
      "INFO - Step 14385, rl-loss: 45.10969161987305\r",
      "INFO - Step 14386, rl-loss: 5.3353376388549805\r",
      "INFO - Step 14387, rl-loss: 368.5836486816406\r",
      "INFO - Step 14388, rl-loss: 5.373447418212891\r",
      "INFO - Step 14389, rl-loss: 155.73069763183594\r",
      "INFO - Step 14390, rl-loss: 125.74368286132812\r",
      "INFO - Step 14391, rl-loss: 111.26219940185547\r",
      "INFO - Step 14392, rl-loss: 52.006072998046875\r",
      "INFO - Step 14393, rl-loss: 275.23358154296875\r",
      "INFO - Step 14394, rl-loss: 138.82923889160156\r",
      "INFO - Step 14395, rl-loss: 9.364439964294434\r",
      "INFO - Step 14396, rl-loss: 108.38835144042969\r",
      "INFO - Step 14397, rl-loss: 11.71631145477295\r",
      "INFO - Step 14398, rl-loss: 72.93476104736328\r",
      "INFO - Step 14399, rl-loss: 200.09103393554688\r",
      "INFO - Step 14400, rl-loss: 538.5775146484375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14401, rl-loss: 93.10411834716797\r",
      "INFO - Step 14402, rl-loss: 35.82065963745117\r",
      "INFO - Step 14403, rl-loss: 10.897926330566406\r",
      "INFO - Step 14404, rl-loss: 309.5401306152344\r",
      "INFO - Step 14405, rl-loss: 7.143593788146973\r",
      "INFO - Step 14406, rl-loss: 76.11675262451172\r",
      "INFO - Step 14407, rl-loss: 161.89901733398438\r",
      "INFO - Step 14408, rl-loss: 151.8137664794922\r",
      "INFO - Step 14409, rl-loss: 189.61744689941406\r",
      "INFO - Step 14410, rl-loss: 685.7604370117188\r",
      "INFO - Step 14411, rl-loss: 28.887475967407227\r",
      "INFO - Step 14412, rl-loss: 207.69679260253906\r",
      "INFO - Step 14413, rl-loss: 384.0700988769531\r",
      "INFO - Step 14414, rl-loss: 99.65897369384766\r",
      "INFO - Step 14415, rl-loss: 195.32046508789062\r",
      "INFO - Step 14416, rl-loss: 122.24755096435547\r",
      "INFO - Step 14417, rl-loss: 100.1212158203125\r",
      "INFO - Step 14418, rl-loss: 96.0452880859375\r",
      "INFO - Step 14419, rl-loss: 5.067941188812256\r",
      "INFO - Step 14420, rl-loss: 57.006980895996094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14421, rl-loss: 326.84234619140625\r",
      "INFO - Step 14422, rl-loss: 159.24298095703125\r",
      "INFO - Step 14423, rl-loss: 182.80889892578125\r",
      "INFO - Step 14424, rl-loss: 3.177830696105957\r",
      "INFO - Step 14425, rl-loss: 791.340576171875\r",
      "INFO - Step 14426, rl-loss: 91.21731567382812\r",
      "INFO - Step 14427, rl-loss: 485.2732238769531\r",
      "INFO - Step 14428, rl-loss: 218.64974975585938\r",
      "INFO - Step 14429, rl-loss: 55.319332122802734\r",
      "INFO - Step 14430, rl-loss: 13.726819038391113\r",
      "INFO - Step 14431, rl-loss: 202.07675170898438\r",
      "INFO - Step 14432, rl-loss: 50.35462951660156\r",
      "INFO - Step 14433, rl-loss: 46.17325210571289\r",
      "INFO - Step 14434, rl-loss: 54.27179718017578\r",
      "INFO - Step 14435, rl-loss: 7.041886329650879\r",
      "INFO - Step 14436, rl-loss: 54.25307846069336\r",
      "INFO - Step 14437, rl-loss: 51.48379135131836\r",
      "INFO - Step 14438, rl-loss: 82.28211975097656\r",
      "INFO - Step 14439, rl-loss: 66.12992858886719\r",
      "INFO - Step 14440, rl-loss: 244.24171447753906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14441, rl-loss: 69.01404571533203\r",
      "INFO - Step 14442, rl-loss: 1.6487431526184082\r",
      "INFO - Step 14443, rl-loss: 774.0220947265625\r",
      "INFO - Step 14444, rl-loss: 272.429443359375\r",
      "INFO - Step 14445, rl-loss: 4.3108344078063965\r",
      "INFO - Step 14446, rl-loss: 145.27743530273438\r",
      "INFO - Step 14447, rl-loss: 240.76101684570312\r",
      "INFO - Step 14448, rl-loss: 1.119160771369934\r",
      "INFO - Step 14449, rl-loss: 114.99374389648438\r",
      "INFO - Step 14450, rl-loss: 44.64063262939453\r",
      "INFO - Step 14451, rl-loss: 4.703792572021484\r",
      "INFO - Step 14452, rl-loss: 135.28121948242188\r",
      "INFO - Step 14453, rl-loss: 304.13983154296875\r",
      "INFO - Step 14454, rl-loss: 4.684272289276123\r",
      "INFO - Step 14455, rl-loss: 236.06451416015625\r",
      "INFO - Step 14456, rl-loss: 4.644804954528809\r",
      "INFO - Step 14457, rl-loss: 167.43556213378906\r",
      "INFO - Step 14458, rl-loss: 1.7103018760681152\r",
      "INFO - Step 14459, rl-loss: 146.7944793701172\r",
      "INFO - Step 14460, rl-loss: 120.89697265625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14461, rl-loss: 253.47415161132812\r",
      "INFO - Step 14462, rl-loss: 81.80757141113281\r",
      "INFO - Step 14463, rl-loss: 102.29757690429688\r",
      "INFO - Step 14464, rl-loss: 1.2648121118545532\r",
      "INFO - Step 14465, rl-loss: 220.93948364257812\r",
      "INFO - Step 14466, rl-loss: 494.79345703125\r",
      "INFO - Step 14467, rl-loss: 146.44482421875\r",
      "INFO - Step 14468, rl-loss: 34.795249938964844\r",
      "INFO - Step 14469, rl-loss: 112.630126953125\r",
      "INFO - Step 14470, rl-loss: 358.7193603515625\r",
      "INFO - Step 14471, rl-loss: 84.89601135253906\r",
      "INFO - Step 14472, rl-loss: 51.58377456665039\r",
      "INFO - Step 14473, rl-loss: 1.1452556848526\r",
      "INFO - Step 14474, rl-loss: 2.4734339714050293\r",
      "INFO - Step 14475, rl-loss: 93.41212463378906\r",
      "INFO - Step 14476, rl-loss: 3.0847859382629395\r",
      "INFO - Step 14477, rl-loss: 104.68428039550781\r",
      "INFO - Step 14478, rl-loss: 218.8903045654297\r",
      "INFO - Step 14479, rl-loss: 393.3341979980469\r",
      "INFO - Step 14480, rl-loss: 338.875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14481, rl-loss: 18.69577407836914\r",
      "INFO - Step 14482, rl-loss: 93.1273193359375\r",
      "INFO - Step 14483, rl-loss: 9.254706382751465\r",
      "INFO - Step 14484, rl-loss: 139.68405151367188\r",
      "INFO - Step 14485, rl-loss: 262.30963134765625\r",
      "INFO - Step 14486, rl-loss: 73.0893783569336\r",
      "INFO - Step 14487, rl-loss: 220.27008056640625\r",
      "INFO - Step 14488, rl-loss: 289.7237243652344\r",
      "INFO - Step 14489, rl-loss: 408.88934326171875\r",
      "INFO - Step 14490, rl-loss: 1.0376065969467163\r",
      "INFO - Step 14491, rl-loss: 2.3530869483947754\r",
      "INFO - Step 14492, rl-loss: 132.74154663085938\r",
      "INFO - Step 14493, rl-loss: 83.23274230957031\r",
      "INFO - Step 14494, rl-loss: 305.9679870605469\r",
      "INFO - Step 14495, rl-loss: 43.6600341796875\r",
      "INFO - Step 14496, rl-loss: 2.4727516174316406\r",
      "INFO - Step 14497, rl-loss: 2.070089101791382\r",
      "INFO - Step 14498, rl-loss: 106.5651626586914\r",
      "INFO - Step 14499, rl-loss: 323.7990417480469\r",
      "INFO - Step 14500, rl-loss: 7.023734092712402"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14501, rl-loss: 385.28839111328125\r",
      "INFO - Step 14502, rl-loss: 89.1814956665039\r",
      "INFO - Step 14503, rl-loss: 339.3384094238281\r",
      "INFO - Step 14504, rl-loss: 4.015710353851318\r",
      "INFO - Step 14505, rl-loss: 8.41935920715332\r",
      "INFO - Step 14506, rl-loss: 295.10107421875\r",
      "INFO - Step 14507, rl-loss: 28.280960083007812\r",
      "INFO - Step 14508, rl-loss: 118.67499542236328\r",
      "INFO - Step 14509, rl-loss: 267.4419860839844\r",
      "INFO - Step 14510, rl-loss: 51.27468490600586\r",
      "INFO - Step 14511, rl-loss: 102.88243865966797\r",
      "INFO - Step 14512, rl-loss: 232.0355987548828\r",
      "INFO - Step 14513, rl-loss: 179.94589233398438\r",
      "INFO - Step 14514, rl-loss: 78.51197052001953\r",
      "INFO - Step 14515, rl-loss: 374.64532470703125\r",
      "INFO - Step 14516, rl-loss: 378.35906982421875\r",
      "INFO - Step 14517, rl-loss: 34.92158126831055\r",
      "INFO - Step 14518, rl-loss: 111.5278091430664\r",
      "INFO - Step 14519, rl-loss: 130.61094665527344\r",
      "INFO - Step 14520, rl-loss: 148.67022705078125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14521, rl-loss: 1.1517539024353027\r",
      "INFO - Step 14522, rl-loss: 215.2014923095703\r",
      "INFO - Step 14523, rl-loss: 90.16193389892578\r",
      "INFO - Step 14524, rl-loss: 211.9422607421875\r",
      "INFO - Step 14525, rl-loss: 9.35191535949707\r",
      "INFO - Step 14526, rl-loss: 538.782958984375\r",
      "INFO - Step 14527, rl-loss: 129.71107482910156\r",
      "INFO - Step 14528, rl-loss: 11.3086519241333\r",
      "INFO - Step 14529, rl-loss: 121.69479370117188\r",
      "INFO - Step 14530, rl-loss: 2.590939521789551\r",
      "INFO - Step 14531, rl-loss: 22.494829177856445\r",
      "INFO - Step 14532, rl-loss: 27.583250045776367\r",
      "INFO - Step 14533, rl-loss: 75.44405364990234\r",
      "INFO - Step 14534, rl-loss: 151.23667907714844\r",
      "INFO - Step 14535, rl-loss: 191.28309631347656\r",
      "INFO - Step 14536, rl-loss: 106.09125518798828\r",
      "INFO - Step 14537, rl-loss: 67.97012329101562\r",
      "INFO - Step 14538, rl-loss: 407.94830322265625\r",
      "INFO - Step 14539, rl-loss: 352.7398986816406\r",
      "INFO - Step 14540, rl-loss: 6.119352340698242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14541, rl-loss: 4.088977336883545\r",
      "INFO - Step 14542, rl-loss: 336.62213134765625\r",
      "INFO - Step 14543, rl-loss: 5.47166633605957\r",
      "INFO - Step 14544, rl-loss: 164.74679565429688\r",
      "INFO - Step 14545, rl-loss: 31.565780639648438\r",
      "INFO - Step 14546, rl-loss: 390.3487548828125\r",
      "INFO - Step 14547, rl-loss: 149.7843017578125\r",
      "INFO - Step 14548, rl-loss: 183.47836303710938\r",
      "INFO - Step 14549, rl-loss: 146.42147827148438\r",
      "INFO - Step 14550, rl-loss: 142.70689392089844\r",
      "INFO - Step 14551, rl-loss: 1.881042242050171\r",
      "INFO - Step 14552, rl-loss: 96.48578643798828\r",
      "INFO - Step 14553, rl-loss: 400.7846374511719\r",
      "INFO - Step 14554, rl-loss: 5.883328914642334\r",
      "INFO - Step 14555, rl-loss: 439.80279541015625\r",
      "INFO - Step 14556, rl-loss: 84.22750854492188\r",
      "INFO - Step 14557, rl-loss: 464.43060302734375\r",
      "INFO - Step 14558, rl-loss: 197.8990478515625\r",
      "INFO - Step 14559, rl-loss: 254.892333984375\r",
      "INFO - Step 14560, rl-loss: 4.460409164428711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14561, rl-loss: 315.01702880859375\r",
      "INFO - Step 14562, rl-loss: 431.58905029296875\r",
      "INFO - Step 14563, rl-loss: 222.9942169189453\r",
      "INFO - Step 14564, rl-loss: 204.99830627441406\r",
      "INFO - Step 14565, rl-loss: 196.08441162109375\r",
      "INFO - Step 14566, rl-loss: 274.7353210449219\r",
      "INFO - Step 14567, rl-loss: 163.21865844726562\r",
      "INFO - Step 14568, rl-loss: 275.4422607421875\r",
      "INFO - Step 14569, rl-loss: 4.654666900634766\r",
      "INFO - Step 14570, rl-loss: 135.05271911621094\r",
      "INFO - Step 14571, rl-loss: 289.9365539550781\r",
      "INFO - Step 14572, rl-loss: 81.37703704833984\r",
      "INFO - Step 14573, rl-loss: 10.434466361999512\r",
      "INFO - Step 14574, rl-loss: 3.4163291454315186\r",
      "INFO - Step 14575, rl-loss: 69.14523315429688\r",
      "INFO - Step 14576, rl-loss: 5.730821132659912\r",
      "INFO - Step 14577, rl-loss: 440.7467041015625\r",
      "INFO - Step 14578, rl-loss: 117.01945495605469\r",
      "INFO - Step 14579, rl-loss: 6.539827346801758\r",
      "INFO - Step 14580, rl-loss: 114.82112121582031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14581, rl-loss: 228.3912353515625\r",
      "INFO - Step 14582, rl-loss: 303.085205078125\r",
      "INFO - Step 14583, rl-loss: 61.135643005371094\r",
      "INFO - Step 14584, rl-loss: 661.11572265625\r",
      "INFO - Step 14585, rl-loss: 285.3081359863281\r",
      "INFO - Step 14586, rl-loss: 6.952570915222168\r",
      "INFO - Step 14587, rl-loss: 10.559842109680176\r",
      "INFO - Step 14588, rl-loss: 284.6102294921875\r",
      "INFO - Step 14589, rl-loss: 3.7438392639160156\r",
      "INFO - Step 14590, rl-loss: 214.69613647460938\r",
      "INFO - Step 14591, rl-loss: 44.9959831237793\r",
      "INFO - Step 14592, rl-loss: 24.95647430419922\r",
      "INFO - Step 14593, rl-loss: 382.01568603515625\r",
      "INFO - Step 14594, rl-loss: 169.49530029296875\r",
      "INFO - Step 14595, rl-loss: 117.4990234375\r",
      "INFO - Step 14596, rl-loss: 21.50470733642578\r",
      "INFO - Step 14597, rl-loss: 208.75570678710938\r",
      "INFO - Step 14598, rl-loss: 297.6799011230469\r",
      "INFO - Step 14599, rl-loss: 79.09291076660156\r",
      "INFO - Step 14600, rl-loss: 119.63838958740234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14601, rl-loss: 73.49434661865234\r",
      "INFO - Step 14602, rl-loss: 157.13668823242188\r",
      "INFO - Step 14603, rl-loss: 327.15472412109375\r",
      "INFO - Step 14604, rl-loss: 5.983473300933838\r",
      "INFO - Step 14605, rl-loss: 250.97576904296875\r",
      "INFO - Step 14606, rl-loss: 1.4398657083511353\r",
      "INFO - Step 14607, rl-loss: 338.1807861328125\r",
      "INFO - Step 14608, rl-loss: 71.69348907470703\r",
      "INFO - Step 14609, rl-loss: 181.97535705566406\r",
      "INFO - Step 14610, rl-loss: 487.9230651855469\r",
      "INFO - Step 14611, rl-loss: 53.7461051940918\r",
      "INFO - Step 14612, rl-loss: 331.5737609863281\r",
      "INFO - Step 14613, rl-loss: 235.51747131347656\r",
      "INFO - Step 14614, rl-loss: 510.04437255859375\r",
      "INFO - Step 14615, rl-loss: 62.356781005859375\r",
      "INFO - Step 14616, rl-loss: 79.2835693359375\r",
      "INFO - Step 14617, rl-loss: 597.7457885742188\r",
      "INFO - Step 14618, rl-loss: 351.366943359375\r",
      "INFO - Step 14619, rl-loss: 364.8587341308594\r",
      "INFO - Step 14620, rl-loss: 227.7509002685547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14621, rl-loss: 239.70518493652344\r",
      "INFO - Step 14622, rl-loss: 411.5359802246094\r",
      "INFO - Step 14623, rl-loss: 15.360573768615723\r",
      "INFO - Step 14624, rl-loss: 470.2843933105469\r",
      "INFO - Step 14625, rl-loss: 3.264089584350586\r",
      "INFO - Step 14626, rl-loss: 261.72235107421875\r",
      "INFO - Step 14627, rl-loss: 133.55860900878906\r",
      "INFO - Step 14628, rl-loss: 184.70648193359375\r",
      "INFO - Step 14629, rl-loss: 216.62863159179688\r",
      "INFO - Step 14630, rl-loss: 244.5979766845703\r",
      "INFO - Step 14631, rl-loss: 7.594271183013916\r",
      "INFO - Step 14632, rl-loss: 332.2582702636719\r",
      "INFO - Step 14633, rl-loss: 260.09503173828125\r",
      "INFO - Step 14634, rl-loss: 158.9608917236328\r",
      "INFO - Step 14635, rl-loss: 607.4254760742188\r",
      "INFO - Step 14636, rl-loss: 47.27433395385742\r",
      "INFO - Step 14637, rl-loss: 267.2742919921875\r",
      "INFO - Step 14638, rl-loss: 97.58149719238281\r",
      "INFO - Step 14639, rl-loss: 102.63468933105469\r",
      "INFO - Step 14640, rl-loss: 196.2505645751953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14641, rl-loss: 108.32264709472656\r",
      "INFO - Step 14642, rl-loss: 154.10716247558594\r",
      "INFO - Step 14643, rl-loss: 150.39501953125\r",
      "INFO - Step 14644, rl-loss: 4.512866973876953\r",
      "INFO - Step 14645, rl-loss: 159.2854461669922\r",
      "INFO - Step 14646, rl-loss: 475.31072998046875\r",
      "INFO - Step 14647, rl-loss: 182.60340881347656\r",
      "INFO - Step 14648, rl-loss: 33.78363800048828\r",
      "INFO - Step 14649, rl-loss: 86.10944366455078\r",
      "INFO - Step 14650, rl-loss: 25.739391326904297\r",
      "INFO - Step 14651, rl-loss: 335.02978515625\r",
      "INFO - Step 14652, rl-loss: 33.976661682128906\r",
      "INFO - Step 14653, rl-loss: 144.57713317871094\r",
      "INFO - Step 14654, rl-loss: 701.6845703125\r",
      "INFO - Step 14655, rl-loss: 38.16257095336914\r",
      "INFO - Step 14656, rl-loss: 87.70655822753906\r",
      "INFO - Step 14657, rl-loss: 109.35010528564453\r",
      "INFO - Step 14658, rl-loss: 208.7906494140625\r",
      "INFO - Step 14659, rl-loss: 98.97657775878906\r",
      "INFO - Step 14660, rl-loss: 210.00393676757812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14661, rl-loss: 32.2517204284668\r",
      "INFO - Step 14662, rl-loss: 763.269775390625\r",
      "INFO - Step 14663, rl-loss: 123.92132568359375\r",
      "INFO - Step 14664, rl-loss: 47.87744140625\r",
      "INFO - Step 14665, rl-loss: 39.802276611328125\r",
      "INFO - Step 14666, rl-loss: 869.9949951171875\r",
      "INFO - Step 14667, rl-loss: 308.17822265625\r",
      "INFO - Step 14668, rl-loss: 3.996006727218628\r",
      "INFO - Step 14669, rl-loss: 185.60531616210938\r",
      "INFO - Step 14670, rl-loss: 338.3388671875\r",
      "INFO - Step 14671, rl-loss: 78.81523895263672\r",
      "INFO - Step 14672, rl-loss: 75.27337646484375\r",
      "INFO - Step 14673, rl-loss: 129.88943481445312\r",
      "INFO - Step 14674, rl-loss: 225.80996704101562\r",
      "INFO - Step 14675, rl-loss: 192.70726013183594\r",
      "INFO - Step 14676, rl-loss: 656.1542358398438\r",
      "INFO - Step 14677, rl-loss: 45.28840255737305\r",
      "INFO - Step 14678, rl-loss: 140.95289611816406\r",
      "INFO - Step 14679, rl-loss: 524.21875\r",
      "INFO - Step 14680, rl-loss: 184.71029663085938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14681, rl-loss: 52.0092887878418\r",
      "INFO - Step 14682, rl-loss: 3.5051658153533936\r",
      "INFO - Step 14683, rl-loss: 32.01931381225586\r",
      "INFO - Step 14684, rl-loss: 451.76318359375\r",
      "INFO - Step 14685, rl-loss: 588.8582763671875\r",
      "INFO - Step 14686, rl-loss: 838.520751953125\r",
      "INFO - Step 14687, rl-loss: 625.7361450195312\r",
      "INFO - Step 14688, rl-loss: 3.1831929683685303\r",
      "INFO - Step 14689, rl-loss: 348.177978515625\r",
      "INFO - Step 14690, rl-loss: 4.201131820678711\r",
      "INFO - Step 14691, rl-loss: 8.908824920654297\r",
      "INFO - Step 14692, rl-loss: 315.5588684082031\r",
      "INFO - Step 14693, rl-loss: 221.0850067138672\r",
      "INFO - Step 14694, rl-loss: 96.91417694091797\r",
      "INFO - Step 14695, rl-loss: 282.4859619140625\r",
      "INFO - Step 14696, rl-loss: 47.98878860473633\r",
      "INFO - Step 14697, rl-loss: 339.7043762207031\r",
      "INFO - Step 14698, rl-loss: 66.53193664550781\r",
      "INFO - Step 14699, rl-loss: 267.8363037109375\r",
      "INFO - Step 14700, rl-loss: 146.35269165039062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14701, rl-loss: 221.28561401367188\r",
      "INFO - Step 14702, rl-loss: 371.21905517578125\r",
      "INFO - Step 14703, rl-loss: 1.1523927450180054\r",
      "INFO - Step 14704, rl-loss: 608.83447265625\r",
      "INFO - Step 14705, rl-loss: 29.634456634521484\r",
      "INFO - Step 14706, rl-loss: 97.83767700195312\r",
      "INFO - Step 14707, rl-loss: 38.356231689453125\r",
      "INFO - Step 14708, rl-loss: 15.798680305480957\r",
      "INFO - Step 14709, rl-loss: 151.4283447265625\r",
      "INFO - Step 14710, rl-loss: 120.02523803710938\r",
      "INFO - Step 14711, rl-loss: 276.77740478515625\r",
      "INFO - Step 14712, rl-loss: 351.1983337402344\r",
      "INFO - Step 14713, rl-loss: 247.6787872314453\r",
      "INFO - Step 14714, rl-loss: 3.8095014095306396\r",
      "INFO - Step 14715, rl-loss: 4.454667568206787\r",
      "INFO - Step 14716, rl-loss: 105.03810119628906\r",
      "INFO - Step 14717, rl-loss: 189.24864196777344\r",
      "INFO - Step 14718, rl-loss: 22.6458797454834\r",
      "INFO - Step 14719, rl-loss: 72.16796875\r",
      "INFO - Step 14720, rl-loss: 3.3855345249176025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14721, rl-loss: 176.07305908203125\r",
      "INFO - Step 14722, rl-loss: 335.031005859375\r",
      "INFO - Step 14723, rl-loss: 157.911376953125\r",
      "INFO - Step 14724, rl-loss: 176.7739715576172\r",
      "INFO - Step 14725, rl-loss: 630.8132934570312\r",
      "INFO - Step 14726, rl-loss: 36.145389556884766\r",
      "INFO - Step 14727, rl-loss: 41.91335678100586\r",
      "INFO - Step 14728, rl-loss: 232.70155334472656\r",
      "INFO - Step 14729, rl-loss: 25.07799530029297\r",
      "INFO - Step 14730, rl-loss: 147.62185668945312\r",
      "INFO - Step 14731, rl-loss: 1.0353466272354126\r",
      "INFO - Step 14732, rl-loss: 38.844512939453125\r",
      "INFO - Step 14733, rl-loss: 107.74037170410156\r",
      "INFO - Step 14734, rl-loss: 377.00244140625\r",
      "INFO - Step 14735, rl-loss: 186.68936157226562\r",
      "INFO - Step 14736, rl-loss: 149.42347717285156\r",
      "INFO - Step 14737, rl-loss: 103.7387466430664\r",
      "INFO - Step 14738, rl-loss: 289.7147216796875\r",
      "INFO - Step 14739, rl-loss: 5.572040557861328\r",
      "INFO - Step 14740, rl-loss: 37.96603012084961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14741, rl-loss: 108.58812713623047\r",
      "INFO - Step 14742, rl-loss: 178.8114471435547\r",
      "INFO - Step 14743, rl-loss: 72.58877563476562\r",
      "INFO - Step 14744, rl-loss: 408.4906921386719\r",
      "INFO - Step 14745, rl-loss: 31.939496994018555\r",
      "INFO - Step 14746, rl-loss: 3.364429235458374\r",
      "INFO - Step 14747, rl-loss: 237.63885498046875\r",
      "INFO - Step 14748, rl-loss: 109.2477798461914\r",
      "INFO - Step 14749, rl-loss: 184.86000061035156\r",
      "INFO - Step 14750, rl-loss: 258.2308349609375\r",
      "INFO - Step 14751, rl-loss: 3.0669100284576416\r",
      "INFO - Step 14752, rl-loss: 221.66966247558594\r",
      "INFO - Step 14753, rl-loss: 2.9605727195739746\r",
      "INFO - Step 14754, rl-loss: 214.2952423095703\r",
      "INFO - Step 14755, rl-loss: 157.66839599609375\r",
      "INFO - Step 14756, rl-loss: 2.9532270431518555\r",
      "INFO - Step 14757, rl-loss: 211.05752563476562\r",
      "INFO - Step 14758, rl-loss: 27.79215431213379\r",
      "INFO - Step 14759, rl-loss: 394.17852783203125\r",
      "INFO - Step 14760, rl-loss: 31.736064910888672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14761, rl-loss: 1.6400978565216064\r",
      "INFO - Step 14762, rl-loss: 34.9122428894043\r",
      "INFO - Step 14763, rl-loss: 525.7791748046875\r",
      "INFO - Step 14764, rl-loss: 50.71076965332031\r",
      "INFO - Step 14765, rl-loss: 207.90208435058594\r",
      "INFO - Step 14766, rl-loss: 93.3951416015625\r",
      "INFO - Step 14767, rl-loss: 158.31692504882812\r",
      "INFO - Step 14768, rl-loss: 28.825037002563477\r",
      "INFO - Step 14769, rl-loss: 91.7459487915039\r",
      "INFO - Step 14770, rl-loss: 186.18966674804688\r",
      "INFO - Step 14771, rl-loss: 285.8590393066406\r",
      "INFO - Step 14772, rl-loss: 132.1265869140625\r",
      "INFO - Step 14773, rl-loss: 602.2744140625\r",
      "INFO - Step 14774, rl-loss: 54.02051544189453\r",
      "INFO - Step 14775, rl-loss: 1.2048118114471436\r",
      "INFO - Step 14776, rl-loss: 7.246670722961426\r",
      "INFO - Step 14777, rl-loss: 3.180516242980957\r",
      "INFO - Step 14778, rl-loss: 5.408999443054199\r",
      "INFO - Step 14779, rl-loss: 96.70270538330078\r",
      "INFO - Step 14780, rl-loss: 376.90032958984375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14781, rl-loss: 351.6620788574219\r",
      "INFO - Step 14782, rl-loss: 508.4236145019531\r",
      "INFO - Step 14783, rl-loss: 61.57379913330078\r",
      "INFO - Step 14784, rl-loss: 51.25707244873047\r",
      "INFO - Step 14785, rl-loss: 419.8402404785156\r",
      "INFO - Step 14786, rl-loss: 350.3165283203125\r",
      "INFO - Step 14787, rl-loss: 198.5308380126953\r",
      "INFO - Step 14788, rl-loss: 80.08194732666016\r",
      "INFO - Step 14789, rl-loss: 231.49481201171875\r",
      "INFO - Step 14790, rl-loss: 264.12432861328125\r",
      "INFO - Step 14791, rl-loss: 331.01202392578125\r",
      "INFO - Step 14792, rl-loss: 250.9972686767578\r",
      "INFO - Step 14793, rl-loss: 56.987003326416016\r",
      "INFO - Step 14794, rl-loss: 90.70671844482422\r",
      "INFO - Step 14795, rl-loss: 6.024713516235352\r",
      "INFO - Step 14796, rl-loss: 228.3262176513672\r",
      "INFO - Step 14797, rl-loss: 720.8277587890625\r",
      "INFO - Step 14798, rl-loss: 407.3746032714844\r",
      "INFO - Step 14799, rl-loss: 93.5798110961914\r",
      "INFO - Step 14800, rl-loss: 169.94879150390625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14801, rl-loss: 16.559307098388672\r",
      "INFO - Step 14802, rl-loss: 89.78446197509766\r",
      "INFO - Step 14803, rl-loss: 441.9545593261719\r",
      "INFO - Step 14804, rl-loss: 185.55123901367188\r",
      "INFO - Step 14805, rl-loss: 67.8568115234375\r",
      "INFO - Step 14806, rl-loss: 97.7093734741211\r",
      "INFO - Step 14807, rl-loss: 3.2561304569244385\r",
      "INFO - Step 14808, rl-loss: 128.6652069091797\r",
      "INFO - Step 14809, rl-loss: 326.5968933105469\r",
      "INFO - Step 14810, rl-loss: 130.07032775878906\r",
      "INFO - Step 14811, rl-loss: 3.35884428024292\r",
      "INFO - Step 14812, rl-loss: 436.4870910644531\r",
      "INFO - Step 14813, rl-loss: 68.70703887939453\r",
      "INFO - Step 14814, rl-loss: 102.42610168457031\r",
      "INFO - Step 14815, rl-loss: 459.09136962890625\r",
      "INFO - Step 14816, rl-loss: 267.88427734375\r",
      "INFO - Step 14817, rl-loss: 1.1277732849121094\r",
      "INFO - Step 14818, rl-loss: 10.138214111328125\r",
      "INFO - Step 14819, rl-loss: 50.58158493041992\r",
      "INFO - Step 14820, rl-loss: 228.9049835205078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14821, rl-loss: 140.587158203125\r",
      "INFO - Step 14822, rl-loss: 225.2926025390625\r",
      "INFO - Step 14823, rl-loss: 19.26362419128418\r",
      "INFO - Step 14824, rl-loss: 69.61677551269531\r",
      "INFO - Step 14825, rl-loss: 4.469480037689209\r",
      "INFO - Step 14826, rl-loss: 109.84959411621094\r",
      "INFO - Step 14827, rl-loss: 386.7792053222656\r",
      "INFO - Step 14828, rl-loss: 65.57300567626953\r",
      "INFO - Step 14829, rl-loss: 195.432861328125\r",
      "INFO - Step 14830, rl-loss: 85.77335357666016\r",
      "INFO - Step 14831, rl-loss: 218.47059631347656\r",
      "INFO - Step 14832, rl-loss: 189.79605102539062\r",
      "INFO - Step 14833, rl-loss: 55.793479919433594\r",
      "INFO - Step 14834, rl-loss: 556.0121459960938\r",
      "INFO - Step 14835, rl-loss: 354.6169128417969\r",
      "INFO - Step 14836, rl-loss: 29.31698226928711\r",
      "INFO - Step 14837, rl-loss: 82.38028717041016\r",
      "INFO - Step 14838, rl-loss: 322.09893798828125\r",
      "INFO - Step 14839, rl-loss: 1.6304818391799927\r",
      "INFO - Step 14840, rl-loss: 1.3905689716339111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14841, rl-loss: 355.36309814453125\r",
      "INFO - Step 14842, rl-loss: 3.2246294021606445\r",
      "INFO - Step 14843, rl-loss: 31.556594848632812\r",
      "INFO - Step 14844, rl-loss: 237.52268981933594\r",
      "INFO - Step 14845, rl-loss: 215.30972290039062\r",
      "INFO - Step 14846, rl-loss: 4.7677764892578125\r",
      "INFO - Step 14847, rl-loss: 50.23390579223633\r",
      "INFO - Step 14848, rl-loss: 146.2700958251953\r",
      "INFO - Step 14849, rl-loss: 145.29306030273438\r",
      "INFO - Step 14850, rl-loss: 280.7775573730469\r",
      "INFO - Step 14851, rl-loss: 5.583059310913086\r",
      "INFO - Step 14852, rl-loss: 178.3769073486328\r",
      "INFO - Step 14853, rl-loss: 36.93505096435547\r",
      "INFO - Step 14854, rl-loss: 293.2113342285156\r",
      "INFO - Step 14855, rl-loss: 4.011354446411133\r",
      "INFO - Step 14856, rl-loss: 50.11761474609375\r",
      "INFO - Step 14857, rl-loss: 28.83936309814453\r",
      "INFO - Step 14858, rl-loss: 139.54034423828125\r",
      "INFO - Step 14859, rl-loss: 127.06294250488281\r",
      "INFO - Step 14860, rl-loss: 173.66336059570312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14861, rl-loss: 326.5522155761719\r",
      "INFO - Step 14862, rl-loss: 1.890555739402771\r",
      "INFO - Step 14863, rl-loss: 18.474166870117188\r",
      "INFO - Step 14864, rl-loss: 6.382880210876465\r",
      "INFO - Step 14865, rl-loss: 427.6266174316406\r",
      "INFO - Step 14866, rl-loss: 320.5572509765625\r",
      "INFO - Step 14867, rl-loss: 7.607117652893066\r",
      "INFO - Step 14868, rl-loss: 239.48001098632812\r",
      "INFO - Step 14869, rl-loss: 243.77830505371094\r",
      "INFO - Step 14870, rl-loss: 96.08734893798828\r",
      "INFO - Step 14871, rl-loss: 254.54922485351562\r",
      "INFO - Step 14872, rl-loss: 297.07391357421875\r",
      "INFO - Step 14873, rl-loss: 300.9368591308594\r",
      "INFO - Step 14874, rl-loss: 364.08172607421875\r",
      "INFO - Step 14875, rl-loss: 67.06604766845703\r",
      "INFO - Step 14876, rl-loss: 89.90962982177734\r",
      "INFO - Step 14877, rl-loss: 57.21420669555664\r",
      "INFO - Step 14878, rl-loss: 230.96176147460938\r",
      "INFO - Step 14879, rl-loss: 199.4731903076172\r",
      "INFO - Step 14880, rl-loss: 255.99867248535156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14881, rl-loss: 167.56947326660156\r",
      "INFO - Step 14882, rl-loss: 32.036590576171875\r",
      "INFO - Step 14883, rl-loss: 679.4697875976562\r",
      "INFO - Step 14884, rl-loss: 54.12799072265625\r",
      "INFO - Step 14885, rl-loss: 114.54190826416016\r",
      "INFO - Step 14886, rl-loss: 5.135625839233398\r",
      "INFO - Step 14887, rl-loss: 41.44080352783203\r",
      "INFO - Step 14888, rl-loss: 25.237110137939453\r",
      "INFO - Step 14889, rl-loss: 170.37802124023438\r",
      "INFO - Step 14890, rl-loss: 59.08078384399414\r",
      "INFO - Step 14891, rl-loss: 341.70123291015625\r",
      "INFO - Step 14892, rl-loss: 4.333566188812256\r",
      "INFO - Step 14893, rl-loss: 81.42587280273438\r",
      "INFO - Step 14894, rl-loss: 133.240478515625\r",
      "INFO - Step 14895, rl-loss: 30.849990844726562\r",
      "INFO - Step 14896, rl-loss: 52.04414749145508\r",
      "INFO - Step 14897, rl-loss: 63.497169494628906\r",
      "INFO - Step 14898, rl-loss: 1.534814715385437\r",
      "INFO - Step 14899, rl-loss: 118.68429565429688\r",
      "INFO - Step 14900, rl-loss: 232.1263885498047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14901, rl-loss: 2.585822105407715\r",
      "INFO - Step 14902, rl-loss: 560.9327392578125\r",
      "INFO - Step 14903, rl-loss: 121.2979736328125\r",
      "INFO - Step 14904, rl-loss: 228.70098876953125\r",
      "INFO - Step 14905, rl-loss: 9.468746185302734\r",
      "INFO - Step 14906, rl-loss: 214.77133178710938\r",
      "INFO - Step 14907, rl-loss: 193.17495727539062\r",
      "INFO - Step 14908, rl-loss: 33.15247344970703\r",
      "INFO - Step 14909, rl-loss: 374.45526123046875\r",
      "INFO - Step 14910, rl-loss: 9.096895217895508\r",
      "INFO - Step 14911, rl-loss: 467.5581359863281\r",
      "INFO - Step 14912, rl-loss: 95.42744445800781\r",
      "INFO - Step 14913, rl-loss: 310.03076171875\r",
      "INFO - Step 14914, rl-loss: 289.7964782714844\r",
      "INFO - Step 14915, rl-loss: 201.37916564941406\r",
      "INFO - Step 14916, rl-loss: 51.94994354248047\r",
      "INFO - Step 14917, rl-loss: 3.788652181625366\r",
      "INFO - Step 14918, rl-loss: 328.15472412109375\r",
      "INFO - Step 14919, rl-loss: 144.902587890625\r",
      "INFO - Step 14920, rl-loss: 6.476334571838379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14921, rl-loss: 26.456192016601562\r",
      "INFO - Step 14922, rl-loss: 150.70521545410156\r",
      "INFO - Step 14923, rl-loss: 547.360595703125\r",
      "INFO - Step 14924, rl-loss: 93.1302719116211\r",
      "INFO - Step 14925, rl-loss: 0.7666215896606445\r",
      "INFO - Step 14926, rl-loss: 146.8798065185547\r",
      "INFO - Step 14927, rl-loss: 156.3699493408203\r",
      "INFO - Step 14928, rl-loss: 320.609619140625\r",
      "INFO - Step 14929, rl-loss: 82.30207061767578\r",
      "INFO - Step 14930, rl-loss: 525.1546020507812\r",
      "INFO - Step 14931, rl-loss: 2.354365825653076\r",
      "INFO - Step 14932, rl-loss: 383.0064697265625\r",
      "INFO - Step 14933, rl-loss: 53.8052978515625\r",
      "INFO - Step 14934, rl-loss: 81.70521545410156\r",
      "INFO - Step 14935, rl-loss: 5.328470230102539\r",
      "INFO - Step 14936, rl-loss: 135.33810424804688\r",
      "INFO - Step 14937, rl-loss: 149.5667266845703\r",
      "INFO - Step 14938, rl-loss: 177.47386169433594\r",
      "INFO - Step 14939, rl-loss: 207.18125915527344\r",
      "INFO - Step 14940, rl-loss: 55.69465637207031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14941, rl-loss: 381.31207275390625\r",
      "INFO - Step 14942, rl-loss: 3.5824503898620605\r",
      "INFO - Step 14943, rl-loss: 96.03604125976562\r",
      "INFO - Step 14944, rl-loss: 22.02698516845703\r",
      "INFO - Step 14945, rl-loss: 91.5516128540039\r",
      "INFO - Step 14946, rl-loss: 97.41101837158203\r",
      "INFO - Step 14947, rl-loss: 226.2519073486328\r",
      "INFO - Step 14948, rl-loss: 6.160933494567871\r",
      "INFO - Step 14949, rl-loss: 374.00115966796875\r",
      "INFO - Step 14950, rl-loss: 264.87939453125\r",
      "INFO - Step 14951, rl-loss: 154.76052856445312\r",
      "INFO - Step 14952, rl-loss: 91.29299926757812\r",
      "INFO - Step 14953, rl-loss: 267.81280517578125\r",
      "INFO - Step 14954, rl-loss: 84.96490478515625\r",
      "INFO - Step 14955, rl-loss: 424.3775329589844\r",
      "INFO - Step 14956, rl-loss: 2.1844193935394287\r",
      "INFO - Step 14957, rl-loss: 435.649658203125\r",
      "INFO - Step 14958, rl-loss: 171.75685119628906\r",
      "INFO - Step 14959, rl-loss: 32.23221969604492\r",
      "INFO - Step 14960, rl-loss: 7.628584861755371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14961, rl-loss: 114.8499984741211\r",
      "INFO - Step 14962, rl-loss: 2.584394693374634\r",
      "INFO - Step 14963, rl-loss: 124.38455200195312\r",
      "INFO - Step 14964, rl-loss: 4.011843681335449\r",
      "INFO - Step 14965, rl-loss: 85.93855285644531\r",
      "INFO - Step 14966, rl-loss: 216.81564331054688\r",
      "INFO - Step 14967, rl-loss: 80.08816528320312\r",
      "INFO - Step 14968, rl-loss: 71.75631713867188\r",
      "INFO - Step 14969, rl-loss: 166.31298828125\r",
      "INFO - Step 14970, rl-loss: 10.526247024536133\r",
      "INFO - Step 14971, rl-loss: 1.1365245580673218\r",
      "INFO - Step 14972, rl-loss: 51.188289642333984\r",
      "INFO - Step 14973, rl-loss: 132.9031982421875\r",
      "INFO - Step 14974, rl-loss: 150.96092224121094\r",
      "INFO - Step 14975, rl-loss: 387.7447509765625\r",
      "INFO - Step 14976, rl-loss: 2.9675893783569336\r",
      "INFO - Step 14977, rl-loss: 159.00363159179688\r",
      "INFO - Step 14978, rl-loss: 3.463118076324463\r",
      "INFO - Step 14979, rl-loss: 5.515554904937744\r",
      "INFO - Step 14980, rl-loss: 206.91397094726562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "INFO - Step 14981, rl-loss: 180.75222778320312\r",
      "INFO - Step 14982, rl-loss: 151.26016235351562\r",
      "INFO - Step 14983, rl-loss: 131.62319946289062\r",
      "INFO - Step 14984, rl-loss: 181.8462371826172\r",
      "INFO - Step 14985, rl-loss: 55.83126449584961\r",
      "INFO - Step 14986, rl-loss: 253.25985717773438\r",
      "INFO - Step 14987, rl-loss: 639.2779541015625\r",
      "INFO - Step 14988, rl-loss: 80.45494079589844\r",
      "INFO - Step 14989, rl-loss: 90.47988891601562\r",
      "INFO - Step 14990, rl-loss: 428.6585693359375\r",
      "INFO - Step 14991, rl-loss: 306.45501708984375\r",
      "INFO - Step 14992, rl-loss: 623.5436401367188\r",
      "INFO - Step 14993, rl-loss: 11.445289611816406\r",
      "INFO - Step 14994, rl-loss: 88.87335968017578\r",
      "INFO - Step 14995, rl-loss: 123.86722564697266\r",
      "INFO - Step 14996, rl-loss: 5.00158166885376\r",
      "INFO - Step 14997, rl-loss: 29.372127532958984\r",
      "INFO - Step 14998, rl-loss: 92.40943145751953\r",
      "INFO - Step 14999, rl-loss: 173.7545623779297\r",
      "INFO - Step 15000, rl-loss: 2.272188663482666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teticio/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 15020, rl-loss: 147.13398742675785\n",
      "Logs saved in .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the performance.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_performance(\n\u001b[1;32m     17\u001b[0m         env\u001b[38;5;241m.\u001b[39mtimestep,\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mtournament\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/utils/utils.py:210\u001b[0m, in \u001b[0;36mtournament\u001b[0;34m(env, num)\u001b[0m\n\u001b[1;32m    208\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m num:\n\u001b[0;32m--> 210\u001b[0m     _, _payoffs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_payoffs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _p \u001b[38;5;129;01min\u001b[39;00m _payoffs:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/envs/env.py:149\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    146\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents[player_id]\u001b[38;5;241m.\u001b[39mstep(state)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Environment steps\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m next_state, next_player_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Save action\u001b[39;00m\n\u001b[1;32m    151\u001b[0m trajectories[player_id]\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/envs/env.py:86\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action, raw_action)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_recorder\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_player_id(), action))\n\u001b[1;32m     84\u001b[0m next_state, player_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m, player_id\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/env.py:57\u001b[0m, in \u001b[0;36mTuteEnv._extract_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     44\u001b[0m follow_suit \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_follow_suit()]\n\u001b[1;32m     45\u001b[0m obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m     46\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m     47\u001b[0m         suit, trump_suit, follow_suit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m ],\n\u001b[1;32m     52\u001b[0m                      axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     54\u001b[0m extracted_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m: obs,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_legal_actions(),\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_legal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_legal_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     58\u001b[0m }\n\u001b[1;32m     59\u001b[0m extracted_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_obs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extracted_state\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/env.py:89\u001b[0m, in \u001b[0;36mTuteEnv._get_legal_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_legal_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124;03m\"\"\"Get all legal actions for current state.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m        dict: a list of legal action ids\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     legal_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_legal_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     legal_actions_ids \u001b[38;5;241m=\u001b[39m {action: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m legal_actions}\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(legal_actions_ids)\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/game.py:124\u001b[0m, in \u001b[0;36mTuteGame.get_legal_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m possible_cards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_move(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player)\n\u001b[1;32m    123\u001b[0m hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [hand\u001b[38;5;241m.\u001b[39miloc[card]\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m possible_cards]\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/game.py:124\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m possible_cards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_move(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player)\n\u001b[1;32m    123\u001b[0m hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mhand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcard\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m possible_cards]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/pandas/core/indexing.py:1522\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/pandas/core/frame.py:3427\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3424\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mfast_xs(i)\n\u001b[1;32m   3426\u001b[0m \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[0;32m-> 3427\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m new_values\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3428\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(\n\u001b[1;32m   3429\u001b[0m     new_values,\n\u001b[1;32m   3430\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m   3431\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[i],\n\u001b[1;32m   3432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnew_values\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   3433\u001b[0m )\n\u001b[1;32m   3434\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_is_copy(\u001b[38;5;28mself\u001b[39m, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Logger(\".\") as logger:\n",
    "    for episode in range(100000):\n",
    "\n",
    "        # Generate data from the environment\n",
    "        trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "        # Reorganaize the data to be state, action, reward, next_state, done\n",
    "        trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "        # Feed transitions into agent memory, and train the agent\n",
    "        for ts in trajectories[0]:\n",
    "            dqn_agent.feed(ts)\n",
    "\n",
    "        # Evaluate the performance.\n",
    "        if episode % 50 == 0:\n",
    "            logger.log_performance(\n",
    "                env.timestep,\n",
    "                tournament(\n",
    "                    env,\n",
    "                    100,\n",
    "                )[0]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a95f7643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "plot_curve(csv_path, fig_path, \"DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2571a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins = 0\n",
    "env.set_agents([dqn_agent, random_agent])\n",
    "for n in range(50):\n",
    "    trajectories, player_wins = env.run(is_training=False)\n",
    "    if player_wins[0] > player_wins[1]:\n",
    "        wins += 1\n",
    "env.set_agents([random_agent, dqn_agent])\n",
    "for n in range(50):\n",
    "    trajectories, player_wins = env.run(is_training=False)\n",
    "    if player_wins[1] > player_wins[0]:\n",
    "        wins += 1\n",
    "wins / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fae84de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 swapped el tres de copas for el dos de copas\n",
      "Player 1 played la sota de copas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "la sota de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el cuatro de bastos\n",
      "3: el tres de oros\n",
      "4: el cinco de oros\n",
      "5: la sota de espadas\n",
      "6: el as de bastos\n",
      "7: el cuatro de oros\n",
      "8: el tres de copas\n",
      "1  2  3  4  5  6  7  8 ? 2\n",
      "\n",
      "Player 2 played el cuatro de bastos\n",
      "Player 1 won trick\n",
      "Player 1 played el cuatro de espadas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el cuatro de espadas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de oros\n",
      "3: el cinco de oros\n",
      "4: la sota de espadas\n",
      "5: el as de bastos\n",
      "6: el cuatro de oros\n",
      "7: el tres de copas\n",
      "8: el seis de oros\n",
      "1  2  3  4  5  6  7  8 ? 4\n",
      "\n",
      "Player 2 played la sota de espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de oros\n",
      "3: el cinco de oros\n",
      "4: el as de bastos\n",
      "5: el cuatro de oros\n",
      "6: el tres de copas\n",
      "7: el seis de oros\n",
      "8: el rey de espadas\n",
      "1  2  3  4  5  6  7  8 ? 7\n",
      "\n",
      "Player 2 played el seis de oros\n",
      "Player 1 played el seis de copas\n",
      "Player 1 won trick\n",
      "Player 1 played el caballo de oros\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el caballo de oros\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de oros\n",
      "3: el cinco de oros\n",
      "4: el as de bastos\n",
      "5: el cuatro de oros\n",
      "6: el tres de copas\n",
      "7: el rey de espadas\n",
      "8: el rey de copas\n",
      "1  2  3  4  5  6  7  8 ? 3\n",
      "\n",
      "Player 2 played el cinco de oros\n",
      "Player 1 won trick\n",
      "Player 1 played el siete de oros\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el siete de oros\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de oros\n",
      "3: el as de bastos\n",
      "4: el cuatro de oros\n",
      "5: el tres de copas\n",
      "6: el rey de espadas\n",
      "7: el rey de copas\n",
      "8: el dos de oros\n",
      "1  2  3  4  5  6  7  8 ? 2\n",
      "\n",
      "Player 2 played el tres de oros\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el as de bastos\n",
      "3: el cuatro de oros\n",
      "4: el tres de copas\n",
      "5: el rey de espadas\n",
      "6: el rey de copas\n",
      "7: el dos de oros\n",
      "8: el cinco de copas\n",
      "1  2  3  4  5  6  7  8 ? 7\n",
      "\n",
      "Player 2 played el dos de oros\n",
      "Player 1 played el as de copas\n",
      "Player 1 won trick\n",
      "Player 1 played el rey de bastos\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el rey de bastos\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el as de bastos\n",
      "3: el cuatro de oros\n",
      "4: el tres de copas\n",
      "5: el rey de espadas\n",
      "6: el rey de copas\n",
      "7: el cinco de copas\n",
      "8: el caballo de espadas\n",
      "1  2  3  4  5  6  7  8 ? 2\n",
      "\n",
      "Player 2 played el as de bastos\n",
      "Player 2 canta espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el cuatro de oros\n",
      "3: el tres de copas\n",
      "4: el rey de espadas\n",
      "5: el rey de copas\n",
      "6: el cinco de copas\n",
      "7: el caballo de espadas\n",
      "8: el rey de oros\n",
      "1  2  3  4  5  6  7  8 ? 2\n",
      "\n",
      "Player 2 played el cuatro de oros\n",
      "Player 1 played el cinco de bastos\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de copas\n",
      "3: el rey de espadas\n",
      "4: el rey de copas\n",
      "5: el cinco de copas\n",
      "6: el caballo de espadas\n",
      "7: el rey de oros\n",
      "8: el seis de bastos\n",
      "1  2  3  4  5  6  7  8 ? 8\n",
      "\n",
      "Player 2 played el seis de bastos\n",
      "Player 1 played el dos de espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de copas\n",
      "3: el rey de espadas\n",
      "4: el rey de copas\n",
      "5: el cinco de copas\n",
      "6: el caballo de espadas\n",
      "7: el rey de oros\n",
      "8: la sota de oros\n",
      "1  2  3  4  5  6  7  8 ? 8\n",
      "\n",
      "Player 2 played la sota de oros\n",
      "Player 1 played el cinco de espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de copas\n",
      "3: el rey de espadas\n",
      "4: el rey de copas\n",
      "5: el cinco de copas\n",
      "6: el caballo de espadas\n",
      "7: el rey de oros\n",
      "8: el siete de bastos\n",
      "1  2  3  4  5  6  7  8 ? 8\n",
      "\n",
      "Player 2 played el siete de bastos\n",
      "Player 1 played el seis de espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el dos de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: la sota de bastos\n",
      "2: el tres de copas\n",
      "3: el rey de espadas\n",
      "4: el rey de copas\n",
      "5: el cinco de copas\n",
      "6: el caballo de espadas\n",
      "7: el rey de oros\n",
      "8: el tres de bastos\n",
      "1  2  3  4  5  6  7  8 ? 1\n",
      "\n",
      "Player 2 played la sota de bastos\n",
      "Player 1 played el siete de espadas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el tres de copas\n",
      "2: el rey de espadas\n",
      "3: el rey de copas\n",
      "4: el cinco de copas\n",
      "5: el caballo de espadas\n",
      "6: el rey de oros\n",
      "7: el tres de bastos\n",
      "8: el cuatro de copas\n",
      "1  2  3  4  5  6  7  8 ? 7\n",
      "\n",
      "Player 2 played el tres de bastos\n",
      "Player 1 played el caballo de bastos\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el tres de copas\n",
      "2: el rey de espadas\n",
      "3: el rey de copas\n",
      "4: el cinco de copas\n",
      "5: el caballo de espadas\n",
      "6: el rey de oros\n",
      "7: el cuatro de copas\n",
      "1  2  3  4  5  6  7 ? 5\n",
      "\n",
      "Player 2 played el caballo de espadas\n",
      "Player 1 played el tres de espadas\n",
      "Player 1 won trick\n",
      "Player 1 played el dos de copas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el dos de copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el tres de copas\n",
      "2: el rey de espadas\n",
      "3: el rey de copas\n",
      "4: el cinco de copas\n",
      "5: el rey de oros\n",
      "6: el cuatro de copas\n",
      "1  3  4  6 ? 1\n",
      "\n",
      "Player 2 played el tres de copas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el rey de espadas\n",
      "2: el rey de copas\n",
      "3: el cinco de copas\n",
      "4: el rey de oros\n",
      "5: el cuatro de copas\n",
      "1  2  3  4  5 ? 1\n",
      "\n",
      "Player 2 played el rey de espadas\n",
      "Player 1 played el as de espadas\n",
      "Player 1 won trick\n",
      "Player 1 played el as de oros\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el as de oros\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el rey de copas\n",
      "2: el cinco de copas\n",
      "3: el rey de oros\n",
      "4: el cuatro de copas\n",
      "3 ? 3\n",
      "\n",
      "Player 2 played el rey de oros\n",
      "Player 1 won trick\n",
      "Player 1 played el caballo de copas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el caballo de copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el rey de copas\n",
      "2: el cinco de copas\n",
      "3: el cuatro de copas\n",
      "1 ? 1\n",
      "\n",
      "Player 2 played el rey de copas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el cinco de copas\n",
      "2: el cuatro de copas\n",
      "1  2 ? 1\n",
      "\n",
      "Player 2 played el cinco de copas\n",
      "Player 1 played el siete de copas\n",
      "Player 2 won trick\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "copas\n",
      "\n",
      "Follow suit\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el cuatro de copas\n",
      "1 ? 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.set_agents([dqn_agent, human_agent])\n",
    "trajectories, player_wins = env.run(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc4aaf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59, 111])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a1a6f",
   "metadata": {},
   "source": [
    "### Train Deep Monte Carlo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ccf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlcard.make('tute')\n",
    "# Initialize the DMC trainer\n",
    "trainer = DMCTrainer(\n",
    "    env,\n",
    "    cuda=\"cuda\",\n",
    "    xpid=\"tute\",\n",
    "    savedir=\"dmc_results\",\n",
    "    save_interval=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c013ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating log directory: dmc_results/tute\n",
      "INFO:palaas/out:Creating log directory: dmc_results/tute\n",
      "Saving arguments to dmc_results/tute/meta.json\n",
      "INFO:palaas/out:Saving arguments to dmc_results/tute/meta.json\n",
      "Saving messages to dmc_results/tute/out.log\n",
      "INFO:palaas/out:Saving messages to dmc_results/tute/out.log\n",
      "Saving logs data to dmc_results/tute/logs.csv\n",
      "INFO:palaas/out:Saving logs data to dmc_results/tute/logs.csv\n",
      "Saving logs' fields to dmc_results/tute/fields.csv\n",
      "INFO:palaas/out:Saving logs' fields to dmc_results/tute/fields.csv\n",
      "[INFO:417927 utils:108 2022-09-17 12:20:32,308] Device 0 Actor 2 started.\n",
      "[INFO:417894 utils:108 2022-09-17 12:20:32,314] Device 0 Actor 0 started.\n",
      "[INFO:417897 utils:108 2022-09-17 12:20:32,344] Device 0 Actor 1 started.\n",
      "[INFO:417991 utils:108 2022-09-17 12:20:32,461] Device 0 Actor 4 started.\n",
      "[INFO:417959 utils:108 2022-09-17 12:20:32,468] Device 0 Actor 3 started.\n",
      "[INFO:417825 trainer:335 2022-09-17 12:20:35,943] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:20:35,995] After 0 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 0,\n",
      " 'loss_1': 0,\n",
      " 'mean_episode_return_0': 0,\n",
      " 'mean_episode_return_1': 0}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:20:41,000] After 0 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 0,\n",
      " 'loss_1': 0,\n",
      " 'mean_episode_return_0': 0,\n",
      " 'mean_episode_return_1': 0}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:20:46,006] After 0 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 0,\n",
      " 'loss_1': 0,\n",
      " 'mean_episode_return_0': 0,\n",
      " 'mean_episode_return_1': 0}\n",
      "Updated log fields: ['_tick', '_time', 'frames', 'mean_episode_return_0', 'loss_0', 'mean_episode_return_1', 'loss_1']\n",
      "INFO:palaas/out:Updated log fields: ['_tick', '_time', 'frames', 'mean_episode_return_0', 'loss_0', 'mean_episode_return_1', 'loss_1']\n",
      "[INFO:417825 trainer:367 2022-09-17 12:20:51,012] After 6400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 4357.560546875,\n",
      " 'loss_1': 7018.6962890625,\n",
      " 'mean_episode_return_0': 60.587501525878906,\n",
      " 'mean_episode_return_1': 78.9437484741211}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:20:56,018] After 6400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4357.560546875,\n",
      " 'loss_1': 7018.6962890625,\n",
      " 'mean_episode_return_0': 60.587501525878906,\n",
      " 'mean_episode_return_1': 78.9437484741211}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:01,024] After 6400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4357.560546875,\n",
      " 'loss_1': 7018.6962890625,\n",
      " 'mean_episode_return_0': 60.587501525878906,\n",
      " 'mean_episode_return_1': 78.9437484741211}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:06,030] After 12800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 4014.693603515625,\n",
      " 'loss_1': 6724.47119140625,\n",
      " 'mean_episode_return_0': 59.15312576293945,\n",
      " 'mean_episode_return_1': 77.82035827636719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:11,036] After 12800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4014.693603515625,\n",
      " 'loss_1': 6724.47119140625,\n",
      " 'mean_episode_return_0': 59.15312576293945,\n",
      " 'mean_episode_return_1': 77.82035827636719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:16,042] After 12800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4014.693603515625,\n",
      " 'loss_1': 6724.47119140625,\n",
      " 'mean_episode_return_0': 59.15312576293945,\n",
      " 'mean_episode_return_1': 77.82035827636719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:21,048] After 19200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 4581.27294921875,\n",
      " 'loss_1': 6964.64990234375,\n",
      " 'mean_episode_return_0': 60.239585876464844,\n",
      " 'mean_episode_return_1': 78.03267669677734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:26,054] After 19200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4581.27294921875,\n",
      " 'loss_1': 6964.64990234375,\n",
      " 'mean_episode_return_0': 60.239585876464844,\n",
      " 'mean_episode_return_1': 78.03267669677734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:31,060] After 22400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 4194.7763671875,\n",
      " 'loss_1': 6964.64990234375,\n",
      " 'mean_episode_return_0': 60.087501525878906,\n",
      " 'mean_episode_return_1': 78.03267669677734}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:21:36,064] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:36,153] After 25600 frames: @ 628.5 fps Stats:\n",
      "{'loss_0': 4194.7763671875,\n",
      " 'loss_1': 6926.5947265625,\n",
      " 'mean_episode_return_0': 60.087501525878906,\n",
      " 'mean_episode_return_1': 78.21996307373047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:41,158] After 25600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 4194.7763671875,\n",
      " 'loss_1': 6926.5947265625,\n",
      " 'mean_episode_return_0': 60.087501525878906,\n",
      " 'mean_episode_return_1': 78.21996307373047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:46,164] After 32000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 3923.4267578125,\n",
      " 'loss_1': 7451.3466796875,\n",
      " 'mean_episode_return_0': 59.842498779296875,\n",
      " 'mean_episode_return_1': 79.27535247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:51,170] After 32000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 3923.4267578125,\n",
      " 'loss_1': 7451.3466796875,\n",
      " 'mean_episode_return_0': 59.842498779296875,\n",
      " 'mean_episode_return_1': 79.27535247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:21:56,176] After 32000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 3923.4267578125,\n",
      " 'loss_1': 7451.3466796875,\n",
      " 'mean_episode_return_0': 59.842498779296875,\n",
      " 'mean_episode_return_1': 79.27535247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:01,177] After 38400 frames: @ 1280.0 fps Stats:\n",
      "{'loss_0': 3934.00244140625,\n",
      " 'loss_1': 6679.966796875,\n",
      " 'mean_episode_return_0': 59.86042022705078,\n",
      " 'mean_episode_return_1': 79.34500885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:06,183] After 38400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 3934.00244140625,\n",
      " 'loss_1': 6679.966796875,\n",
      " 'mean_episode_return_0': 59.86042022705078,\n",
      " 'mean_episode_return_1': 79.34500885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:11,189] After 41600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 3900.653564453125,\n",
      " 'loss_1': 6679.966796875,\n",
      " 'mean_episode_return_0': 60.243751525878906,\n",
      " 'mean_episode_return_1': 79.34500885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:16,195] After 44800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 3900.653564453125,\n",
      " 'loss_1': 6438.9375,\n",
      " 'mean_episode_return_0': 60.243751525878906,\n",
      " 'mean_episode_return_1': 79.74620056152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:21,201] After 44800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 3900.653564453125,\n",
      " 'loss_1': 6438.9375,\n",
      " 'mean_episode_return_0': 60.243751525878906,\n",
      " 'mean_episode_return_1': 79.74620056152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:26,207] After 51200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 2967.248779296875,\n",
      " 'loss_1': 5261.138671875,\n",
      " 'mean_episode_return_0': 60.060157775878906,\n",
      " 'mean_episode_return_1': 79.79393005371094}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:31,213] After 51200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 2967.248779296875,\n",
      " 'loss_1': 5261.138671875,\n",
      " 'mean_episode_return_0': 60.060157775878906,\n",
      " 'mean_episode_return_1': 79.79393005371094}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:22:36,219] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:36,315] After 51200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 2967.248779296875,\n",
      " 'loss_1': 5261.138671875,\n",
      " 'mean_episode_return_0': 60.060157775878906,\n",
      " 'mean_episode_return_1': 79.79393005371094}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:41,321] After 57600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 2866.042724609375,\n",
      " 'loss_1': 4831.86572265625,\n",
      " 'mean_episode_return_0': 60.24514389038086,\n",
      " 'mean_episode_return_1': 80.11493682861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:46,324] After 57600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 2866.042724609375,\n",
      " 'loss_1': 4831.86572265625,\n",
      " 'mean_episode_return_0': 60.24514389038086,\n",
      " 'mean_episode_return_1': 80.11493682861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:51,330] After 60800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 1532.9952392578125,\n",
      " 'loss_1': 4831.86572265625,\n",
      " 'mean_episode_return_0': 59.8487548828125,\n",
      " 'mean_episode_return_1': 80.11493682861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:22:56,335] After 64000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 1532.9952392578125,\n",
      " 'loss_1': 4351.015625,\n",
      " 'mean_episode_return_0': 59.8487548828125,\n",
      " 'mean_episode_return_1': 80.9089584350586}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:01,341] After 64000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 1532.9952392578125,\n",
      " 'loss_1': 4351.015625,\n",
      " 'mean_episode_return_0': 59.8487548828125,\n",
      " 'mean_episode_return_1': 80.9089584350586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:23:06,347] After 67200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 1348.5743408203125,\n",
      " 'loss_1': 4351.015625,\n",
      " 'mean_episode_return_0': 59.885799407958984,\n",
      " 'mean_episode_return_1': 80.9089584350586}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:11,351] After 70400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 1348.5743408203125,\n",
      " 'loss_1': 2665.51123046875,\n",
      " 'mean_episode_return_0': 59.885799407958984,\n",
      " 'mean_episode_return_1': 80.99018859863281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:16,356] After 70400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 1348.5743408203125,\n",
      " 'loss_1': 2665.51123046875,\n",
      " 'mean_episode_return_0': 59.885799407958984,\n",
      " 'mean_episode_return_1': 80.99018859863281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:21,360] After 73600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 854.1663818359375,\n",
      " 'loss_1': 2665.51123046875,\n",
      " 'mean_episode_return_0': 59.92812728881836,\n",
      " 'mean_episode_return_1': 80.99018859863281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:26,366] After 76800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 854.1663818359375,\n",
      " 'loss_1': 1463.9459228515625,\n",
      " 'mean_episode_return_0': 59.92812728881836,\n",
      " 'mean_episode_return_1': 80.87086486816406}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:31,371] After 76800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 854.1663818359375,\n",
      " 'loss_1': 1463.9459228515625,\n",
      " 'mean_episode_return_0': 59.92812728881836,\n",
      " 'mean_episode_return_1': 80.87086486816406}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:23:36,377] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:36,437] After 83200 frames: @ 1263.7 fps Stats:\n",
      "{'loss_0': 652.2804565429688,\n",
      " 'loss_1': 1222.9815673828125,\n",
      " 'mean_episode_return_0': 60.013465881347656,\n",
      " 'mean_episode_return_1': 81.1387710571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:41,440] After 83200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 652.2804565429688,\n",
      " 'loss_1': 1222.9815673828125,\n",
      " 'mean_episode_return_0': 60.013465881347656,\n",
      " 'mean_episode_return_1': 81.1387710571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:46,444] After 86400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 724.2280883789062,\n",
      " 'loss_1': 1222.9815673828125,\n",
      " 'mean_episode_return_0': 60.0741081237793,\n",
      " 'mean_episode_return_1': 81.1387710571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:51,450] After 89600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 724.2280883789062,\n",
      " 'loss_1': 742.5575561523438,\n",
      " 'mean_episode_return_0': 60.0741081237793,\n",
      " 'mean_episode_return_1': 81.32329559326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:23:56,452] After 89600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 724.2280883789062,\n",
      " 'loss_1': 742.5575561523438,\n",
      " 'mean_episode_return_0': 60.0741081237793,\n",
      " 'mean_episode_return_1': 81.32329559326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:01,458] After 92800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 512.9442138671875,\n",
      " 'loss_1': 742.5575561523438,\n",
      " 'mean_episode_return_0': 59.919166564941406,\n",
      " 'mean_episode_return_1': 81.32329559326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:06,464] After 96000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 512.9442138671875,\n",
      " 'loss_1': 698.4278564453125,\n",
      " 'mean_episode_return_0': 59.919166564941406,\n",
      " 'mean_episode_return_1': 81.63140106201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:11,468] After 96000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 512.9442138671875,\n",
      " 'loss_1': 698.4278564453125,\n",
      " 'mean_episode_return_0': 59.919166564941406,\n",
      " 'mean_episode_return_1': 81.63140106201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:16,472] After 99200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 646.5225830078125,\n",
      " 'loss_1': 698.4278564453125,\n",
      " 'mean_episode_return_0': 59.798439025878906,\n",
      " 'mean_episode_return_1': 81.63140106201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:21,476] After 102400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 646.5225830078125,\n",
      " 'loss_1': 622.3618774414062,\n",
      " 'mean_episode_return_0': 59.798439025878906,\n",
      " 'mean_episode_return_1': 81.74361419677734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:26,480] After 102400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 646.5225830078125,\n",
      " 'loss_1': 622.3618774414062,\n",
      " 'mean_episode_return_0': 59.798439025878906,\n",
      " 'mean_episode_return_1': 81.74361419677734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:31,484] After 105600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 558.7764892578125,\n",
      " 'loss_1': 622.3618774414062,\n",
      " 'mean_episode_return_0': 59.786399841308594,\n",
      " 'mean_episode_return_1': 81.74361419677734}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:24:36,489] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:36,570] After 108800 frames: @ 629.2 fps Stats:\n",
      "{'loss_0': 558.7764892578125,\n",
      " 'loss_1': 968.56640625,\n",
      " 'mean_episode_return_0': 59.786399841308594,\n",
      " 'mean_episode_return_1': 81.57325744628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:41,576] After 108800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 558.7764892578125,\n",
      " 'loss_1': 968.56640625,\n",
      " 'mean_episode_return_0': 59.786399841308594,\n",
      " 'mean_episode_return_1': 81.57325744628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:46,580] After 112000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 604.3690185546875,\n",
      " 'loss_1': 968.56640625,\n",
      " 'mean_episode_return_0': 59.70695114135742,\n",
      " 'mean_episode_return_1': 81.57325744628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:51,585] After 115200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 604.3690185546875,\n",
      " 'loss_1': 547.79833984375,\n",
      " 'mean_episode_return_0': 59.70695114135742,\n",
      " 'mean_episode_return_1': 81.7005386352539}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:24:56,591] After 115200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 604.3690185546875,\n",
      " 'loss_1': 547.79833984375,\n",
      " 'mean_episode_return_0': 59.70695114135742,\n",
      " 'mean_episode_return_1': 81.7005386352539}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:01,597] After 118400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 462.80511474609375,\n",
      " 'loss_1': 547.79833984375,\n",
      " 'mean_episode_return_0': 59.637168884277344,\n",
      " 'mean_episode_return_1': 81.7005386352539}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:06,603] After 121600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 462.80511474609375,\n",
      " 'loss_1': 798.49755859375,\n",
      " 'mean_episode_return_0': 59.637168884277344,\n",
      " 'mean_episode_return_1': 81.41278076171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:11,608] After 124800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 492.76605224609375,\n",
      " 'loss_1': 798.49755859375,\n",
      " 'mean_episode_return_0': 59.65281295776367,\n",
      " 'mean_episode_return_1': 81.41278076171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:16,613] After 124800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 492.76605224609375,\n",
      " 'loss_1': 798.49755859375,\n",
      " 'mean_episode_return_0': 59.65281295776367,\n",
      " 'mean_episode_return_1': 81.41278076171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:21,619] After 128000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 492.76605224609375,\n",
      " 'loss_1': 713.8717651367188,\n",
      " 'mean_episode_return_0': 59.65281295776367,\n",
      " 'mean_episode_return_1': 81.38726043701172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:26,625] After 131200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 534.3642578125,\n",
      " 'loss_1': 713.8717651367188,\n",
      " 'mean_episode_return_0': 59.68185043334961,\n",
      " 'mean_episode_return_1': 81.38726043701172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:31,631] After 134400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 534.3642578125,\n",
      " 'loss_1': 731.5732421875,\n",
      " 'mean_episode_return_0': 59.68185043334961,\n",
      " 'mean_episode_return_1': 81.07258605957031}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:25:36,637] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:36,719] After 134400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 534.3642578125,\n",
      " 'loss_1': 731.5732421875,\n",
      " 'mean_episode_return_0': 59.68185043334961,\n",
      " 'mean_episode_return_1': 81.07258605957031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:41,724] After 137600 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 451.3758850097656,\n",
      " 'loss_1': 731.5732421875,\n",
      " 'mean_episode_return_0': 59.65909194946289,\n",
      " 'mean_episode_return_1': 81.07258605957031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:46,728] After 140800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 451.3758850097656,\n",
      " 'loss_1': 475.06536865234375,\n",
      " 'mean_episode_return_0': 59.65909194946289,\n",
      " 'mean_episode_return_1': 80.87250518798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:25:51,732] After 140800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.3758850097656,\n",
      " 'loss_1': 475.06536865234375,\n",
      " 'mean_episode_return_0': 59.65909194946289,\n",
      " 'mean_episode_return_1': 80.87250518798828}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:25:56,736] After 144000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 610.1395874023438,\n",
      " 'loss_1': 475.06536865234375,\n",
      " 'mean_episode_return_0': 59.82853317260742,\n",
      " 'mean_episode_return_1': 80.87250518798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:01,740] After 147200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 610.1395874023438,\n",
      " 'loss_1': 716.870849609375,\n",
      " 'mean_episode_return_0': 59.82853317260742,\n",
      " 'mean_episode_return_1': 80.73091125488281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:06,744] After 147200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 610.1395874023438,\n",
      " 'loss_1': 716.870849609375,\n",
      " 'mean_episode_return_0': 59.82853317260742,\n",
      " 'mean_episode_return_1': 80.73091125488281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:11,748] After 150400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 530.1473999023438,\n",
      " 'loss_1': 716.870849609375,\n",
      " 'mean_episode_return_0': 59.78411865234375,\n",
      " 'mean_episode_return_1': 80.73091125488281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:16,752] After 153600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 530.1473999023438,\n",
      " 'loss_1': 534.9699096679688,\n",
      " 'mean_episode_return_0': 59.78411865234375,\n",
      " 'mean_episode_return_1': 80.44029235839844}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:21,757] After 153600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 530.1473999023438,\n",
      " 'loss_1': 534.9699096679688,\n",
      " 'mean_episode_return_0': 59.78411865234375,\n",
      " 'mean_episode_return_1': 80.44029235839844}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:26,760] After 156800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 593.8222045898438,\n",
      " 'loss_1': 534.9699096679688,\n",
      " 'mean_episode_return_0': 59.95075225830078,\n",
      " 'mean_episode_return_1': 80.44029235839844}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:31,764] After 160000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 593.8222045898438,\n",
      " 'loss_1': 576.8004150390625,\n",
      " 'mean_episode_return_0': 59.95075225830078,\n",
      " 'mean_episode_return_1': 80.1181411743164}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:26:36,767] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:36,855] After 163200 frames: @ 628.6 fps Stats:\n",
      "{'loss_0': 450.1481628417969,\n",
      " 'loss_1': 576.8004150390625,\n",
      " 'mean_episode_return_0': 60.075965881347656,\n",
      " 'mean_episode_return_1': 80.1181411743164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:41,860] After 163200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 450.1481628417969,\n",
      " 'loss_1': 576.8004150390625,\n",
      " 'mean_episode_return_0': 60.075965881347656,\n",
      " 'mean_episode_return_1': 80.1181411743164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:46,864] After 166400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 450.1481628417969,\n",
      " 'loss_1': 584.3057861328125,\n",
      " 'mean_episode_return_0': 60.075965881347656,\n",
      " 'mean_episode_return_1': 79.72358703613281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:51,868] After 169600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 489.6723327636719,\n",
      " 'loss_1': 584.3057861328125,\n",
      " 'mean_episode_return_0': 60.0828742980957,\n",
      " 'mean_episode_return_1': 79.72358703613281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:26:56,872] After 169600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 489.6723327636719,\n",
      " 'loss_1': 584.3057861328125,\n",
      " 'mean_episode_return_0': 60.0828742980957,\n",
      " 'mean_episode_return_1': 79.72358703613281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:01,876] After 172800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 489.6723327636719,\n",
      " 'loss_1': 595.0132446289062,\n",
      " 'mean_episode_return_0': 60.0828742980957,\n",
      " 'mean_episode_return_1': 79.57512664794922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:06,880] After 176000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 576.3930053710938,\n",
      " 'loss_1': 595.0132446289062,\n",
      " 'mean_episode_return_0': 60.195091247558594,\n",
      " 'mean_episode_return_1': 79.57512664794922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:11,885] After 176000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 576.3930053710938,\n",
      " 'loss_1': 595.0132446289062,\n",
      " 'mean_episode_return_0': 60.195091247558594,\n",
      " 'mean_episode_return_1': 79.57512664794922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:16,891] After 179200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 576.3930053710938,\n",
      " 'loss_1': 518.1325073242188,\n",
      " 'mean_episode_return_0': 60.195091247558594,\n",
      " 'mean_episode_return_1': 79.34030151367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:21,896] After 182400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 425.1751403808594,\n",
      " 'loss_1': 518.1325073242188,\n",
      " 'mean_episode_return_0': 60.1607780456543,\n",
      " 'mean_episode_return_1': 79.34030151367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:26,901] After 182400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 425.1751403808594,\n",
      " 'loss_1': 518.1325073242188,\n",
      " 'mean_episode_return_0': 60.1607780456543,\n",
      " 'mean_episode_return_1': 79.34030151367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:31,904] After 185600 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 425.1751403808594,\n",
      " 'loss_1': 518.732666015625,\n",
      " 'mean_episode_return_0': 60.1607780456543,\n",
      " 'mean_episode_return_1': 79.20893096923828}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:27:36,907] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:36,990] After 188800 frames: @ 629.2 fps Stats:\n",
      "{'loss_0': 469.07708740234375,\n",
      " 'loss_1': 518.732666015625,\n",
      " 'mean_episode_return_0': 60.099796295166016,\n",
      " 'mean_episode_return_1': 79.20893096923828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:41,996] After 192000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 469.07708740234375,\n",
      " 'loss_1': 490.88531494140625,\n",
      " 'mean_episode_return_0': 60.099796295166016,\n",
      " 'mean_episode_return_1': 79.01131439208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:47,000] After 195200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 458.6950378417969,\n",
      " 'loss_1': 490.88531494140625,\n",
      " 'mean_episode_return_0': 60.22378921508789,\n",
      " 'mean_episode_return_1': 79.01131439208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:52,004] After 195200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 458.6950378417969,\n",
      " 'loss_1': 490.88531494140625,\n",
      " 'mean_episode_return_0': 60.22378921508789,\n",
      " 'mean_episode_return_1': 79.01131439208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:27:57,008] After 198400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 458.6950378417969,\n",
      " 'loss_1': 499.8715515136719,\n",
      " 'mean_episode_return_0': 60.22378921508789,\n",
      " 'mean_episode_return_1': 78.8113021850586}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:02,012] After 201600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 479.1681823730469,\n",
      " 'loss_1': 499.8715515136719,\n",
      " 'mean_episode_return_0': 60.26699447631836,\n",
      " 'mean_episode_return_1': 78.8113021850586}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:07,016] After 201600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 479.1681823730469,\n",
      " 'loss_1': 499.8715515136719,\n",
      " 'mean_episode_return_0': 60.26699447631836,\n",
      " 'mean_episode_return_1': 78.8113021850586}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:12,022] After 204800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 479.1681823730469,\n",
      " 'loss_1': 509.751953125,\n",
      " 'mean_episode_return_0': 60.26699447631836,\n",
      " 'mean_episode_return_1': 78.60209655761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:17,028] After 208000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 441.5218505859375,\n",
      " 'loss_1': 509.751953125,\n",
      " 'mean_episode_return_0': 60.408145904541016,\n",
      " 'mean_episode_return_1': 78.60209655761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:22,032] After 208000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 441.5218505859375,\n",
      " 'loss_1': 509.751953125,\n",
      " 'mean_episode_return_0': 60.408145904541016,\n",
      " 'mean_episode_return_1': 78.60209655761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:27,038] After 211200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 441.5218505859375,\n",
      " 'loss_1': 415.4261474609375,\n",
      " 'mean_episode_return_0': 60.408145904541016,\n",
      " 'mean_episode_return_1': 78.3021240234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:32,044] After 214400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 395.1898498535156,\n",
      " 'loss_1': 415.4261474609375,\n",
      " 'mean_episode_return_0': 60.384742736816406,\n",
      " 'mean_episode_return_1': 78.3021240234375}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:28:37,050] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:37,127] After 214400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.1898498535156,\n",
      " 'loss_1': 415.4261474609375,\n",
      " 'mean_episode_return_0': 60.384742736816406,\n",
      " 'mean_episode_return_1': 78.3021240234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:42,128] After 220800 frames: @ 1280.0 fps Stats:\n",
      "{'loss_0': 442.8182678222656,\n",
      " 'loss_1': 440.5427551269531,\n",
      " 'mean_episode_return_0': 60.31071472167969,\n",
      " 'mean_episode_return_1': 78.21005249023438}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:28:47,134] After 220800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.8182678222656,\n",
      " 'loss_1': 440.5427551269531,\n",
      " 'mean_episode_return_0': 60.31071472167969,\n",
      " 'mean_episode_return_1': 78.21005249023438}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:52,139] After 220800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.8182678222656,\n",
      " 'loss_1': 440.5427551269531,\n",
      " 'mean_episode_return_0': 60.31071472167969,\n",
      " 'mean_episode_return_1': 78.21005249023438}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:28:57,145] After 227200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 318.6097106933594,\n",
      " 'loss_1': 389.4335021972656,\n",
      " 'mean_episode_return_0': 60.31528091430664,\n",
      " 'mean_episode_return_1': 78.09669494628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:02,151] After 227200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.6097106933594,\n",
      " 'loss_1': 389.4335021972656,\n",
      " 'mean_episode_return_0': 60.31528091430664,\n",
      " 'mean_episode_return_1': 78.09669494628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:07,156] After 227200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.6097106933594,\n",
      " 'loss_1': 389.4335021972656,\n",
      " 'mean_episode_return_0': 60.31528091430664,\n",
      " 'mean_episode_return_1': 78.09669494628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:12,162] After 233600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 457.7581787109375,\n",
      " 'loss_1': 478.7994384765625,\n",
      " 'mean_episode_return_0': 60.35574722290039,\n",
      " 'mean_episode_return_1': 78.00641632080078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:17,164] After 233600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 457.7581787109375,\n",
      " 'loss_1': 478.7994384765625,\n",
      " 'mean_episode_return_0': 60.35574722290039,\n",
      " 'mean_episode_return_1': 78.00641632080078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:22,168] After 236800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 457.7581787109375,\n",
      " 'loss_1': 587.6300048828125,\n",
      " 'mean_episode_return_0': 60.35574722290039,\n",
      " 'mean_episode_return_1': 77.96002960205078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:27,172] After 240000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 423.7554626464844,\n",
      " 'loss_1': 587.6300048828125,\n",
      " 'mean_episode_return_0': 60.31447982788086,\n",
      " 'mean_episode_return_1': 77.96002960205078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:32,178] After 240000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 423.7554626464844,\n",
      " 'loss_1': 587.6300048828125,\n",
      " 'mean_episode_return_0': 60.31447982788086,\n",
      " 'mean_episode_return_1': 77.96002960205078}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:29:37,179] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:37,270] After 246400 frames: @ 1257.1 fps Stats:\n",
      "{'loss_0': 453.86480712890625,\n",
      " 'loss_1': 486.57122802734375,\n",
      " 'mean_episode_return_0': 60.39599609375,\n",
      " 'mean_episode_return_1': 77.70446014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:42,272] After 246400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.86480712890625,\n",
      " 'loss_1': 486.57122802734375,\n",
      " 'mean_episode_return_0': 60.39599609375,\n",
      " 'mean_episode_return_1': 77.70446014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:47,278] After 246400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.86480712890625,\n",
      " 'loss_1': 486.57122802734375,\n",
      " 'mean_episode_return_0': 60.39599609375,\n",
      " 'mean_episode_return_1': 77.70446014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:52,284] After 252800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 451.07904052734375,\n",
      " 'loss_1': 535.6142578125,\n",
      " 'mean_episode_return_0': 60.42109298706055,\n",
      " 'mean_episode_return_1': 77.59446716308594}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:29:57,290] After 252800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.07904052734375,\n",
      " 'loss_1': 535.6142578125,\n",
      " 'mean_episode_return_0': 60.42109298706055,\n",
      " 'mean_episode_return_1': 77.59446716308594}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:02,296] After 256000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 451.07904052734375,\n",
      " 'loss_1': 381.39703369140625,\n",
      " 'mean_episode_return_0': 60.42109298706055,\n",
      " 'mean_episode_return_1': 77.49339294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:07,301] After 259200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 362.358154296875,\n",
      " 'loss_1': 381.39703369140625,\n",
      " 'mean_episode_return_0': 60.42317199707031,\n",
      " 'mean_episode_return_1': 77.49339294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:12,304] After 259200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 362.358154296875,\n",
      " 'loss_1': 381.39703369140625,\n",
      " 'mean_episode_return_0': 60.42317199707031,\n",
      " 'mean_episode_return_1': 77.49339294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:17,308] After 265600 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 319.9563293457031,\n",
      " 'loss_1': 362.1358947753906,\n",
      " 'mean_episode_return_0': 60.45059585571289,\n",
      " 'mean_episode_return_1': 77.33979797363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:22,310] After 265600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 319.9563293457031,\n",
      " 'loss_1': 362.1358947753906,\n",
      " 'mean_episode_return_0': 60.45059585571289,\n",
      " 'mean_episode_return_1': 77.33979797363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:27,316] After 265600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 319.9563293457031,\n",
      " 'loss_1': 362.1358947753906,\n",
      " 'mean_episode_return_0': 60.45059585571289,\n",
      " 'mean_episode_return_1': 77.33979797363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:32,322] After 272000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 324.6802978515625,\n",
      " 'loss_1': 400.8278503417969,\n",
      " 'mean_episode_return_0': 60.42732620239258,\n",
      " 'mean_episode_return_1': 77.21585845947266}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:30:37,328] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:37,421] After 272000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 324.6802978515625,\n",
      " 'loss_1': 400.8278503417969,\n",
      " 'mean_episode_return_0': 60.42732620239258,\n",
      " 'mean_episode_return_1': 77.21585845947266}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:42,426] After 272000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 324.6802978515625,\n",
      " 'loss_1': 400.8278503417969,\n",
      " 'mean_episode_return_0': 60.42732620239258,\n",
      " 'mean_episode_return_1': 77.21585845947266}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:47,432] After 278400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 346.80963134765625,\n",
      " 'loss_1': 457.1522521972656,\n",
      " 'mean_episode_return_0': 60.435935974121094,\n",
      " 'mean_episode_return_1': 77.0562744140625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:52,436] After 278400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 346.80963134765625,\n",
      " 'loss_1': 457.1522521972656,\n",
      " 'mean_episode_return_0': 60.435935974121094,\n",
      " 'mean_episode_return_1': 77.0562744140625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:30:57,440] After 278400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 346.80963134765625,\n",
      " 'loss_1': 457.1522521972656,\n",
      " 'mean_episode_return_0': 60.435935974121094,\n",
      " 'mean_episode_return_1': 77.0562744140625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:02,446] After 284800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 362.9646301269531,\n",
      " 'loss_1': 394.3035888671875,\n",
      " 'mean_episode_return_0': 60.4605598449707,\n",
      " 'mean_episode_return_1': 76.9559555053711}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:07,452] After 284800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 362.9646301269531,\n",
      " 'loss_1': 394.3035888671875,\n",
      " 'mean_episode_return_0': 60.4605598449707,\n",
      " 'mean_episode_return_1': 76.9559555053711}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:12,458] After 291200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 295.5633239746094,\n",
      " 'loss_1': 387.0713195800781,\n",
      " 'mean_episode_return_0': 60.47160720825195,\n",
      " 'mean_episode_return_1': 76.8433609008789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:17,463] After 291200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 295.5633239746094,\n",
      " 'loss_1': 387.0713195800781,\n",
      " 'mean_episode_return_0': 60.47160720825195,\n",
      " 'mean_episode_return_1': 76.8433609008789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:22,469] After 291200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 295.5633239746094,\n",
      " 'loss_1': 387.0713195800781,\n",
      " 'mean_episode_return_0': 60.47160720825195,\n",
      " 'mean_episode_return_1': 76.8433609008789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:27,475] After 297600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 341.65460205078125,\n",
      " 'loss_1': 372.32318115234375,\n",
      " 'mean_episode_return_0': 60.50545120239258,\n",
      " 'mean_episode_return_1': 76.6648941040039}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:32,480] After 297600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.65460205078125,\n",
      " 'loss_1': 372.32318115234375,\n",
      " 'mean_episode_return_0': 60.50545120239258,\n",
      " 'mean_episode_return_1': 76.6648941040039}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 12:31:37,486] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:37,559] After 297600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.65460205078125,\n",
      " 'loss_1': 372.32318115234375,\n",
      " 'mean_episode_return_0': 60.50545120239258,\n",
      " 'mean_episode_return_1': 76.6648941040039}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:42,565] After 304000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 332.5693664550781,\n",
      " 'loss_1': 488.8091735839844,\n",
      " 'mean_episode_return_0': 60.50482177734375,\n",
      " 'mean_episode_return_1': 76.62098693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:47,568] After 304000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 332.5693664550781,\n",
      " 'loss_1': 488.8091735839844,\n",
      " 'mean_episode_return_0': 60.50482177734375,\n",
      " 'mean_episode_return_1': 76.62098693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:52,574] After 307200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 355.0056457519531,\n",
      " 'loss_1': 488.8091735839844,\n",
      " 'mean_episode_return_0': 60.53711700439453,\n",
      " 'mean_episode_return_1': 76.62098693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:31:57,579] After 310400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 355.0056457519531,\n",
      " 'loss_1': 328.66302490234375,\n",
      " 'mean_episode_return_0': 60.53711700439453,\n",
      " 'mean_episode_return_1': 76.49092864990234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:02,585] After 310400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.0056457519531,\n",
      " 'loss_1': 328.66302490234375,\n",
      " 'mean_episode_return_0': 60.53711700439453,\n",
      " 'mean_episode_return_1': 76.49092864990234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:07,591] After 316800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 282.2646179199219,\n",
      " 'loss_1': 383.7082214355469,\n",
      " 'mean_episode_return_0': 60.466121673583984,\n",
      " 'mean_episode_return_1': 76.42752075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:12,597] After 316800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.2646179199219,\n",
      " 'loss_1': 383.7082214355469,\n",
      " 'mean_episode_return_0': 60.466121673583984,\n",
      " 'mean_episode_return_1': 76.42752075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:17,603] After 316800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.2646179199219,\n",
      " 'loss_1': 383.7082214355469,\n",
      " 'mean_episode_return_0': 60.466121673583984,\n",
      " 'mean_episode_return_1': 76.42752075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:22,608] After 323200 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 439.1350402832031,\n",
      " 'loss_1': 412.4856262207031,\n",
      " 'mean_episode_return_0': 60.476966857910156,\n",
      " 'mean_episode_return_1': 76.36664581298828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:27,609] After 323200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.1350402832031,\n",
      " 'loss_1': 412.4856262207031,\n",
      " 'mean_episode_return_0': 60.476966857910156,\n",
      " 'mean_episode_return_1': 76.36664581298828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:32,612] After 323200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.1350402832031,\n",
      " 'loss_1': 412.4856262207031,\n",
      " 'mean_episode_return_0': 60.476966857910156,\n",
      " 'mean_episode_return_1': 76.36664581298828}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:32:37,617] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:37,707] After 329600 frames: @ 1256.2 fps Stats:\n",
      "{'loss_0': 334.9119567871094,\n",
      " 'loss_1': 372.34881591796875,\n",
      " 'mean_episode_return_0': 60.434139251708984,\n",
      " 'mean_episode_return_1': 76.28498077392578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:42,713] After 329600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 334.9119567871094,\n",
      " 'loss_1': 372.34881591796875,\n",
      " 'mean_episode_return_0': 60.434139251708984,\n",
      " 'mean_episode_return_1': 76.28498077392578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:47,719] After 332800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 298.7652587890625,\n",
      " 'loss_1': 372.34881591796875,\n",
      " 'mean_episode_return_0': 60.50046920776367,\n",
      " 'mean_episode_return_1': 76.28498077392578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:52,724] After 336000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 298.7652587890625,\n",
      " 'loss_1': 341.48480224609375,\n",
      " 'mean_episode_return_0': 60.50046920776367,\n",
      " 'mean_episode_return_1': 76.14887237548828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:32:57,729] After 336000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 298.7652587890625,\n",
      " 'loss_1': 341.48480224609375,\n",
      " 'mean_episode_return_0': 60.50046920776367,\n",
      " 'mean_episode_return_1': 76.14887237548828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:02,735] After 342400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 309.4172058105469,\n",
      " 'loss_1': 392.4192199707031,\n",
      " 'mean_episode_return_0': 60.49745559692383,\n",
      " 'mean_episode_return_1': 76.10566711425781}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:07,740] After 342400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 309.4172058105469,\n",
      " 'loss_1': 392.4192199707031,\n",
      " 'mean_episode_return_0': 60.49745559692383,\n",
      " 'mean_episode_return_1': 76.10566711425781}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:12,746] After 342400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 309.4172058105469,\n",
      " 'loss_1': 392.4192199707031,\n",
      " 'mean_episode_return_0': 60.49745559692383,\n",
      " 'mean_episode_return_1': 76.10566711425781}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:17,751] After 348800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 345.43109130859375,\n",
      " 'loss_1': 388.2999267578125,\n",
      " 'mean_episode_return_0': 60.53091049194336,\n",
      " 'mean_episode_return_1': 75.99861907958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:22,757] After 348800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.43109130859375,\n",
      " 'loss_1': 388.2999267578125,\n",
      " 'mean_episode_return_0': 60.53091049194336,\n",
      " 'mean_episode_return_1': 75.99861907958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:27,763] After 352000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 325.44207763671875,\n",
      " 'loss_1': 388.2999267578125,\n",
      " 'mean_episode_return_0': 60.48862075805664,\n",
      " 'mean_episode_return_1': 75.99861907958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:32,769] After 355200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 325.44207763671875,\n",
      " 'loss_1': 335.23687744140625,\n",
      " 'mean_episode_return_0': 60.48862075805664,\n",
      " 'mean_episode_return_1': 75.92713165283203}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:33:37,773] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:37,864] After 355200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 325.44207763671875,\n",
      " 'loss_1': 335.23687744140625,\n",
      " 'mean_episode_return_0': 60.48862075805664,\n",
      " 'mean_episode_return_1': 75.92713165283203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:42,870] After 358400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 344.0233459472656,\n",
      " 'loss_1': 335.23687744140625,\n",
      " 'mean_episode_return_0': 60.553070068359375,\n",
      " 'mean_episode_return_1': 75.92713165283203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:47,876] After 361600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 344.0233459472656,\n",
      " 'loss_1': 352.6156921386719,\n",
      " 'mean_episode_return_0': 60.553070068359375,\n",
      " 'mean_episode_return_1': 75.74012756347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:52,882] After 361600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 344.0233459472656,\n",
      " 'loss_1': 352.6156921386719,\n",
      " 'mean_episode_return_0': 60.553070068359375,\n",
      " 'mean_episode_return_1': 75.74012756347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:33:57,888] After 364800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 303.6951904296875,\n",
      " 'loss_1': 352.6156921386719,\n",
      " 'mean_episode_return_0': 60.6131477355957,\n",
      " 'mean_episode_return_1': 75.74012756347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:02,894] After 368000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 303.6951904296875,\n",
      " 'loss_1': 364.00445556640625,\n",
      " 'mean_episode_return_0': 60.6131477355957,\n",
      " 'mean_episode_return_1': 75.6424331665039}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:07,900] After 371200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 325.29132080078125,\n",
      " 'loss_1': 364.00445556640625,\n",
      " 'mean_episode_return_0': 60.631675720214844,\n",
      " 'mean_episode_return_1': 75.6424331665039}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:12,906] After 374400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 325.29132080078125,\n",
      " 'loss_1': 371.8299255371094,\n",
      " 'mean_episode_return_0': 60.631675720214844,\n",
      " 'mean_episode_return_1': 75.57117462158203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:17,912] After 374400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 325.29132080078125,\n",
      " 'loss_1': 371.8299255371094,\n",
      " 'mean_episode_return_0': 60.631675720214844,\n",
      " 'mean_episode_return_1': 75.57117462158203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:34:22,916] After 377600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 318.2760925292969,\n",
      " 'loss_1': 371.8299255371094,\n",
      " 'mean_episode_return_0': 60.70490264892578,\n",
      " 'mean_episode_return_1': 75.57117462158203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:27,921] After 380800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 318.2760925292969,\n",
      " 'loss_1': 338.0424499511719,\n",
      " 'mean_episode_return_0': 60.70490264892578,\n",
      " 'mean_episode_return_1': 75.45771026611328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:32,924] After 380800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.2760925292969,\n",
      " 'loss_1': 338.0424499511719,\n",
      " 'mean_episode_return_0': 60.70490264892578,\n",
      " 'mean_episode_return_1': 75.45771026611328}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:34:37,927] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:37,997] After 384000 frames: @ 630.8 fps Stats:\n",
      "{'loss_0': 348.6032409667969,\n",
      " 'loss_1': 338.0424499511719,\n",
      " 'mean_episode_return_0': 60.7378044128418,\n",
      " 'mean_episode_return_1': 75.45771026611328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:43,000] After 387200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 348.6032409667969,\n",
      " 'loss_1': 316.8680725097656,\n",
      " 'mean_episode_return_0': 60.7378044128418,\n",
      " 'mean_episode_return_1': 75.3404312133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:48,005] After 387200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.6032409667969,\n",
      " 'loss_1': 316.8680725097656,\n",
      " 'mean_episode_return_0': 60.7378044128418,\n",
      " 'mean_episode_return_1': 75.3404312133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:53,011] After 390400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 305.2115478515625,\n",
      " 'loss_1': 316.8680725097656,\n",
      " 'mean_episode_return_0': 60.76471710205078,\n",
      " 'mean_episode_return_1': 75.3404312133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:34:58,017] After 393600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 305.2115478515625,\n",
      " 'loss_1': 389.2955322265625,\n",
      " 'mean_episode_return_0': 60.76471710205078,\n",
      " 'mean_episode_return_1': 75.2447280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:03,020] After 396800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 299.7406005859375,\n",
      " 'loss_1': 389.2955322265625,\n",
      " 'mean_episode_return_0': 60.80466842651367,\n",
      " 'mean_episode_return_1': 75.2447280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:08,024] After 400000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 299.7406005859375,\n",
      " 'loss_1': 322.04296875,\n",
      " 'mean_episode_return_0': 60.80466842651367,\n",
      " 'mean_episode_return_1': 75.13032531738281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:13,028] After 400000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 299.7406005859375,\n",
      " 'loss_1': 322.04296875,\n",
      " 'mean_episode_return_0': 60.80466842651367,\n",
      " 'mean_episode_return_1': 75.13032531738281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:18,032] After 403200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 300.4885559082031,\n",
      " 'loss_1': 322.04296875,\n",
      " 'mean_episode_return_0': 60.82177734375,\n",
      " 'mean_episode_return_1': 75.13032531738281}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:23,036] After 406400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 300.4885559082031,\n",
      " 'loss_1': 428.3703308105469,\n",
      " 'mean_episode_return_0': 60.82177734375,\n",
      " 'mean_episode_return_1': 75.07586669921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:28,040] After 406400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 300.4885559082031,\n",
      " 'loss_1': 428.3703308105469,\n",
      " 'mean_episode_return_0': 60.82177734375,\n",
      " 'mean_episode_return_1': 75.07586669921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:33,045] After 409600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 258.48193359375,\n",
      " 'loss_1': 428.3703308105469,\n",
      " 'mean_episode_return_0': 60.85394287109375,\n",
      " 'mean_episode_return_1': 75.07586669921875}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:35:38,051] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:38,141] After 412800 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 258.48193359375,\n",
      " 'loss_1': 321.132568359375,\n",
      " 'mean_episode_return_0': 60.85394287109375,\n",
      " 'mean_episode_return_1': 75.00360107421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:43,146] After 416000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 338.0285339355469,\n",
      " 'loss_1': 321.132568359375,\n",
      " 'mean_episode_return_0': 60.84223937988281,\n",
      " 'mean_episode_return_1': 75.00360107421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:48,152] After 416000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 338.0285339355469,\n",
      " 'loss_1': 321.132568359375,\n",
      " 'mean_episode_return_0': 60.84223937988281,\n",
      " 'mean_episode_return_1': 75.00360107421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:53,158] After 419200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 338.0285339355469,\n",
      " 'loss_1': 370.42974853515625,\n",
      " 'mean_episode_return_0': 60.84223937988281,\n",
      " 'mean_episode_return_1': 74.98321533203125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:35:58,164] After 422400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 303.1852722167969,\n",
      " 'loss_1': 370.42974853515625,\n",
      " 'mean_episode_return_0': 60.850276947021484,\n",
      " 'mean_episode_return_1': 74.98321533203125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:03,168] After 425600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 303.1852722167969,\n",
      " 'loss_1': 258.1146545410156,\n",
      " 'mean_episode_return_0': 60.850276947021484,\n",
      " 'mean_episode_return_1': 74.92425537109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:08,174] After 425600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 303.1852722167969,\n",
      " 'loss_1': 258.1146545410156,\n",
      " 'mean_episode_return_0': 60.850276947021484,\n",
      " 'mean_episode_return_1': 74.92425537109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:13,179] After 428800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 259.23858642578125,\n",
      " 'loss_1': 258.1146545410156,\n",
      " 'mean_episode_return_0': 60.82959747314453,\n",
      " 'mean_episode_return_1': 74.92425537109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:18,186] After 432000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 259.23858642578125,\n",
      " 'loss_1': 293.2246398925781,\n",
      " 'mean_episode_return_0': 60.82959747314453,\n",
      " 'mean_episode_return_1': 74.85913848876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:23,192] After 432000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 259.23858642578125,\n",
      " 'loss_1': 293.2246398925781,\n",
      " 'mean_episode_return_0': 60.82959747314453,\n",
      " 'mean_episode_return_1': 74.85913848876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:28,198] After 435200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 314.4871826171875,\n",
      " 'loss_1': 293.2246398925781,\n",
      " 'mean_episode_return_0': 60.85561752319336,\n",
      " 'mean_episode_return_1': 74.85913848876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:33,204] After 438400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 314.4871826171875,\n",
      " 'loss_1': 288.9431457519531,\n",
      " 'mean_episode_return_0': 60.85561752319336,\n",
      " 'mean_episode_return_1': 74.79080200195312}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:36:38,210] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:38,302] After 441600 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 260.89019775390625,\n",
      " 'loss_1': 288.9431457519531,\n",
      " 'mean_episode_return_0': 60.86249923706055,\n",
      " 'mean_episode_return_1': 74.79080200195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:43,308] After 441600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 260.89019775390625,\n",
      " 'loss_1': 288.9431457519531,\n",
      " 'mean_episode_return_0': 60.86249923706055,\n",
      " 'mean_episode_return_1': 74.79080200195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:48,314] After 444800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 260.89019775390625,\n",
      " 'loss_1': 271.11090087890625,\n",
      " 'mean_episode_return_0': 60.86249923706055,\n",
      " 'mean_episode_return_1': 74.68018341064453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:53,320] After 448000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 302.7052917480469,\n",
      " 'loss_1': 271.11090087890625,\n",
      " 'mean_episode_return_0': 60.91162109375,\n",
      " 'mean_episode_return_1': 74.68018341064453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:36:58,326] After 448000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 302.7052917480469,\n",
      " 'loss_1': 271.11090087890625,\n",
      " 'mean_episode_return_0': 60.91162109375,\n",
      " 'mean_episode_return_1': 74.68018341064453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:03,332] After 451200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 302.7052917480469,\n",
      " 'loss_1': 333.62823486328125,\n",
      " 'mean_episode_return_0': 60.91162109375,\n",
      " 'mean_episode_return_1': 74.60165405273438}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:08,338] After 454400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 261.3167419433594,\n",
      " 'loss_1': 333.62823486328125,\n",
      " 'mean_episode_return_0': 60.91145706176758,\n",
      " 'mean_episode_return_1': 74.60165405273438}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:37:13,344] After 457600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 261.3167419433594,\n",
      " 'loss_1': 366.18072509765625,\n",
      " 'mean_episode_return_0': 60.91145706176758,\n",
      " 'mean_episode_return_1': 74.56596374511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:18,348] After 460800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 314.5307922363281,\n",
      " 'loss_1': 366.18072509765625,\n",
      " 'mean_episode_return_0': 60.921661376953125,\n",
      " 'mean_episode_return_1': 74.56596374511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:23,354] After 460800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 314.5307922363281,\n",
      " 'loss_1': 366.18072509765625,\n",
      " 'mean_episode_return_0': 60.921661376953125,\n",
      " 'mean_episode_return_1': 74.56596374511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:28,360] After 464000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 314.5307922363281,\n",
      " 'loss_1': 390.451171875,\n",
      " 'mean_episode_return_0': 60.921661376953125,\n",
      " 'mean_episode_return_1': 74.55288696289062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:33,366] After 467200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 293.3240051269531,\n",
      " 'loss_1': 390.451171875,\n",
      " 'mean_episode_return_0': 60.94350051879883,\n",
      " 'mean_episode_return_1': 74.55288696289062}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:37:38,372] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:38,464] After 467200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 293.3240051269531,\n",
      " 'loss_1': 390.451171875,\n",
      " 'mean_episode_return_0': 60.94350051879883,\n",
      " 'mean_episode_return_1': 74.55288696289062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:43,470] After 470400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 293.3240051269531,\n",
      " 'loss_1': 370.55474853515625,\n",
      " 'mean_episode_return_0': 60.94350051879883,\n",
      " 'mean_episode_return_1': 74.48453521728516}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:48,476] After 473600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 274.676513671875,\n",
      " 'loss_1': 370.55474853515625,\n",
      " 'mean_episode_return_0': 60.97433853149414,\n",
      " 'mean_episode_return_1': 74.48453521728516}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:53,478] After 473600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 274.676513671875,\n",
      " 'loss_1': 370.55474853515625,\n",
      " 'mean_episode_return_0': 60.97433853149414,\n",
      " 'mean_episode_return_1': 74.48453521728516}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:37:58,484] After 480000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 275.98406982421875,\n",
      " 'loss_1': 296.6730651855469,\n",
      " 'mean_episode_return_0': 60.971961975097656,\n",
      " 'mean_episode_return_1': 74.42326354980469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:03,488] After 480000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 275.98406982421875,\n",
      " 'loss_1': 296.6730651855469,\n",
      " 'mean_episode_return_0': 60.971961975097656,\n",
      " 'mean_episode_return_1': 74.42326354980469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:08,494] After 483200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 275.98406982421875,\n",
      " 'loss_1': 484.07513427734375,\n",
      " 'mean_episode_return_0': 60.971961975097656,\n",
      " 'mean_episode_return_1': 74.43347930908203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:13,500] After 486400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 308.7544860839844,\n",
      " 'loss_1': 484.07513427734375,\n",
      " 'mean_episode_return_0': 60.971107482910156,\n",
      " 'mean_episode_return_1': 74.43347930908203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:18,504] After 486400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 308.7544860839844,\n",
      " 'loss_1': 484.07513427734375,\n",
      " 'mean_episode_return_0': 60.971107482910156,\n",
      " 'mean_episode_return_1': 74.43347930908203}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:23,510] After 489600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 308.7544860839844,\n",
      " 'loss_1': 272.3108215332031,\n",
      " 'mean_episode_return_0': 60.971107482910156,\n",
      " 'mean_episode_return_1': 74.38264465332031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:28,516] After 492800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.75811767578125,\n",
      " 'loss_1': 272.3108215332031,\n",
      " 'mean_episode_return_0': 60.99607467651367,\n",
      " 'mean_episode_return_1': 74.38264465332031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:33,522] After 492800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 327.75811767578125,\n",
      " 'loss_1': 272.3108215332031,\n",
      " 'mean_episode_return_0': 60.99607467651367,\n",
      " 'mean_episode_return_1': 74.38264465332031}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:38:38,527] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:38,620] After 499200 frames: @ 1255.6 fps Stats:\n",
      "{'loss_0': 273.7488708496094,\n",
      " 'loss_1': 315.1649169921875,\n",
      " 'mean_episode_return_0': 61.02769088745117,\n",
      " 'mean_episode_return_1': 74.336181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:43,626] After 499200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 273.7488708496094,\n",
      " 'loss_1': 315.1649169921875,\n",
      " 'mean_episode_return_0': 61.02769088745117,\n",
      " 'mean_episode_return_1': 74.336181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:48,632] After 499200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 273.7488708496094,\n",
      " 'loss_1': 315.1649169921875,\n",
      " 'mean_episode_return_0': 61.02769088745117,\n",
      " 'mean_episode_return_1': 74.336181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:53,637] After 505600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 332.1054992675781,\n",
      " 'loss_1': 238.62252807617188,\n",
      " 'mean_episode_return_0': 61.07343673706055,\n",
      " 'mean_episode_return_1': 74.24267578125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:38:58,643] After 505600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 332.1054992675781,\n",
      " 'loss_1': 238.62252807617188,\n",
      " 'mean_episode_return_0': 61.07343673706055,\n",
      " 'mean_episode_return_1': 74.24267578125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:03,648] After 508800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 332.1054992675781,\n",
      " 'loss_1': 386.9586486816406,\n",
      " 'mean_episode_return_0': 61.07343673706055,\n",
      " 'mean_episode_return_1': 74.23358917236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:08,652] After 512000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 310.6972351074219,\n",
      " 'loss_1': 386.9586486816406,\n",
      " 'mean_episode_return_0': 61.10956954956055,\n",
      " 'mean_episode_return_1': 74.23358917236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:13,657] After 512000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 310.6972351074219,\n",
      " 'loss_1': 386.9586486816406,\n",
      " 'mean_episode_return_0': 61.10956954956055,\n",
      " 'mean_episode_return_1': 74.23358917236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:18,663] After 515200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 310.6972351074219,\n",
      " 'loss_1': 293.8351135253906,\n",
      " 'mean_episode_return_0': 61.10956954956055,\n",
      " 'mean_episode_return_1': 74.181884765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:23,670] After 518400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 283.8174133300781,\n",
      " 'loss_1': 293.8351135253906,\n",
      " 'mean_episode_return_0': 61.12446212768555,\n",
      " 'mean_episode_return_1': 74.181884765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:28,676] After 518400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 283.8174133300781,\n",
      " 'loss_1': 293.8351135253906,\n",
      " 'mean_episode_return_0': 61.12446212768555,\n",
      " 'mean_episode_return_1': 74.181884765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:33,680] After 524800 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 272.7553405761719,\n",
      " 'loss_1': 368.6531066894531,\n",
      " 'mean_episode_return_0': 61.11859893798828,\n",
      " 'mean_episode_return_1': 74.14073181152344}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:39:38,685] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:38,762] After 524800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 272.7553405761719,\n",
      " 'loss_1': 368.6531066894531,\n",
      " 'mean_episode_return_0': 61.11859893798828,\n",
      " 'mean_episode_return_1': 74.14073181152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:43,768] After 524800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 272.7553405761719,\n",
      " 'loss_1': 368.6531066894531,\n",
      " 'mean_episode_return_0': 61.11859893798828,\n",
      " 'mean_episode_return_1': 74.14073181152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:48,774] After 531200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 394.8684387207031,\n",
      " 'loss_1': 348.1507263183594,\n",
      " 'mean_episode_return_0': 61.166744232177734,\n",
      " 'mean_episode_return_1': 74.12853240966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:53,780] After 531200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 394.8684387207031,\n",
      " 'loss_1': 348.1507263183594,\n",
      " 'mean_episode_return_0': 61.166744232177734,\n",
      " 'mean_episode_return_1': 74.12853240966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:39:58,784] After 534400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 394.8684387207031,\n",
      " 'loss_1': 407.9398498535156,\n",
      " 'mean_episode_return_0': 61.166744232177734,\n",
      " 'mean_episode_return_1': 74.07908630371094}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:40:03,788] After 537600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 307.7543640136719,\n",
      " 'loss_1': 407.9398498535156,\n",
      " 'mean_episode_return_0': 61.196617126464844,\n",
      " 'mean_episode_return_1': 74.07908630371094}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:08,792] After 537600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 307.7543640136719,\n",
      " 'loss_1': 407.9398498535156,\n",
      " 'mean_episode_return_0': 61.196617126464844,\n",
      " 'mean_episode_return_1': 74.07908630371094}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:13,796] After 544000 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 296.3932189941406,\n",
      " 'loss_1': 329.7781066894531,\n",
      " 'mean_episode_return_0': 61.197235107421875,\n",
      " 'mean_episode_return_1': 74.05258178710938}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:18,802] After 544000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 296.3932189941406,\n",
      " 'loss_1': 329.7781066894531,\n",
      " 'mean_episode_return_0': 61.197235107421875,\n",
      " 'mean_episode_return_1': 74.05258178710938}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:23,808] After 544000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 296.3932189941406,\n",
      " 'loss_1': 329.7781066894531,\n",
      " 'mean_episode_return_0': 61.197235107421875,\n",
      " 'mean_episode_return_1': 74.05258178710938}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:28,814] After 550400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 278.3184509277344,\n",
      " 'loss_1': 359.3689880371094,\n",
      " 'mean_episode_return_0': 61.21867752075195,\n",
      " 'mean_episode_return_1': 73.98729705810547}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:33,820] After 550400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 278.3184509277344,\n",
      " 'loss_1': 359.3689880371094,\n",
      " 'mean_episode_return_0': 61.21867752075195,\n",
      " 'mean_episode_return_1': 73.98729705810547}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:40:38,823] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:38,917] After 550400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 278.3184509277344,\n",
      " 'loss_1': 359.3689880371094,\n",
      " 'mean_episode_return_0': 61.21867752075195,\n",
      " 'mean_episode_return_1': 73.98729705810547}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:43,923] After 556800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 289.5581970214844,\n",
      " 'loss_1': 276.8498229980469,\n",
      " 'mean_episode_return_0': 61.22258758544922,\n",
      " 'mean_episode_return_1': 73.97000122070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:48,929] After 556800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.5581970214844,\n",
      " 'loss_1': 276.8498229980469,\n",
      " 'mean_episode_return_0': 61.22258758544922,\n",
      " 'mean_episode_return_1': 73.97000122070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:53,934] After 556800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.5581970214844,\n",
      " 'loss_1': 276.8498229980469,\n",
      " 'mean_episode_return_0': 61.22258758544922,\n",
      " 'mean_episode_return_1': 73.97000122070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:40:58,936] After 563200 frames: @ 1279.7 fps Stats:\n",
      "{'loss_0': 281.16168212890625,\n",
      " 'loss_1': 300.2514343261719,\n",
      " 'mean_episode_return_0': 61.22935104370117,\n",
      " 'mean_episode_return_1': 73.9222640991211}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:03,942] After 563200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 281.16168212890625,\n",
      " 'loss_1': 300.2514343261719,\n",
      " 'mean_episode_return_0': 61.22935104370117,\n",
      " 'mean_episode_return_1': 73.9222640991211}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:08,948] After 569600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 276.8324890136719,\n",
      " 'loss_1': 260.0950927734375,\n",
      " 'mean_episode_return_0': 61.25319290161133,\n",
      " 'mean_episode_return_1': 73.86772155761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:13,954] After 569600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 276.8324890136719,\n",
      " 'loss_1': 260.0950927734375,\n",
      " 'mean_episode_return_0': 61.25319290161133,\n",
      " 'mean_episode_return_1': 73.86772155761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:18,958] After 569600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 276.8324890136719,\n",
      " 'loss_1': 260.0950927734375,\n",
      " 'mean_episode_return_0': 61.25319290161133,\n",
      " 'mean_episode_return_1': 73.86772155761719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:23,964] After 576000 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 286.9809265136719,\n",
      " 'loss_1': 318.1372375488281,\n",
      " 'mean_episode_return_0': 61.27843475341797,\n",
      " 'mean_episode_return_1': 73.83618927001953}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:28,970] After 576000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 286.9809265136719,\n",
      " 'loss_1': 318.1372375488281,\n",
      " 'mean_episode_return_0': 61.27843475341797,\n",
      " 'mean_episode_return_1': 73.83618927001953}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:33,972] After 576000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 286.9809265136719,\n",
      " 'loss_1': 318.1372375488281,\n",
      " 'mean_episode_return_0': 61.27843475341797,\n",
      " 'mean_episode_return_1': 73.83618927001953}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:41:38,975] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:39,062] After 582400 frames: @ 1257.4 fps Stats:\n",
      "{'loss_0': 267.6469421386719,\n",
      " 'loss_1': 407.14483642578125,\n",
      " 'mean_episode_return_0': 61.25693130493164,\n",
      " 'mean_episode_return_1': 73.85592651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:44,068] After 582400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 267.6469421386719,\n",
      " 'loss_1': 407.14483642578125,\n",
      " 'mean_episode_return_0': 61.25693130493164,\n",
      " 'mean_episode_return_1': 73.85592651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:49,074] After 582400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 267.6469421386719,\n",
      " 'loss_1': 407.14483642578125,\n",
      " 'mean_episode_return_0': 61.25693130493164,\n",
      " 'mean_episode_return_1': 73.85592651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:54,080] After 588800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 244.4019012451172,\n",
      " 'loss_1': 275.2527160644531,\n",
      " 'mean_episode_return_0': 61.260684967041016,\n",
      " 'mean_episode_return_1': 73.83261108398438}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:41:59,086] After 588800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 244.4019012451172,\n",
      " 'loss_1': 275.2527160644531,\n",
      " 'mean_episode_return_0': 61.260684967041016,\n",
      " 'mean_episode_return_1': 73.83261108398438}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:04,092] After 595200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 301.6232604980469,\n",
      " 'loss_1': 291.1579895019531,\n",
      " 'mean_episode_return_0': 61.31256103515625,\n",
      " 'mean_episode_return_1': 73.770263671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:09,096] After 595200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 301.6232604980469,\n",
      " 'loss_1': 291.1579895019531,\n",
      " 'mean_episode_return_0': 61.31256103515625,\n",
      " 'mean_episode_return_1': 73.770263671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:14,102] After 595200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 301.6232604980469,\n",
      " 'loss_1': 291.1579895019531,\n",
      " 'mean_episode_return_0': 61.31256103515625,\n",
      " 'mean_episode_return_1': 73.770263671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:19,108] After 601600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 282.1775207519531,\n",
      " 'loss_1': 294.8145751953125,\n",
      " 'mean_episode_return_0': 61.32341766357422,\n",
      " 'mean_episode_return_1': 73.71680450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:24,114] After 601600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.1775207519531,\n",
      " 'loss_1': 294.8145751953125,\n",
      " 'mean_episode_return_0': 61.32341766357422,\n",
      " 'mean_episode_return_1': 73.71680450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:29,120] After 601600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.1775207519531,\n",
      " 'loss_1': 294.8145751953125,\n",
      " 'mean_episode_return_0': 61.32341766357422,\n",
      " 'mean_episode_return_1': 73.71680450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:34,124] After 608000 frames: @ 1279.0 fps Stats:\n",
      "{'loss_0': 231.16995239257812,\n",
      " 'loss_1': 307.2403564453125,\n",
      " 'mean_episode_return_0': 61.31978988647461,\n",
      " 'mean_episode_return_1': 73.68712615966797}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:42:39,127] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:39,220] After 608000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 231.16995239257812,\n",
      " 'loss_1': 307.2403564453125,\n",
      " 'mean_episode_return_0': 61.31978988647461,\n",
      " 'mean_episode_return_1': 73.68712615966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:44,224] After 611200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 264.1239318847656,\n",
      " 'loss_1': 307.2403564453125,\n",
      " 'mean_episode_return_0': 61.34890365600586,\n",
      " 'mean_episode_return_1': 73.68712615966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:49,225] After 614400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 264.1239318847656,\n",
      " 'loss_1': 282.355712890625,\n",
      " 'mean_episode_return_0': 61.34890365600586,\n",
      " 'mean_episode_return_1': 73.60498809814453}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:42:54,231] After 614400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 264.1239318847656,\n",
      " 'loss_1': 282.355712890625,\n",
      " 'mean_episode_return_0': 61.34890365600586,\n",
      " 'mean_episode_return_1': 73.60498809814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:42:59,236] After 617600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 297.7020263671875,\n",
      " 'loss_1': 282.355712890625,\n",
      " 'mean_episode_return_0': 61.407203674316406,\n",
      " 'mean_episode_return_1': 73.60498809814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:04,242] After 620800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 297.7020263671875,\n",
      " 'loss_1': 283.24560546875,\n",
      " 'mean_episode_return_0': 61.407203674316406,\n",
      " 'mean_episode_return_1': 73.57015991210938}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:09,247] After 620800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 297.7020263671875,\n",
      " 'loss_1': 283.24560546875,\n",
      " 'mean_episode_return_0': 61.407203674316406,\n",
      " 'mean_episode_return_1': 73.57015991210938}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:14,253] After 627200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 311.42572021484375,\n",
      " 'loss_1': 260.4497985839844,\n",
      " 'mean_episode_return_0': 61.41899871826172,\n",
      " 'mean_episode_return_1': 73.53547668457031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:19,256] After 627200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 311.42572021484375,\n",
      " 'loss_1': 260.4497985839844,\n",
      " 'mean_episode_return_0': 61.41899871826172,\n",
      " 'mean_episode_return_1': 73.53547668457031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:24,261] After 627200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 311.42572021484375,\n",
      " 'loss_1': 260.4497985839844,\n",
      " 'mean_episode_return_0': 61.41899871826172,\n",
      " 'mean_episode_return_1': 73.53547668457031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:29,267] After 633600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 246.2676544189453,\n",
      " 'loss_1': 302.2786560058594,\n",
      " 'mean_episode_return_0': 61.4162483215332,\n",
      " 'mean_episode_return_1': 73.52413177490234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:34,273] After 633600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 246.2676544189453,\n",
      " 'loss_1': 302.2786560058594,\n",
      " 'mean_episode_return_0': 61.4162483215332,\n",
      " 'mean_episode_return_1': 73.52413177490234}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:43:39,279] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:39,370] After 636800 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 258.8749084472656,\n",
      " 'loss_1': 302.2786560058594,\n",
      " 'mean_episode_return_0': 61.42900085449219,\n",
      " 'mean_episode_return_1': 73.52413177490234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:44,376] After 640000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 258.8749084472656,\n",
      " 'loss_1': 322.4798278808594,\n",
      " 'mean_episode_return_0': 61.42900085449219,\n",
      " 'mean_episode_return_1': 73.50578308105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:49,382] After 640000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 258.8749084472656,\n",
      " 'loss_1': 322.4798278808594,\n",
      " 'mean_episode_return_0': 61.42900085449219,\n",
      " 'mean_episode_return_1': 73.50578308105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:54,387] After 643200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 307.5877685546875,\n",
      " 'loss_1': 322.4798278808594,\n",
      " 'mean_episode_return_0': 61.52174758911133,\n",
      " 'mean_episode_return_1': 73.50578308105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:43:59,393] After 646400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 307.5877685546875,\n",
      " 'loss_1': 283.383544921875,\n",
      " 'mean_episode_return_0': 61.52174758911133,\n",
      " 'mean_episode_return_1': 73.42826843261719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:04,399] After 646400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 307.5877685546875,\n",
      " 'loss_1': 283.383544921875,\n",
      " 'mean_episode_return_0': 61.52174758911133,\n",
      " 'mean_episode_return_1': 73.42826843261719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:09,405] After 649600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 291.8760681152344,\n",
      " 'loss_1': 283.383544921875,\n",
      " 'mean_episode_return_0': 61.5286865234375,\n",
      " 'mean_episode_return_1': 73.42826843261719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:14,411] After 652800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 291.8760681152344,\n",
      " 'loss_1': 293.2312927246094,\n",
      " 'mean_episode_return_0': 61.5286865234375,\n",
      " 'mean_episode_return_1': 73.35101318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:19,417] After 652800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 291.8760681152344,\n",
      " 'loss_1': 293.2312927246094,\n",
      " 'mean_episode_return_0': 61.5286865234375,\n",
      " 'mean_episode_return_1': 73.35101318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:24,423] After 659200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 281.4083251953125,\n",
      " 'loss_1': 321.3214111328125,\n",
      " 'mean_episode_return_0': 61.51712417602539,\n",
      " 'mean_episode_return_1': 73.31813049316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:29,429] After 659200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 281.4083251953125,\n",
      " 'loss_1': 321.3214111328125,\n",
      " 'mean_episode_return_0': 61.51712417602539,\n",
      " 'mean_episode_return_1': 73.31813049316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:34,433] After 662400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 229.6598358154297,\n",
      " 'loss_1': 321.3214111328125,\n",
      " 'mean_episode_return_0': 61.512001037597656,\n",
      " 'mean_episode_return_1': 73.31813049316406}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:44:39,436] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:39,525] After 665600 frames: @ 628.6 fps Stats:\n",
      "{'loss_0': 229.6598358154297,\n",
      " 'loss_1': 247.57289123535156,\n",
      " 'mean_episode_return_0': 61.512001037597656,\n",
      " 'mean_episode_return_1': 73.25625610351562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:44,531] After 665600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 229.6598358154297,\n",
      " 'loss_1': 247.57289123535156,\n",
      " 'mean_episode_return_0': 61.512001037597656,\n",
      " 'mean_episode_return_1': 73.25625610351562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:49,533] After 668800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 247.23779296875,\n",
      " 'loss_1': 247.57289123535156,\n",
      " 'mean_episode_return_0': 61.53268814086914,\n",
      " 'mean_episode_return_1': 73.25625610351562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:54,539] After 672000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 247.23779296875,\n",
      " 'loss_1': 319.8121337890625,\n",
      " 'mean_episode_return_0': 61.53268814086914,\n",
      " 'mean_episode_return_1': 73.18080139160156}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:44:59,545] After 672000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 247.23779296875,\n",
      " 'loss_1': 319.8121337890625,\n",
      " 'mean_episode_return_0': 61.53268814086914,\n",
      " 'mean_episode_return_1': 73.18080139160156}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:04,551] After 675200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.5969543457031,\n",
      " 'loss_1': 319.8121337890625,\n",
      " 'mean_episode_return_0': 61.539939880371094,\n",
      " 'mean_episode_return_1': 73.18080139160156}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:09,557] After 678400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.5969543457031,\n",
      " 'loss_1': 299.1320495605469,\n",
      " 'mean_episode_return_0': 61.539939880371094,\n",
      " 'mean_episode_return_1': 73.05150604248047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:14,563] After 681600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 251.1108856201172,\n",
      " 'loss_1': 299.1320495605469,\n",
      " 'mean_episode_return_0': 61.56418991088867,\n",
      " 'mean_episode_return_1': 73.05150604248047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:19,568] After 681600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 251.1108856201172,\n",
      " 'loss_1': 299.1320495605469,\n",
      " 'mean_episode_return_0': 61.56418991088867,\n",
      " 'mean_episode_return_1': 73.05150604248047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:24,569] After 684800 frames: @ 640.0 fps Stats:\n",
      "{'loss_0': 251.1108856201172,\n",
      " 'loss_1': 345.65484619140625,\n",
      " 'mean_episode_return_0': 61.56418991088867,\n",
      " 'mean_episode_return_1': 72.95867919921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:29,575] After 688000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 328.06610107421875,\n",
      " 'loss_1': 345.65484619140625,\n",
      " 'mean_episode_return_0': 61.59206008911133,\n",
      " 'mean_episode_return_1': 72.95867919921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:34,581] After 691200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 328.06610107421875,\n",
      " 'loss_1': 335.7108459472656,\n",
      " 'mean_episode_return_0': 61.59206008911133,\n",
      " 'mean_episode_return_1': 72.80419921875}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:45:39,583] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:39,655] After 691200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 328.06610107421875,\n",
      " 'loss_1': 335.7108459472656,\n",
      " 'mean_episode_return_0': 61.59206008911133,\n",
      " 'mean_episode_return_1': 72.80419921875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:45:44,661] After 694400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 322.83953857421875,\n",
      " 'loss_1': 335.7108459472656,\n",
      " 'mean_episode_return_0': 61.670623779296875,\n",
      " 'mean_episode_return_1': 72.80419921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:49,667] After 697600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 322.83953857421875,\n",
      " 'loss_1': 314.81878662109375,\n",
      " 'mean_episode_return_0': 61.670623779296875,\n",
      " 'mean_episode_return_1': 72.73818969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:54,671] After 700800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 263.1881103515625,\n",
      " 'loss_1': 314.81878662109375,\n",
      " 'mean_episode_return_0': 61.70893478393555,\n",
      " 'mean_episode_return_1': 72.73818969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:45:59,677] After 700800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 263.1881103515625,\n",
      " 'loss_1': 314.81878662109375,\n",
      " 'mean_episode_return_0': 61.70893478393555,\n",
      " 'mean_episode_return_1': 72.73818969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:04,683] After 704000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 263.1881103515625,\n",
      " 'loss_1': 294.7624206542969,\n",
      " 'mean_episode_return_0': 61.70893478393555,\n",
      " 'mean_episode_return_1': 72.5938720703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:09,688] After 707200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 270.5528564453125,\n",
      " 'loss_1': 294.7624206542969,\n",
      " 'mean_episode_return_0': 61.72074890136719,\n",
      " 'mean_episode_return_1': 72.5938720703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:14,693] After 707200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 270.5528564453125,\n",
      " 'loss_1': 294.7624206542969,\n",
      " 'mean_episode_return_0': 61.72074890136719,\n",
      " 'mean_episode_return_1': 72.5938720703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:19,699] After 710400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 270.5528564453125,\n",
      " 'loss_1': 363.8102722167969,\n",
      " 'mean_episode_return_0': 61.72074890136719,\n",
      " 'mean_episode_return_1': 72.44332885742188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:24,705] After 713600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 292.9151916503906,\n",
      " 'loss_1': 363.8102722167969,\n",
      " 'mean_episode_return_0': 61.748313903808594,\n",
      " 'mean_episode_return_1': 72.44332885742188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:29,706] After 713600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 292.9151916503906,\n",
      " 'loss_1': 363.8102722167969,\n",
      " 'mean_episode_return_0': 61.748313903808594,\n",
      " 'mean_episode_return_1': 72.44332885742188}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:34,712] After 716800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 292.9151916503906,\n",
      " 'loss_1': 323.8401794433594,\n",
      " 'mean_episode_return_0': 61.748313903808594,\n",
      " 'mean_episode_return_1': 72.33748626708984}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:46:39,718] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:39,803] After 720000 frames: @ 628.7 fps Stats:\n",
      "{'loss_0': 318.2248840332031,\n",
      " 'loss_1': 323.8401794433594,\n",
      " 'mean_episode_return_0': 61.804500579833984,\n",
      " 'mean_episode_return_1': 72.33748626708984}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:44,809] After 723200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 318.2248840332031,\n",
      " 'loss_1': 302.83245849609375,\n",
      " 'mean_episode_return_0': 61.804500579833984,\n",
      " 'mean_episode_return_1': 72.24208068847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:49,812] After 726400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 222.16136169433594,\n",
      " 'loss_1': 302.83245849609375,\n",
      " 'mean_episode_return_0': 61.83681106567383,\n",
      " 'mean_episode_return_1': 72.24208068847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:54,817] After 726400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 222.16136169433594,\n",
      " 'loss_1': 302.83245849609375,\n",
      " 'mean_episode_return_0': 61.83681106567383,\n",
      " 'mean_episode_return_1': 72.24208068847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:46:59,823] After 729600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 222.16136169433594,\n",
      " 'loss_1': 285.05914306640625,\n",
      " 'mean_episode_return_0': 61.83681106567383,\n",
      " 'mean_episode_return_1': 72.10285186767578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:04,828] After 732800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 340.4268798828125,\n",
      " 'loss_1': 285.05914306640625,\n",
      " 'mean_episode_return_0': 61.907249450683594,\n",
      " 'mean_episode_return_1': 72.10285186767578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:09,832] After 732800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 340.4268798828125,\n",
      " 'loss_1': 285.05914306640625,\n",
      " 'mean_episode_return_0': 61.907249450683594,\n",
      " 'mean_episode_return_1': 72.10285186767578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:14,836] After 736000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 340.4268798828125,\n",
      " 'loss_1': 366.20648193359375,\n",
      " 'mean_episode_return_0': 61.907249450683594,\n",
      " 'mean_episode_return_1': 71.96649169921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:19,840] After 739200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 316.6894226074219,\n",
      " 'loss_1': 366.20648193359375,\n",
      " 'mean_episode_return_0': 61.952186584472656,\n",
      " 'mean_episode_return_1': 71.96649169921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:24,844] After 739200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 316.6894226074219,\n",
      " 'loss_1': 366.20648193359375,\n",
      " 'mean_episode_return_0': 61.952186584472656,\n",
      " 'mean_episode_return_1': 71.96649169921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:29,850] After 742400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 316.6894226074219,\n",
      " 'loss_1': 316.2657165527344,\n",
      " 'mean_episode_return_0': 61.952186584472656,\n",
      " 'mean_episode_return_1': 71.81037902832031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:34,856] After 745600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 281.5338134765625,\n",
      " 'loss_1': 316.2657165527344,\n",
      " 'mean_episode_return_0': 61.985877990722656,\n",
      " 'mean_episode_return_1': 71.81037902832031}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:47:39,861] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:39,952] After 748800 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 281.5338134765625,\n",
      " 'loss_1': 316.2298889160156,\n",
      " 'mean_episode_return_0': 61.985877990722656,\n",
      " 'mean_episode_return_1': 71.67095184326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:44,958] After 752000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 270.59661865234375,\n",
      " 'loss_1': 316.2298889160156,\n",
      " 'mean_episode_return_0': 62.06087112426758,\n",
      " 'mean_episode_return_1': 71.67095184326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:49,964] After 752000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 270.59661865234375,\n",
      " 'loss_1': 316.2298889160156,\n",
      " 'mean_episode_return_0': 62.06087112426758,\n",
      " 'mean_episode_return_1': 71.67095184326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:54,970] After 755200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 270.59661865234375,\n",
      " 'loss_1': 281.3143310546875,\n",
      " 'mean_episode_return_0': 62.06087112426758,\n",
      " 'mean_episode_return_1': 71.62052917480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:47:59,972] After 758400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 279.1549377441406,\n",
      " 'loss_1': 281.3143310546875,\n",
      " 'mean_episode_return_0': 62.10487747192383,\n",
      " 'mean_episode_return_1': 71.62052917480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:04,978] After 758400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.1549377441406,\n",
      " 'loss_1': 281.3143310546875,\n",
      " 'mean_episode_return_0': 62.10487747192383,\n",
      " 'mean_episode_return_1': 71.62052917480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:09,982] After 761600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 279.1549377441406,\n",
      " 'loss_1': 305.3762512207031,\n",
      " 'mean_episode_return_0': 62.10487747192383,\n",
      " 'mean_episode_return_1': 71.51991271972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:14,988] After 764800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 243.86537170410156,\n",
      " 'loss_1': 305.3762512207031,\n",
      " 'mean_episode_return_0': 62.089500427246094,\n",
      " 'mean_episode_return_1': 71.51991271972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:19,994] After 764800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 243.86537170410156,\n",
      " 'loss_1': 305.3762512207031,\n",
      " 'mean_episode_return_0': 62.089500427246094,\n",
      " 'mean_episode_return_1': 71.51991271972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:25,000] After 768000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 243.86537170410156,\n",
      " 'loss_1': 393.9615478515625,\n",
      " 'mean_episode_return_0': 62.089500427246094,\n",
      " 'mean_episode_return_1': 71.5206069946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:30,006] After 771200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 304.9992370605469,\n",
      " 'loss_1': 393.9615478515625,\n",
      " 'mean_episode_return_0': 62.10443878173828,\n",
      " 'mean_episode_return_1': 71.5206069946289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:48:35,012] After 771200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 304.9992370605469,\n",
      " 'loss_1': 393.9615478515625,\n",
      " 'mean_episode_return_0': 62.10443878173828,\n",
      " 'mean_episode_return_1': 71.5206069946289}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:48:40,016] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:40,095] After 777600 frames: @ 1259.2 fps Stats:\n",
      "{'loss_0': 259.2247314453125,\n",
      " 'loss_1': 375.7250671386719,\n",
      " 'mean_episode_return_0': 62.073123931884766,\n",
      " 'mean_episode_return_1': 71.48577117919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:45,100] After 777600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 259.2247314453125,\n",
      " 'loss_1': 375.7250671386719,\n",
      " 'mean_episode_return_0': 62.073123931884766,\n",
      " 'mean_episode_return_1': 71.48577117919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:50,104] After 780800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 259.2247314453125,\n",
      " 'loss_1': 405.572021484375,\n",
      " 'mean_episode_return_0': 62.073123931884766,\n",
      " 'mean_episode_return_1': 71.4857177734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:48:55,110] After 784000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 258.6964111328125,\n",
      " 'loss_1': 405.572021484375,\n",
      " 'mean_episode_return_0': 62.10225296020508,\n",
      " 'mean_episode_return_1': 71.4857177734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:00,112] After 784000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 258.6964111328125,\n",
      " 'loss_1': 405.572021484375,\n",
      " 'mean_episode_return_0': 62.10225296020508,\n",
      " 'mean_episode_return_1': 71.4857177734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:05,118] After 787200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 258.6964111328125,\n",
      " 'loss_1': 262.0560607910156,\n",
      " 'mean_episode_return_0': 62.10225296020508,\n",
      " 'mean_episode_return_1': 71.41938018798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:10,124] After 790400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 255.5990447998047,\n",
      " 'loss_1': 262.0560607910156,\n",
      " 'mean_episode_return_0': 62.108436584472656,\n",
      " 'mean_episode_return_1': 71.41938018798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:15,130] After 790400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 255.5990447998047,\n",
      " 'loss_1': 262.0560607910156,\n",
      " 'mean_episode_return_0': 62.108436584472656,\n",
      " 'mean_episode_return_1': 71.41938018798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:20,136] After 796800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 342.3045349121094,\n",
      " 'loss_1': 361.1405334472656,\n",
      " 'mean_episode_return_0': 62.12018585205078,\n",
      " 'mean_episode_return_1': 71.35498809814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:25,142] After 796800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 342.3045349121094,\n",
      " 'loss_1': 361.1405334472656,\n",
      " 'mean_episode_return_0': 62.12018585205078,\n",
      " 'mean_episode_return_1': 71.35498809814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:30,148] After 796800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 342.3045349121094,\n",
      " 'loss_1': 361.1405334472656,\n",
      " 'mean_episode_return_0': 62.12018585205078,\n",
      " 'mean_episode_return_1': 71.35498809814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:35,154] After 803200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 398.238037109375,\n",
      " 'loss_1': 329.38409423828125,\n",
      " 'mean_episode_return_0': 62.14968490600586,\n",
      " 'mean_episode_return_1': 71.37035369873047}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:49:40,155] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:40,231] After 803200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 398.238037109375,\n",
      " 'loss_1': 329.38409423828125,\n",
      " 'mean_episode_return_0': 62.14968490600586,\n",
      " 'mean_episode_return_1': 71.37035369873047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:45,236] After 803200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 398.238037109375,\n",
      " 'loss_1': 329.38409423828125,\n",
      " 'mean_episode_return_0': 62.14968490600586,\n",
      " 'mean_episode_return_1': 71.37035369873047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:50,242] After 809600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 241.9207305908203,\n",
      " 'loss_1': 364.3510437011719,\n",
      " 'mean_episode_return_0': 62.17562484741211,\n",
      " 'mean_episode_return_1': 71.34723663330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:49:55,247] After 809600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 241.9207305908203,\n",
      " 'loss_1': 364.3510437011719,\n",
      " 'mean_episode_return_0': 62.17562484741211,\n",
      " 'mean_episode_return_1': 71.34723663330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:00,253] After 816000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 283.1535339355469,\n",
      " 'loss_1': 316.02618408203125,\n",
      " 'mean_episode_return_0': 62.19562911987305,\n",
      " 'mean_episode_return_1': 71.38288116455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:05,256] After 816000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 283.1535339355469,\n",
      " 'loss_1': 316.02618408203125,\n",
      " 'mean_episode_return_0': 62.19562911987305,\n",
      " 'mean_episode_return_1': 71.38288116455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:10,258] After 816000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 283.1535339355469,\n",
      " 'loss_1': 316.02618408203125,\n",
      " 'mean_episode_return_0': 62.19562911987305,\n",
      " 'mean_episode_return_1': 71.38288116455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:15,264] After 822400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 279.28564453125,\n",
      " 'loss_1': 271.97430419921875,\n",
      " 'mean_episode_return_0': 62.22624969482422,\n",
      " 'mean_episode_return_1': 71.36742401123047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:20,270] After 822400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.28564453125,\n",
      " 'loss_1': 271.97430419921875,\n",
      " 'mean_episode_return_0': 62.22624969482422,\n",
      " 'mean_episode_return_1': 71.36742401123047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:25,276] After 822400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.28564453125,\n",
      " 'loss_1': 271.97430419921875,\n",
      " 'mean_episode_return_0': 62.22624969482422,\n",
      " 'mean_episode_return_1': 71.36742401123047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:30,282] After 828800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 313.3960876464844,\n",
      " 'loss_1': 270.7071533203125,\n",
      " 'mean_episode_return_0': 62.21050262451172,\n",
      " 'mean_episode_return_1': 71.348876953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:35,288] After 828800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 313.3960876464844,\n",
      " 'loss_1': 270.7071533203125,\n",
      " 'mean_episode_return_0': 62.21050262451172,\n",
      " 'mean_episode_return_1': 71.348876953125}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:50:40,294] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:40,384] After 828800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 313.3960876464844,\n",
      " 'loss_1': 270.7071533203125,\n",
      " 'mean_episode_return_0': 62.21050262451172,\n",
      " 'mean_episode_return_1': 71.348876953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:45,390] After 835200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 276.8175354003906,\n",
      " 'loss_1': 324.6249084472656,\n",
      " 'mean_episode_return_0': 62.23543930053711,\n",
      " 'mean_episode_return_1': 71.31301879882812}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:50,396] After 835200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 276.8175354003906,\n",
      " 'loss_1': 324.6249084472656,\n",
      " 'mean_episode_return_0': 62.23543930053711,\n",
      " 'mean_episode_return_1': 71.31301879882812}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:50:55,400] After 841600 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 287.1199645996094,\n",
      " 'loss_1': 293.6334228515625,\n",
      " 'mean_episode_return_0': 62.21212387084961,\n",
      " 'mean_episode_return_1': 71.29424285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:00,404] After 841600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 287.1199645996094,\n",
      " 'loss_1': 293.6334228515625,\n",
      " 'mean_episode_return_0': 62.21212387084961,\n",
      " 'mean_episode_return_1': 71.29424285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:05,409] After 841600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 287.1199645996094,\n",
      " 'loss_1': 293.6334228515625,\n",
      " 'mean_episode_return_0': 62.21212387084961,\n",
      " 'mean_episode_return_1': 71.29424285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:10,415] After 848000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 271.0433654785156,\n",
      " 'loss_1': 337.52044677734375,\n",
      " 'mean_episode_return_0': 62.23018264770508,\n",
      " 'mean_episode_return_1': 71.29711151123047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:15,421] After 848000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 271.0433654785156,\n",
      " 'loss_1': 337.52044677734375,\n",
      " 'mean_episode_return_0': 62.23018264770508,\n",
      " 'mean_episode_return_1': 71.29711151123047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:20,427] After 848000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 271.0433654785156,\n",
      " 'loss_1': 337.52044677734375,\n",
      " 'mean_episode_return_0': 62.23018264770508,\n",
      " 'mean_episode_return_1': 71.29711151123047}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:51:25,433] After 854400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 279.3696594238281,\n",
      " 'loss_1': 299.05706787109375,\n",
      " 'mean_episode_return_0': 62.26262664794922,\n",
      " 'mean_episode_return_1': 71.3284912109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:30,439] After 854400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.3696594238281,\n",
      " 'loss_1': 299.05706787109375,\n",
      " 'mean_episode_return_0': 62.26262664794922,\n",
      " 'mean_episode_return_1': 71.3284912109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:35,445] After 854400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.3696594238281,\n",
      " 'loss_1': 299.05706787109375,\n",
      " 'mean_episode_return_0': 62.26262664794922,\n",
      " 'mean_episode_return_1': 71.3284912109375}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:51:40,451] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:40,543] After 860800 frames: @ 1255.7 fps Stats:\n",
      "{'loss_0': 270.51080322265625,\n",
      " 'loss_1': 309.8444519042969,\n",
      " 'mean_episode_return_0': 62.272254943847656,\n",
      " 'mean_episode_return_1': 71.36892700195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:45,549] After 860800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 270.51080322265625,\n",
      " 'loss_1': 309.8444519042969,\n",
      " 'mean_episode_return_0': 62.272254943847656,\n",
      " 'mean_episode_return_1': 71.36892700195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:50,552] After 864000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 247.6432342529297,\n",
      " 'loss_1': 309.8444519042969,\n",
      " 'mean_episode_return_0': 62.315189361572266,\n",
      " 'mean_episode_return_1': 71.36892700195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:51:55,558] After 867200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 247.6432342529297,\n",
      " 'loss_1': 312.7975769042969,\n",
      " 'mean_episode_return_0': 62.315189361572266,\n",
      " 'mean_episode_return_1': 71.31612396240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:00,564] After 867200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 247.6432342529297,\n",
      " 'loss_1': 312.7975769042969,\n",
      " 'mean_episode_return_0': 62.315189361572266,\n",
      " 'mean_episode_return_1': 71.31612396240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:05,570] After 870400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 303.0379943847656,\n",
      " 'loss_1': 312.7975769042969,\n",
      " 'mean_episode_return_0': 62.34812545776367,\n",
      " 'mean_episode_return_1': 71.31612396240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:10,575] After 873600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 303.0379943847656,\n",
      " 'loss_1': 382.8227233886719,\n",
      " 'mean_episode_return_0': 62.34812545776367,\n",
      " 'mean_episode_return_1': 71.33448791503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:15,580] After 873600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 303.0379943847656,\n",
      " 'loss_1': 382.8227233886719,\n",
      " 'mean_episode_return_0': 62.34812545776367,\n",
      " 'mean_episode_return_1': 71.33448791503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:20,584] After 880000 frames: @ 1279.3 fps Stats:\n",
      "{'loss_0': 423.431640625,\n",
      " 'loss_1': 315.1340026855469,\n",
      " 'mean_episode_return_0': 62.3824348449707,\n",
      " 'mean_episode_return_1': 71.29789733886719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:25,589] After 880000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 423.431640625,\n",
      " 'loss_1': 315.1340026855469,\n",
      " 'mean_episode_return_0': 62.3824348449707,\n",
      " 'mean_episode_return_1': 71.29789733886719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:30,596] After 883200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 279.0941467285156,\n",
      " 'loss_1': 315.1340026855469,\n",
      " 'mean_episode_return_0': 62.43318176269531,\n",
      " 'mean_episode_return_1': 71.29789733886719}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:35,600] After 886400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 279.0941467285156,\n",
      " 'loss_1': 404.7694091796875,\n",
      " 'mean_episode_return_0': 62.43318176269531,\n",
      " 'mean_episode_return_1': 71.2624282836914}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:52:40,606] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:40,703] After 886400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 279.0941467285156,\n",
      " 'loss_1': 404.7694091796875,\n",
      " 'mean_episode_return_0': 62.43318176269531,\n",
      " 'mean_episode_return_1': 71.2624282836914}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:45,709] After 889600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 296.7408447265625,\n",
      " 'loss_1': 404.7694091796875,\n",
      " 'mean_episode_return_0': 62.45249938964844,\n",
      " 'mean_episode_return_1': 71.2624282836914}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:50,715] After 892800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 296.7408447265625,\n",
      " 'loss_1': 292.76483154296875,\n",
      " 'mean_episode_return_0': 62.45249938964844,\n",
      " 'mean_episode_return_1': 71.31055450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:52:55,721] After 892800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 296.7408447265625,\n",
      " 'loss_1': 292.76483154296875,\n",
      " 'mean_episode_return_0': 62.45249938964844,\n",
      " 'mean_episode_return_1': 71.31055450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:00,727] After 896000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 282.84686279296875,\n",
      " 'loss_1': 292.76483154296875,\n",
      " 'mean_episode_return_0': 62.49868392944336,\n",
      " 'mean_episode_return_1': 71.31055450439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:05,733] After 899200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 282.84686279296875,\n",
      " 'loss_1': 301.8948669433594,\n",
      " 'mean_episode_return_0': 62.49868392944336,\n",
      " 'mean_episode_return_1': 71.2745132446289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:10,739] After 899200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.84686279296875,\n",
      " 'loss_1': 301.8948669433594,\n",
      " 'mean_episode_return_0': 62.49868392944336,\n",
      " 'mean_episode_return_1': 71.2745132446289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:15,745] After 902400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 351.7323303222656,\n",
      " 'loss_1': 301.8948669433594,\n",
      " 'mean_episode_return_0': 62.55318069458008,\n",
      " 'mean_episode_return_1': 71.2745132446289}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:20,748] After 905600 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 351.7323303222656,\n",
      " 'loss_1': 316.3111572265625,\n",
      " 'mean_episode_return_0': 62.55318069458008,\n",
      " 'mean_episode_return_1': 71.24198913574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:25,752] After 908800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 293.672607421875,\n",
      " 'loss_1': 316.3111572265625,\n",
      " 'mean_episode_return_0': 62.577938079833984,\n",
      " 'mean_episode_return_1': 71.24198913574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:30,756] After 912000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 293.672607421875,\n",
      " 'loss_1': 359.94573974609375,\n",
      " 'mean_episode_return_0': 62.577938079833984,\n",
      " 'mean_episode_return_1': 71.2823486328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:35,760] After 912000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 293.672607421875,\n",
      " 'loss_1': 359.94573974609375,\n",
      " 'mean_episode_return_0': 62.577938079833984,\n",
      " 'mean_episode_return_1': 71.2823486328125}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:53:40,766] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:40,856] After 915200 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 280.819091796875,\n",
      " 'loss_1': 359.94573974609375,\n",
      " 'mean_episode_return_0': 62.63312530517578,\n",
      " 'mean_episode_return_1': 71.2823486328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:45,860] After 918400 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 280.819091796875,\n",
      " 'loss_1': 329.2602233886719,\n",
      " 'mean_episode_return_0': 62.63312530517578,\n",
      " 'mean_episode_return_1': 71.2538833618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:50,865] After 918400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 280.819091796875,\n",
      " 'loss_1': 329.2602233886719,\n",
      " 'mean_episode_return_0': 62.63312530517578,\n",
      " 'mean_episode_return_1': 71.2538833618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:53:55,871] After 921600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 267.2267761230469,\n",
      " 'loss_1': 329.2602233886719,\n",
      " 'mean_episode_return_0': 62.65156173706055,\n",
      " 'mean_episode_return_1': 71.2538833618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:00,876] After 924800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 267.2267761230469,\n",
      " 'loss_1': 292.2946472167969,\n",
      " 'mean_episode_return_0': 62.65156173706055,\n",
      " 'mean_episode_return_1': 71.26741027832031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:05,882] After 924800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 267.2267761230469,\n",
      " 'loss_1': 292.2946472167969,\n",
      " 'mean_episode_return_0': 62.65156173706055,\n",
      " 'mean_episode_return_1': 71.26741027832031}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:10,888] After 928000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 295.61798095703125,\n",
      " 'loss_1': 292.2946472167969,\n",
      " 'mean_episode_return_0': 62.67593765258789,\n",
      " 'mean_episode_return_1': 71.26741027832031}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:54:15,894] After 931200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 295.61798095703125,\n",
      " 'loss_1': 305.8628845214844,\n",
      " 'mean_episode_return_0': 62.67593765258789,\n",
      " 'mean_episode_return_1': 71.263916015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:20,898] After 931200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 295.61798095703125,\n",
      " 'loss_1': 305.8628845214844,\n",
      " 'mean_episode_return_0': 62.67593765258789,\n",
      " 'mean_episode_return_1': 71.263916015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:25,904] After 934400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 273.02752685546875,\n",
      " 'loss_1': 305.8628845214844,\n",
      " 'mean_episode_return_0': 62.69024658203125,\n",
      " 'mean_episode_return_1': 71.263916015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:30,910] After 937600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 273.02752685546875,\n",
      " 'loss_1': 400.92242431640625,\n",
      " 'mean_episode_return_0': 62.69024658203125,\n",
      " 'mean_episode_return_1': 71.29999542236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:35,916] After 940800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 286.6513671875,\n",
      " 'loss_1': 400.92242431640625,\n",
      " 'mean_episode_return_0': 62.71406173706055,\n",
      " 'mean_episode_return_1': 71.29999542236328}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:54:40,919] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:41,001] After 944000 frames: @ 629.3 fps Stats:\n",
      "{'loss_0': 286.6513671875,\n",
      " 'loss_1': 290.4255065917969,\n",
      " 'mean_episode_return_0': 62.71406173706055,\n",
      " 'mean_episode_return_1': 71.29413604736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:46,008] After 944000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 286.6513671875,\n",
      " 'loss_1': 290.4255065917969,\n",
      " 'mean_episode_return_0': 62.71406173706055,\n",
      " 'mean_episode_return_1': 71.29413604736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:51,014] After 947200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 275.7825012207031,\n",
      " 'loss_1': 290.4255065917969,\n",
      " 'mean_episode_return_0': 62.79737091064453,\n",
      " 'mean_episode_return_1': 71.29413604736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:54:56,020] After 950400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 275.7825012207031,\n",
      " 'loss_1': 386.6473388671875,\n",
      " 'mean_episode_return_0': 62.79737091064453,\n",
      " 'mean_episode_return_1': 71.2978286743164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:01,025] After 950400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 275.7825012207031,\n",
      " 'loss_1': 386.6473388671875,\n",
      " 'mean_episode_return_0': 62.79737091064453,\n",
      " 'mean_episode_return_1': 71.2978286743164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:06,031] After 953600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 297.6293029785156,\n",
      " 'loss_1': 386.6473388671875,\n",
      " 'mean_episode_return_0': 62.855125427246094,\n",
      " 'mean_episode_return_1': 71.2978286743164}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:11,034] After 956800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 297.6293029785156,\n",
      " 'loss_1': 284.193359375,\n",
      " 'mean_episode_return_0': 62.855125427246094,\n",
      " 'mean_episode_return_1': 71.29702758789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:16,040] After 960000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 256.36578369140625,\n",
      " 'loss_1': 284.193359375,\n",
      " 'mean_episode_return_0': 62.91361999511719,\n",
      " 'mean_episode_return_1': 71.29702758789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:21,042] After 960000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 256.36578369140625,\n",
      " 'loss_1': 284.193359375,\n",
      " 'mean_episode_return_0': 62.91361999511719,\n",
      " 'mean_episode_return_1': 71.29702758789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:26,048] After 963200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 256.36578369140625,\n",
      " 'loss_1': 425.87646484375,\n",
      " 'mean_episode_return_0': 62.91361999511719,\n",
      " 'mean_episode_return_1': 71.28246307373047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:31,052] After 966400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 261.5036315917969,\n",
      " 'loss_1': 425.87646484375,\n",
      " 'mean_episode_return_0': 62.914188385009766,\n",
      " 'mean_episode_return_1': 71.28246307373047}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:36,058] After 966400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 261.5036315917969,\n",
      " 'loss_1': 425.87646484375,\n",
      " 'mean_episode_return_0': 62.914188385009766,\n",
      " 'mean_episode_return_1': 71.28246307373047}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:55:41,064] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:41,142] After 969600 frames: @ 629.6 fps Stats:\n",
      "{'loss_0': 261.5036315917969,\n",
      " 'loss_1': 304.59820556640625,\n",
      " 'mean_episode_return_0': 62.914188385009766,\n",
      " 'mean_episode_return_1': 71.24270629882812}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:46,148] After 972800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 288.4225769042969,\n",
      " 'loss_1': 304.59820556640625,\n",
      " 'mean_episode_return_0': 62.97343444824219,\n",
      " 'mean_episode_return_1': 71.24270629882812}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:51,152] After 976000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 288.4225769042969,\n",
      " 'loss_1': 346.3706970214844,\n",
      " 'mean_episode_return_0': 62.97343444824219,\n",
      " 'mean_episode_return_1': 71.2389907836914}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:55:56,156] After 976000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 288.4225769042969,\n",
      " 'loss_1': 346.3706970214844,\n",
      " 'mean_episode_return_0': 62.97343444824219,\n",
      " 'mean_episode_return_1': 71.2389907836914}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:01,160] After 979200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 344.6304931640625,\n",
      " 'loss_1': 346.3706970214844,\n",
      " 'mean_episode_return_0': 62.97494125366211,\n",
      " 'mean_episode_return_1': 71.2389907836914}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:06,164] After 982400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 344.6304931640625,\n",
      " 'loss_1': 339.33837890625,\n",
      " 'mean_episode_return_0': 62.97494125366211,\n",
      " 'mean_episode_return_1': 71.29924774169922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:11,168] After 985600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 289.2068176269531,\n",
      " 'loss_1': 339.33837890625,\n",
      " 'mean_episode_return_0': 63.07400131225586,\n",
      " 'mean_episode_return_1': 71.29924774169922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:16,171] After 985600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.2068176269531,\n",
      " 'loss_1': 339.33837890625,\n",
      " 'mean_episode_return_0': 63.07400131225586,\n",
      " 'mean_episode_return_1': 71.29924774169922}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:21,178] After 988800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 289.2068176269531,\n",
      " 'loss_1': 303.16107177734375,\n",
      " 'mean_episode_return_0': 63.07400131225586,\n",
      " 'mean_episode_return_1': 71.27925872802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:26,183] After 992000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 320.4234313964844,\n",
      " 'loss_1': 303.16107177734375,\n",
      " 'mean_episode_return_0': 63.0595588684082,\n",
      " 'mean_episode_return_1': 71.27925872802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:31,188] After 992000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 320.4234313964844,\n",
      " 'loss_1': 303.16107177734375,\n",
      " 'mean_episode_return_0': 63.0595588684082,\n",
      " 'mean_episode_return_1': 71.27925872802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:36,194] After 995200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 320.4234313964844,\n",
      " 'loss_1': 321.28717041015625,\n",
      " 'mean_episode_return_0': 63.0595588684082,\n",
      " 'mean_episode_return_1': 71.32667541503906}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:56:41,200] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:41,289] After 998400 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 288.9909362792969,\n",
      " 'loss_1': 321.28717041015625,\n",
      " 'mean_episode_return_0': 63.01043701171875,\n",
      " 'mean_episode_return_1': 71.32667541503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:46,295] After 1001600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 288.9909362792969,\n",
      " 'loss_1': 328.6619567871094,\n",
      " 'mean_episode_return_0': 63.01043701171875,\n",
      " 'mean_episode_return_1': 71.33625030517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:51,301] After 1004800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 328.7926940917969,\n",
      " 'loss_1': 328.6619567871094,\n",
      " 'mean_episode_return_0': 63.077186584472656,\n",
      " 'mean_episode_return_1': 71.33625030517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:56:56,304] After 1004800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 328.7926940917969,\n",
      " 'loss_1': 328.6619567871094,\n",
      " 'mean_episode_return_0': 63.077186584472656,\n",
      " 'mean_episode_return_1': 71.33625030517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:01,308] After 1008000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 328.7926940917969,\n",
      " 'loss_1': 308.8255310058594,\n",
      " 'mean_episode_return_0': 63.077186584472656,\n",
      " 'mean_episode_return_1': 71.4322509765625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:57:06,312] After 1011200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 295.14642333984375,\n",
      " 'loss_1': 308.8255310058594,\n",
      " 'mean_episode_return_0': 63.04924774169922,\n",
      " 'mean_episode_return_1': 71.4322509765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:11,318] After 1011200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 295.14642333984375,\n",
      " 'loss_1': 308.8255310058594,\n",
      " 'mean_episode_return_0': 63.04924774169922,\n",
      " 'mean_episode_return_1': 71.4322509765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:16,324] After 1014400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 295.14642333984375,\n",
      " 'loss_1': 296.62530517578125,\n",
      " 'mean_episode_return_0': 63.04924774169922,\n",
      " 'mean_episode_return_1': 71.4481430053711}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:21,329] After 1017600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 293.82232666015625,\n",
      " 'loss_1': 296.62530517578125,\n",
      " 'mean_episode_return_0': 63.0422477722168,\n",
      " 'mean_episode_return_1': 71.4481430053711}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:26,335] After 1017600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 293.82232666015625,\n",
      " 'loss_1': 296.62530517578125,\n",
      " 'mean_episode_return_0': 63.0422477722168,\n",
      " 'mean_episode_return_1': 71.4481430053711}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:31,340] After 1024000 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 257.1683044433594,\n",
      " 'loss_1': 330.69281005859375,\n",
      " 'mean_episode_return_0': 63.0517463684082,\n",
      " 'mean_episode_return_1': 71.50979614257812}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:36,345] After 1024000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 257.1683044433594,\n",
      " 'loss_1': 330.69281005859375,\n",
      " 'mean_episode_return_0': 63.0517463684082,\n",
      " 'mean_episode_return_1': 71.50979614257812}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:57:41,351] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:41,446] After 1027200 frames: @ 627.5 fps Stats:\n",
      "{'loss_0': 257.1683044433594,\n",
      " 'loss_1': 317.3515625,\n",
      " 'mean_episode_return_0': 63.0517463684082,\n",
      " 'mean_episode_return_1': 71.55472564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:46,452] After 1030400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 273.56842041015625,\n",
      " 'loss_1': 317.3515625,\n",
      " 'mean_episode_return_0': 63.04900360107422,\n",
      " 'mean_episode_return_1': 71.55472564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:51,458] After 1030400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 273.56842041015625,\n",
      " 'loss_1': 317.3515625,\n",
      " 'mean_episode_return_0': 63.04900360107422,\n",
      " 'mean_episode_return_1': 71.55472564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:57:56,460] After 1033600 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 273.56842041015625,\n",
      " 'loss_1': 372.2723693847656,\n",
      " 'mean_episode_return_0': 63.04900360107422,\n",
      " 'mean_episode_return_1': 71.63917541503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:01,464] After 1036800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 310.51727294921875,\n",
      " 'loss_1': 372.2723693847656,\n",
      " 'mean_episode_return_0': 63.07075119018555,\n",
      " 'mean_episode_return_1': 71.63917541503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:06,468] After 1036800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 310.51727294921875,\n",
      " 'loss_1': 372.2723693847656,\n",
      " 'mean_episode_return_0': 63.07075119018555,\n",
      " 'mean_episode_return_1': 71.63917541503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:11,474] After 1040000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 310.51727294921875,\n",
      " 'loss_1': 382.1485900878906,\n",
      " 'mean_episode_return_0': 63.07075119018555,\n",
      " 'mean_episode_return_1': 71.71218872070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:16,480] After 1043200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 355.5517578125,\n",
      " 'loss_1': 382.1485900878906,\n",
      " 'mean_episode_return_0': 63.059871673583984,\n",
      " 'mean_episode_return_1': 71.71218872070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:21,484] After 1043200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.5517578125,\n",
      " 'loss_1': 382.1485900878906,\n",
      " 'mean_episode_return_0': 63.059871673583984,\n",
      " 'mean_episode_return_1': 71.71218872070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:26,490] After 1049600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 336.0509948730469,\n",
      " 'loss_1': 386.7515869140625,\n",
      " 'mean_episode_return_0': 63.1124382019043,\n",
      " 'mean_episode_return_1': 71.78157806396484}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:31,496] After 1049600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 336.0509948730469,\n",
      " 'loss_1': 386.7515869140625,\n",
      " 'mean_episode_return_0': 63.1124382019043,\n",
      " 'mean_episode_return_1': 71.78157806396484}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:36,501] After 1049600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 336.0509948730469,\n",
      " 'loss_1': 386.7515869140625,\n",
      " 'mean_episode_return_0': 63.1124382019043,\n",
      " 'mean_episode_return_1': 71.78157806396484}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:58:41,507] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:41,601] After 1056000 frames: @ 1255.2 fps Stats:\n",
      "{'loss_0': 354.6999816894531,\n",
      " 'loss_1': 397.6424560546875,\n",
      " 'mean_episode_return_0': 63.165184020996094,\n",
      " 'mean_episode_return_1': 71.83120727539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:46,607] After 1056000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 354.6999816894531,\n",
      " 'loss_1': 397.6424560546875,\n",
      " 'mean_episode_return_0': 63.165184020996094,\n",
      " 'mean_episode_return_1': 71.83120727539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:51,613] After 1059200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 354.6999816894531,\n",
      " 'loss_1': 346.4505310058594,\n",
      " 'mean_episode_return_0': 63.165184020996094,\n",
      " 'mean_episode_return_1': 71.90251922607422}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:58:56,619] After 1062400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 282.921875,\n",
      " 'loss_1': 346.4505310058594,\n",
      " 'mean_episode_return_0': 63.186248779296875,\n",
      " 'mean_episode_return_1': 71.90251922607422}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:01,625] After 1062400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 282.921875,\n",
      " 'loss_1': 346.4505310058594,\n",
      " 'mean_episode_return_0': 63.186248779296875,\n",
      " 'mean_episode_return_1': 71.90251922607422}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:06,631] After 1068800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 318.323486328125,\n",
      " 'loss_1': 367.3341369628906,\n",
      " 'mean_episode_return_0': 63.19562530517578,\n",
      " 'mean_episode_return_1': 71.90750885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:11,637] After 1068800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.323486328125,\n",
      " 'loss_1': 367.3341369628906,\n",
      " 'mean_episode_return_0': 63.19562530517578,\n",
      " 'mean_episode_return_1': 71.90750885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:16,643] After 1068800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.323486328125,\n",
      " 'loss_1': 367.3341369628906,\n",
      " 'mean_episode_return_0': 63.19562530517578,\n",
      " 'mean_episode_return_1': 71.90750885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:21,649] After 1075200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 316.11688232421875,\n",
      " 'loss_1': 334.3121032714844,\n",
      " 'mean_episode_return_0': 63.2636833190918,\n",
      " 'mean_episode_return_1': 71.91543579101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:26,655] After 1075200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 316.11688232421875,\n",
      " 'loss_1': 334.3121032714844,\n",
      " 'mean_episode_return_0': 63.2636833190918,\n",
      " 'mean_episode_return_1': 71.91543579101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:31,661] After 1078400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 316.11688232421875,\n",
      " 'loss_1': 350.6863098144531,\n",
      " 'mean_episode_return_0': 63.2636833190918,\n",
      " 'mean_episode_return_1': 71.93415069580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:36,667] After 1081600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 338.226318359375,\n",
      " 'loss_1': 350.6863098144531,\n",
      " 'mean_episode_return_0': 63.28556442260742,\n",
      " 'mean_episode_return_1': 71.93415069580078}\n",
      "[INFO:417825 trainer:335 2022-09-17 12:59:41,671] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:41,763] After 1081600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 338.226318359375,\n",
      " 'loss_1': 350.6863098144531,\n",
      " 'mean_episode_return_0': 63.28556442260742,\n",
      " 'mean_episode_return_1': 71.93415069580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:46,768] After 1088000 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 350.7405090332031,\n",
      " 'loss_1': 345.4025573730469,\n",
      " 'mean_episode_return_0': 63.34756088256836,\n",
      " 'mean_episode_return_1': 71.93046569824219}\n",
      "[INFO:417825 trainer:367 2022-09-17 12:59:51,774] After 1088000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.7405090332031,\n",
      " 'loss_1': 345.4025573730469,\n",
      " 'mean_episode_return_0': 63.34756088256836,\n",
      " 'mean_episode_return_1': 71.93046569824219}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 12:59:56,780] After 1088000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.7405090332031,\n",
      " 'loss_1': 345.4025573730469,\n",
      " 'mean_episode_return_0': 63.34756088256836,\n",
      " 'mean_episode_return_1': 71.93046569824219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:01,786] After 1094400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 299.1362609863281,\n",
      " 'loss_1': 326.2107849121094,\n",
      " 'mean_episode_return_0': 63.35006332397461,\n",
      " 'mean_episode_return_1': 72.003173828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:06,792] After 1094400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 299.1362609863281,\n",
      " 'loss_1': 326.2107849121094,\n",
      " 'mean_episode_return_0': 63.35006332397461,\n",
      " 'mean_episode_return_1': 72.003173828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:11,797] After 1094400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 299.1362609863281,\n",
      " 'loss_1': 326.2107849121094,\n",
      " 'mean_episode_return_0': 63.35006332397461,\n",
      " 'mean_episode_return_1': 72.003173828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:16,803] After 1100800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 355.4318542480469,\n",
      " 'loss_1': 332.88006591796875,\n",
      " 'mean_episode_return_0': 63.393123626708984,\n",
      " 'mean_episode_return_1': 72.05207824707031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:21,806] After 1100800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.4318542480469,\n",
      " 'loss_1': 332.88006591796875,\n",
      " 'mean_episode_return_0': 63.393123626708984,\n",
      " 'mean_episode_return_1': 72.05207824707031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:26,811] After 1104000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 288.28948974609375,\n",
      " 'loss_1': 332.88006591796875,\n",
      " 'mean_episode_return_0': 63.41830825805664,\n",
      " 'mean_episode_return_1': 72.05207824707031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:31,817] After 1107200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 288.28948974609375,\n",
      " 'loss_1': 362.8497314453125,\n",
      " 'mean_episode_return_0': 63.41830825805664,\n",
      " 'mean_episode_return_1': 72.07782745361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:36,823] After 1107200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 288.28948974609375,\n",
      " 'loss_1': 362.8497314453125,\n",
      " 'mean_episode_return_0': 63.41830825805664,\n",
      " 'mean_episode_return_1': 72.07782745361328}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:00:41,829] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:41,921] After 1113600 frames: @ 1255.7 fps Stats:\n",
      "{'loss_0': 271.1034240722656,\n",
      " 'loss_1': 327.9298400878906,\n",
      " 'mean_episode_return_0': 63.44737243652344,\n",
      " 'mean_episode_return_1': 72.08824920654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:46,927] After 1113600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 271.1034240722656,\n",
      " 'loss_1': 327.9298400878906,\n",
      " 'mean_episode_return_0': 63.44737243652344,\n",
      " 'mean_episode_return_1': 72.08824920654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:51,933] After 1113600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 271.1034240722656,\n",
      " 'loss_1': 327.9298400878906,\n",
      " 'mean_episode_return_0': 63.44737243652344,\n",
      " 'mean_episode_return_1': 72.08824920654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:00:56,936] After 1120000 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 272.5733337402344,\n",
      " 'loss_1': 326.6001892089844,\n",
      " 'mean_episode_return_0': 63.49081039428711,\n",
      " 'mean_episode_return_1': 72.1400146484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:01,942] After 1120000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 272.5733337402344,\n",
      " 'loss_1': 326.6001892089844,\n",
      " 'mean_episode_return_0': 63.49081039428711,\n",
      " 'mean_episode_return_1': 72.1400146484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:06,944] After 1120000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 272.5733337402344,\n",
      " 'loss_1': 326.6001892089844,\n",
      " 'mean_episode_return_0': 63.49081039428711,\n",
      " 'mean_episode_return_1': 72.1400146484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:11,950] After 1126400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 318.9630126953125,\n",
      " 'loss_1': 331.7681579589844,\n",
      " 'mean_episode_return_0': 63.49055862426758,\n",
      " 'mean_episode_return_1': 72.20842742919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:16,956] After 1126400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.9630126953125,\n",
      " 'loss_1': 331.7681579589844,\n",
      " 'mean_episode_return_0': 63.49055862426758,\n",
      " 'mean_episode_return_1': 72.20842742919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:21,962] After 1129600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 330.87249755859375,\n",
      " 'loss_1': 331.7681579589844,\n",
      " 'mean_episode_return_0': 63.49568557739258,\n",
      " 'mean_episode_return_1': 72.20842742919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:26,968] After 1132800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 330.87249755859375,\n",
      " 'loss_1': 357.9666442871094,\n",
      " 'mean_episode_return_0': 63.49568557739258,\n",
      " 'mean_episode_return_1': 72.2080078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:31,974] After 1132800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 330.87249755859375,\n",
      " 'loss_1': 357.9666442871094,\n",
      " 'mean_episode_return_0': 63.49568557739258,\n",
      " 'mean_episode_return_1': 72.2080078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:36,980] After 1139200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 268.54351806640625,\n",
      " 'loss_1': 366.5509948730469,\n",
      " 'mean_episode_return_0': 63.49119186401367,\n",
      " 'mean_episode_return_1': 72.28290557861328}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:01:41,986] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:42,073] After 1139200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 268.54351806640625,\n",
      " 'loss_1': 366.5509948730469,\n",
      " 'mean_episode_return_0': 63.49119186401367,\n",
      " 'mean_episode_return_1': 72.28290557861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:47,079] After 1139200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 268.54351806640625,\n",
      " 'loss_1': 366.5509948730469,\n",
      " 'mean_episode_return_0': 63.49119186401367,\n",
      " 'mean_episode_return_1': 72.28290557861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:52,085] After 1145600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 378.8678894042969,\n",
      " 'loss_1': 383.42333984375,\n",
      " 'mean_episode_return_0': 63.532440185546875,\n",
      " 'mean_episode_return_1': 72.29503631591797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:01:57,090] After 1145600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 378.8678894042969,\n",
      " 'loss_1': 383.42333984375,\n",
      " 'mean_episode_return_0': 63.532440185546875,\n",
      " 'mean_episode_return_1': 72.29503631591797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:02,096] After 1148800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 331.04248046875,\n",
      " 'loss_1': 383.42333984375,\n",
      " 'mean_episode_return_0': 63.55656051635742,\n",
      " 'mean_episode_return_1': 72.29503631591797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:07,102] After 1152000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 331.04248046875,\n",
      " 'loss_1': 333.15789794921875,\n",
      " 'mean_episode_return_0': 63.55656051635742,\n",
      " 'mean_episode_return_1': 72.3705825805664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:12,109] After 1152000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 331.04248046875,\n",
      " 'loss_1': 333.15789794921875,\n",
      " 'mean_episode_return_0': 63.55656051635742,\n",
      " 'mean_episode_return_1': 72.3705825805664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:17,115] After 1155200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 285.00738525390625,\n",
      " 'loss_1': 333.15789794921875,\n",
      " 'mean_episode_return_0': 63.62118911743164,\n",
      " 'mean_episode_return_1': 72.3705825805664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:22,120] After 1158400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 285.00738525390625,\n",
      " 'loss_1': 299.9388122558594,\n",
      " 'mean_episode_return_0': 63.62118911743164,\n",
      " 'mean_episode_return_1': 72.34058380126953}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:27,124] After 1158400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 285.00738525390625,\n",
      " 'loss_1': 299.9388122558594,\n",
      " 'mean_episode_return_0': 63.62118911743164,\n",
      " 'mean_episode_return_1': 72.34058380126953}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:32,128] After 1164800 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 274.552490234375,\n",
      " 'loss_1': 326.1039123535156,\n",
      " 'mean_episode_return_0': 63.66337585449219,\n",
      " 'mean_episode_return_1': 72.36719512939453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:37,132] After 1164800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 274.552490234375,\n",
      " 'loss_1': 326.1039123535156,\n",
      " 'mean_episode_return_0': 63.66337585449219,\n",
      " 'mean_episode_return_1': 72.36719512939453}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:02:42,137] Saving checkpoint to dmc_results/tute/model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:02:42,236] After 1168000 frames: @ 627.0 fps Stats:\n",
      "{'loss_0': 294.1035461425781,\n",
      " 'loss_1': 326.1039123535156,\n",
      " 'mean_episode_return_0': 63.68099594116211,\n",
      " 'mean_episode_return_1': 72.36719512939453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:47,240] After 1171200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 294.1035461425781,\n",
      " 'loss_1': 338.3587951660156,\n",
      " 'mean_episode_return_0': 63.68099594116211,\n",
      " 'mean_episode_return_1': 72.38633728027344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:52,244] After 1171200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 294.1035461425781,\n",
      " 'loss_1': 338.3587951660156,\n",
      " 'mean_episode_return_0': 63.68099594116211,\n",
      " 'mean_episode_return_1': 72.38633728027344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:02:57,250] After 1174400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 341.66851806640625,\n",
      " 'loss_1': 338.3587951660156,\n",
      " 'mean_episode_return_0': 63.769622802734375,\n",
      " 'mean_episode_return_1': 72.38633728027344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:02,255] After 1177600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 341.66851806640625,\n",
      " 'loss_1': 454.72943115234375,\n",
      " 'mean_episode_return_0': 63.769622802734375,\n",
      " 'mean_episode_return_1': 72.40185546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:07,261] After 1177600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.66851806640625,\n",
      " 'loss_1': 454.72943115234375,\n",
      " 'mean_episode_return_0': 63.769622802734375,\n",
      " 'mean_episode_return_1': 72.40185546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:12,267] After 1180800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 317.22808837890625,\n",
      " 'loss_1': 454.72943115234375,\n",
      " 'mean_episode_return_0': 63.8121223449707,\n",
      " 'mean_episode_return_1': 72.40185546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:17,273] After 1184000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 317.22808837890625,\n",
      " 'loss_1': 357.1658630371094,\n",
      " 'mean_episode_return_0': 63.8121223449707,\n",
      " 'mean_episode_return_1': 72.45246887207031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:22,276] After 1187200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 325.1932678222656,\n",
      " 'loss_1': 357.1658630371094,\n",
      " 'mean_episode_return_0': 63.90218734741211,\n",
      " 'mean_episode_return_1': 72.45246887207031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:27,281] After 1190400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 325.1932678222656,\n",
      " 'loss_1': 372.8763122558594,\n",
      " 'mean_episode_return_0': 63.90218734741211,\n",
      " 'mean_episode_return_1': 72.447998046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:32,288] After 1190400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 325.1932678222656,\n",
      " 'loss_1': 372.8763122558594,\n",
      " 'mean_episode_return_0': 63.90218734741211,\n",
      " 'mean_episode_return_1': 72.447998046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:37,292] After 1193600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 306.4128112792969,\n",
      " 'loss_1': 372.8763122558594,\n",
      " 'mean_episode_return_0': 63.972312927246094,\n",
      " 'mean_episode_return_1': 72.447998046875}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:03:42,297] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:42,390] After 1196800 frames: @ 627.7 fps Stats:\n",
      "{'loss_0': 306.4128112792969,\n",
      " 'loss_1': 322.524169921875,\n",
      " 'mean_episode_return_0': 63.972312927246094,\n",
      " 'mean_episode_return_1': 72.50627136230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:47,396] After 1196800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 306.4128112792969,\n",
      " 'loss_1': 322.524169921875,\n",
      " 'mean_episode_return_0': 63.972312927246094,\n",
      " 'mean_episode_return_1': 72.50627136230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:52,402] After 1200000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 277.8138732910156,\n",
      " 'loss_1': 322.524169921875,\n",
      " 'mean_episode_return_0': 63.98831558227539,\n",
      " 'mean_episode_return_1': 72.50627136230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:03:57,408] After 1203200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 277.8138732910156,\n",
      " 'loss_1': 367.7471923828125,\n",
      " 'mean_episode_return_0': 63.98831558227539,\n",
      " 'mean_episode_return_1': 72.51945495605469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:02,414] After 1206400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 315.7471008300781,\n",
      " 'loss_1': 367.7471923828125,\n",
      " 'mean_episode_return_0': 64.01587677001953,\n",
      " 'mean_episode_return_1': 72.51945495605469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:07,420] After 1206400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 315.7471008300781,\n",
      " 'loss_1': 367.7471923828125,\n",
      " 'mean_episode_return_0': 64.01587677001953,\n",
      " 'mean_episode_return_1': 72.51945495605469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:12,424] After 1209600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 315.7471008300781,\n",
      " 'loss_1': 347.64892578125,\n",
      " 'mean_episode_return_0': 64.01587677001953,\n",
      " 'mean_episode_return_1': 72.5625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:17,430] After 1212800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 379.2278747558594,\n",
      " 'loss_1': 347.64892578125,\n",
      " 'mean_episode_return_0': 64.11262512207031,\n",
      " 'mean_episode_return_1': 72.5625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:22,436] After 1216000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 379.2278747558594,\n",
      " 'loss_1': 389.22991943359375,\n",
      " 'mean_episode_return_0': 64.11262512207031,\n",
      " 'mean_episode_return_1': 72.57158660888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:27,442] After 1216000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 379.2278747558594,\n",
      " 'loss_1': 389.22991943359375,\n",
      " 'mean_episode_return_0': 64.11262512207031,\n",
      " 'mean_episode_return_1': 72.57158660888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:32,448] After 1219200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 320.46881103515625,\n",
      " 'loss_1': 389.22991943359375,\n",
      " 'mean_episode_return_0': 64.20281219482422,\n",
      " 'mean_episode_return_1': 72.57158660888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:37,453] After 1222400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 320.46881103515625,\n",
      " 'loss_1': 360.4394836425781,\n",
      " 'mean_episode_return_0': 64.20281219482422,\n",
      " 'mean_episode_return_1': 72.58031463623047}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:04:42,459] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:42,555] After 1225600 frames: @ 627.4 fps Stats:\n",
      "{'loss_0': 313.88446044921875,\n",
      " 'loss_1': 360.4394836425781,\n",
      " 'mean_episode_return_0': 64.19987487792969,\n",
      " 'mean_episode_return_1': 72.58031463623047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:47,560] After 1225600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 313.88446044921875,\n",
      " 'loss_1': 360.4394836425781,\n",
      " 'mean_episode_return_0': 64.19987487792969,\n",
      " 'mean_episode_return_1': 72.58031463623047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:52,566] After 1228800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 313.88446044921875,\n",
      " 'loss_1': 362.0172119140625,\n",
      " 'mean_episode_return_0': 64.19987487792969,\n",
      " 'mean_episode_return_1': 72.54430389404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:04:57,572] After 1232000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 390.7281799316406,\n",
      " 'loss_1': 362.0172119140625,\n",
      " 'mean_episode_return_0': 64.28056335449219,\n",
      " 'mean_episode_return_1': 72.54430389404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:02,578] After 1232000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 390.7281799316406,\n",
      " 'loss_1': 362.0172119140625,\n",
      " 'mean_episode_return_0': 64.28056335449219,\n",
      " 'mean_episode_return_1': 72.54430389404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:07,584] After 1235200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 390.7281799316406,\n",
      " 'loss_1': 371.6090393066406,\n",
      " 'mean_episode_return_0': 64.28056335449219,\n",
      " 'mean_episode_return_1': 72.5708999633789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:12,590] After 1238400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 323.34375,\n",
      " 'loss_1': 371.6090393066406,\n",
      " 'mean_episode_return_0': 64.3829345703125,\n",
      " 'mean_episode_return_1': 72.5708999633789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:17,595] After 1241600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 323.34375,\n",
      " 'loss_1': 394.0371398925781,\n",
      " 'mean_episode_return_0': 64.3829345703125,\n",
      " 'mean_episode_return_1': 72.63127899169922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:22,601] After 1241600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 323.34375,\n",
      " 'loss_1': 394.0371398925781,\n",
      " 'mean_episode_return_0': 64.3829345703125,\n",
      " 'mean_episode_return_1': 72.63127899169922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:27,607] After 1244800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 326.7840576171875,\n",
      " 'loss_1': 394.0371398925781,\n",
      " 'mean_episode_return_0': 64.40325164794922,\n",
      " 'mean_episode_return_1': 72.63127899169922}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:05:32,612] After 1248000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 326.7840576171875,\n",
      " 'loss_1': 420.2825622558594,\n",
      " 'mean_episode_return_0': 64.40325164794922,\n",
      " 'mean_episode_return_1': 72.74768829345703}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:37,618] After 1251200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.32952880859375,\n",
      " 'loss_1': 420.2825622558594,\n",
      " 'mean_episode_return_0': 64.36856079101562,\n",
      " 'mean_episode_return_1': 72.74768829345703}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:05:42,620] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:42,715] After 1251200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 327.32952880859375,\n",
      " 'loss_1': 420.2825622558594,\n",
      " 'mean_episode_return_0': 64.36856079101562,\n",
      " 'mean_episode_return_1': 72.74768829345703}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:47,721] After 1254400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.32952880859375,\n",
      " 'loss_1': 367.3205261230469,\n",
      " 'mean_episode_return_0': 64.36856079101562,\n",
      " 'mean_episode_return_1': 72.75140380859375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:52,724] After 1257600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 329.0716247558594,\n",
      " 'loss_1': 367.3205261230469,\n",
      " 'mean_episode_return_0': 64.44225311279297,\n",
      " 'mean_episode_return_1': 72.75140380859375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:05:57,726] After 1257600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 329.0716247558594,\n",
      " 'loss_1': 367.3205261230469,\n",
      " 'mean_episode_return_0': 64.44225311279297,\n",
      " 'mean_episode_return_1': 72.75140380859375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:02,732] After 1260800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 329.0716247558594,\n",
      " 'loss_1': 349.28997802734375,\n",
      " 'mean_episode_return_0': 64.44225311279297,\n",
      " 'mean_episode_return_1': 72.83850860595703}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:07,738] After 1264000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 323.59967041015625,\n",
      " 'loss_1': 349.28997802734375,\n",
      " 'mean_episode_return_0': 64.510498046875,\n",
      " 'mean_episode_return_1': 72.83850860595703}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:12,744] After 1264000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 323.59967041015625,\n",
      " 'loss_1': 349.28997802734375,\n",
      " 'mean_episode_return_0': 64.510498046875,\n",
      " 'mean_episode_return_1': 72.83850860595703}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:17,748] After 1270400 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 327.5399475097656,\n",
      " 'loss_1': 388.50335693359375,\n",
      " 'mean_episode_return_0': 64.56443786621094,\n",
      " 'mean_episode_return_1': 72.89094543457031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:22,754] After 1270400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 327.5399475097656,\n",
      " 'loss_1': 388.50335693359375,\n",
      " 'mean_episode_return_0': 64.56443786621094,\n",
      " 'mean_episode_return_1': 72.89094543457031}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:27,760] After 1273600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 327.5399475097656,\n",
      " 'loss_1': 377.19482421875,\n",
      " 'mean_episode_return_0': 64.56443786621094,\n",
      " 'mean_episode_return_1': 72.922607421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:32,764] After 1276800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 348.1805114746094,\n",
      " 'loss_1': 377.19482421875,\n",
      " 'mean_episode_return_0': 64.54393768310547,\n",
      " 'mean_episode_return_1': 72.922607421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:37,768] After 1276800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.1805114746094,\n",
      " 'loss_1': 377.19482421875,\n",
      " 'mean_episode_return_0': 64.54393768310547,\n",
      " 'mean_episode_return_1': 72.922607421875}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:06:42,772] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:42,865] After 1280000 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 348.1805114746094,\n",
      " 'loss_1': 351.9480895996094,\n",
      " 'mean_episode_return_0': 64.54393768310547,\n",
      " 'mean_episode_return_1': 72.98261260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:47,868] After 1283200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 366.8026428222656,\n",
      " 'loss_1': 351.9480895996094,\n",
      " 'mean_episode_return_0': 64.57899475097656,\n",
      " 'mean_episode_return_1': 72.98261260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:52,874] After 1283200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 366.8026428222656,\n",
      " 'loss_1': 351.9480895996094,\n",
      " 'mean_episode_return_0': 64.57899475097656,\n",
      " 'mean_episode_return_1': 72.98261260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:06:57,880] After 1289600 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 360.9852294921875,\n",
      " 'loss_1': 356.3662414550781,\n",
      " 'mean_episode_return_0': 64.70987701416016,\n",
      " 'mean_episode_return_1': 72.98048400878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:02,884] After 1289600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 360.9852294921875,\n",
      " 'loss_1': 356.3662414550781,\n",
      " 'mean_episode_return_0': 64.70987701416016,\n",
      " 'mean_episode_return_1': 72.98048400878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:07,888] After 1289600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 360.9852294921875,\n",
      " 'loss_1': 356.3662414550781,\n",
      " 'mean_episode_return_0': 64.70987701416016,\n",
      " 'mean_episode_return_1': 72.98048400878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:12,894] After 1296000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 317.51800537109375,\n",
      " 'loss_1': 384.5328674316406,\n",
      " 'mean_episode_return_0': 64.83343505859375,\n",
      " 'mean_episode_return_1': 73.07860565185547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:17,900] After 1296000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 317.51800537109375,\n",
      " 'loss_1': 384.5328674316406,\n",
      " 'mean_episode_return_0': 64.83343505859375,\n",
      " 'mean_episode_return_1': 73.07860565185547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:22,906] After 1299200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 317.51800537109375,\n",
      " 'loss_1': 350.89544677734375,\n",
      " 'mean_episode_return_0': 64.83343505859375,\n",
      " 'mean_episode_return_1': 73.06685638427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:27,912] After 1302400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 328.5340270996094,\n",
      " 'loss_1': 350.89544677734375,\n",
      " 'mean_episode_return_0': 64.92406463623047,\n",
      " 'mean_episode_return_1': 73.06685638427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:32,918] After 1302400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 328.5340270996094,\n",
      " 'loss_1': 350.89544677734375,\n",
      " 'mean_episode_return_0': 64.92406463623047,\n",
      " 'mean_episode_return_1': 73.06685638427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:37,924] After 1308800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 337.1424560546875,\n",
      " 'loss_1': 383.52044677734375,\n",
      " 'mean_episode_return_0': 64.99781036376953,\n",
      " 'mean_episode_return_1': 73.07306671142578}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:07:42,929] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:43,029] After 1308800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 337.1424560546875,\n",
      " 'loss_1': 383.52044677734375,\n",
      " 'mean_episode_return_0': 64.99781036376953,\n",
      " 'mean_episode_return_1': 73.07306671142578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:48,035] After 1308800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 337.1424560546875,\n",
      " 'loss_1': 383.52044677734375,\n",
      " 'mean_episode_return_0': 64.99781036376953,\n",
      " 'mean_episode_return_1': 73.07306671142578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:53,041] After 1315200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 347.8171081542969,\n",
      " 'loss_1': 435.0135498046875,\n",
      " 'mean_episode_return_0': 65.05874633789062,\n",
      " 'mean_episode_return_1': 73.14042663574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:07:58,047] After 1315200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 347.8171081542969,\n",
      " 'loss_1': 435.0135498046875,\n",
      " 'mean_episode_return_0': 65.05874633789062,\n",
      " 'mean_episode_return_1': 73.14042663574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:03,053] After 1315200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 347.8171081542969,\n",
      " 'loss_1': 435.0135498046875,\n",
      " 'mean_episode_return_0': 65.05874633789062,\n",
      " 'mean_episode_return_1': 73.14042663574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:08,059] After 1321600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 384.4674987792969,\n",
      " 'loss_1': 347.5194091796875,\n",
      " 'mean_episode_return_0': 65.13412475585938,\n",
      " 'mean_episode_return_1': 73.16264343261719}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:13,065] After 1321600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.4674987792969,\n",
      " 'loss_1': 347.5194091796875,\n",
      " 'mean_episode_return_0': 65.13412475585938,\n",
      " 'mean_episode_return_1': 73.16264343261719}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:08:18,071] After 1328000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 338.7250671386719,\n",
      " 'loss_1': 399.47772216796875,\n",
      " 'mean_episode_return_0': 65.1630630493164,\n",
      " 'mean_episode_return_1': 73.2435531616211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:23,076] After 1328000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 338.7250671386719,\n",
      " 'loss_1': 399.47772216796875,\n",
      " 'mean_episode_return_0': 65.1630630493164,\n",
      " 'mean_episode_return_1': 73.2435531616211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:28,082] After 1328000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 338.7250671386719,\n",
      " 'loss_1': 399.47772216796875,\n",
      " 'mean_episode_return_0': 65.1630630493164,\n",
      " 'mean_episode_return_1': 73.2435531616211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:33,088] After 1334400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 289.40399169921875,\n",
      " 'loss_1': 370.76129150390625,\n",
      " 'mean_episode_return_0': 65.2099380493164,\n",
      " 'mean_episode_return_1': 73.27519989013672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:38,094] After 1334400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.40399169921875,\n",
      " 'loss_1': 370.76129150390625,\n",
      " 'mean_episode_return_0': 65.2099380493164,\n",
      " 'mean_episode_return_1': 73.27519989013672}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:08:43,096] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:43,166] After 1334400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.40399169921875,\n",
      " 'loss_1': 370.76129150390625,\n",
      " 'mean_episode_return_0': 65.2099380493164,\n",
      " 'mean_episode_return_1': 73.27519989013672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:48,172] After 1340800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 333.19500732421875,\n",
      " 'loss_1': 364.0252380371094,\n",
      " 'mean_episode_return_0': 65.296630859375,\n",
      " 'mean_episode_return_1': 73.36209869384766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:53,178] After 1340800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 333.19500732421875,\n",
      " 'loss_1': 364.0252380371094,\n",
      " 'mean_episode_return_0': 65.296630859375,\n",
      " 'mean_episode_return_1': 73.36209869384766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:08:58,183] After 1340800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 333.19500732421875,\n",
      " 'loss_1': 364.0252380371094,\n",
      " 'mean_episode_return_0': 65.296630859375,\n",
      " 'mean_episode_return_1': 73.36209869384766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:03,188] After 1347200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 338.37115478515625,\n",
      " 'loss_1': 344.3299865722656,\n",
      " 'mean_episode_return_0': 65.3165054321289,\n",
      " 'mean_episode_return_1': 73.40188598632812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:08,195] After 1347200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 338.37115478515625,\n",
      " 'loss_1': 344.3299865722656,\n",
      " 'mean_episode_return_0': 65.3165054321289,\n",
      " 'mean_episode_return_1': 73.40188598632812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:13,200] After 1353600 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 370.1529541015625,\n",
      " 'loss_1': 432.48248291015625,\n",
      " 'mean_episode_return_0': 65.32799530029297,\n",
      " 'mean_episode_return_1': 73.48714447021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:18,206] After 1353600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 370.1529541015625,\n",
      " 'loss_1': 432.48248291015625,\n",
      " 'mean_episode_return_0': 65.32799530029297,\n",
      " 'mean_episode_return_1': 73.48714447021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:23,208] After 1353600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 370.1529541015625,\n",
      " 'loss_1': 432.48248291015625,\n",
      " 'mean_episode_return_0': 65.32799530029297,\n",
      " 'mean_episode_return_1': 73.48714447021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:28,212] After 1360000 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 350.47760009765625,\n",
      " 'loss_1': 406.1187438964844,\n",
      " 'mean_episode_return_0': 65.38880920410156,\n",
      " 'mean_episode_return_1': 73.51830291748047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:33,218] After 1360000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.47760009765625,\n",
      " 'loss_1': 406.1187438964844,\n",
      " 'mean_episode_return_0': 65.38880920410156,\n",
      " 'mean_episode_return_1': 73.51830291748047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:38,220] After 1360000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.47760009765625,\n",
      " 'loss_1': 406.1187438964844,\n",
      " 'mean_episode_return_0': 65.38880920410156,\n",
      " 'mean_episode_return_1': 73.51830291748047}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:09:43,226] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:43,287] After 1366400 frames: @ 1263.3 fps Stats:\n",
      "{'loss_0': 375.0466003417969,\n",
      " 'loss_1': 357.0587158203125,\n",
      " 'mean_episode_return_0': 65.42049407958984,\n",
      " 'mean_episode_return_1': 73.54167175292969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:48,293] After 1366400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.0466003417969,\n",
      " 'loss_1': 357.0587158203125,\n",
      " 'mean_episode_return_0': 65.42049407958984,\n",
      " 'mean_episode_return_1': 73.54167175292969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:53,298] After 1369600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.72802734375,\n",
      " 'loss_1': 357.0587158203125,\n",
      " 'mean_episode_return_0': 65.4974365234375,\n",
      " 'mean_episode_return_1': 73.54167175292969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:09:58,304] After 1372800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.72802734375,\n",
      " 'loss_1': 399.1397705078125,\n",
      " 'mean_episode_return_0': 65.4974365234375,\n",
      " 'mean_episode_return_1': 73.61798858642578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:03,310] After 1372800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 374.72802734375,\n",
      " 'loss_1': 399.1397705078125,\n",
      " 'mean_episode_return_0': 65.4974365234375,\n",
      " 'mean_episode_return_1': 73.61798858642578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:08,316] After 1376000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 353.5040588378906,\n",
      " 'loss_1': 399.1397705078125,\n",
      " 'mean_episode_return_0': 65.53424835205078,\n",
      " 'mean_episode_return_1': 73.61798858642578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:13,322] After 1379200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 353.5040588378906,\n",
      " 'loss_1': 441.86834716796875,\n",
      " 'mean_episode_return_0': 65.53424835205078,\n",
      " 'mean_episode_return_1': 73.68820190429688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:18,328] After 1379200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 353.5040588378906,\n",
      " 'loss_1': 441.86834716796875,\n",
      " 'mean_episode_return_0': 65.53424835205078,\n",
      " 'mean_episode_return_1': 73.68820190429688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:23,334] After 1385600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 289.70843505859375,\n",
      " 'loss_1': 387.3483581542969,\n",
      " 'mean_episode_return_0': 65.55037689208984,\n",
      " 'mean_episode_return_1': 73.71073150634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:28,337] After 1385600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.70843505859375,\n",
      " 'loss_1': 387.3483581542969,\n",
      " 'mean_episode_return_0': 65.55037689208984,\n",
      " 'mean_episode_return_1': 73.71073150634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:33,343] After 1385600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 289.70843505859375,\n",
      " 'loss_1': 387.3483581542969,\n",
      " 'mean_episode_return_0': 65.55037689208984,\n",
      " 'mean_episode_return_1': 73.71073150634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:38,349] After 1392000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 431.95220947265625,\n",
      " 'loss_1': 362.3343811035156,\n",
      " 'mean_episode_return_0': 65.66100311279297,\n",
      " 'mean_episode_return_1': 73.73776245117188}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:10:43,355] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:43,447] After 1392000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 431.95220947265625,\n",
      " 'loss_1': 362.3343811035156,\n",
      " 'mean_episode_return_0': 65.66100311279297,\n",
      " 'mean_episode_return_1': 73.73776245117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:48,453] After 1395200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.49700927734375,\n",
      " 'loss_1': 362.3343811035156,\n",
      " 'mean_episode_return_0': 65.77806091308594,\n",
      " 'mean_episode_return_1': 73.73776245117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:53,459] After 1398400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.49700927734375,\n",
      " 'loss_1': 364.8515625,\n",
      " 'mean_episode_return_0': 65.77806091308594,\n",
      " 'mean_episode_return_1': 73.77794647216797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:10:58,465] After 1398400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.49700927734375,\n",
      " 'loss_1': 364.8515625,\n",
      " 'mean_episode_return_0': 65.77806091308594,\n",
      " 'mean_episode_return_1': 73.77794647216797}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:11:03,471] After 1401600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 312.3265686035156,\n",
      " 'loss_1': 364.8515625,\n",
      " 'mean_episode_return_0': 65.84593963623047,\n",
      " 'mean_episode_return_1': 73.77794647216797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:08,477] After 1404800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 312.3265686035156,\n",
      " 'loss_1': 355.36151123046875,\n",
      " 'mean_episode_return_0': 65.84593963623047,\n",
      " 'mean_episode_return_1': 73.7880630493164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:13,483] After 1404800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 312.3265686035156,\n",
      " 'loss_1': 355.36151123046875,\n",
      " 'mean_episode_return_0': 65.84593963623047,\n",
      " 'mean_episode_return_1': 73.7880630493164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:18,488] After 1411200 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 426.2867126464844,\n",
      " 'loss_1': 391.27545166015625,\n",
      " 'mean_episode_return_0': 65.95780944824219,\n",
      " 'mean_episode_return_1': 73.77491760253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:23,494] After 1411200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.2867126464844,\n",
      " 'loss_1': 391.27545166015625,\n",
      " 'mean_episode_return_0': 65.95780944824219,\n",
      " 'mean_episode_return_1': 73.77491760253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:28,500] After 1414400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 339.3738098144531,\n",
      " 'loss_1': 391.27545166015625,\n",
      " 'mean_episode_return_0': 66.0340576171875,\n",
      " 'mean_episode_return_1': 73.77491760253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:33,506] After 1417600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 339.3738098144531,\n",
      " 'loss_1': 343.4611511230469,\n",
      " 'mean_episode_return_0': 66.0340576171875,\n",
      " 'mean_episode_return_1': 73.7472152709961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:38,512] After 1417600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 339.3738098144531,\n",
      " 'loss_1': 343.4611511230469,\n",
      " 'mean_episode_return_0': 66.0340576171875,\n",
      " 'mean_episode_return_1': 73.7472152709961}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:11:43,518] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:43,608] After 1420800 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 403.4878845214844,\n",
      " 'loss_1': 343.4611511230469,\n",
      " 'mean_episode_return_0': 66.1121826171875,\n",
      " 'mean_episode_return_1': 73.7472152709961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:48,613] After 1424000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.4878845214844,\n",
      " 'loss_1': 417.0990905761719,\n",
      " 'mean_episode_return_0': 66.1121826171875,\n",
      " 'mean_episode_return_1': 73.71610260009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:53,619] After 1424000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.4878845214844,\n",
      " 'loss_1': 417.0990905761719,\n",
      " 'mean_episode_return_0': 66.1121826171875,\n",
      " 'mean_episode_return_1': 73.71610260009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:11:58,625] After 1427200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.90777587890625,\n",
      " 'loss_1': 417.0990905761719,\n",
      " 'mean_episode_return_0': 66.1866226196289,\n",
      " 'mean_episode_return_1': 73.71610260009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:03,631] After 1430400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.90777587890625,\n",
      " 'loss_1': 390.5811767578125,\n",
      " 'mean_episode_return_0': 66.1866226196289,\n",
      " 'mean_episode_return_1': 73.6947021484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:08,636] After 1433600 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 393.9831848144531,\n",
      " 'loss_1': 390.5811767578125,\n",
      " 'mean_episode_return_0': 66.27481079101562,\n",
      " 'mean_episode_return_1': 73.6947021484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:13,642] After 1436800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 393.9831848144531,\n",
      " 'loss_1': 447.5655517578125,\n",
      " 'mean_episode_return_0': 66.27481079101562,\n",
      " 'mean_episode_return_1': 73.75446319580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:18,648] After 1436800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 393.9831848144531,\n",
      " 'loss_1': 447.5655517578125,\n",
      " 'mean_episode_return_0': 66.27481079101562,\n",
      " 'mean_episode_return_1': 73.75446319580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:23,652] After 1440000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 377.4620361328125,\n",
      " 'loss_1': 447.5655517578125,\n",
      " 'mean_episode_return_0': 66.2840576171875,\n",
      " 'mean_episode_return_1': 73.75446319580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:28,656] After 1443200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 377.4620361328125,\n",
      " 'loss_1': 412.1346740722656,\n",
      " 'mean_episode_return_0': 66.2840576171875,\n",
      " 'mean_episode_return_1': 73.80677795410156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:33,661] After 1443200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 377.4620361328125,\n",
      " 'loss_1': 412.1346740722656,\n",
      " 'mean_episode_return_0': 66.2840576171875,\n",
      " 'mean_episode_return_1': 73.80677795410156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:38,664] After 1446400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 401.2690124511719,\n",
      " 'loss_1': 412.1346740722656,\n",
      " 'mean_episode_return_0': 66.37349700927734,\n",
      " 'mean_episode_return_1': 73.80677795410156}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:12:43,668] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:43,759] After 1449600 frames: @ 628.2 fps Stats:\n",
      "{'loss_0': 401.2690124511719,\n",
      " 'loss_1': 326.3724365234375,\n",
      " 'mean_episode_return_0': 66.37349700927734,\n",
      " 'mean_episode_return_1': 73.79312133789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:48,764] After 1449600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.2690124511719,\n",
      " 'loss_1': 326.3724365234375,\n",
      " 'mean_episode_return_0': 66.37349700927734,\n",
      " 'mean_episode_return_1': 73.79312133789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:53,770] After 1452800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.05859375,\n",
      " 'loss_1': 326.3724365234375,\n",
      " 'mean_episode_return_0': 66.44218444824219,\n",
      " 'mean_episode_return_1': 73.79312133789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:12:58,776] After 1456000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.05859375,\n",
      " 'loss_1': 383.3228454589844,\n",
      " 'mean_episode_return_0': 66.44218444824219,\n",
      " 'mean_episode_return_1': 73.871337890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:03,782] After 1459200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.1904602050781,\n",
      " 'loss_1': 383.3228454589844,\n",
      " 'mean_episode_return_0': 66.49012756347656,\n",
      " 'mean_episode_return_1': 73.871337890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:08,788] After 1459200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.1904602050781,\n",
      " 'loss_1': 383.3228454589844,\n",
      " 'mean_episode_return_0': 66.49012756347656,\n",
      " 'mean_episode_return_1': 73.871337890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:13,794] After 1462400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.1904602050781,\n",
      " 'loss_1': 369.96826171875,\n",
      " 'mean_episode_return_0': 66.49012756347656,\n",
      " 'mean_episode_return_1': 73.8810043334961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:18,800] After 1465600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 355.7417907714844,\n",
      " 'loss_1': 369.96826171875,\n",
      " 'mean_episode_return_0': 66.54718780517578,\n",
      " 'mean_episode_return_1': 73.8810043334961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:23,804] After 1468800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 355.7417907714844,\n",
      " 'loss_1': 453.8780517578125,\n",
      " 'mean_episode_return_0': 66.54718780517578,\n",
      " 'mean_episode_return_1': 73.901123046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:28,810] After 1468800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.7417907714844,\n",
      " 'loss_1': 453.8780517578125,\n",
      " 'mean_episode_return_0': 66.54718780517578,\n",
      " 'mean_episode_return_1': 73.901123046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:33,816] After 1472000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 496.0085144042969,\n",
      " 'loss_1': 453.8780517578125,\n",
      " 'mean_episode_return_0': 66.62068176269531,\n",
      " 'mean_episode_return_1': 73.901123046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:38,822] After 1475200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 496.0085144042969,\n",
      " 'loss_1': 339.6937561035156,\n",
      " 'mean_episode_return_0': 66.62068176269531,\n",
      " 'mean_episode_return_1': 73.94710540771484}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:13:43,824] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:43,914] After 1478400 frames: @ 628.5 fps Stats:\n",
      "{'loss_0': 341.4134826660156,\n",
      " 'loss_1': 339.6937561035156,\n",
      " 'mean_episode_return_0': 66.71306610107422,\n",
      " 'mean_episode_return_1': 73.94710540771484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:48,920] After 1478400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.4134826660156,\n",
      " 'loss_1': 339.6937561035156,\n",
      " 'mean_episode_return_0': 66.71306610107422,\n",
      " 'mean_episode_return_1': 73.94710540771484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:13:53,926] After 1481600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 341.4134826660156,\n",
      " 'loss_1': 316.404296875,\n",
      " 'mean_episode_return_0': 66.71306610107422,\n",
      " 'mean_episode_return_1': 73.93489837646484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:13:58,932] After 1484800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 371.8145751953125,\n",
      " 'loss_1': 316.404296875,\n",
      " 'mean_episode_return_0': 66.82393646240234,\n",
      " 'mean_episode_return_1': 73.93489837646484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:03,938] After 1484800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 371.8145751953125,\n",
      " 'loss_1': 316.404296875,\n",
      " 'mean_episode_return_0': 66.82393646240234,\n",
      " 'mean_episode_return_1': 73.93489837646484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:08,944] After 1488000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 371.8145751953125,\n",
      " 'loss_1': 362.3956604003906,\n",
      " 'mean_episode_return_0': 66.82393646240234,\n",
      " 'mean_episode_return_1': 73.97051239013672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:13,950] After 1491200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 355.11358642578125,\n",
      " 'loss_1': 362.3956604003906,\n",
      " 'mean_episode_return_0': 66.90018463134766,\n",
      " 'mean_episode_return_1': 73.97051239013672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:18,956] After 1491200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.11358642578125,\n",
      " 'loss_1': 362.3956604003906,\n",
      " 'mean_episode_return_0': 66.90018463134766,\n",
      " 'mean_episode_return_1': 73.97051239013672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:23,962] After 1497600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 384.0280456542969,\n",
      " 'loss_1': 388.6430358886719,\n",
      " 'mean_episode_return_0': 66.94124603271484,\n",
      " 'mean_episode_return_1': 74.0040054321289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:28,968] After 1497600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.0280456542969,\n",
      " 'loss_1': 388.6430358886719,\n",
      " 'mean_episode_return_0': 66.94124603271484,\n",
      " 'mean_episode_return_1': 74.0040054321289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:33,974] After 1500800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 384.0280456542969,\n",
      " 'loss_1': 420.8601379394531,\n",
      " 'mean_episode_return_0': 66.94124603271484,\n",
      " 'mean_episode_return_1': 74.03145599365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:38,980] After 1504000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.4420166015625,\n",
      " 'loss_1': 420.8601379394531,\n",
      " 'mean_episode_return_0': 67.05437469482422,\n",
      " 'mean_episode_return_1': 74.03145599365234}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:14:43,986] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:44,076] After 1504000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 374.4420166015625,\n",
      " 'loss_1': 420.8601379394531,\n",
      " 'mean_episode_return_0': 67.05437469482422,\n",
      " 'mean_episode_return_1': 74.03145599365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:49,083] After 1507200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.4420166015625,\n",
      " 'loss_1': 375.41656494140625,\n",
      " 'mean_episode_return_0': 67.05437469482422,\n",
      " 'mean_episode_return_1': 74.05004119873047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:54,089] After 1510400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 356.2691955566406,\n",
      " 'loss_1': 375.41656494140625,\n",
      " 'mean_episode_return_0': 67.0545654296875,\n",
      " 'mean_episode_return_1': 74.05004119873047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:14:59,095] After 1510400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 356.2691955566406,\n",
      " 'loss_1': 375.41656494140625,\n",
      " 'mean_episode_return_0': 67.0545654296875,\n",
      " 'mean_episode_return_1': 74.05004119873047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:04,100] After 1513600 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 356.2691955566406,\n",
      " 'loss_1': 432.413818359375,\n",
      " 'mean_episode_return_0': 67.0545654296875,\n",
      " 'mean_episode_return_1': 74.11343383789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:09,104] After 1516800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 355.30596923828125,\n",
      " 'loss_1': 432.413818359375,\n",
      " 'mean_episode_return_0': 67.08518981933594,\n",
      " 'mean_episode_return_1': 74.11343383789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:14,110] After 1516800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 355.30596923828125,\n",
      " 'loss_1': 432.413818359375,\n",
      " 'mean_episode_return_0': 67.08518981933594,\n",
      " 'mean_episode_return_1': 74.11343383789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:19,115] After 1523200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 367.821044921875,\n",
      " 'loss_1': 495.7647399902344,\n",
      " 'mean_episode_return_0': 67.12150573730469,\n",
      " 'mean_episode_return_1': 74.14752197265625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:24,120] After 1523200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.821044921875,\n",
      " 'loss_1': 495.7647399902344,\n",
      " 'mean_episode_return_0': 67.12150573730469,\n",
      " 'mean_episode_return_1': 74.14752197265625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:29,124] After 1526400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 367.821044921875,\n",
      " 'loss_1': 474.4886474609375,\n",
      " 'mean_episode_return_0': 67.12150573730469,\n",
      " 'mean_episode_return_1': 74.23400115966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:34,128] After 1529600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 457.7554626464844,\n",
      " 'loss_1': 474.4886474609375,\n",
      " 'mean_episode_return_0': 67.17518615722656,\n",
      " 'mean_episode_return_1': 74.23400115966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:39,132] After 1529600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 457.7554626464844,\n",
      " 'loss_1': 474.4886474609375,\n",
      " 'mean_episode_return_0': 67.17518615722656,\n",
      " 'mean_episode_return_1': 74.23400115966797}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:15:44,135] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:44,226] After 1532800 frames: @ 628.3 fps Stats:\n",
      "{'loss_0': 457.7554626464844,\n",
      " 'loss_1': 365.388671875,\n",
      " 'mean_episode_return_0': 67.17518615722656,\n",
      " 'mean_episode_return_1': 74.24193572998047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:49,232] After 1536000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 400.51788330078125,\n",
      " 'loss_1': 365.388671875,\n",
      " 'mean_episode_return_0': 67.22643280029297,\n",
      " 'mean_episode_return_1': 74.24193572998047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:54,237] After 1536000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.51788330078125,\n",
      " 'loss_1': 365.388671875,\n",
      " 'mean_episode_return_0': 67.22643280029297,\n",
      " 'mean_episode_return_1': 74.24193572998047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:15:59,242] After 1542400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 305.74700927734375,\n",
      " 'loss_1': 392.6585388183594,\n",
      " 'mean_episode_return_0': 67.25606536865234,\n",
      " 'mean_episode_return_1': 74.2613296508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:04,248] After 1542400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 305.74700927734375,\n",
      " 'loss_1': 392.6585388183594,\n",
      " 'mean_episode_return_0': 67.25606536865234,\n",
      " 'mean_episode_return_1': 74.2613296508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:09,254] After 1542400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 305.74700927734375,\n",
      " 'loss_1': 392.6585388183594,\n",
      " 'mean_episode_return_0': 67.25606536865234,\n",
      " 'mean_episode_return_1': 74.2613296508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:14,260] After 1548800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 404.9010009765625,\n",
      " 'loss_1': 529.4732055664062,\n",
      " 'mean_episode_return_0': 67.28324890136719,\n",
      " 'mean_episode_return_1': 74.3824234008789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:19,266] After 1548800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.9010009765625,\n",
      " 'loss_1': 529.4732055664062,\n",
      " 'mean_episode_return_0': 67.28324890136719,\n",
      " 'mean_episode_return_1': 74.3824234008789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:24,272] After 1552000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.9010009765625,\n",
      " 'loss_1': 349.6839904785156,\n",
      " 'mean_episode_return_0': 67.28324890136719,\n",
      " 'mean_episode_return_1': 74.43407440185547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:29,278] After 1555200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 424.275390625,\n",
      " 'loss_1': 349.6839904785156,\n",
      " 'mean_episode_return_0': 67.39268493652344,\n",
      " 'mean_episode_return_1': 74.43407440185547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:34,284] After 1555200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 424.275390625,\n",
      " 'loss_1': 349.6839904785156,\n",
      " 'mean_episode_return_0': 67.39268493652344,\n",
      " 'mean_episode_return_1': 74.43407440185547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:39,290] After 1558400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 424.275390625,\n",
      " 'loss_1': 408.19366455078125,\n",
      " 'mean_episode_return_0': 67.39268493652344,\n",
      " 'mean_episode_return_1': 74.43356323242188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 13:16:44,296] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:44,386] After 1561600 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 379.160888671875,\n",
      " 'loss_1': 408.19366455078125,\n",
      " 'mean_episode_return_0': 67.4505615234375,\n",
      " 'mean_episode_return_1': 74.43356323242188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:49,392] After 1561600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 379.160888671875,\n",
      " 'loss_1': 408.19366455078125,\n",
      " 'mean_episode_return_0': 67.4505615234375,\n",
      " 'mean_episode_return_1': 74.43356323242188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:54,398] After 1568000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 406.2564392089844,\n",
      " 'loss_1': 347.3529968261719,\n",
      " 'mean_episode_return_0': 67.55325317382812,\n",
      " 'mean_episode_return_1': 74.47493743896484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:16:59,404] After 1568000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.2564392089844,\n",
      " 'loss_1': 347.3529968261719,\n",
      " 'mean_episode_return_0': 67.55325317382812,\n",
      " 'mean_episode_return_1': 74.47493743896484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:04,410] After 1568000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.2564392089844,\n",
      " 'loss_1': 347.3529968261719,\n",
      " 'mean_episode_return_0': 67.55325317382812,\n",
      " 'mean_episode_return_1': 74.47493743896484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:09,416] After 1574400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 367.9156188964844,\n",
      " 'loss_1': 409.9146728515625,\n",
      " 'mean_episode_return_0': 67.61038208007812,\n",
      " 'mean_episode_return_1': 74.51360321044922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:14,420] After 1574400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.9156188964844,\n",
      " 'loss_1': 409.9146728515625,\n",
      " 'mean_episode_return_0': 67.61038208007812,\n",
      " 'mean_episode_return_1': 74.51360321044922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:19,424] After 1577600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 367.9156188964844,\n",
      " 'loss_1': 421.63787841796875,\n",
      " 'mean_episode_return_0': 67.61038208007812,\n",
      " 'mean_episode_return_1': 74.51713562011719}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:24,430] After 1580800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 469.5978088378906,\n",
      " 'loss_1': 421.63787841796875,\n",
      " 'mean_episode_return_0': 67.69993591308594,\n",
      " 'mean_episode_return_1': 74.51713562011719}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:29,433] After 1580800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 469.5978088378906,\n",
      " 'loss_1': 421.63787841796875,\n",
      " 'mean_episode_return_0': 67.69993591308594,\n",
      " 'mean_episode_return_1': 74.51713562011719}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:34,439] After 1587200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 343.9652404785156,\n",
      " 'loss_1': 364.2271728515625,\n",
      " 'mean_episode_return_0': 67.75337219238281,\n",
      " 'mean_episode_return_1': 74.4737548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:39,445] After 1587200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 343.9652404785156,\n",
      " 'loss_1': 364.2271728515625,\n",
      " 'mean_episode_return_0': 67.75337219238281,\n",
      " 'mean_episode_return_1': 74.4737548828125}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:17:44,451] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:44,514] After 1587200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 343.9652404785156,\n",
      " 'loss_1': 364.2271728515625,\n",
      " 'mean_episode_return_0': 67.75337219238281,\n",
      " 'mean_episode_return_1': 74.4737548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:49,520] After 1593600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 392.5368347167969,\n",
      " 'loss_1': 463.24212646484375,\n",
      " 'mean_episode_return_0': 67.8136215209961,\n",
      " 'mean_episode_return_1': 74.5600357055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:54,524] After 1593600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 392.5368347167969,\n",
      " 'loss_1': 463.24212646484375,\n",
      " 'mean_episode_return_0': 67.8136215209961,\n",
      " 'mean_episode_return_1': 74.5600357055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:17:59,530] After 1593600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 392.5368347167969,\n",
      " 'loss_1': 463.24212646484375,\n",
      " 'mean_episode_return_0': 67.8136215209961,\n",
      " 'mean_episode_return_1': 74.5600357055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:04,533] After 1600000 frames: @ 1279.3 fps Stats:\n",
      "{'loss_0': 388.5174865722656,\n",
      " 'loss_1': 387.3707275390625,\n",
      " 'mean_episode_return_0': 67.86611938476562,\n",
      " 'mean_episode_return_1': 74.57997131347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:09,539] After 1600000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 388.5174865722656,\n",
      " 'loss_1': 387.3707275390625,\n",
      " 'mean_episode_return_0': 67.86611938476562,\n",
      " 'mean_episode_return_1': 74.57997131347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:14,545] After 1603200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 351.7701110839844,\n",
      " 'loss_1': 387.3707275390625,\n",
      " 'mean_episode_return_0': 67.87737274169922,\n",
      " 'mean_episode_return_1': 74.57997131347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:19,548] After 1606400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 351.7701110839844,\n",
      " 'loss_1': 422.52587890625,\n",
      " 'mean_episode_return_0': 67.87737274169922,\n",
      " 'mean_episode_return_1': 74.62144470214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:24,553] After 1606400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 351.7701110839844,\n",
      " 'loss_1': 422.52587890625,\n",
      " 'mean_episode_return_0': 67.87737274169922,\n",
      " 'mean_episode_return_1': 74.62144470214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:29,556] After 1612800 frames: @ 1279.7 fps Stats:\n",
      "{'loss_0': 348.1475830078125,\n",
      " 'loss_1': 360.0997619628906,\n",
      " 'mean_episode_return_0': 67.94393157958984,\n",
      " 'mean_episode_return_1': 74.671630859375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:34,561] After 1612800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.1475830078125,\n",
      " 'loss_1': 360.0997619628906,\n",
      " 'mean_episode_return_0': 67.94393157958984,\n",
      " 'mean_episode_return_1': 74.671630859375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:39,567] After 1612800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.1475830078125,\n",
      " 'loss_1': 360.0997619628906,\n",
      " 'mean_episode_return_0': 67.94393157958984,\n",
      " 'mean_episode_return_1': 74.671630859375}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:18:44,573] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:44,641] After 1619200 frames: @ 1261.6 fps Stats:\n",
      "{'loss_0': 374.5743713378906,\n",
      " 'loss_1': 360.5398254394531,\n",
      " 'mean_episode_return_0': 67.99868774414062,\n",
      " 'mean_episode_return_1': 74.73571014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:49,644] After 1619200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 374.5743713378906,\n",
      " 'loss_1': 360.5398254394531,\n",
      " 'mean_episode_return_0': 67.99868774414062,\n",
      " 'mean_episode_return_1': 74.73571014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:54,648] After 1622400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 403.05010986328125,\n",
      " 'loss_1': 360.5398254394531,\n",
      " 'mean_episode_return_0': 68.11893463134766,\n",
      " 'mean_episode_return_1': 74.73571014404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:18:59,653] After 1625600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.05010986328125,\n",
      " 'loss_1': 337.7763977050781,\n",
      " 'mean_episode_return_0': 68.11893463134766,\n",
      " 'mean_episode_return_1': 74.726318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:04,656] After 1625600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.05010986328125,\n",
      " 'loss_1': 337.7763977050781,\n",
      " 'mean_episode_return_0': 68.11893463134766,\n",
      " 'mean_episode_return_1': 74.726318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:09,660] After 1628800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 371.7591857910156,\n",
      " 'loss_1': 337.7763977050781,\n",
      " 'mean_episode_return_0': 68.24312591552734,\n",
      " 'mean_episode_return_1': 74.726318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:14,666] After 1632000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 371.7591857910156,\n",
      " 'loss_1': 420.73529052734375,\n",
      " 'mean_episode_return_0': 68.24312591552734,\n",
      " 'mean_episode_return_1': 74.71703338623047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:19,668] After 1632000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 371.7591857910156,\n",
      " 'loss_1': 420.73529052734375,\n",
      " 'mean_episode_return_0': 68.24312591552734,\n",
      " 'mean_episode_return_1': 74.71703338623047}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:24,674] After 1638400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 435.8653564453125,\n",
      " 'loss_1': 388.44659423828125,\n",
      " 'mean_episode_return_0': 68.2985610961914,\n",
      " 'mean_episode_return_1': 74.74715423583984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:19:29,680] After 1638400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 435.8653564453125,\n",
      " 'loss_1': 388.44659423828125,\n",
      " 'mean_episode_return_0': 68.2985610961914,\n",
      " 'mean_episode_return_1': 74.74715423583984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:34,686] After 1638400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 435.8653564453125,\n",
      " 'loss_1': 388.44659423828125,\n",
      " 'mean_episode_return_0': 68.2985610961914,\n",
      " 'mean_episode_return_1': 74.74715423583984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:39,692] After 1644800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 357.5675048828125,\n",
      " 'loss_1': 418.1136474609375,\n",
      " 'mean_episode_return_0': 68.38624572753906,\n",
      " 'mean_episode_return_1': 74.7659912109375}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:19:44,697] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:44,789] After 1644800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 357.5675048828125,\n",
      " 'loss_1': 418.1136474609375,\n",
      " 'mean_episode_return_0': 68.38624572753906,\n",
      " 'mean_episode_return_1': 74.7659912109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:49,795] After 1648000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 392.0534362792969,\n",
      " 'loss_1': 418.1136474609375,\n",
      " 'mean_episode_return_0': 68.4651870727539,\n",
      " 'mean_episode_return_1': 74.7659912109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:54,801] After 1651200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 392.0534362792969,\n",
      " 'loss_1': 348.1731872558594,\n",
      " 'mean_episode_return_0': 68.4651870727539,\n",
      " 'mean_episode_return_1': 74.80738830566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:19:59,807] After 1651200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 392.0534362792969,\n",
      " 'loss_1': 348.1731872558594,\n",
      " 'mean_episode_return_0': 68.4651870727539,\n",
      " 'mean_episode_return_1': 74.80738830566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:04,813] After 1654400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 385.5984191894531,\n",
      " 'loss_1': 348.1731872558594,\n",
      " 'mean_episode_return_0': 68.56288146972656,\n",
      " 'mean_episode_return_1': 74.80738830566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:09,819] After 1657600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 385.5984191894531,\n",
      " 'loss_1': 410.63671875,\n",
      " 'mean_episode_return_0': 68.56288146972656,\n",
      " 'mean_episode_return_1': 74.82763671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:14,825] After 1657600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 385.5984191894531,\n",
      " 'loss_1': 410.63671875,\n",
      " 'mean_episode_return_0': 68.56288146972656,\n",
      " 'mean_episode_return_1': 74.82763671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:19,831] After 1664000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 373.50311279296875,\n",
      " 'loss_1': 355.4582214355469,\n",
      " 'mean_episode_return_0': 68.6343765258789,\n",
      " 'mean_episode_return_1': 74.86469268798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:24,837] After 1664000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 373.50311279296875,\n",
      " 'loss_1': 355.4582214355469,\n",
      " 'mean_episode_return_0': 68.6343765258789,\n",
      " 'mean_episode_return_1': 74.86469268798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:29,840] After 1667200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 451.8671875,\n",
      " 'loss_1': 355.4582214355469,\n",
      " 'mean_episode_return_0': 68.70269012451172,\n",
      " 'mean_episode_return_1': 74.86469268798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:34,846] After 1670400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 451.8671875,\n",
      " 'loss_1': 402.7478942871094,\n",
      " 'mean_episode_return_0': 68.70269012451172,\n",
      " 'mean_episode_return_1': 74.87061309814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:39,852] After 1670400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.8671875,\n",
      " 'loss_1': 402.7478942871094,\n",
      " 'mean_episode_return_0': 68.70269012451172,\n",
      " 'mean_episode_return_1': 74.87061309814453}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:20:44,857] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:44,949] After 1673600 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 372.6181945800781,\n",
      " 'loss_1': 402.7478942871094,\n",
      " 'mean_episode_return_0': 68.76781463623047,\n",
      " 'mean_episode_return_1': 74.87061309814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:49,955] After 1676800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 372.6181945800781,\n",
      " 'loss_1': 438.9048767089844,\n",
      " 'mean_episode_return_0': 68.76781463623047,\n",
      " 'mean_episode_return_1': 74.94703674316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:54,961] After 1676800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.6181945800781,\n",
      " 'loss_1': 438.9048767089844,\n",
      " 'mean_episode_return_0': 68.76781463623047,\n",
      " 'mean_episode_return_1': 74.94703674316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:20:59,964] After 1680000 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 348.4674987792969,\n",
      " 'loss_1': 438.9048767089844,\n",
      " 'mean_episode_return_0': 68.81087493896484,\n",
      " 'mean_episode_return_1': 74.94703674316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:04,968] After 1683200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 348.4674987792969,\n",
      " 'loss_1': 413.8209228515625,\n",
      " 'mean_episode_return_0': 68.81087493896484,\n",
      " 'mean_episode_return_1': 74.98640441894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:09,974] After 1683200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.4674987792969,\n",
      " 'loss_1': 413.8209228515625,\n",
      " 'mean_episode_return_0': 68.81087493896484,\n",
      " 'mean_episode_return_1': 74.98640441894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:14,980] After 1689600 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 395.009521484375,\n",
      " 'loss_1': 396.4123229980469,\n",
      " 'mean_episode_return_0': 68.8724365234375,\n",
      " 'mean_episode_return_1': 74.98340606689453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:19,986] After 1689600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.009521484375,\n",
      " 'loss_1': 396.4123229980469,\n",
      " 'mean_episode_return_0': 68.8724365234375,\n",
      " 'mean_episode_return_1': 74.98340606689453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:24,992] After 1692800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.259765625,\n",
      " 'loss_1': 396.4123229980469,\n",
      " 'mean_episode_return_0': 68.95087432861328,\n",
      " 'mean_episode_return_1': 74.98340606689453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:29,998] After 1696000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.259765625,\n",
      " 'loss_1': 413.4676208496094,\n",
      " 'mean_episode_return_0': 68.95087432861328,\n",
      " 'mean_episode_return_1': 74.9940185546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:35,002] After 1696000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.259765625,\n",
      " 'loss_1': 413.4676208496094,\n",
      " 'mean_episode_return_0': 68.95087432861328,\n",
      " 'mean_episode_return_1': 74.9940185546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:40,008] After 1699200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 442.29437255859375,\n",
      " 'loss_1': 413.4676208496094,\n",
      " 'mean_episode_return_0': 69.09200286865234,\n",
      " 'mean_episode_return_1': 74.9940185546875}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:21:45,014] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:45,105] After 1702400 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 442.29437255859375,\n",
      " 'loss_1': 370.9164733886719,\n",
      " 'mean_episode_return_0': 69.09200286865234,\n",
      " 'mean_episode_return_1': 74.93770599365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:50,111] After 1705600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 345.97381591796875,\n",
      " 'loss_1': 370.9164733886719,\n",
      " 'mean_episode_return_0': 69.11556243896484,\n",
      " 'mean_episode_return_1': 74.93770599365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:21:55,117] After 1705600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.97381591796875,\n",
      " 'loss_1': 370.9164733886719,\n",
      " 'mean_episode_return_0': 69.11556243896484,\n",
      " 'mean_episode_return_1': 74.93770599365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:00,123] After 1708800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 345.97381591796875,\n",
      " 'loss_1': 477.7170104980469,\n",
      " 'mean_episode_return_0': 69.11556243896484,\n",
      " 'mean_episode_return_1': 74.93895721435547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:05,129] After 1712000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.61944580078125,\n",
      " 'loss_1': 477.7170104980469,\n",
      " 'mean_episode_return_0': 69.17281341552734,\n",
      " 'mean_episode_return_1': 74.93895721435547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:10,135] After 1715200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.61944580078125,\n",
      " 'loss_1': 403.9206237792969,\n",
      " 'mean_episode_return_0': 69.17281341552734,\n",
      " 'mean_episode_return_1': 74.95948791503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:15,141] After 1715200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.61944580078125,\n",
      " 'loss_1': 403.9206237792969,\n",
      " 'mean_episode_return_0': 69.17281341552734,\n",
      " 'mean_episode_return_1': 74.95948791503906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:22:20,147] After 1718400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.2935485839844,\n",
      " 'loss_1': 403.9206237792969,\n",
      " 'mean_episode_return_0': 69.22537231445312,\n",
      " 'mean_episode_return_1': 74.95948791503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:25,153] After 1721600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.2935485839844,\n",
      " 'loss_1': 389.9698486328125,\n",
      " 'mean_episode_return_0': 69.22537231445312,\n",
      " 'mean_episode_return_1': 75.0460433959961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:30,159] After 1724800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 358.1968994140625,\n",
      " 'loss_1': 389.9698486328125,\n",
      " 'mean_episode_return_0': 69.286376953125,\n",
      " 'mean_episode_return_1': 75.0460433959961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:35,165] After 1724800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.1968994140625,\n",
      " 'loss_1': 389.9698486328125,\n",
      " 'mean_episode_return_0': 69.286376953125,\n",
      " 'mean_episode_return_1': 75.0460433959961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:40,171] After 1728000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 358.1968994140625,\n",
      " 'loss_1': 419.521240234375,\n",
      " 'mean_episode_return_0': 69.286376953125,\n",
      " 'mean_episode_return_1': 75.09500885009766}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:22:45,176] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:45,268] After 1731200 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 485.2613525390625,\n",
      " 'loss_1': 419.521240234375,\n",
      " 'mean_episode_return_0': 69.3400650024414,\n",
      " 'mean_episode_return_1': 75.09500885009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:50,274] After 1734400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 485.2613525390625,\n",
      " 'loss_1': 399.0138244628906,\n",
      " 'mean_episode_return_0': 69.3400650024414,\n",
      " 'mean_episode_return_1': 75.15699768066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:22:55,280] After 1734400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 485.2613525390625,\n",
      " 'loss_1': 399.0138244628906,\n",
      " 'mean_episode_return_0': 69.3400650024414,\n",
      " 'mean_episode_return_1': 75.15699768066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:00,286] After 1737600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 379.403076171875,\n",
      " 'loss_1': 399.0138244628906,\n",
      " 'mean_episode_return_0': 69.42724609375,\n",
      " 'mean_episode_return_1': 75.15699768066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:05,292] After 1740800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 379.403076171875,\n",
      " 'loss_1': 365.6593933105469,\n",
      " 'mean_episode_return_0': 69.42724609375,\n",
      " 'mean_episode_return_1': 75.13694763183594}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:10,298] After 1744000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.87261962890625,\n",
      " 'loss_1': 365.6593933105469,\n",
      " 'mean_episode_return_0': 69.49562072753906,\n",
      " 'mean_episode_return_1': 75.13694763183594}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:15,304] After 1744000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 340.87261962890625,\n",
      " 'loss_1': 365.6593933105469,\n",
      " 'mean_episode_return_0': 69.49562072753906,\n",
      " 'mean_episode_return_1': 75.13694763183594}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:20,310] After 1747200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.87261962890625,\n",
      " 'loss_1': 404.0948486328125,\n",
      " 'mean_episode_return_0': 69.49562072753906,\n",
      " 'mean_episode_return_1': 75.14810180664062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:25,316] After 1750400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.4288330078125,\n",
      " 'loss_1': 404.0948486328125,\n",
      " 'mean_episode_return_0': 69.56881713867188,\n",
      " 'mean_episode_return_1': 75.14810180664062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:30,322] After 1750400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 378.4288330078125,\n",
      " 'loss_1': 404.0948486328125,\n",
      " 'mean_episode_return_0': 69.56881713867188,\n",
      " 'mean_episode_return_1': 75.14810180664062}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:35,328] After 1753600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.4288330078125,\n",
      " 'loss_1': 462.6824951171875,\n",
      " 'mean_episode_return_0': 69.56881713867188,\n",
      " 'mean_episode_return_1': 75.22804260253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:40,334] After 1756800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 352.9048767089844,\n",
      " 'loss_1': 462.6824951171875,\n",
      " 'mean_episode_return_0': 69.63587188720703,\n",
      " 'mean_episode_return_1': 75.22804260253906}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:23:45,340] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:45,429] After 1760000 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 352.9048767089844,\n",
      " 'loss_1': 402.1212158203125,\n",
      " 'mean_episode_return_0': 69.63587188720703,\n",
      " 'mean_episode_return_1': 75.248291015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:50,435] After 1763200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 351.3790588378906,\n",
      " 'loss_1': 402.1212158203125,\n",
      " 'mean_episode_return_0': 69.73412322998047,\n",
      " 'mean_episode_return_1': 75.248291015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:23:55,440] After 1763200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 351.3790588378906,\n",
      " 'loss_1': 402.1212158203125,\n",
      " 'mean_episode_return_0': 69.73412322998047,\n",
      " 'mean_episode_return_1': 75.248291015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:00,446] After 1766400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 351.3790588378906,\n",
      " 'loss_1': 445.4725341796875,\n",
      " 'mean_episode_return_0': 69.73412322998047,\n",
      " 'mean_episode_return_1': 75.30271911621094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:05,452] After 1769600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.4817810058594,\n",
      " 'loss_1': 445.4725341796875,\n",
      " 'mean_episode_return_0': 69.7848129272461,\n",
      " 'mean_episode_return_1': 75.30271911621094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:10,456] After 1769600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.4817810058594,\n",
      " 'loss_1': 445.4725341796875,\n",
      " 'mean_episode_return_0': 69.7848129272461,\n",
      " 'mean_episode_return_1': 75.30271911621094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:15,462] After 1776000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 329.2926940917969,\n",
      " 'loss_1': 500.90728759765625,\n",
      " 'mean_episode_return_0': 69.77375030517578,\n",
      " 'mean_episode_return_1': 75.3460922241211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:20,468] After 1776000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 329.2926940917969,\n",
      " 'loss_1': 500.90728759765625,\n",
      " 'mean_episode_return_0': 69.77375030517578,\n",
      " 'mean_episode_return_1': 75.3460922241211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:25,474] After 1779200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 329.2926940917969,\n",
      " 'loss_1': 471.0045166015625,\n",
      " 'mean_episode_return_0': 69.77375030517578,\n",
      " 'mean_episode_return_1': 75.41262817382812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:30,480] After 1782400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.6141357421875,\n",
      " 'loss_1': 471.0045166015625,\n",
      " 'mean_episode_return_0': 69.80762481689453,\n",
      " 'mean_episode_return_1': 75.41262817382812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:35,486] After 1782400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.6141357421875,\n",
      " 'loss_1': 471.0045166015625,\n",
      " 'mean_episode_return_0': 69.80762481689453,\n",
      " 'mean_episode_return_1': 75.41262817382812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:40,492] After 1785600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.6141357421875,\n",
      " 'loss_1': 376.7370300292969,\n",
      " 'mean_episode_return_0': 69.80762481689453,\n",
      " 'mean_episode_return_1': 75.39789581298828}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:24:45,498] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:45,586] After 1788800 frames: @ 628.2 fps Stats:\n",
      "{'loss_0': 362.3960876464844,\n",
      " 'loss_1': 376.7370300292969,\n",
      " 'mean_episode_return_0': 69.86031341552734,\n",
      " 'mean_episode_return_1': 75.39789581298828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:50,593] After 1788800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 362.3960876464844,\n",
      " 'loss_1': 376.7370300292969,\n",
      " 'mean_episode_return_0': 69.86031341552734,\n",
      " 'mean_episode_return_1': 75.39789581298828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:24:55,599] After 1795200 frames: @ 1278.6 fps Stats:\n",
      "{'loss_0': 382.23687744140625,\n",
      " 'loss_1': 434.0174865722656,\n",
      " 'mean_episode_return_0': 69.88662719726562,\n",
      " 'mean_episode_return_1': 75.45280456542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:00,605] After 1795200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 382.23687744140625,\n",
      " 'loss_1': 434.0174865722656,\n",
      " 'mean_episode_return_0': 69.88662719726562,\n",
      " 'mean_episode_return_1': 75.45280456542969}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:25:05,611] After 1798400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 382.23687744140625,\n",
      " 'loss_1': 453.962890625,\n",
      " 'mean_episode_return_0': 69.88662719726562,\n",
      " 'mean_episode_return_1': 75.45207214355469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:10,614] After 1801600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 388.6062316894531,\n",
      " 'loss_1': 453.962890625,\n",
      " 'mean_episode_return_0': 69.96187591552734,\n",
      " 'mean_episode_return_1': 75.45207214355469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:15,620] After 1801600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 388.6062316894531,\n",
      " 'loss_1': 453.962890625,\n",
      " 'mean_episode_return_0': 69.96187591552734,\n",
      " 'mean_episode_return_1': 75.45207214355469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:20,624] After 1804800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 388.6062316894531,\n",
      " 'loss_1': 416.4874267578125,\n",
      " 'mean_episode_return_0': 69.96187591552734,\n",
      " 'mean_episode_return_1': 75.4944076538086}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:25,630] After 1808000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 397.79693603515625,\n",
      " 'loss_1': 416.4874267578125,\n",
      " 'mean_episode_return_0': 69.98993682861328,\n",
      " 'mean_episode_return_1': 75.4944076538086}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:30,636] After 1808000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.79693603515625,\n",
      " 'loss_1': 416.4874267578125,\n",
      " 'mean_episode_return_0': 69.98993682861328,\n",
      " 'mean_episode_return_1': 75.4944076538086}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:35,641] After 1814400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 369.78643798828125,\n",
      " 'loss_1': 432.3602294921875,\n",
      " 'mean_episode_return_0': 70.04049682617188,\n",
      " 'mean_episode_return_1': 75.5287094116211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:40,647] After 1814400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 369.78643798828125,\n",
      " 'loss_1': 432.3602294921875,\n",
      " 'mean_episode_return_0': 70.04049682617188,\n",
      " 'mean_episode_return_1': 75.5287094116211}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:25:45,653] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:45,742] After 1814400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 369.78643798828125,\n",
      " 'loss_1': 432.3602294921875,\n",
      " 'mean_episode_return_0': 70.04049682617188,\n",
      " 'mean_episode_return_1': 75.5287094116211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:50,748] After 1820800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 386.9588317871094,\n",
      " 'loss_1': 469.0052185058594,\n",
      " 'mean_episode_return_0': 70.03569030761719,\n",
      " 'mean_episode_return_1': 75.60993194580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:25:55,754] After 1820800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.9588317871094,\n",
      " 'loss_1': 469.0052185058594,\n",
      " 'mean_episode_return_0': 70.03569030761719,\n",
      " 'mean_episode_return_1': 75.60993194580078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:00,756] After 1824000 frames: @ 640.0 fps Stats:\n",
      "{'loss_0': 386.9588317871094,\n",
      " 'loss_1': 462.3724060058594,\n",
      " 'mean_episode_return_0': 70.03569030761719,\n",
      " 'mean_episode_return_1': 75.6622085571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:05,762] After 1827200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 456.6898193359375,\n",
      " 'loss_1': 462.3724060058594,\n",
      " 'mean_episode_return_0': 70.07093811035156,\n",
      " 'mean_episode_return_1': 75.6622085571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:10,767] After 1827200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 456.6898193359375,\n",
      " 'loss_1': 462.3724060058594,\n",
      " 'mean_episode_return_0': 70.07093811035156,\n",
      " 'mean_episode_return_1': 75.6622085571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:15,774] After 1833600 frames: @ 1278.6 fps Stats:\n",
      "{'loss_0': 461.2825622558594,\n",
      " 'loss_1': 403.2094421386719,\n",
      " 'mean_episode_return_0': 70.16231536865234,\n",
      " 'mean_episode_return_1': 75.69292449951172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:20,780] After 1833600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 461.2825622558594,\n",
      " 'loss_1': 403.2094421386719,\n",
      " 'mean_episode_return_0': 70.16231536865234,\n",
      " 'mean_episode_return_1': 75.69292449951172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:25,786] After 1833600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 461.2825622558594,\n",
      " 'loss_1': 403.2094421386719,\n",
      " 'mean_episode_return_0': 70.16231536865234,\n",
      " 'mean_episode_return_1': 75.69292449951172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:30,791] After 1840000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 405.2392883300781,\n",
      " 'loss_1': 457.57232666015625,\n",
      " 'mean_episode_return_0': 70.2126235961914,\n",
      " 'mean_episode_return_1': 75.7412109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:35,798] After 1840000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.2392883300781,\n",
      " 'loss_1': 457.57232666015625,\n",
      " 'mean_episode_return_0': 70.2126235961914,\n",
      " 'mean_episode_return_1': 75.7412109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:40,803] After 1840000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.2392883300781,\n",
      " 'loss_1': 457.57232666015625,\n",
      " 'mean_episode_return_0': 70.2126235961914,\n",
      " 'mean_episode_return_1': 75.7412109375}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:26:45,810] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:45,898] After 1846400 frames: @ 1256.5 fps Stats:\n",
      "{'loss_0': 405.8096923828125,\n",
      " 'loss_1': 443.5422668457031,\n",
      " 'mean_episode_return_0': 70.2231216430664,\n",
      " 'mean_episode_return_1': 75.80403900146484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:50,904] After 1846400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.8096923828125,\n",
      " 'loss_1': 443.5422668457031,\n",
      " 'mean_episode_return_0': 70.2231216430664,\n",
      " 'mean_episode_return_1': 75.80403900146484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:26:55,908] After 1849600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 385.36328125,\n",
      " 'loss_1': 443.5422668457031,\n",
      " 'mean_episode_return_0': 70.2614974975586,\n",
      " 'mean_episode_return_1': 75.80403900146484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:00,914] After 1852800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 385.36328125,\n",
      " 'loss_1': 400.6667175292969,\n",
      " 'mean_episode_return_0': 70.2614974975586,\n",
      " 'mean_episode_return_1': 75.82073211669922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:05,920] After 1852800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 385.36328125,\n",
      " 'loss_1': 400.6667175292969,\n",
      " 'mean_episode_return_0': 70.2614974975586,\n",
      " 'mean_episode_return_1': 75.82073211669922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:10,926] After 1859200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 402.2999267578125,\n",
      " 'loss_1': 443.59765625,\n",
      " 'mean_episode_return_0': 70.32793426513672,\n",
      " 'mean_episode_return_1': 75.87932586669922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:15,928] After 1859200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.2999267578125,\n",
      " 'loss_1': 443.59765625,\n",
      " 'mean_episode_return_0': 70.32793426513672,\n",
      " 'mean_episode_return_1': 75.87932586669922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:20,934] After 1859200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.2999267578125,\n",
      " 'loss_1': 443.59765625,\n",
      " 'mean_episode_return_0': 70.32793426513672,\n",
      " 'mean_episode_return_1': 75.87932586669922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:25,940] After 1865600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 395.4071350097656,\n",
      " 'loss_1': 478.8965148925781,\n",
      " 'mean_episode_return_0': 70.29556274414062,\n",
      " 'mean_episode_return_1': 76.00206756591797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:30,946] After 1865600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.4071350097656,\n",
      " 'loss_1': 478.8965148925781,\n",
      " 'mean_episode_return_0': 70.29556274414062,\n",
      " 'mean_episode_return_1': 76.00206756591797}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:35,952] After 1872000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 379.9705810546875,\n",
      " 'loss_1': 511.308349609375,\n",
      " 'mean_episode_return_0': 70.27337646484375,\n",
      " 'mean_episode_return_1': 76.112548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:40,958] After 1872000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 379.9705810546875,\n",
      " 'loss_1': 511.308349609375,\n",
      " 'mean_episode_return_0': 70.27337646484375,\n",
      " 'mean_episode_return_1': 76.112548828125}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:27:45,964] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:46,054] After 1872000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 379.9705810546875,\n",
      " 'loss_1': 511.308349609375,\n",
      " 'mean_episode_return_0': 70.27337646484375,\n",
      " 'mean_episode_return_1': 76.112548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:27:51,056] After 1878400 frames: @ 1279.8 fps Stats:\n",
      "{'loss_0': 390.2044982910156,\n",
      " 'loss_1': 479.753662109375,\n",
      " 'mean_episode_return_0': 70.35518646240234,\n",
      " 'mean_episode_return_1': 76.14862060546875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:27:56,062] After 1878400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 390.2044982910156,\n",
      " 'loss_1': 479.753662109375,\n",
      " 'mean_episode_return_0': 70.35518646240234,\n",
      " 'mean_episode_return_1': 76.14862060546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:01,067] After 1878400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 390.2044982910156,\n",
      " 'loss_1': 479.753662109375,\n",
      " 'mean_episode_return_0': 70.35518646240234,\n",
      " 'mean_episode_return_1': 76.14862060546875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:06,073] After 1884800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 442.9769287109375,\n",
      " 'loss_1': 450.0350646972656,\n",
      " 'mean_episode_return_0': 70.45637512207031,\n",
      " 'mean_episode_return_1': 76.19808197021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:11,079] After 1884800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.9769287109375,\n",
      " 'loss_1': 450.0350646972656,\n",
      " 'mean_episode_return_0': 70.45637512207031,\n",
      " 'mean_episode_return_1': 76.19808197021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:16,085] After 1888000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.9391174316406,\n",
      " 'loss_1': 450.0350646972656,\n",
      " 'mean_episode_return_0': 70.46687316894531,\n",
      " 'mean_episode_return_1': 76.19808197021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:21,091] After 1891200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.9391174316406,\n",
      " 'loss_1': 398.8879089355469,\n",
      " 'mean_episode_return_0': 70.46687316894531,\n",
      " 'mean_episode_return_1': 76.22740173339844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:26,097] After 1891200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.9391174316406,\n",
      " 'loss_1': 398.8879089355469,\n",
      " 'mean_episode_return_0': 70.46687316894531,\n",
      " 'mean_episode_return_1': 76.22740173339844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:31,103] After 1897600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 345.6647644042969,\n",
      " 'loss_1': 486.07427978515625,\n",
      " 'mean_episode_return_0': 70.50556182861328,\n",
      " 'mean_episode_return_1': 76.20355987548828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:36,109] After 1897600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.6647644042969,\n",
      " 'loss_1': 486.07427978515625,\n",
      " 'mean_episode_return_0': 70.50556182861328,\n",
      " 'mean_episode_return_1': 76.20355987548828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:41,115] After 1897600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.6647644042969,\n",
      " 'loss_1': 486.07427978515625,\n",
      " 'mean_episode_return_0': 70.50556182861328,\n",
      " 'mean_episode_return_1': 76.20355987548828}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:28:46,121] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:46,211] After 1904000 frames: @ 1256.1 fps Stats:\n",
      "{'loss_0': 445.9275207519531,\n",
      " 'loss_1': 479.7471618652344,\n",
      " 'mean_episode_return_0': 70.53362274169922,\n",
      " 'mean_episode_return_1': 76.2952651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:51,216] After 1904000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 445.9275207519531,\n",
      " 'loss_1': 479.7471618652344,\n",
      " 'mean_episode_return_0': 70.53362274169922,\n",
      " 'mean_episode_return_1': 76.2952651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:28:56,222] After 1907200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.40765380859375,\n",
      " 'loss_1': 479.7471618652344,\n",
      " 'mean_episode_return_0': 70.62950134277344,\n",
      " 'mean_episode_return_1': 76.2952651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:01,228] After 1910400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 403.40765380859375,\n",
      " 'loss_1': 473.8746643066406,\n",
      " 'mean_episode_return_0': 70.62950134277344,\n",
      " 'mean_episode_return_1': 76.29371643066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:06,234] After 1910400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.40765380859375,\n",
      " 'loss_1': 473.8746643066406,\n",
      " 'mean_episode_return_0': 70.62950134277344,\n",
      " 'mean_episode_return_1': 76.29371643066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:11,238] After 1916800 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 394.4670104980469,\n",
      " 'loss_1': 479.7395935058594,\n",
      " 'mean_episode_return_0': 70.6866226196289,\n",
      " 'mean_episode_return_1': 76.33963775634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:16,244] After 1916800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 394.4670104980469,\n",
      " 'loss_1': 479.7395935058594,\n",
      " 'mean_episode_return_0': 70.6866226196289,\n",
      " 'mean_episode_return_1': 76.33963775634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:21,250] After 1920000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 394.64019775390625,\n",
      " 'loss_1': 479.7395935058594,\n",
      " 'mean_episode_return_0': 70.72661590576172,\n",
      " 'mean_episode_return_1': 76.33963775634766}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:26,255] After 1923200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 394.64019775390625,\n",
      " 'loss_1': 428.00970458984375,\n",
      " 'mean_episode_return_0': 70.72661590576172,\n",
      " 'mean_episode_return_1': 76.37885284423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:31,260] After 1923200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 394.64019775390625,\n",
      " 'loss_1': 428.00970458984375,\n",
      " 'mean_episode_return_0': 70.72661590576172,\n",
      " 'mean_episode_return_1': 76.37885284423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:36,266] After 1926400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 348.3104248046875,\n",
      " 'loss_1': 428.00970458984375,\n",
      " 'mean_episode_return_0': 70.71300506591797,\n",
      " 'mean_episode_return_1': 76.37885284423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:41,272] After 1929600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 348.3104248046875,\n",
      " 'loss_1': 458.8871765136719,\n",
      " 'mean_episode_return_0': 70.71300506591797,\n",
      " 'mean_episode_return_1': 76.3814468383789}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:29:46,278] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:46,400] After 1929600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 348.3104248046875,\n",
      " 'loss_1': 458.8871765136719,\n",
      " 'mean_episode_return_0': 70.71300506591797,\n",
      " 'mean_episode_return_1': 76.3814468383789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:51,406] After 1932800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.59686279296875,\n",
      " 'loss_1': 458.8871765136719,\n",
      " 'mean_episode_return_0': 70.74506378173828,\n",
      " 'mean_episode_return_1': 76.3814468383789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:29:56,412] After 1936000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.59686279296875,\n",
      " 'loss_1': 404.6588439941406,\n",
      " 'mean_episode_return_0': 70.74506378173828,\n",
      " 'mean_episode_return_1': 76.45919036865234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:01,418] After 1939200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 452.5602111816406,\n",
      " 'loss_1': 404.6588439941406,\n",
      " 'mean_episode_return_0': 70.76181030273438,\n",
      " 'mean_episode_return_1': 76.45919036865234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:06,424] After 1942400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 452.5602111816406,\n",
      " 'loss_1': 474.4715881347656,\n",
      " 'mean_episode_return_0': 70.76181030273438,\n",
      " 'mean_episode_return_1': 76.49542236328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:11,428] After 1942400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 452.5602111816406,\n",
      " 'loss_1': 474.4715881347656,\n",
      " 'mean_episode_return_0': 70.76181030273438,\n",
      " 'mean_episode_return_1': 76.49542236328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:16,432] After 1945600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 400.40960693359375,\n",
      " 'loss_1': 474.4715881347656,\n",
      " 'mean_episode_return_0': 70.82356262207031,\n",
      " 'mean_episode_return_1': 76.49542236328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:21,438] After 1948800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 400.40960693359375,\n",
      " 'loss_1': 383.3497619628906,\n",
      " 'mean_episode_return_0': 70.82356262207031,\n",
      " 'mean_episode_return_1': 76.57212829589844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:26,443] After 1948800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.40960693359375,\n",
      " 'loss_1': 383.3497619628906,\n",
      " 'mean_episode_return_0': 70.82356262207031,\n",
      " 'mean_episode_return_1': 76.57212829589844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:31,449] After 1952000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 405.20843505859375,\n",
      " 'loss_1': 383.3497619628906,\n",
      " 'mean_episode_return_0': 70.8505630493164,\n",
      " 'mean_episode_return_1': 76.57212829589844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:36,452] After 1955200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 405.20843505859375,\n",
      " 'loss_1': 443.455078125,\n",
      " 'mean_episode_return_0': 70.8505630493164,\n",
      " 'mean_episode_return_1': 76.60895538330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:41,458] After 1958400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 387.9803771972656,\n",
      " 'loss_1': 443.455078125,\n",
      " 'mean_episode_return_0': 70.93193054199219,\n",
      " 'mean_episode_return_1': 76.60895538330078}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 13:30:46,463] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:46,550] After 1961600 frames: @ 628.5 fps Stats:\n",
      "{'loss_0': 387.9803771972656,\n",
      " 'loss_1': 409.71490478515625,\n",
      " 'mean_episode_return_0': 70.93193054199219,\n",
      " 'mean_episode_return_1': 76.60309600830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:51,556] After 1961600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 387.9803771972656,\n",
      " 'loss_1': 409.71490478515625,\n",
      " 'mean_episode_return_0': 70.93193054199219,\n",
      " 'mean_episode_return_1': 76.60309600830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:30:56,562] After 1964800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 468.86456298828125,\n",
      " 'loss_1': 409.71490478515625,\n",
      " 'mean_episode_return_0': 70.93987274169922,\n",
      " 'mean_episode_return_1': 76.60309600830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:01,568] After 1968000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 468.86456298828125,\n",
      " 'loss_1': 407.13800048828125,\n",
      " 'mean_episode_return_0': 70.93987274169922,\n",
      " 'mean_episode_return_1': 76.62809753417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:06,574] After 1968000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 468.86456298828125,\n",
      " 'loss_1': 407.13800048828125,\n",
      " 'mean_episode_return_0': 70.93987274169922,\n",
      " 'mean_episode_return_1': 76.62809753417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:11,580] After 1971200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 425.3946838378906,\n",
      " 'loss_1': 407.13800048828125,\n",
      " 'mean_episode_return_0': 70.99687194824219,\n",
      " 'mean_episode_return_1': 76.62809753417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:16,586] After 1974400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 425.3946838378906,\n",
      " 'loss_1': 399.54534912109375,\n",
      " 'mean_episode_return_0': 70.99687194824219,\n",
      " 'mean_episode_return_1': 76.63992309570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:21,591] After 1977600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 407.5049133300781,\n",
      " 'loss_1': 399.54534912109375,\n",
      " 'mean_episode_return_0': 71.0613784790039,\n",
      " 'mean_episode_return_1': 76.63992309570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:26,597] After 1977600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 407.5049133300781,\n",
      " 'loss_1': 399.54534912109375,\n",
      " 'mean_episode_return_0': 71.0613784790039,\n",
      " 'mean_episode_return_1': 76.63992309570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:31,602] After 1980800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 407.5049133300781,\n",
      " 'loss_1': 399.81756591796875,\n",
      " 'mean_episode_return_0': 71.0613784790039,\n",
      " 'mean_episode_return_1': 76.66465759277344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:36,608] After 1984000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 343.947021484375,\n",
      " 'loss_1': 399.81756591796875,\n",
      " 'mean_episode_return_0': 71.08881378173828,\n",
      " 'mean_episode_return_1': 76.66465759277344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:41,612] After 1987200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 343.947021484375,\n",
      " 'loss_1': 471.1412353515625,\n",
      " 'mean_episode_return_0': 71.08881378173828,\n",
      " 'mean_episode_return_1': 76.68174743652344}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:31:46,617] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:46,706] After 1987200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 343.947021484375,\n",
      " 'loss_1': 471.1412353515625,\n",
      " 'mean_episode_return_0': 71.08881378173828,\n",
      " 'mean_episode_return_1': 76.68174743652344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:51,708] After 1990400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 459.73968505859375,\n",
      " 'loss_1': 471.1412353515625,\n",
      " 'mean_episode_return_0': 71.1631851196289,\n",
      " 'mean_episode_return_1': 76.68174743652344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:31:56,714] After 1993600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 459.73968505859375,\n",
      " 'loss_1': 475.52447509765625,\n",
      " 'mean_episode_return_0': 71.1631851196289,\n",
      " 'mean_episode_return_1': 76.68156433105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:01,720] After 1996800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.90679931640625,\n",
      " 'loss_1': 475.52447509765625,\n",
      " 'mean_episode_return_0': 71.2178726196289,\n",
      " 'mean_episode_return_1': 76.68156433105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:06,726] After 1996800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.90679931640625,\n",
      " 'loss_1': 475.52447509765625,\n",
      " 'mean_episode_return_0': 71.2178726196289,\n",
      " 'mean_episode_return_1': 76.68156433105469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:11,732] After 2000000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 375.90679931640625,\n",
      " 'loss_1': 412.52044677734375,\n",
      " 'mean_episode_return_0': 71.2178726196289,\n",
      " 'mean_episode_return_1': 76.7430648803711}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:16,738] After 2003200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 377.568115234375,\n",
      " 'loss_1': 412.52044677734375,\n",
      " 'mean_episode_return_0': 71.2179946899414,\n",
      " 'mean_episode_return_1': 76.7430648803711}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:21,740] After 2006400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 377.568115234375,\n",
      " 'loss_1': 432.3184814453125,\n",
      " 'mean_episode_return_0': 71.2179946899414,\n",
      " 'mean_episode_return_1': 76.76499938964844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:26,746] After 2006400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 377.568115234375,\n",
      " 'loss_1': 432.3184814453125,\n",
      " 'mean_episode_return_0': 71.2179946899414,\n",
      " 'mean_episode_return_1': 76.76499938964844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:31,752] After 2009600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 380.3130798339844,\n",
      " 'loss_1': 432.3184814453125,\n",
      " 'mean_episode_return_0': 71.25299835205078,\n",
      " 'mean_episode_return_1': 76.76499938964844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:36,754] After 2012800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 380.3130798339844,\n",
      " 'loss_1': 466.2164611816406,\n",
      " 'mean_episode_return_0': 71.25299835205078,\n",
      " 'mean_episode_return_1': 76.80889129638672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:41,760] After 2016000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.99249267578125,\n",
      " 'loss_1': 466.2164611816406,\n",
      " 'mean_episode_return_0': 71.29499816894531,\n",
      " 'mean_episode_return_1': 76.80889129638672}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:32:46,764] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:46,856] After 2016000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 410.99249267578125,\n",
      " 'loss_1': 466.2164611816406,\n",
      " 'mean_episode_return_0': 71.29499816894531,\n",
      " 'mean_episode_return_1': 76.80889129638672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:51,862] After 2019200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.99249267578125,\n",
      " 'loss_1': 419.97454833984375,\n",
      " 'mean_episode_return_0': 71.29499816894531,\n",
      " 'mean_episode_return_1': 76.80428314208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:32:56,868] After 2022400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 367.8463134765625,\n",
      " 'loss_1': 419.97454833984375,\n",
      " 'mean_episode_return_0': 71.26018524169922,\n",
      " 'mean_episode_return_1': 76.80428314208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:01,874] After 2022400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.8463134765625,\n",
      " 'loss_1': 419.97454833984375,\n",
      " 'mean_episode_return_0': 71.26018524169922,\n",
      " 'mean_episode_return_1': 76.80428314208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:06,880] After 2025600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 367.8463134765625,\n",
      " 'loss_1': 399.794677734375,\n",
      " 'mean_episode_return_0': 71.26018524169922,\n",
      " 'mean_episode_return_1': 76.8487319946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:11,886] After 2028800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 385.0890808105469,\n",
      " 'loss_1': 399.794677734375,\n",
      " 'mean_episode_return_0': 71.29212951660156,\n",
      " 'mean_episode_return_1': 76.8487319946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:16,888] After 2032000 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 385.0890808105469,\n",
      " 'loss_1': 477.4849853515625,\n",
      " 'mean_episode_return_0': 71.29212951660156,\n",
      " 'mean_episode_return_1': 76.932373046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:21,893] After 2035200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.8184814453125,\n",
      " 'loss_1': 477.4849853515625,\n",
      " 'mean_episode_return_0': 71.35218811035156,\n",
      " 'mean_episode_return_1': 76.932373046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:26,899] After 2035200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 454.8184814453125,\n",
      " 'loss_1': 477.4849853515625,\n",
      " 'mean_episode_return_0': 71.35218811035156,\n",
      " 'mean_episode_return_1': 76.932373046875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:33:31,905] After 2038400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.8184814453125,\n",
      " 'loss_1': 537.60302734375,\n",
      " 'mean_episode_return_0': 71.35218811035156,\n",
      " 'mean_episode_return_1': 77.0209732055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:36,911] After 2041600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 363.276123046875,\n",
      " 'loss_1': 537.60302734375,\n",
      " 'mean_episode_return_0': 71.32830810546875,\n",
      " 'mean_episode_return_1': 77.0209732055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:41,917] After 2041600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 363.276123046875,\n",
      " 'loss_1': 537.60302734375,\n",
      " 'mean_episode_return_0': 71.32830810546875,\n",
      " 'mean_episode_return_1': 77.0209732055664}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:33:46,923] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:47,016] After 2044800 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 363.276123046875,\n",
      " 'loss_1': 442.8282470703125,\n",
      " 'mean_episode_return_0': 71.32830810546875,\n",
      " 'mean_episode_return_1': 77.02483367919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:52,019] After 2048000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 383.65863037109375,\n",
      " 'loss_1': 442.8282470703125,\n",
      " 'mean_episode_return_0': 71.37049865722656,\n",
      " 'mean_episode_return_1': 77.02483367919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:33:57,025] After 2048000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.65863037109375,\n",
      " 'loss_1': 442.8282470703125,\n",
      " 'mean_episode_return_0': 71.37049865722656,\n",
      " 'mean_episode_return_1': 77.02483367919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:02,031] After 2051200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.65863037109375,\n",
      " 'loss_1': 412.3093566894531,\n",
      " 'mean_episode_return_0': 71.37049865722656,\n",
      " 'mean_episode_return_1': 77.05508422851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:07,036] After 2054400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.4423828125,\n",
      " 'loss_1': 412.3093566894531,\n",
      " 'mean_episode_return_0': 71.32463073730469,\n",
      " 'mean_episode_return_1': 77.05508422851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:12,042] After 2054400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 378.4423828125,\n",
      " 'loss_1': 412.3093566894531,\n",
      " 'mean_episode_return_0': 71.32463073730469,\n",
      " 'mean_episode_return_1': 77.05508422851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:17,048] After 2057600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.4423828125,\n",
      " 'loss_1': 441.21185302734375,\n",
      " 'mean_episode_return_0': 71.32463073730469,\n",
      " 'mean_episode_return_1': 77.1279525756836}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:22,054] After 2060800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.88592529296875,\n",
      " 'loss_1': 441.21185302734375,\n",
      " 'mean_episode_return_0': 71.29231262207031,\n",
      " 'mean_episode_return_1': 77.1279525756836}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:27,060] After 2060800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 378.88592529296875,\n",
      " 'loss_1': 441.21185302734375,\n",
      " 'mean_episode_return_0': 71.29231262207031,\n",
      " 'mean_episode_return_1': 77.1279525756836}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:32,066] After 2064000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.88592529296875,\n",
      " 'loss_1': 398.2342834472656,\n",
      " 'mean_episode_return_0': 71.29231262207031,\n",
      " 'mean_episode_return_1': 77.17864990234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:37,072] After 2067200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 509.4591979980469,\n",
      " 'loss_1': 398.2342834472656,\n",
      " 'mean_episode_return_0': 71.3351821899414,\n",
      " 'mean_episode_return_1': 77.17864990234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:42,078] After 2067200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 509.4591979980469,\n",
      " 'loss_1': 398.2342834472656,\n",
      " 'mean_episode_return_0': 71.3351821899414,\n",
      " 'mean_episode_return_1': 77.17864990234375}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:34:47,079] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:47,172] After 2073600 frames: @ 1256.6 fps Stats:\n",
      "{'loss_0': 357.6654968261719,\n",
      " 'loss_1': 455.5146789550781,\n",
      " 'mean_episode_return_0': 71.39781188964844,\n",
      " 'mean_episode_return_1': 77.2327651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:52,178] After 2073600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 357.6654968261719,\n",
      " 'loss_1': 455.5146789550781,\n",
      " 'mean_episode_return_0': 71.39781188964844,\n",
      " 'mean_episode_return_1': 77.2327651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:34:57,184] After 2073600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 357.6654968261719,\n",
      " 'loss_1': 455.5146789550781,\n",
      " 'mean_episode_return_0': 71.39781188964844,\n",
      " 'mean_episode_return_1': 77.2327651977539}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:02,190] After 2080000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 407.13507080078125,\n",
      " 'loss_1': 394.12249755859375,\n",
      " 'mean_episode_return_0': 71.42162322998047,\n",
      " 'mean_episode_return_1': 77.29691314697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:07,196] After 2080000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 407.13507080078125,\n",
      " 'loss_1': 394.12249755859375,\n",
      " 'mean_episode_return_0': 71.42162322998047,\n",
      " 'mean_episode_return_1': 77.29691314697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:12,200] After 2080000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 407.13507080078125,\n",
      " 'loss_1': 394.12249755859375,\n",
      " 'mean_episode_return_0': 71.42162322998047,\n",
      " 'mean_episode_return_1': 77.29691314697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:17,204] After 2086400 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 414.298828125,\n",
      " 'loss_1': 396.72491455078125,\n",
      " 'mean_episode_return_0': 71.43556213378906,\n",
      " 'mean_episode_return_1': 77.34282684326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:22,210] After 2086400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 414.298828125,\n",
      " 'loss_1': 396.72491455078125,\n",
      " 'mean_episode_return_0': 71.43556213378906,\n",
      " 'mean_episode_return_1': 77.34282684326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:27,216] After 2086400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 414.298828125,\n",
      " 'loss_1': 396.72491455078125,\n",
      " 'mean_episode_return_0': 71.43556213378906,\n",
      " 'mean_episode_return_1': 77.34282684326172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:32,222] After 2092800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 442.0914611816406,\n",
      " 'loss_1': 374.2451477050781,\n",
      " 'mean_episode_return_0': 71.48981475830078,\n",
      " 'mean_episode_return_1': 77.3569107055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:37,228] After 2092800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.0914611816406,\n",
      " 'loss_1': 374.2451477050781,\n",
      " 'mean_episode_return_0': 71.48981475830078,\n",
      " 'mean_episode_return_1': 77.3569107055664}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:42,232] After 2092800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.0914611816406,\n",
      " 'loss_1': 374.2451477050781,\n",
      " 'mean_episode_return_0': 71.48981475830078,\n",
      " 'mean_episode_return_1': 77.3569107055664}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:35:47,235] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:47,336] After 2099200 frames: @ 1253.9 fps Stats:\n",
      "{'loss_0': 417.35247802734375,\n",
      " 'loss_1': 397.2869567871094,\n",
      " 'mean_episode_return_0': 71.53356170654297,\n",
      " 'mean_episode_return_1': 77.43359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:52,342] After 2099200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.35247802734375,\n",
      " 'loss_1': 397.2869567871094,\n",
      " 'mean_episode_return_0': 71.53356170654297,\n",
      " 'mean_episode_return_1': 77.43359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:35:57,348] After 2102400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.35247802734375,\n",
      " 'loss_1': 463.4303894042969,\n",
      " 'mean_episode_return_0': 71.53356170654297,\n",
      " 'mean_episode_return_1': 77.44001770019531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:02,352] After 2105600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 402.2621765136719,\n",
      " 'loss_1': 463.4303894042969,\n",
      " 'mean_episode_return_0': 71.57569122314453,\n",
      " 'mean_episode_return_1': 77.44001770019531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:07,354] After 2105600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.2621765136719,\n",
      " 'loss_1': 463.4303894042969,\n",
      " 'mean_episode_return_0': 71.57569122314453,\n",
      " 'mean_episode_return_1': 77.44001770019531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:12,360] After 2108800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 402.2621765136719,\n",
      " 'loss_1': 477.86151123046875,\n",
      " 'mean_episode_return_0': 71.57569122314453,\n",
      " 'mean_episode_return_1': 77.4581298828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:17,366] After 2112000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 371.4039001464844,\n",
      " 'loss_1': 477.86151123046875,\n",
      " 'mean_episode_return_0': 71.5790023803711,\n",
      " 'mean_episode_return_1': 77.4581298828125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:36:22,372] After 2112000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 371.4039001464844,\n",
      " 'loss_1': 477.86151123046875,\n",
      " 'mean_episode_return_0': 71.5790023803711,\n",
      " 'mean_episode_return_1': 77.4581298828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:27,377] After 2118400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 402.8358459472656,\n",
      " 'loss_1': 437.0516662597656,\n",
      " 'mean_episode_return_0': 71.56743621826172,\n",
      " 'mean_episode_return_1': 77.4933090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:32,383] After 2118400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.8358459472656,\n",
      " 'loss_1': 437.0516662597656,\n",
      " 'mean_episode_return_0': 71.56743621826172,\n",
      " 'mean_episode_return_1': 77.4933090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:37,389] After 2118400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.8358459472656,\n",
      " 'loss_1': 437.0516662597656,\n",
      " 'mean_episode_return_0': 71.56743621826172,\n",
      " 'mean_episode_return_1': 77.4933090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:42,395] After 2124800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 358.0777282714844,\n",
      " 'loss_1': 440.2183532714844,\n",
      " 'mean_episode_return_0': 71.5953140258789,\n",
      " 'mean_episode_return_1': 77.53838348388672}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:36:47,397] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:47,480] After 2124800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.0777282714844,\n",
      " 'loss_1': 440.2183532714844,\n",
      " 'mean_episode_return_0': 71.5953140258789,\n",
      " 'mean_episode_return_1': 77.53838348388672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:52,486] After 2124800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.0777282714844,\n",
      " 'loss_1': 440.2183532714844,\n",
      " 'mean_episode_return_0': 71.5953140258789,\n",
      " 'mean_episode_return_1': 77.53838348388672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:36:57,492] After 2131200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 354.5699462890625,\n",
      " 'loss_1': 413.0135192871094,\n",
      " 'mean_episode_return_0': 71.59850311279297,\n",
      " 'mean_episode_return_1': 77.64705657958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:02,498] After 2131200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 354.5699462890625,\n",
      " 'loss_1': 413.0135192871094,\n",
      " 'mean_episode_return_0': 71.59850311279297,\n",
      " 'mean_episode_return_1': 77.64705657958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:07,504] After 2131200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 354.5699462890625,\n",
      " 'loss_1': 413.0135192871094,\n",
      " 'mean_episode_return_0': 71.59850311279297,\n",
      " 'mean_episode_return_1': 77.64705657958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:12,510] After 2137600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 432.2942199707031,\n",
      " 'loss_1': 407.1965637207031,\n",
      " 'mean_episode_return_0': 71.55838012695312,\n",
      " 'mean_episode_return_1': 77.71285247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:17,516] After 2137600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.2942199707031,\n",
      " 'loss_1': 407.1965637207031,\n",
      " 'mean_episode_return_0': 71.55838012695312,\n",
      " 'mean_episode_return_1': 77.71285247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:22,522] After 2140800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.2955322265625,\n",
      " 'loss_1': 407.1965637207031,\n",
      " 'mean_episode_return_0': 71.55787658691406,\n",
      " 'mean_episode_return_1': 77.71285247802734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:27,527] After 2144000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.2955322265625,\n",
      " 'loss_1': 559.406005859375,\n",
      " 'mean_episode_return_0': 71.55787658691406,\n",
      " 'mean_episode_return_1': 77.77278900146484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:32,533] After 2144000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 340.2955322265625,\n",
      " 'loss_1': 559.406005859375,\n",
      " 'mean_episode_return_0': 71.55787658691406,\n",
      " 'mean_episode_return_1': 77.77278900146484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:37,539] After 2150400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 335.88201904296875,\n",
      " 'loss_1': 477.57513427734375,\n",
      " 'mean_episode_return_0': 71.58012390136719,\n",
      " 'mean_episode_return_1': 77.79004669189453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:42,545] After 2150400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 335.88201904296875,\n",
      " 'loss_1': 477.57513427734375,\n",
      " 'mean_episode_return_0': 71.58012390136719,\n",
      " 'mean_episode_return_1': 77.79004669189453}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:37:47,551] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:47,643] After 2150400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 335.88201904296875,\n",
      " 'loss_1': 477.57513427734375,\n",
      " 'mean_episode_return_0': 71.58012390136719,\n",
      " 'mean_episode_return_1': 77.79004669189453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:52,649] After 2156800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 380.84429931640625,\n",
      " 'loss_1': 446.0067138671875,\n",
      " 'mean_episode_return_0': 71.61369323730469,\n",
      " 'mean_episode_return_1': 77.83694458007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:37:57,652] After 2156800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 380.84429931640625,\n",
      " 'loss_1': 446.0067138671875,\n",
      " 'mean_episode_return_0': 71.61369323730469,\n",
      " 'mean_episode_return_1': 77.83694458007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:02,656] After 2156800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 380.84429931640625,\n",
      " 'loss_1': 446.0067138671875,\n",
      " 'mean_episode_return_0': 71.61369323730469,\n",
      " 'mean_episode_return_1': 77.83694458007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:07,660] After 2160000 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 381.357177734375,\n",
      " 'loss_1': 446.0067138671875,\n",
      " 'mean_episode_return_0': 71.6133804321289,\n",
      " 'mean_episode_return_1': 77.83694458007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:12,664] After 2163200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 381.357177734375,\n",
      " 'loss_1': 408.7906188964844,\n",
      " 'mean_episode_return_0': 71.6133804321289,\n",
      " 'mean_episode_return_1': 77.885009765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:17,668] After 2163200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 381.357177734375,\n",
      " 'loss_1': 408.7906188964844,\n",
      " 'mean_episode_return_0': 71.6133804321289,\n",
      " 'mean_episode_return_1': 77.885009765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:22,671] After 2166400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 388.0066833496094,\n",
      " 'loss_1': 408.7906188964844,\n",
      " 'mean_episode_return_0': 71.63899993896484,\n",
      " 'mean_episode_return_1': 77.885009765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:27,677] After 2169600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 388.0066833496094,\n",
      " 'loss_1': 427.069091796875,\n",
      " 'mean_episode_return_0': 71.63899993896484,\n",
      " 'mean_episode_return_1': 77.86598205566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:32,680] After 2169600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 388.0066833496094,\n",
      " 'loss_1': 427.069091796875,\n",
      " 'mean_episode_return_0': 71.63899993896484,\n",
      " 'mean_episode_return_1': 77.86598205566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:37,686] After 2172800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 418.7003479003906,\n",
      " 'loss_1': 427.069091796875,\n",
      " 'mean_episode_return_0': 71.70006561279297,\n",
      " 'mean_episode_return_1': 77.86598205566406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:42,691] After 2176000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 418.7003479003906,\n",
      " 'loss_1': 521.2254638671875,\n",
      " 'mean_episode_return_0': 71.70006561279297,\n",
      " 'mean_episode_return_1': 77.880126953125}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:38:47,697] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:47,792] After 2176000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 418.7003479003906,\n",
      " 'loss_1': 521.2254638671875,\n",
      " 'mean_episode_return_0': 71.70006561279297,\n",
      " 'mean_episode_return_1': 77.880126953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:52,797] After 2182400 frames: @ 1279.0 fps Stats:\n",
      "{'loss_0': 430.0847473144531,\n",
      " 'loss_1': 356.7783508300781,\n",
      " 'mean_episode_return_0': 71.73756408691406,\n",
      " 'mean_episode_return_1': 77.939453125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:38:57,803] After 2182400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 430.0847473144531,\n",
      " 'loss_1': 356.7783508300781,\n",
      " 'mean_episode_return_0': 71.73756408691406,\n",
      " 'mean_episode_return_1': 77.939453125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:02,809] After 2182400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 430.0847473144531,\n",
      " 'loss_1': 356.7783508300781,\n",
      " 'mean_episode_return_0': 71.73756408691406,\n",
      " 'mean_episode_return_1': 77.939453125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:07,815] After 2188800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 389.61419677734375,\n",
      " 'loss_1': 498.2582702636719,\n",
      " 'mean_episode_return_0': 71.72331237792969,\n",
      " 'mean_episode_return_1': 78.00436401367188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:39:12,820] After 2188800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.61419677734375,\n",
      " 'loss_1': 498.2582702636719,\n",
      " 'mean_episode_return_0': 71.72331237792969,\n",
      " 'mean_episode_return_1': 78.00436401367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:17,826] After 2188800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.61419677734375,\n",
      " 'loss_1': 498.2582702636719,\n",
      " 'mean_episode_return_0': 71.72331237792969,\n",
      " 'mean_episode_return_1': 78.00436401367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:22,832] After 2195200 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 443.18280029296875,\n",
      " 'loss_1': 486.1953125,\n",
      " 'mean_episode_return_0': 71.78418731689453,\n",
      " 'mean_episode_return_1': 77.96766662597656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:27,836] After 2195200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 443.18280029296875,\n",
      " 'loss_1': 486.1953125,\n",
      " 'mean_episode_return_0': 71.78418731689453,\n",
      " 'mean_episode_return_1': 77.96766662597656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:32,842] After 2198400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.1349182128906,\n",
      " 'loss_1': 486.1953125,\n",
      " 'mean_episode_return_0': 71.7925033569336,\n",
      " 'mean_episode_return_1': 77.96766662597656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:37,847] After 2201600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.1349182128906,\n",
      " 'loss_1': 529.4467163085938,\n",
      " 'mean_episode_return_0': 71.7925033569336,\n",
      " 'mean_episode_return_1': 78.00885772705078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:42,853] After 2201600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.1349182128906,\n",
      " 'loss_1': 529.4467163085938,\n",
      " 'mean_episode_return_0': 71.7925033569336,\n",
      " 'mean_episode_return_1': 78.00885772705078}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:39:47,859] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:47,955] After 2204800 frames: @ 627.4 fps Stats:\n",
      "{'loss_0': 393.0492248535156,\n",
      " 'loss_1': 529.4467163085938,\n",
      " 'mean_episode_return_0': 71.80574798583984,\n",
      " 'mean_episode_return_1': 78.00885772705078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:52,961] After 2208000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 393.0492248535156,\n",
      " 'loss_1': 448.15740966796875,\n",
      " 'mean_episode_return_0': 71.80574798583984,\n",
      " 'mean_episode_return_1': 78.05628967285156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:39:57,967] After 2208000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 393.0492248535156,\n",
      " 'loss_1': 448.15740966796875,\n",
      " 'mean_episode_return_0': 71.80574798583984,\n",
      " 'mean_episode_return_1': 78.05628967285156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:02,972] After 2211200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 400.1603088378906,\n",
      " 'loss_1': 448.15740966796875,\n",
      " 'mean_episode_return_0': 71.76856231689453,\n",
      " 'mean_episode_return_1': 78.05628967285156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:07,976] After 2214400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 400.1603088378906,\n",
      " 'loss_1': 415.7302551269531,\n",
      " 'mean_episode_return_0': 71.76856231689453,\n",
      " 'mean_episode_return_1': 78.11277770996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:12,984] After 2214400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.1603088378906,\n",
      " 'loss_1': 415.7302551269531,\n",
      " 'mean_episode_return_0': 71.76856231689453,\n",
      " 'mean_episode_return_1': 78.11277770996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:17,989] After 2217600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 369.5979919433594,\n",
      " 'loss_1': 415.7302551269531,\n",
      " 'mean_episode_return_0': 71.76443481445312,\n",
      " 'mean_episode_return_1': 78.11277770996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:22,995] After 2220800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 369.5979919433594,\n",
      " 'loss_1': 533.2960205078125,\n",
      " 'mean_episode_return_0': 71.76443481445312,\n",
      " 'mean_episode_return_1': 78.12284851074219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:28,001] After 2220800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 369.5979919433594,\n",
      " 'loss_1': 533.2960205078125,\n",
      " 'mean_episode_return_0': 71.76443481445312,\n",
      " 'mean_episode_return_1': 78.12284851074219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:33,007] After 2224000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 367.6265869140625,\n",
      " 'loss_1': 533.2960205078125,\n",
      " 'mean_episode_return_0': 71.80243682861328,\n",
      " 'mean_episode_return_1': 78.12284851074219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:38,013] After 2227200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 367.6265869140625,\n",
      " 'loss_1': 470.1394348144531,\n",
      " 'mean_episode_return_0': 71.80243682861328,\n",
      " 'mean_episode_return_1': 78.18366241455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:43,019] After 2227200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.6265869140625,\n",
      " 'loss_1': 470.1394348144531,\n",
      " 'mean_episode_return_0': 71.80243682861328,\n",
      " 'mean_episode_return_1': 78.18366241455078}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:40:48,025] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:48,109] After 2230400 frames: @ 628.8 fps Stats:\n",
      "{'loss_0': 402.5436706542969,\n",
      " 'loss_1': 470.1394348144531,\n",
      " 'mean_episode_return_0': 71.8046875,\n",
      " 'mean_episode_return_1': 78.18366241455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:53,115] After 2233600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 402.5436706542969,\n",
      " 'loss_1': 521.881103515625,\n",
      " 'mean_episode_return_0': 71.8046875,\n",
      " 'mean_episode_return_1': 78.25753784179688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:40:58,121] After 2233600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.5436706542969,\n",
      " 'loss_1': 521.881103515625,\n",
      " 'mean_episode_return_0': 71.8046875,\n",
      " 'mean_episode_return_1': 78.25753784179688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:03,128] After 2236800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 467.087890625,\n",
      " 'loss_1': 521.881103515625,\n",
      " 'mean_episode_return_0': 71.85025024414062,\n",
      " 'mean_episode_return_1': 78.25753784179688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:08,133] After 2240000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 467.087890625,\n",
      " 'loss_1': 461.9393005371094,\n",
      " 'mean_episode_return_0': 71.85025024414062,\n",
      " 'mean_episode_return_1': 78.29423522949219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:13,139] After 2240000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 467.087890625,\n",
      " 'loss_1': 461.9393005371094,\n",
      " 'mean_episode_return_0': 71.85025024414062,\n",
      " 'mean_episode_return_1': 78.29423522949219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:18,145] After 2243200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 431.7427978515625,\n",
      " 'loss_1': 461.9393005371094,\n",
      " 'mean_episode_return_0': 71.89412689208984,\n",
      " 'mean_episode_return_1': 78.29423522949219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:23,148] After 2246400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 431.7427978515625,\n",
      " 'loss_1': 537.0323486328125,\n",
      " 'mean_episode_return_0': 71.89412689208984,\n",
      " 'mean_episode_return_1': 78.32331848144531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:28,154] After 2249600 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 386.2004089355469,\n",
      " 'loss_1': 537.0323486328125,\n",
      " 'mean_episode_return_0': 71.85706329345703,\n",
      " 'mean_episode_return_1': 78.32331848144531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:33,160] After 2252800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 386.2004089355469,\n",
      " 'loss_1': 437.6959533691406,\n",
      " 'mean_episode_return_0': 71.85706329345703,\n",
      " 'mean_episode_return_1': 78.35407257080078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:38,165] After 2252800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.2004089355469,\n",
      " 'loss_1': 437.6959533691406,\n",
      " 'mean_episode_return_0': 71.85706329345703,\n",
      " 'mean_episode_return_1': 78.35407257080078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:43,171] After 2256000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 453.46380615234375,\n",
      " 'loss_1': 437.6959533691406,\n",
      " 'mean_episode_return_0': 71.84668731689453,\n",
      " 'mean_episode_return_1': 78.35407257080078}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:41:48,177] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:48,271] After 2259200 frames: @ 627.6 fps Stats:\n",
      "{'loss_0': 453.46380615234375,\n",
      " 'loss_1': 489.7271728515625,\n",
      " 'mean_episode_return_0': 71.84668731689453,\n",
      " 'mean_episode_return_1': 78.35810089111328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:53,277] After 2259200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.46380615234375,\n",
      " 'loss_1': 489.7271728515625,\n",
      " 'mean_episode_return_0': 71.84668731689453,\n",
      " 'mean_episode_return_1': 78.35810089111328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:41:58,283] After 2262400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 394.06488037109375,\n",
      " 'loss_1': 489.7271728515625,\n",
      " 'mean_episode_return_0': 71.87569427490234,\n",
      " 'mean_episode_return_1': 78.35810089111328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:42:03,289] After 2265600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 394.06488037109375,\n",
      " 'loss_1': 431.4189758300781,\n",
      " 'mean_episode_return_0': 71.87569427490234,\n",
      " 'mean_episode_return_1': 78.4023208618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:08,292] After 2268800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 396.4123229980469,\n",
      " 'loss_1': 431.4189758300781,\n",
      " 'mean_episode_return_0': 71.84862518310547,\n",
      " 'mean_episode_return_1': 78.4023208618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:13,296] After 2268800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 396.4123229980469,\n",
      " 'loss_1': 431.4189758300781,\n",
      " 'mean_episode_return_0': 71.84862518310547,\n",
      " 'mean_episode_return_1': 78.4023208618164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:18,300] After 2272000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 396.4123229980469,\n",
      " 'loss_1': 461.86334228515625,\n",
      " 'mean_episode_return_0': 71.84862518310547,\n",
      " 'mean_episode_return_1': 78.4842758178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:23,304] After 2272000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 396.4123229980469,\n",
      " 'loss_1': 461.86334228515625,\n",
      " 'mean_episode_return_0': 71.84862518310547,\n",
      " 'mean_episode_return_1': 78.4842758178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:28,310] After 2275200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.63568115234375,\n",
      " 'loss_1': 461.86334228515625,\n",
      " 'mean_episode_return_0': 71.8635025024414,\n",
      " 'mean_episode_return_1': 78.4842758178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:33,316] After 2278400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.63568115234375,\n",
      " 'loss_1': 469.36273193359375,\n",
      " 'mean_episode_return_0': 71.8635025024414,\n",
      " 'mean_episode_return_1': 78.56415557861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:38,320] After 2278400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.63568115234375,\n",
      " 'loss_1': 469.36273193359375,\n",
      " 'mean_episode_return_0': 71.8635025024414,\n",
      " 'mean_episode_return_1': 78.56415557861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:43,326] After 2281600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 437.42889404296875,\n",
      " 'loss_1': 469.36273193359375,\n",
      " 'mean_episode_return_0': 71.86949920654297,\n",
      " 'mean_episode_return_1': 78.56415557861328}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:42:48,332] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:48,420] After 2284800 frames: @ 628.3 fps Stats:\n",
      "{'loss_0': 437.42889404296875,\n",
      " 'loss_1': 502.2122497558594,\n",
      " 'mean_episode_return_0': 71.86949920654297,\n",
      " 'mean_episode_return_1': 78.64203643798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:53,426] After 2284800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.42889404296875,\n",
      " 'loss_1': 502.2122497558594,\n",
      " 'mean_episode_return_0': 71.86949920654297,\n",
      " 'mean_episode_return_1': 78.64203643798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:42:58,432] After 2288000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 354.3637390136719,\n",
      " 'loss_1': 502.2122497558594,\n",
      " 'mean_episode_return_0': 71.84581756591797,\n",
      " 'mean_episode_return_1': 78.64203643798828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:03,437] After 2291200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 354.3637390136719,\n",
      " 'loss_1': 490.9108581542969,\n",
      " 'mean_episode_return_0': 71.84581756591797,\n",
      " 'mean_episode_return_1': 78.65740966796875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:08,443] After 2291200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 354.3637390136719,\n",
      " 'loss_1': 490.9108581542969,\n",
      " 'mean_episode_return_0': 71.84581756591797,\n",
      " 'mean_episode_return_1': 78.65740966796875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:13,448] After 2294400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 352.48211669921875,\n",
      " 'loss_1': 490.9108581542969,\n",
      " 'mean_episode_return_0': 71.85606384277344,\n",
      " 'mean_episode_return_1': 78.65740966796875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:18,452] After 2297600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 352.48211669921875,\n",
      " 'loss_1': 606.6691284179688,\n",
      " 'mean_episode_return_0': 71.85606384277344,\n",
      " 'mean_episode_return_1': 78.70905303955078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:23,456] After 2300800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 432.1110534667969,\n",
      " 'loss_1': 606.6691284179688,\n",
      " 'mean_episode_return_0': 71.85100555419922,\n",
      " 'mean_episode_return_1': 78.70905303955078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:28,462] After 2300800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.1110534667969,\n",
      " 'loss_1': 606.6691284179688,\n",
      " 'mean_episode_return_0': 71.85100555419922,\n",
      " 'mean_episode_return_1': 78.70905303955078}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:33,468] After 2304000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 432.1110534667969,\n",
      " 'loss_1': 446.1589660644531,\n",
      " 'mean_episode_return_0': 71.85100555419922,\n",
      " 'mean_episode_return_1': 78.78072357177734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:38,470] After 2307200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 346.02154541015625,\n",
      " 'loss_1': 446.1589660644531,\n",
      " 'mean_episode_return_0': 71.84593963623047,\n",
      " 'mean_episode_return_1': 78.78072357177734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:43,471] After 2307200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 346.02154541015625,\n",
      " 'loss_1': 446.1589660644531,\n",
      " 'mean_episode_return_0': 71.84593963623047,\n",
      " 'mean_episode_return_1': 78.78072357177734}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:43:48,477] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:48,572] After 2310400 frames: @ 627.4 fps Stats:\n",
      "{'loss_0': 346.02154541015625,\n",
      " 'loss_1': 464.2323303222656,\n",
      " 'mean_episode_return_0': 71.84593963623047,\n",
      " 'mean_episode_return_1': 78.85759735107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:53,578] After 2313600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 427.9732666015625,\n",
      " 'loss_1': 464.2323303222656,\n",
      " 'mean_episode_return_0': 71.8446273803711,\n",
      " 'mean_episode_return_1': 78.85759735107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:43:58,584] After 2313600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 427.9732666015625,\n",
      " 'loss_1': 464.2323303222656,\n",
      " 'mean_episode_return_0': 71.8446273803711,\n",
      " 'mean_episode_return_1': 78.85759735107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:03,590] After 2316800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 427.9732666015625,\n",
      " 'loss_1': 481.1648254394531,\n",
      " 'mean_episode_return_0': 71.8446273803711,\n",
      " 'mean_episode_return_1': 78.86247253417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:08,596] After 2320000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.41265869140625,\n",
      " 'loss_1': 481.1648254394531,\n",
      " 'mean_episode_return_0': 71.83087921142578,\n",
      " 'mean_episode_return_1': 78.86247253417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:13,602] After 2320000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.41265869140625,\n",
      " 'loss_1': 481.1648254394531,\n",
      " 'mean_episode_return_0': 71.83087921142578,\n",
      " 'mean_episode_return_1': 78.86247253417969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:18,608] After 2323200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.41265869140625,\n",
      " 'loss_1': 511.8932189941406,\n",
      " 'mean_episode_return_0': 71.83087921142578,\n",
      " 'mean_episode_return_1': 78.87471771240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:23,614] After 2326400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 358.8035583496094,\n",
      " 'loss_1': 511.8932189941406,\n",
      " 'mean_episode_return_0': 71.85968780517578,\n",
      " 'mean_episode_return_1': 78.87471771240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:28,620] After 2326400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.8035583496094,\n",
      " 'loss_1': 511.8932189941406,\n",
      " 'mean_episode_return_0': 71.85968780517578,\n",
      " 'mean_episode_return_1': 78.87471771240234}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:33,626] After 2329600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 358.8035583496094,\n",
      " 'loss_1': 481.8051452636719,\n",
      " 'mean_episode_return_0': 71.85968780517578,\n",
      " 'mean_episode_return_1': 78.84085083007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:38,632] After 2332800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 427.6990661621094,\n",
      " 'loss_1': 481.8051452636719,\n",
      " 'mean_episode_return_0': 71.80562591552734,\n",
      " 'mean_episode_return_1': 78.84085083007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:43,638] After 2332800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 427.6990661621094,\n",
      " 'loss_1': 481.8051452636719,\n",
      " 'mean_episode_return_0': 71.80562591552734,\n",
      " 'mean_episode_return_1': 78.84085083007812}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:44:48,644] Saving checkpoint to dmc_results/tute/model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:44:48,733] After 2336000 frames: @ 628.2 fps Stats:\n",
      "{'loss_0': 427.6990661621094,\n",
      " 'loss_1': 402.3697814941406,\n",
      " 'mean_episode_return_0': 71.80562591552734,\n",
      " 'mean_episode_return_1': 78.84062957763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:53,737] After 2339200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 360.88995361328125,\n",
      " 'loss_1': 402.3697814941406,\n",
      " 'mean_episode_return_0': 71.7959976196289,\n",
      " 'mean_episode_return_1': 78.84062957763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:44:58,743] After 2339200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 360.88995361328125,\n",
      " 'loss_1': 402.3697814941406,\n",
      " 'mean_episode_return_0': 71.7959976196289,\n",
      " 'mean_episode_return_1': 78.84062957763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:03,748] After 2342400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 360.88995361328125,\n",
      " 'loss_1': 503.5065002441406,\n",
      " 'mean_episode_return_0': 71.7959976196289,\n",
      " 'mean_episode_return_1': 78.87480926513672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:08,753] After 2345600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 422.40380859375,\n",
      " 'loss_1': 503.5065002441406,\n",
      " 'mean_episode_return_0': 71.79275512695312,\n",
      " 'mean_episode_return_1': 78.87480926513672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:13,755] After 2345600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 422.40380859375,\n",
      " 'loss_1': 503.5065002441406,\n",
      " 'mean_episode_return_0': 71.79275512695312,\n",
      " 'mean_episode_return_1': 78.87480926513672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:18,760] After 2352000 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 382.2169189453125,\n",
      " 'loss_1': 567.191162109375,\n",
      " 'mean_episode_return_0': 71.75450134277344,\n",
      " 'mean_episode_return_1': 78.95176696777344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:23,765] After 2352000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 382.2169189453125,\n",
      " 'loss_1': 567.191162109375,\n",
      " 'mean_episode_return_0': 71.75450134277344,\n",
      " 'mean_episode_return_1': 78.95176696777344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:28,771] After 2352000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 382.2169189453125,\n",
      " 'loss_1': 567.191162109375,\n",
      " 'mean_episode_return_0': 71.75450134277344,\n",
      " 'mean_episode_return_1': 78.95176696777344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:33,777] After 2358400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 376.7135925292969,\n",
      " 'loss_1': 451.2137451171875,\n",
      " 'mean_episode_return_0': 71.74724578857422,\n",
      " 'mean_episode_return_1': 78.99034118652344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:38,780] After 2358400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 376.7135925292969,\n",
      " 'loss_1': 451.2137451171875,\n",
      " 'mean_episode_return_0': 71.74724578857422,\n",
      " 'mean_episode_return_1': 78.99034118652344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:43,785] After 2361600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 376.7135925292969,\n",
      " 'loss_1': 534.2744140625,\n",
      " 'mean_episode_return_0': 71.74724578857422,\n",
      " 'mean_episode_return_1': 79.06924438476562}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:45:48,791] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:48,885] After 2364800 frames: @ 627.6 fps Stats:\n",
      "{'loss_0': 318.7626037597656,\n",
      " 'loss_1': 534.2744140625,\n",
      " 'mean_episode_return_0': 71.72624969482422,\n",
      " 'mean_episode_return_1': 79.06924438476562}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:53,888] After 2364800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 318.7626037597656,\n",
      " 'loss_1': 534.2744140625,\n",
      " 'mean_episode_return_0': 71.72624969482422,\n",
      " 'mean_episode_return_1': 79.06924438476562}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:45:58,894] After 2368000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 318.7626037597656,\n",
      " 'loss_1': 406.0117492675781,\n",
      " 'mean_episode_return_0': 71.72624969482422,\n",
      " 'mean_episode_return_1': 79.12384796142578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:03,900] After 2371200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.7586975097656,\n",
      " 'loss_1': 406.0117492675781,\n",
      " 'mean_episode_return_0': 71.64693450927734,\n",
      " 'mean_episode_return_1': 79.12384796142578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:08,905] After 2371200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.7586975097656,\n",
      " 'loss_1': 406.0117492675781,\n",
      " 'mean_episode_return_0': 71.64693450927734,\n",
      " 'mean_episode_return_1': 79.12384796142578}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:13,911] After 2374400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.7586975097656,\n",
      " 'loss_1': 416.16827392578125,\n",
      " 'mean_episode_return_0': 71.64693450927734,\n",
      " 'mean_episode_return_1': 79.15849304199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:18,917] After 2377600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 439.5767517089844,\n",
      " 'loss_1': 416.16827392578125,\n",
      " 'mean_episode_return_0': 71.67724609375,\n",
      " 'mean_episode_return_1': 79.15849304199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:23,923] After 2377600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.5767517089844,\n",
      " 'loss_1': 416.16827392578125,\n",
      " 'mean_episode_return_0': 71.67724609375,\n",
      " 'mean_episode_return_1': 79.15849304199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:28,928] After 2380800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 439.5767517089844,\n",
      " 'loss_1': 400.6689758300781,\n",
      " 'mean_episode_return_0': 71.67724609375,\n",
      " 'mean_episode_return_1': 79.20217895507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:33,934] After 2384000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 427.0916442871094,\n",
      " 'loss_1': 400.6689758300781,\n",
      " 'mean_episode_return_0': 71.6588134765625,\n",
      " 'mean_episode_return_1': 79.20217895507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:38,940] After 2384000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 427.0916442871094,\n",
      " 'loss_1': 400.6689758300781,\n",
      " 'mean_episode_return_0': 71.6588134765625,\n",
      " 'mean_episode_return_1': 79.20217895507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:43,944] After 2387200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 427.0916442871094,\n",
      " 'loss_1': 475.695068359375,\n",
      " 'mean_episode_return_0': 71.6588134765625,\n",
      " 'mean_episode_return_1': 79.31922149658203}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:46:48,950] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:49,042] After 2390400 frames: @ 627.7 fps Stats:\n",
      "{'loss_0': 439.5623779296875,\n",
      " 'loss_1': 475.695068359375,\n",
      " 'mean_episode_return_0': 71.65918731689453,\n",
      " 'mean_episode_return_1': 79.31922149658203}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:54,048] After 2390400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.5623779296875,\n",
      " 'loss_1': 475.695068359375,\n",
      " 'mean_episode_return_0': 71.65918731689453,\n",
      " 'mean_episode_return_1': 79.31922149658203}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:46:59,053] After 2396800 frames: @ 1279.0 fps Stats:\n",
      "{'loss_0': 444.5267333984375,\n",
      " 'loss_1': 586.6602783203125,\n",
      " 'mean_episode_return_0': 71.64700317382812,\n",
      " 'mean_episode_return_1': 79.38121032714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:04,060] After 2396800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.5267333984375,\n",
      " 'loss_1': 586.6602783203125,\n",
      " 'mean_episode_return_0': 71.64700317382812,\n",
      " 'mean_episode_return_1': 79.38121032714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:09,064] After 2396800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.5267333984375,\n",
      " 'loss_1': 586.6602783203125,\n",
      " 'mean_episode_return_0': 71.64700317382812,\n",
      " 'mean_episode_return_1': 79.38121032714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:14,068] After 2403200 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 406.55718994140625,\n",
      " 'loss_1': 491.4504699707031,\n",
      " 'mean_episode_return_0': 71.64506530761719,\n",
      " 'mean_episode_return_1': 79.37030792236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:19,074] After 2403200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.55718994140625,\n",
      " 'loss_1': 491.4504699707031,\n",
      " 'mean_episode_return_0': 71.64506530761719,\n",
      " 'mean_episode_return_1': 79.37030792236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:24,079] After 2403200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.55718994140625,\n",
      " 'loss_1': 491.4504699707031,\n",
      " 'mean_episode_return_0': 71.64506530761719,\n",
      " 'mean_episode_return_1': 79.37030792236328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:29,082] After 2409600 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 434.97412109375,\n",
      " 'loss_1': 533.8204345703125,\n",
      " 'mean_episode_return_0': 71.6915054321289,\n",
      " 'mean_episode_return_1': 79.42583465576172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:34,088] After 2409600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.97412109375,\n",
      " 'loss_1': 533.8204345703125,\n",
      " 'mean_episode_return_0': 71.6915054321289,\n",
      " 'mean_episode_return_1': 79.42583465576172}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:47:39,093] After 2409600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.97412109375,\n",
      " 'loss_1': 533.8204345703125,\n",
      " 'mean_episode_return_0': 71.6915054321289,\n",
      " 'mean_episode_return_1': 79.42583465576172}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:44,099] After 2416000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 421.27740478515625,\n",
      " 'loss_1': 522.1132202148438,\n",
      " 'mean_episode_return_0': 71.74593353271484,\n",
      " 'mean_episode_return_1': 79.42327117919922}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:47:49,104] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:49,197] After 2416000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 421.27740478515625,\n",
      " 'loss_1': 522.1132202148438,\n",
      " 'mean_episode_return_0': 71.74593353271484,\n",
      " 'mean_episode_return_1': 79.42327117919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:54,199] After 2416000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 421.27740478515625,\n",
      " 'loss_1': 522.1132202148438,\n",
      " 'mean_episode_return_0': 71.74593353271484,\n",
      " 'mean_episode_return_1': 79.42327117919922}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:47:59,205] After 2422400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 365.2892150878906,\n",
      " 'loss_1': 438.34283447265625,\n",
      " 'mean_episode_return_0': 71.72512817382812,\n",
      " 'mean_episode_return_1': 79.42656707763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:04,211] After 2422400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 365.2892150878906,\n",
      " 'loss_1': 438.34283447265625,\n",
      " 'mean_episode_return_0': 71.72512817382812,\n",
      " 'mean_episode_return_1': 79.42656707763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:09,214] After 2422400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 365.2892150878906,\n",
      " 'loss_1': 438.34283447265625,\n",
      " 'mean_episode_return_0': 71.72512817382812,\n",
      " 'mean_episode_return_1': 79.42656707763672}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:14,220] After 2428800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 440.2809753417969,\n",
      " 'loss_1': 464.5955810546875,\n",
      " 'mean_episode_return_0': 71.71244049072266,\n",
      " 'mean_episode_return_1': 79.42021179199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:19,225] After 2428800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 440.2809753417969,\n",
      " 'loss_1': 464.5955810546875,\n",
      " 'mean_episode_return_0': 71.71244049072266,\n",
      " 'mean_episode_return_1': 79.42021179199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:24,231] After 2432000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 347.226318359375,\n",
      " 'loss_1': 464.5955810546875,\n",
      " 'mean_episode_return_0': 71.69768524169922,\n",
      " 'mean_episode_return_1': 79.42021179199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:29,237] After 2435200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 347.226318359375,\n",
      " 'loss_1': 484.45660400390625,\n",
      " 'mean_episode_return_0': 71.69768524169922,\n",
      " 'mean_episode_return_1': 79.44851684570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:34,243] After 2435200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 347.226318359375,\n",
      " 'loss_1': 484.45660400390625,\n",
      " 'mean_episode_return_0': 71.69768524169922,\n",
      " 'mean_episode_return_1': 79.44851684570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:39,249] After 2441600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 387.9736633300781,\n",
      " 'loss_1': 514.6487426757812,\n",
      " 'mean_episode_return_0': 71.68286895751953,\n",
      " 'mean_episode_return_1': 79.48017120361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:44,254] After 2441600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 387.9736633300781,\n",
      " 'loss_1': 514.6487426757812,\n",
      " 'mean_episode_return_0': 71.68286895751953,\n",
      " 'mean_episode_return_1': 79.48017120361328}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:48:49,260] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:49,350] After 2441600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 387.9736633300781,\n",
      " 'loss_1': 514.6487426757812,\n",
      " 'mean_episode_return_0': 71.68286895751953,\n",
      " 'mean_episode_return_1': 79.48017120361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:54,355] After 2448000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 417.0665283203125,\n",
      " 'loss_1': 445.9304504394531,\n",
      " 'mean_episode_return_0': 71.6785659790039,\n",
      " 'mean_episode_return_1': 79.55894470214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:48:59,361] After 2448000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.0665283203125,\n",
      " 'loss_1': 445.9304504394531,\n",
      " 'mean_episode_return_0': 71.6785659790039,\n",
      " 'mean_episode_return_1': 79.55894470214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:04,367] After 2448000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.0665283203125,\n",
      " 'loss_1': 445.9304504394531,\n",
      " 'mean_episode_return_0': 71.6785659790039,\n",
      " 'mean_episode_return_1': 79.55894470214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:09,373] After 2454400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 426.31170654296875,\n",
      " 'loss_1': 558.747314453125,\n",
      " 'mean_episode_return_0': 71.6838150024414,\n",
      " 'mean_episode_return_1': 79.60392761230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:14,376] After 2454400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.31170654296875,\n",
      " 'loss_1': 558.747314453125,\n",
      " 'mean_episode_return_0': 71.6838150024414,\n",
      " 'mean_episode_return_1': 79.60392761230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:19,381] After 2457600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 435.3802185058594,\n",
      " 'loss_1': 558.747314453125,\n",
      " 'mean_episode_return_0': 71.65481567382812,\n",
      " 'mean_episode_return_1': 79.60392761230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:24,384] After 2460800 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 435.3802185058594,\n",
      " 'loss_1': 495.6822509765625,\n",
      " 'mean_episode_return_0': 71.65481567382812,\n",
      " 'mean_episode_return_1': 79.65860748291016}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:29,388] After 2460800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 435.3802185058594,\n",
      " 'loss_1': 495.6822509765625,\n",
      " 'mean_episode_return_0': 71.65481567382812,\n",
      " 'mean_episode_return_1': 79.65860748291016}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:34,392] After 2460800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 435.3802185058594,\n",
      " 'loss_1': 495.6822509765625,\n",
      " 'mean_episode_return_0': 71.65481567382812,\n",
      " 'mean_episode_return_1': 79.65860748291016}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:39,398] After 2464000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 406.3307189941406,\n",
      " 'loss_1': 495.6822509765625,\n",
      " 'mean_episode_return_0': 71.61162567138672,\n",
      " 'mean_episode_return_1': 79.65860748291016}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:44,402] After 2467200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 406.3307189941406,\n",
      " 'loss_1': 441.5177917480469,\n",
      " 'mean_episode_return_0': 71.61162567138672,\n",
      " 'mean_episode_return_1': 79.64230346679688}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:49:49,407] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:49,500] After 2467200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.3307189941406,\n",
      " 'loss_1': 441.5177917480469,\n",
      " 'mean_episode_return_0': 71.61162567138672,\n",
      " 'mean_episode_return_1': 79.64230346679688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:54,506] After 2473600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 409.10076904296875,\n",
      " 'loss_1': 523.1103515625,\n",
      " 'mean_episode_return_0': 71.67256164550781,\n",
      " 'mean_episode_return_1': 79.614013671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:49:59,511] After 2473600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 409.10076904296875,\n",
      " 'loss_1': 523.1103515625,\n",
      " 'mean_episode_return_0': 71.67256164550781,\n",
      " 'mean_episode_return_1': 79.614013671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:04,516] After 2473600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 409.10076904296875,\n",
      " 'loss_1': 523.1103515625,\n",
      " 'mean_episode_return_0': 71.67256164550781,\n",
      " 'mean_episode_return_1': 79.614013671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:09,520] After 2476800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 454.1097412109375,\n",
      " 'loss_1': 523.1103515625,\n",
      " 'mean_episode_return_0': 71.6708755493164,\n",
      " 'mean_episode_return_1': 79.614013671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:14,526] After 2480000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 454.1097412109375,\n",
      " 'loss_1': 483.51617431640625,\n",
      " 'mean_episode_return_0': 71.6708755493164,\n",
      " 'mean_episode_return_1': 79.65255737304688}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:19,532] After 2480000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 454.1097412109375,\n",
      " 'loss_1': 483.51617431640625,\n",
      " 'mean_episode_return_0': 71.6708755493164,\n",
      " 'mean_episode_return_1': 79.65255737304688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:50:24,538] After 2486400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 410.7403869628906,\n",
      " 'loss_1': 494.77655029296875,\n",
      " 'mean_episode_return_0': 71.65699768066406,\n",
      " 'mean_episode_return_1': 79.70252990722656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:29,544] After 2486400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 410.7403869628906,\n",
      " 'loss_1': 494.77655029296875,\n",
      " 'mean_episode_return_0': 71.65699768066406,\n",
      " 'mean_episode_return_1': 79.70252990722656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:34,549] After 2489600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 359.2994689941406,\n",
      " 'loss_1': 494.77655029296875,\n",
      " 'mean_episode_return_0': 71.64212799072266,\n",
      " 'mean_episode_return_1': 79.70252990722656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:39,552] After 2492800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 359.2994689941406,\n",
      " 'loss_1': 438.2234191894531,\n",
      " 'mean_episode_return_0': 71.64212799072266,\n",
      " 'mean_episode_return_1': 79.68609619140625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:44,556] After 2492800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 359.2994689941406,\n",
      " 'loss_1': 438.2234191894531,\n",
      " 'mean_episode_return_0': 71.64212799072266,\n",
      " 'mean_episode_return_1': 79.68609619140625}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:50:49,559] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:49,631] After 2496000 frames: @ 630.6 fps Stats:\n",
      "{'loss_0': 360.18084716796875,\n",
      " 'loss_1': 438.2234191894531,\n",
      " 'mean_episode_return_0': 71.66413116455078,\n",
      " 'mean_episode_return_1': 79.68609619140625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:54,637] After 2499200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 360.18084716796875,\n",
      " 'loss_1': 470.2923583984375,\n",
      " 'mean_episode_return_0': 71.66413116455078,\n",
      " 'mean_episode_return_1': 79.7291488647461}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:50:59,643] After 2499200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 360.18084716796875,\n",
      " 'loss_1': 470.2923583984375,\n",
      " 'mean_episode_return_0': 71.66413116455078,\n",
      " 'mean_episode_return_1': 79.7291488647461}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:04,648] After 2502400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 393.6236267089844,\n",
      " 'loss_1': 470.2923583984375,\n",
      " 'mean_episode_return_0': 71.65731048583984,\n",
      " 'mean_episode_return_1': 79.7291488647461}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:09,652] After 2505600 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 393.6236267089844,\n",
      " 'loss_1': 530.0189819335938,\n",
      " 'mean_episode_return_0': 71.65731048583984,\n",
      " 'mean_episode_return_1': 79.73786926269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:14,658] After 2505600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 393.6236267089844,\n",
      " 'loss_1': 530.0189819335938,\n",
      " 'mean_episode_return_0': 71.65731048583984,\n",
      " 'mean_episode_return_1': 79.73786926269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:19,664] After 2508800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 413.5787658691406,\n",
      " 'loss_1': 530.0189819335938,\n",
      " 'mean_episode_return_0': 71.64824676513672,\n",
      " 'mean_episode_return_1': 79.73786926269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:24,670] After 2512000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 413.5787658691406,\n",
      " 'loss_1': 488.9606628417969,\n",
      " 'mean_episode_return_0': 71.64824676513672,\n",
      " 'mean_episode_return_1': 79.66921997070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:29,676] After 2512000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.5787658691406,\n",
      " 'loss_1': 488.9606628417969,\n",
      " 'mean_episode_return_0': 71.64824676513672,\n",
      " 'mean_episode_return_1': 79.66921997070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:34,682] After 2515200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 433.8518371582031,\n",
      " 'loss_1': 488.9606628417969,\n",
      " 'mean_episode_return_0': 71.6552505493164,\n",
      " 'mean_episode_return_1': 79.66921997070312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:39,688] After 2518400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 433.8518371582031,\n",
      " 'loss_1': 402.9325256347656,\n",
      " 'mean_episode_return_0': 71.6552505493164,\n",
      " 'mean_episode_return_1': 79.6363296508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:44,694] After 2518400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.8518371582031,\n",
      " 'loss_1': 402.9325256347656,\n",
      " 'mean_episode_return_0': 71.6552505493164,\n",
      " 'mean_episode_return_1': 79.6363296508789}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:51:49,695] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:49,789] After 2521600 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 396.3335876464844,\n",
      " 'loss_1': 402.9325256347656,\n",
      " 'mean_episode_return_0': 71.61775207519531,\n",
      " 'mean_episode_return_1': 79.6363296508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:54,795] After 2524800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 396.3335876464844,\n",
      " 'loss_1': 521.3642578125,\n",
      " 'mean_episode_return_0': 71.61775207519531,\n",
      " 'mean_episode_return_1': 79.71722412109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:51:59,801] After 2528000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 465.3902893066406,\n",
      " 'loss_1': 521.3642578125,\n",
      " 'mean_episode_return_0': 71.6041259765625,\n",
      " 'mean_episode_return_1': 79.71722412109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:04,804] After 2528000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 465.3902893066406,\n",
      " 'loss_1': 521.3642578125,\n",
      " 'mean_episode_return_0': 71.6041259765625,\n",
      " 'mean_episode_return_1': 79.71722412109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:09,811] After 2531200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 465.3902893066406,\n",
      " 'loss_1': 512.743408203125,\n",
      " 'mean_episode_return_0': 71.6041259765625,\n",
      " 'mean_episode_return_1': 79.7191390991211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:14,817] After 2534400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 400.9254455566406,\n",
      " 'loss_1': 512.743408203125,\n",
      " 'mean_episode_return_0': 71.59319305419922,\n",
      " 'mean_episode_return_1': 79.7191390991211}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:19,820] After 2537600 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 400.9254455566406,\n",
      " 'loss_1': 511.2961120605469,\n",
      " 'mean_episode_return_0': 71.59319305419922,\n",
      " 'mean_episode_return_1': 79.80056762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:24,826] After 2537600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.9254455566406,\n",
      " 'loss_1': 511.2961120605469,\n",
      " 'mean_episode_return_0': 71.59319305419922,\n",
      " 'mean_episode_return_1': 79.80056762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:29,832] After 2540800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 441.4039611816406,\n",
      " 'loss_1': 511.2961120605469,\n",
      " 'mean_episode_return_0': 71.54369354248047,\n",
      " 'mean_episode_return_1': 79.80056762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:34,838] After 2544000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 441.4039611816406,\n",
      " 'loss_1': 489.9114074707031,\n",
      " 'mean_episode_return_0': 71.54369354248047,\n",
      " 'mean_episode_return_1': 79.82698822021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:39,843] After 2544000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 441.4039611816406,\n",
      " 'loss_1': 489.9114074707031,\n",
      " 'mean_episode_return_0': 71.54369354248047,\n",
      " 'mean_episode_return_1': 79.82698822021484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:44,849] After 2547200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 414.7076416015625,\n",
      " 'loss_1': 489.9114074707031,\n",
      " 'mean_episode_return_0': 71.54049682617188,\n",
      " 'mean_episode_return_1': 79.82698822021484}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:52:49,855] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:49,930] After 2550400 frames: @ 629.9 fps Stats:\n",
      "{'loss_0': 414.7076416015625,\n",
      " 'loss_1': 442.6246337890625,\n",
      " 'mean_episode_return_0': 71.54049682617188,\n",
      " 'mean_episode_return_1': 79.81198120117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:54,936] After 2553600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 432.4293518066406,\n",
      " 'loss_1': 442.6246337890625,\n",
      " 'mean_episode_return_0': 71.484375,\n",
      " 'mean_episode_return_1': 79.81198120117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:52:59,942] After 2553600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.4293518066406,\n",
      " 'loss_1': 442.6246337890625,\n",
      " 'mean_episode_return_0': 71.484375,\n",
      " 'mean_episode_return_1': 79.81198120117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:04,948] After 2556800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 432.4293518066406,\n",
      " 'loss_1': 456.3421630859375,\n",
      " 'mean_episode_return_0': 71.484375,\n",
      " 'mean_episode_return_1': 79.82350158691406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:09,954] After 2560000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.48919677734375,\n",
      " 'loss_1': 456.3421630859375,\n",
      " 'mean_episode_return_0': 71.5496826171875,\n",
      " 'mean_episode_return_1': 79.82350158691406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:53:14,960] After 2560000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 454.48919677734375,\n",
      " 'loss_1': 456.3421630859375,\n",
      " 'mean_episode_return_0': 71.5496826171875,\n",
      " 'mean_episode_return_1': 79.82350158691406}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:19,964] After 2563200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 454.48919677734375,\n",
      " 'loss_1': 520.0376586914062,\n",
      " 'mean_episode_return_0': 71.5496826171875,\n",
      " 'mean_episode_return_1': 79.84272003173828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:24,968] After 2566400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 394.5901794433594,\n",
      " 'loss_1': 520.0376586914062,\n",
      " 'mean_episode_return_0': 71.46012878417969,\n",
      " 'mean_episode_return_1': 79.84272003173828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:29,972] After 2566400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 394.5901794433594,\n",
      " 'loss_1': 520.0376586914062,\n",
      " 'mean_episode_return_0': 71.46012878417969,\n",
      " 'mean_episode_return_1': 79.84272003173828}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:34,976] After 2569600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 394.5901794433594,\n",
      " 'loss_1': 570.4978637695312,\n",
      " 'mean_episode_return_0': 71.46012878417969,\n",
      " 'mean_episode_return_1': 79.91807556152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:39,982] After 2569600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 394.5901794433594,\n",
      " 'loss_1': 570.4978637695312,\n",
      " 'mean_episode_return_0': 71.46012878417969,\n",
      " 'mean_episode_return_1': 79.91807556152344}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:44,987] After 2572800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 504.8160400390625,\n",
      " 'loss_1': 570.4978637695312,\n",
      " 'mean_episode_return_0': 71.46062469482422,\n",
      " 'mean_episode_return_1': 79.91807556152344}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:53:49,993] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:50,065] After 2576000 frames: @ 630.3 fps Stats:\n",
      "{'loss_0': 504.8160400390625,\n",
      " 'loss_1': 515.7435302734375,\n",
      " 'mean_episode_return_0': 71.46062469482422,\n",
      " 'mean_episode_return_1': 79.94178771972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:53:55,072] After 2579200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 399.9547424316406,\n",
      " 'loss_1': 515.7435302734375,\n",
      " 'mean_episode_return_0': 71.4234390258789,\n",
      " 'mean_episode_return_1': 79.94178771972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:00,078] After 2579200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 399.9547424316406,\n",
      " 'loss_1': 515.7435302734375,\n",
      " 'mean_episode_return_0': 71.4234390258789,\n",
      " 'mean_episode_return_1': 79.94178771972656}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:05,080] After 2582400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 399.9547424316406,\n",
      " 'loss_1': 573.8909301757812,\n",
      " 'mean_episode_return_0': 71.4234390258789,\n",
      " 'mean_episode_return_1': 80.01386260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:10,086] After 2585600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 419.0164794921875,\n",
      " 'loss_1': 573.8909301757812,\n",
      " 'mean_episode_return_0': 71.33174896240234,\n",
      " 'mean_episode_return_1': 80.01386260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:15,088] After 2585600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 419.0164794921875,\n",
      " 'loss_1': 573.8909301757812,\n",
      " 'mean_episode_return_0': 71.33174896240234,\n",
      " 'mean_episode_return_1': 80.01386260986328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:20,093] After 2588800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 419.0164794921875,\n",
      " 'loss_1': 480.0054931640625,\n",
      " 'mean_episode_return_0': 71.33174896240234,\n",
      " 'mean_episode_return_1': 80.03511047363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:25,099] After 2592000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 366.8625183105469,\n",
      " 'loss_1': 480.0054931640625,\n",
      " 'mean_episode_return_0': 71.29237365722656,\n",
      " 'mean_episode_return_1': 80.03511047363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:30,105] After 2592000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 366.8625183105469,\n",
      " 'loss_1': 480.0054931640625,\n",
      " 'mean_episode_return_0': 71.29237365722656,\n",
      " 'mean_episode_return_1': 80.03511047363281}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:35,111] After 2595200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 366.8625183105469,\n",
      " 'loss_1': 460.9847106933594,\n",
      " 'mean_episode_return_0': 71.29237365722656,\n",
      " 'mean_episode_return_1': 80.07392120361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:40,117] After 2598400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 424.6579284667969,\n",
      " 'loss_1': 460.9847106933594,\n",
      " 'mean_episode_return_0': 71.3055648803711,\n",
      " 'mean_episode_return_1': 80.07392120361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:45,123] After 2598400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 424.6579284667969,\n",
      " 'loss_1': 460.9847106933594,\n",
      " 'mean_episode_return_0': 71.3055648803711,\n",
      " 'mean_episode_return_1': 80.07392120361328}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:54:50,128] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:50,219] After 2601600 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 424.6579284667969,\n",
      " 'loss_1': 534.904052734375,\n",
      " 'mean_episode_return_0': 71.3055648803711,\n",
      " 'mean_episode_return_1': 80.09214782714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:54:55,224] After 2604800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 476.43902587890625,\n",
      " 'loss_1': 534.904052734375,\n",
      " 'mean_episode_return_0': 71.28424835205078,\n",
      " 'mean_episode_return_1': 80.09214782714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:00,230] After 2604800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 476.43902587890625,\n",
      " 'loss_1': 534.904052734375,\n",
      " 'mean_episode_return_0': 71.28424835205078,\n",
      " 'mean_episode_return_1': 80.09214782714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:05,236] After 2608000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 476.43902587890625,\n",
      " 'loss_1': 522.8313598632812,\n",
      " 'mean_episode_return_0': 71.28424835205078,\n",
      " 'mean_episode_return_1': 80.1004638671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:10,242] After 2611200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 405.14263916015625,\n",
      " 'loss_1': 522.8313598632812,\n",
      " 'mean_episode_return_0': 71.26950073242188,\n",
      " 'mean_episode_return_1': 80.1004638671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:15,247] After 2611200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.14263916015625,\n",
      " 'loss_1': 522.8313598632812,\n",
      " 'mean_episode_return_0': 71.26950073242188,\n",
      " 'mean_episode_return_1': 80.1004638671875}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:20,256] After 2614400 frames: @ 639.1 fps Stats:\n",
      "{'loss_0': 405.14263916015625,\n",
      " 'loss_1': 467.0149230957031,\n",
      " 'mean_episode_return_0': 71.26950073242188,\n",
      " 'mean_episode_return_1': 80.15279388427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:25,262] After 2617600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.8503723144531,\n",
      " 'loss_1': 467.0149230957031,\n",
      " 'mean_episode_return_0': 71.2491226196289,\n",
      " 'mean_episode_return_1': 80.15279388427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:30,268] After 2617600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.8503723144531,\n",
      " 'loss_1': 467.0149230957031,\n",
      " 'mean_episode_return_0': 71.2491226196289,\n",
      " 'mean_episode_return_1': 80.15279388427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:35,272] After 2620800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 397.8503723144531,\n",
      " 'loss_1': 465.134521484375,\n",
      " 'mean_episode_return_0': 71.2491226196289,\n",
      " 'mean_episode_return_1': 80.15589904785156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:40,277] After 2624000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 426.7228088378906,\n",
      " 'loss_1': 465.134521484375,\n",
      " 'mean_episode_return_0': 71.25074768066406,\n",
      " 'mean_episode_return_1': 80.15589904785156}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:45,280] After 2624000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.7228088378906,\n",
      " 'loss_1': 465.134521484375,\n",
      " 'mean_episode_return_0': 71.25074768066406,\n",
      " 'mean_episode_return_1': 80.15589904785156}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:55:50,283] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:50,375] After 2630400 frames: @ 1256.2 fps Stats:\n",
      "{'loss_0': 401.5495300292969,\n",
      " 'loss_1': 506.0989685058594,\n",
      " 'mean_episode_return_0': 71.23143768310547,\n",
      " 'mean_episode_return_1': 80.18083953857422}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:55:55,381] After 2630400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.5495300292969,\n",
      " 'loss_1': 506.0989685058594,\n",
      " 'mean_episode_return_0': 71.23143768310547,\n",
      " 'mean_episode_return_1': 80.18083953857422}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 13:56:00,384] After 2630400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.5495300292969,\n",
      " 'loss_1': 506.0989685058594,\n",
      " 'mean_episode_return_0': 71.23143768310547,\n",
      " 'mean_episode_return_1': 80.18083953857422}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:05,390] After 2633600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.5495300292969,\n",
      " 'loss_1': 446.2557067871094,\n",
      " 'mean_episode_return_0': 71.23143768310547,\n",
      " 'mean_episode_return_1': 80.20906829833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:10,391] After 2636800 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 402.5992126464844,\n",
      " 'loss_1': 446.2557067871094,\n",
      " 'mean_episode_return_0': 71.21611785888672,\n",
      " 'mean_episode_return_1': 80.20906829833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:15,397] After 2636800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.5992126464844,\n",
      " 'loss_1': 446.2557067871094,\n",
      " 'mean_episode_return_0': 71.21611785888672,\n",
      " 'mean_episode_return_1': 80.20906829833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:20,403] After 2643200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 451.6443786621094,\n",
      " 'loss_1': 439.94256591796875,\n",
      " 'mean_episode_return_0': 71.20075225830078,\n",
      " 'mean_episode_return_1': 80.27497863769531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:25,409] After 2643200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.6443786621094,\n",
      " 'loss_1': 439.94256591796875,\n",
      " 'mean_episode_return_0': 71.20075225830078,\n",
      " 'mean_episode_return_1': 80.27497863769531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:30,415] After 2643200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.6443786621094,\n",
      " 'loss_1': 439.94256591796875,\n",
      " 'mean_episode_return_0': 71.20075225830078,\n",
      " 'mean_episode_return_1': 80.27497863769531}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:35,421] After 2649600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 327.0606384277344,\n",
      " 'loss_1': 522.8508911132812,\n",
      " 'mean_episode_return_0': 71.15937805175781,\n",
      " 'mean_episode_return_1': 80.25472259521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:40,424] After 2649600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 327.0606384277344,\n",
      " 'loss_1': 522.8508911132812,\n",
      " 'mean_episode_return_0': 71.15937805175781,\n",
      " 'mean_episode_return_1': 80.25472259521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:45,428] After 2649600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 327.0606384277344,\n",
      " 'loss_1': 522.8508911132812,\n",
      " 'mean_episode_return_0': 71.15937805175781,\n",
      " 'mean_episode_return_1': 80.25472259521484}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:56:50,433] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:50,523] After 2656000 frames: @ 1256.3 fps Stats:\n",
      "{'loss_0': 472.4420471191406,\n",
      " 'loss_1': 550.4457397460938,\n",
      " 'mean_episode_return_0': 71.1302490234375,\n",
      " 'mean_episode_return_1': 80.3126220703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:56:55,524] After 2656000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 472.4420471191406,\n",
      " 'loss_1': 550.4457397460938,\n",
      " 'mean_episode_return_0': 71.1302490234375,\n",
      " 'mean_episode_return_1': 80.3126220703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:00,530] After 2656000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 472.4420471191406,\n",
      " 'loss_1': 550.4457397460938,\n",
      " 'mean_episode_return_0': 71.1302490234375,\n",
      " 'mean_episode_return_1': 80.3126220703125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:05,536] After 2662400 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 464.0306091308594,\n",
      " 'loss_1': 453.46441650390625,\n",
      " 'mean_episode_return_0': 71.1068115234375,\n",
      " 'mean_episode_return_1': 80.34733581542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:10,540] After 2662400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 464.0306091308594,\n",
      " 'loss_1': 453.46441650390625,\n",
      " 'mean_episode_return_0': 71.1068115234375,\n",
      " 'mean_episode_return_1': 80.34733581542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:15,546] After 2668800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 400.5627136230469,\n",
      " 'loss_1': 442.6899108886719,\n",
      " 'mean_episode_return_0': 71.07125091552734,\n",
      " 'mean_episode_return_1': 80.40458679199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:20,548] After 2668800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.5627136230469,\n",
      " 'loss_1': 442.6899108886719,\n",
      " 'mean_episode_return_0': 71.07125091552734,\n",
      " 'mean_episode_return_1': 80.40458679199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:25,554] After 2668800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.5627136230469,\n",
      " 'loss_1': 442.6899108886719,\n",
      " 'mean_episode_return_0': 71.07125091552734,\n",
      " 'mean_episode_return_1': 80.40458679199219}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:30,556] After 2675200 frames: @ 1279.7 fps Stats:\n",
      "{'loss_0': 437.78118896484375,\n",
      " 'loss_1': 540.3782958984375,\n",
      " 'mean_episode_return_0': 71.07881164550781,\n",
      " 'mean_episode_return_1': 80.35897064208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:35,562] After 2675200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.78118896484375,\n",
      " 'loss_1': 540.3782958984375,\n",
      " 'mean_episode_return_0': 71.07881164550781,\n",
      " 'mean_episode_return_1': 80.35897064208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:40,567] After 2675200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.78118896484375,\n",
      " 'loss_1': 540.3782958984375,\n",
      " 'mean_episode_return_0': 71.07881164550781,\n",
      " 'mean_episode_return_1': 80.35897064208984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:45,573] After 2681600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 386.3582763671875,\n",
      " 'loss_1': 619.1513671875,\n",
      " 'mean_episode_return_0': 71.03606414794922,\n",
      " 'mean_episode_return_1': 80.416015625}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:57:50,578] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:50,671] After 2681600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.3582763671875,\n",
      " 'loss_1': 619.1513671875,\n",
      " 'mean_episode_return_0': 71.03606414794922,\n",
      " 'mean_episode_return_1': 80.416015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:57:55,677] After 2681600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.3582763671875,\n",
      " 'loss_1': 619.1513671875,\n",
      " 'mean_episode_return_0': 71.03606414794922,\n",
      " 'mean_episode_return_1': 80.416015625}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:00,681] After 2688000 frames: @ 1279.3 fps Stats:\n",
      "{'loss_0': 413.2908630371094,\n",
      " 'loss_1': 454.72943115234375,\n",
      " 'mean_episode_return_0': 71.09111785888672,\n",
      " 'mean_episode_return_1': 80.43505096435547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:05,686] After 2688000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.2908630371094,\n",
      " 'loss_1': 454.72943115234375,\n",
      " 'mean_episode_return_0': 71.09111785888672,\n",
      " 'mean_episode_return_1': 80.43505096435547}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:10,692] After 2694400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 449.0877685546875,\n",
      " 'loss_1': 492.6173400878906,\n",
      " 'mean_episode_return_0': 71.10712432861328,\n",
      " 'mean_episode_return_1': 80.51139831542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:15,696] After 2694400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 449.0877685546875,\n",
      " 'loss_1': 492.6173400878906,\n",
      " 'mean_episode_return_0': 71.10712432861328,\n",
      " 'mean_episode_return_1': 80.51139831542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:20,700] After 2694400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 449.0877685546875,\n",
      " 'loss_1': 492.6173400878906,\n",
      " 'mean_episode_return_0': 71.10712432861328,\n",
      " 'mean_episode_return_1': 80.51139831542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:25,705] After 2694400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 449.0877685546875,\n",
      " 'loss_1': 492.6173400878906,\n",
      " 'mean_episode_return_0': 71.10712432861328,\n",
      " 'mean_episode_return_1': 80.51139831542969}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:30,711] After 2700800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 326.74749755859375,\n",
      " 'loss_1': 440.07080078125,\n",
      " 'mean_episode_return_0': 70.9964370727539,\n",
      " 'mean_episode_return_1': 80.553955078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:35,716] After 2700800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 326.74749755859375,\n",
      " 'loss_1': 440.07080078125,\n",
      " 'mean_episode_return_0': 70.9964370727539,\n",
      " 'mean_episode_return_1': 80.553955078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:40,722] After 2700800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 326.74749755859375,\n",
      " 'loss_1': 440.07080078125,\n",
      " 'mean_episode_return_0': 70.9964370727539,\n",
      " 'mean_episode_return_1': 80.553955078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:45,728] After 2707200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 403.3775634765625,\n",
      " 'loss_1': 455.20538330078125,\n",
      " 'mean_episode_return_0': 70.99993896484375,\n",
      " 'mean_episode_return_1': 80.53302001953125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 13:58:50,731] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:50,823] After 2707200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.3775634765625,\n",
      " 'loss_1': 455.20538330078125,\n",
      " 'mean_episode_return_0': 70.99993896484375,\n",
      " 'mean_episode_return_1': 80.53302001953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:58:55,828] After 2707200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.3775634765625,\n",
      " 'loss_1': 455.20538330078125,\n",
      " 'mean_episode_return_0': 70.99993896484375,\n",
      " 'mean_episode_return_1': 80.53302001953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:00,834] After 2713600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 427.22686767578125,\n",
      " 'loss_1': 444.8771667480469,\n",
      " 'mean_episode_return_0': 70.96156311035156,\n",
      " 'mean_episode_return_1': 80.59229278564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:05,840] After 2713600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 427.22686767578125,\n",
      " 'loss_1': 444.8771667480469,\n",
      " 'mean_episode_return_0': 70.96156311035156,\n",
      " 'mean_episode_return_1': 80.59229278564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:10,846] After 2716800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 347.2005920410156,\n",
      " 'loss_1': 444.8771667480469,\n",
      " 'mean_episode_return_0': 70.93568420410156,\n",
      " 'mean_episode_return_1': 80.59229278564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:15,852] After 2720000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 347.2005920410156,\n",
      " 'loss_1': 466.6482849121094,\n",
      " 'mean_episode_return_0': 70.93568420410156,\n",
      " 'mean_episode_return_1': 80.6191177368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:20,858] After 2720000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 347.2005920410156,\n",
      " 'loss_1': 466.6482849121094,\n",
      " 'mean_episode_return_0': 70.93568420410156,\n",
      " 'mean_episode_return_1': 80.6191177368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:25,864] After 2723200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 357.52923583984375,\n",
      " 'loss_1': 466.6482849121094,\n",
      " 'mean_episode_return_0': 70.95818328857422,\n",
      " 'mean_episode_return_1': 80.6191177368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:30,870] After 2726400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 357.52923583984375,\n",
      " 'loss_1': 405.04718017578125,\n",
      " 'mean_episode_return_0': 70.95818328857422,\n",
      " 'mean_episode_return_1': 80.59622955322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:35,872] After 2726400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 357.52923583984375,\n",
      " 'loss_1': 405.04718017578125,\n",
      " 'mean_episode_return_0': 70.95818328857422,\n",
      " 'mean_episode_return_1': 80.59622955322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:40,878] After 2729600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.1784362792969,\n",
      " 'loss_1': 405.04718017578125,\n",
      " 'mean_episode_return_0': 70.92525482177734,\n",
      " 'mean_episode_return_1': 80.59622955322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:45,884] After 2732800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.1784362792969,\n",
      " 'loss_1': 427.513671875,\n",
      " 'mean_episode_return_0': 70.92525482177734,\n",
      " 'mean_episode_return_1': 80.60994720458984}\n",
      "[INFO:417825 trainer:335 2022-09-17 13:59:50,889] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:50,984] After 2732800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.1784362792969,\n",
      " 'loss_1': 427.513671875,\n",
      " 'mean_episode_return_0': 70.92525482177734,\n",
      " 'mean_episode_return_1': 80.60994720458984}\n",
      "[INFO:417825 trainer:367 2022-09-17 13:59:55,988] After 2739200 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 408.9095153808594,\n",
      " 'loss_1': 586.1741943359375,\n",
      " 'mean_episode_return_0': 70.8935546875,\n",
      " 'mean_episode_return_1': 80.62196350097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:00,994] After 2739200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 408.9095153808594,\n",
      " 'loss_1': 586.1741943359375,\n",
      " 'mean_episode_return_0': 70.8935546875,\n",
      " 'mean_episode_return_1': 80.62196350097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:06,000] After 2739200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 408.9095153808594,\n",
      " 'loss_1': 586.1741943359375,\n",
      " 'mean_episode_return_0': 70.8935546875,\n",
      " 'mean_episode_return_1': 80.62196350097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:11,006] After 2745600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 381.0428161621094,\n",
      " 'loss_1': 510.8470153808594,\n",
      " 'mean_episode_return_0': 70.87825012207031,\n",
      " 'mean_episode_return_1': 80.62564086914062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:16,008] After 2745600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 381.0428161621094,\n",
      " 'loss_1': 510.8470153808594,\n",
      " 'mean_episode_return_0': 70.87825012207031,\n",
      " 'mean_episode_return_1': 80.62564086914062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:21,014] After 2745600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 381.0428161621094,\n",
      " 'loss_1': 510.8470153808594,\n",
      " 'mean_episode_return_0': 70.87825012207031,\n",
      " 'mean_episode_return_1': 80.62564086914062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:26,020] After 2752000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 400.9242248535156,\n",
      " 'loss_1': 516.2693481445312,\n",
      " 'mean_episode_return_0': 70.8699951171875,\n",
      " 'mean_episode_return_1': 80.69677734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:31,026] After 2752000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.9242248535156,\n",
      " 'loss_1': 516.2693481445312,\n",
      " 'mean_episode_return_0': 70.8699951171875,\n",
      " 'mean_episode_return_1': 80.69677734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:36,032] After 2755200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 465.68121337890625,\n",
      " 'loss_1': 516.2693481445312,\n",
      " 'mean_episode_return_0': 70.85631561279297,\n",
      " 'mean_episode_return_1': 80.69677734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:41,038] After 2758400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 465.68121337890625,\n",
      " 'loss_1': 393.3653869628906,\n",
      " 'mean_episode_return_0': 70.85631561279297,\n",
      " 'mean_episode_return_1': 80.76758575439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:46,044] After 2758400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 465.68121337890625,\n",
      " 'loss_1': 393.3653869628906,\n",
      " 'mean_episode_return_0': 70.85631561279297,\n",
      " 'mean_episode_return_1': 80.76758575439453}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:00:51,049] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:51,144] After 2761600 frames: @ 627.6 fps Stats:\n",
      "{'loss_0': 424.2828674316406,\n",
      " 'loss_1': 393.3653869628906,\n",
      " 'mean_episode_return_0': 70.85124969482422,\n",
      " 'mean_episode_return_1': 80.76758575439453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:00:56,149] After 2764800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 424.2828674316406,\n",
      " 'loss_1': 430.1006164550781,\n",
      " 'mean_episode_return_0': 70.85124969482422,\n",
      " 'mean_episode_return_1': 80.80076599121094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:01,155] After 2764800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 424.2828674316406,\n",
      " 'loss_1': 430.1006164550781,\n",
      " 'mean_episode_return_0': 70.85124969482422,\n",
      " 'mean_episode_return_1': 80.80076599121094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:06,161] After 2768000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 407.4925231933594,\n",
      " 'loss_1': 430.1006164550781,\n",
      " 'mean_episode_return_0': 70.89593505859375,\n",
      " 'mean_episode_return_1': 80.80076599121094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:11,167] After 2771200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 407.4925231933594,\n",
      " 'loss_1': 504.28424072265625,\n",
      " 'mean_episode_return_0': 70.89593505859375,\n",
      " 'mean_episode_return_1': 80.80801391601562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:16,173] After 2771200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 407.4925231933594,\n",
      " 'loss_1': 504.28424072265625,\n",
      " 'mean_episode_return_0': 70.89593505859375,\n",
      " 'mean_episode_return_1': 80.80801391601562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:21,178] After 2774400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 478.8698425292969,\n",
      " 'loss_1': 504.28424072265625,\n",
      " 'mean_episode_return_0': 70.96231079101562,\n",
      " 'mean_episode_return_1': 80.80801391601562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:26,184] After 2777600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 478.8698425292969,\n",
      " 'loss_1': 485.913818359375,\n",
      " 'mean_episode_return_0': 70.96231079101562,\n",
      " 'mean_episode_return_1': 80.77825164794922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:31,190] After 2777600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 478.8698425292969,\n",
      " 'loss_1': 485.913818359375,\n",
      " 'mean_episode_return_0': 70.96231079101562,\n",
      " 'mean_episode_return_1': 80.77825164794922}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:01:36,196] After 2780800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.1842346191406,\n",
      " 'loss_1': 485.913818359375,\n",
      " 'mean_episode_return_0': 70.94168853759766,\n",
      " 'mean_episode_return_1': 80.77825164794922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:41,202] After 2784000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.1842346191406,\n",
      " 'loss_1': 498.2940368652344,\n",
      " 'mean_episode_return_0': 70.94168853759766,\n",
      " 'mean_episode_return_1': 80.7333755493164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:46,207] After 2784000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 399.1842346191406,\n",
      " 'loss_1': 498.2940368652344,\n",
      " 'mean_episode_return_0': 70.94168853759766,\n",
      " 'mean_episode_return_1': 80.7333755493164}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:01:51,213] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:51,301] After 2787200 frames: @ 628.4 fps Stats:\n",
      "{'loss_0': 415.9824523925781,\n",
      " 'loss_1': 498.2940368652344,\n",
      " 'mean_episode_return_0': 71.00262451171875,\n",
      " 'mean_episode_return_1': 80.7333755493164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:01:56,307] After 2790400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 415.9824523925781,\n",
      " 'loss_1': 472.56927490234375,\n",
      " 'mean_episode_return_0': 71.00262451171875,\n",
      " 'mean_episode_return_1': 80.70555114746094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:01,313] After 2790400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.9824523925781,\n",
      " 'loss_1': 472.56927490234375,\n",
      " 'mean_episode_return_0': 71.00262451171875,\n",
      " 'mean_episode_return_1': 80.70555114746094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:06,319] After 2793600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 418.66888427734375,\n",
      " 'loss_1': 472.56927490234375,\n",
      " 'mean_episode_return_0': 71.02200317382812,\n",
      " 'mean_episode_return_1': 80.70555114746094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:11,325] After 2796800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 418.66888427734375,\n",
      " 'loss_1': 447.3546447753906,\n",
      " 'mean_episode_return_0': 71.02200317382812,\n",
      " 'mean_episode_return_1': 80.67483520507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:16,331] After 2800000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 420.43505859375,\n",
      " 'loss_1': 447.3546447753906,\n",
      " 'mean_episode_return_0': 71.00218963623047,\n",
      " 'mean_episode_return_1': 80.67483520507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:21,336] After 2800000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 420.43505859375,\n",
      " 'loss_1': 447.3546447753906,\n",
      " 'mean_episode_return_0': 71.00218963623047,\n",
      " 'mean_episode_return_1': 80.67483520507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:26,340] After 2803200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 420.43505859375,\n",
      " 'loss_1': 438.2674255371094,\n",
      " 'mean_episode_return_0': 71.00218963623047,\n",
      " 'mean_episode_return_1': 80.66097259521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:31,346] After 2806400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 458.06884765625,\n",
      " 'loss_1': 438.2674255371094,\n",
      " 'mean_episode_return_0': 71.01249694824219,\n",
      " 'mean_episode_return_1': 80.66097259521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:36,352] After 2806400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 458.06884765625,\n",
      " 'loss_1': 438.2674255371094,\n",
      " 'mean_episode_return_0': 71.01249694824219,\n",
      " 'mean_episode_return_1': 80.66097259521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:41,357] After 2809600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 458.06884765625,\n",
      " 'loss_1': 622.089599609375,\n",
      " 'mean_episode_return_0': 71.01249694824219,\n",
      " 'mean_episode_return_1': 80.67194366455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:46,360] After 2812800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 447.92095947265625,\n",
      " 'loss_1': 622.089599609375,\n",
      " 'mean_episode_return_0': 70.9957504272461,\n",
      " 'mean_episode_return_1': 80.67194366455078}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:02:51,366] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:51,455] After 2812800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 447.92095947265625,\n",
      " 'loss_1': 622.089599609375,\n",
      " 'mean_episode_return_0': 70.9957504272461,\n",
      " 'mean_episode_return_1': 80.67194366455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:02:56,461] After 2816000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 447.92095947265625,\n",
      " 'loss_1': 466.8543701171875,\n",
      " 'mean_episode_return_0': 70.9957504272461,\n",
      " 'mean_episode_return_1': 80.67151641845703}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:01,467] After 2819200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 443.53729248046875,\n",
      " 'loss_1': 466.8543701171875,\n",
      " 'mean_episode_return_0': 70.96755981445312,\n",
      " 'mean_episode_return_1': 80.67151641845703}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:06,473] After 2822400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 443.53729248046875,\n",
      " 'loss_1': 437.8417053222656,\n",
      " 'mean_episode_return_0': 70.96755981445312,\n",
      " 'mean_episode_return_1': 80.7104263305664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:11,476] After 2822400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 443.53729248046875,\n",
      " 'loss_1': 437.8417053222656,\n",
      " 'mean_episode_return_0': 70.96755981445312,\n",
      " 'mean_episode_return_1': 80.7104263305664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:16,482] After 2825600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 485.06756591796875,\n",
      " 'loss_1': 437.8417053222656,\n",
      " 'mean_episode_return_0': 70.93806457519531,\n",
      " 'mean_episode_return_1': 80.7104263305664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:21,488] After 2828800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 485.06756591796875,\n",
      " 'loss_1': 503.37982177734375,\n",
      " 'mean_episode_return_0': 70.93806457519531,\n",
      " 'mean_episode_return_1': 80.71640014648438}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:26,494] After 2828800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 485.06756591796875,\n",
      " 'loss_1': 503.37982177734375,\n",
      " 'mean_episode_return_0': 70.93806457519531,\n",
      " 'mean_episode_return_1': 80.71640014648438}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:31,500] After 2832000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 420.0081481933594,\n",
      " 'loss_1': 503.37982177734375,\n",
      " 'mean_episode_return_0': 70.88768768310547,\n",
      " 'mean_episode_return_1': 80.71640014648438}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:36,506] After 2835200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 420.0081481933594,\n",
      " 'loss_1': 502.6852111816406,\n",
      " 'mean_episode_return_0': 70.88768768310547,\n",
      " 'mean_episode_return_1': 80.7122802734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:41,511] After 2838400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 496.59185791015625,\n",
      " 'loss_1': 502.6852111816406,\n",
      " 'mean_episode_return_0': 70.89830780029297,\n",
      " 'mean_episode_return_1': 80.7122802734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:46,517] After 2838400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 496.59185791015625,\n",
      " 'loss_1': 502.6852111816406,\n",
      " 'mean_episode_return_0': 70.89830780029297,\n",
      " 'mean_episode_return_1': 80.7122802734375}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:03:51,523] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:51,614] After 2841600 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 496.59185791015625,\n",
      " 'loss_1': 479.6086120605469,\n",
      " 'mean_episode_return_0': 70.89830780029297,\n",
      " 'mean_episode_return_1': 80.73942565917969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:03:56,620] After 2844800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 433.2779541015625,\n",
      " 'loss_1': 479.6086120605469,\n",
      " 'mean_episode_return_0': 70.89981079101562,\n",
      " 'mean_episode_return_1': 80.73942565917969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:01,626] After 2844800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.2779541015625,\n",
      " 'loss_1': 479.6086120605469,\n",
      " 'mean_episode_return_0': 70.89981079101562,\n",
      " 'mean_episode_return_1': 80.73942565917969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:06,632] After 2848000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 433.2779541015625,\n",
      " 'loss_1': 441.87005615234375,\n",
      " 'mean_episode_return_0': 70.89981079101562,\n",
      " 'mean_episode_return_1': 80.75199127197266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:11,636] After 2851200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 333.7069396972656,\n",
      " 'loss_1': 441.87005615234375,\n",
      " 'mean_episode_return_0': 70.88118743896484,\n",
      " 'mean_episode_return_1': 80.75199127197266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:16,642] After 2851200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 333.7069396972656,\n",
      " 'loss_1': 441.87005615234375,\n",
      " 'mean_episode_return_0': 70.88118743896484,\n",
      " 'mean_episode_return_1': 80.75199127197266}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:04:21,648] After 2854400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 333.7069396972656,\n",
      " 'loss_1': 550.6563720703125,\n",
      " 'mean_episode_return_0': 70.88118743896484,\n",
      " 'mean_episode_return_1': 80.75611877441406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:26,654] After 2857600 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 391.6486511230469,\n",
      " 'loss_1': 550.6563720703125,\n",
      " 'mean_episode_return_0': 70.85543823242188,\n",
      " 'mean_episode_return_1': 80.75611877441406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:31,660] After 2857600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 391.6486511230469,\n",
      " 'loss_1': 550.6563720703125,\n",
      " 'mean_episode_return_0': 70.85543823242188,\n",
      " 'mean_episode_return_1': 80.75611877441406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:36,664] After 2860800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 391.6486511230469,\n",
      " 'loss_1': 434.5965576171875,\n",
      " 'mean_episode_return_0': 70.85543823242188,\n",
      " 'mean_episode_return_1': 80.77587890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:41,670] After 2864000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 442.5828552246094,\n",
      " 'loss_1': 434.5965576171875,\n",
      " 'mean_episode_return_0': 70.87774658203125,\n",
      " 'mean_episode_return_1': 80.77587890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:46,672] After 2864000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.5828552246094,\n",
      " 'loss_1': 434.5965576171875,\n",
      " 'mean_episode_return_0': 70.87774658203125,\n",
      " 'mean_episode_return_1': 80.77587890625}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:04:51,678] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:51,767] After 2870400 frames: @ 1256.1 fps Stats:\n",
      "{'loss_0': 358.7706298828125,\n",
      " 'loss_1': 544.9264526367188,\n",
      " 'mean_episode_return_0': 70.81169128417969,\n",
      " 'mean_episode_return_1': 80.8444595336914}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:04:56,773] After 2870400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.7706298828125,\n",
      " 'loss_1': 544.9264526367188,\n",
      " 'mean_episode_return_0': 70.81169128417969,\n",
      " 'mean_episode_return_1': 80.8444595336914}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:01,779] After 2870400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 358.7706298828125,\n",
      " 'loss_1': 544.9264526367188,\n",
      " 'mean_episode_return_0': 70.81169128417969,\n",
      " 'mean_episode_return_1': 80.8444595336914}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:06,785] After 2876800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 399.77569580078125,\n",
      " 'loss_1': 500.35406494140625,\n",
      " 'mean_episode_return_0': 70.81312561035156,\n",
      " 'mean_episode_return_1': 80.90139770507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:11,791] After 2876800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 399.77569580078125,\n",
      " 'loss_1': 500.35406494140625,\n",
      " 'mean_episode_return_0': 70.81312561035156,\n",
      " 'mean_episode_return_1': 80.90139770507812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:16,797] After 2880000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.77569580078125,\n",
      " 'loss_1': 533.634765625,\n",
      " 'mean_episode_return_0': 70.81312561035156,\n",
      " 'mean_episode_return_1': 80.9220962524414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:21,803] After 2883200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.72869873046875,\n",
      " 'loss_1': 533.634765625,\n",
      " 'mean_episode_return_0': 70.8333740234375,\n",
      " 'mean_episode_return_1': 80.9220962524414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:26,809] After 2883200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.72869873046875,\n",
      " 'loss_1': 533.634765625,\n",
      " 'mean_episode_return_0': 70.8333740234375,\n",
      " 'mean_episode_return_1': 80.9220962524414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:31,812] After 2883200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.72869873046875,\n",
      " 'loss_1': 533.634765625,\n",
      " 'mean_episode_return_0': 70.8333740234375,\n",
      " 'mean_episode_return_1': 80.9220962524414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:36,817] After 2889600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 526.408203125,\n",
      " 'loss_1': 484.8079833984375,\n",
      " 'mean_episode_return_0': 70.81712341308594,\n",
      " 'mean_episode_return_1': 80.9149398803711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:41,820] After 2889600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 526.408203125,\n",
      " 'loss_1': 484.8079833984375,\n",
      " 'mean_episode_return_0': 70.81712341308594,\n",
      " 'mean_episode_return_1': 80.9149398803711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:46,826] After 2892800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 526.408203125,\n",
      " 'loss_1': 456.67962646484375,\n",
      " 'mean_episode_return_0': 70.81712341308594,\n",
      " 'mean_episode_return_1': 80.9423599243164}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:05:51,828] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:51,895] After 2896000 frames: @ 631.3 fps Stats:\n",
      "{'loss_0': 492.98138427734375,\n",
      " 'loss_1': 456.67962646484375,\n",
      " 'mean_episode_return_0': 70.79499816894531,\n",
      " 'mean_episode_return_1': 80.9423599243164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:05:56,901] After 2896000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 492.98138427734375,\n",
      " 'loss_1': 456.67962646484375,\n",
      " 'mean_episode_return_0': 70.79499816894531,\n",
      " 'mean_episode_return_1': 80.9423599243164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:01,907] After 2899200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 492.98138427734375,\n",
      " 'loss_1': 402.64093017578125,\n",
      " 'mean_episode_return_0': 70.79499816894531,\n",
      " 'mean_episode_return_1': 81.00273132324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:06,913] After 2902400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 335.7091369628906,\n",
      " 'loss_1': 402.64093017578125,\n",
      " 'mean_episode_return_0': 70.74018096923828,\n",
      " 'mean_episode_return_1': 81.00273132324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:11,919] After 2902400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 335.7091369628906,\n",
      " 'loss_1': 402.64093017578125,\n",
      " 'mean_episode_return_0': 70.74018096923828,\n",
      " 'mean_episode_return_1': 81.00273132324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:16,925] After 2905600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 335.7091369628906,\n",
      " 'loss_1': 472.5389709472656,\n",
      " 'mean_episode_return_0': 70.74018096923828,\n",
      " 'mean_episode_return_1': 81.04351806640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:21,931] After 2908800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 486.62646484375,\n",
      " 'loss_1': 472.5389709472656,\n",
      " 'mean_episode_return_0': 70.73300170898438,\n",
      " 'mean_episode_return_1': 81.04351806640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:26,936] After 2908800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 486.62646484375,\n",
      " 'loss_1': 472.5389709472656,\n",
      " 'mean_episode_return_0': 70.73300170898438,\n",
      " 'mean_episode_return_1': 81.04351806640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:31,939] After 2912000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 486.62646484375,\n",
      " 'loss_1': 476.6123352050781,\n",
      " 'mean_episode_return_0': 70.73300170898438,\n",
      " 'mean_episode_return_1': 81.0341567993164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:36,945] After 2915200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 501.8350524902344,\n",
      " 'loss_1': 476.6123352050781,\n",
      " 'mean_episode_return_0': 70.74768829345703,\n",
      " 'mean_episode_return_1': 81.0341567993164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:41,948] After 2915200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 501.8350524902344,\n",
      " 'loss_1': 476.6123352050781,\n",
      " 'mean_episode_return_0': 70.74768829345703,\n",
      " 'mean_episode_return_1': 81.0341567993164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:46,952] After 2921600 frames: @ 1279.4 fps Stats:\n",
      "{'loss_0': 493.75433349609375,\n",
      " 'loss_1': 536.2675170898438,\n",
      " 'mean_episode_return_0': 70.76368713378906,\n",
      " 'mean_episode_return_1': 81.05848693847656}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:06:51,957] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:52,052] After 2921600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 493.75433349609375,\n",
      " 'loss_1': 536.2675170898438,\n",
      " 'mean_episode_return_0': 70.76368713378906,\n",
      " 'mean_episode_return_1': 81.05848693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:06:57,058] After 2921600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 493.75433349609375,\n",
      " 'loss_1': 536.2675170898438,\n",
      " 'mean_episode_return_0': 70.76368713378906,\n",
      " 'mean_episode_return_1': 81.05848693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:02,064] After 2928000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 433.60467529296875,\n",
      " 'loss_1': 429.2920227050781,\n",
      " 'mean_episode_return_0': 70.74800109863281,\n",
      " 'mean_episode_return_1': 81.0477294921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:07,068] After 2928000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.60467529296875,\n",
      " 'loss_1': 429.2920227050781,\n",
      " 'mean_episode_return_0': 70.74800109863281,\n",
      " 'mean_episode_return_1': 81.0477294921875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:07:12,072] After 2928000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.60467529296875,\n",
      " 'loss_1': 429.2920227050781,\n",
      " 'mean_episode_return_0': 70.74800109863281,\n",
      " 'mean_episode_return_1': 81.0477294921875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:17,074] After 2934400 frames: @ 1279.6 fps Stats:\n",
      "{'loss_0': 413.6296691894531,\n",
      " 'loss_1': 458.7017517089844,\n",
      " 'mean_episode_return_0': 70.7248764038086,\n",
      " 'mean_episode_return_1': 81.057861328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:22,080] After 2934400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.6296691894531,\n",
      " 'loss_1': 458.7017517089844,\n",
      " 'mean_episode_return_0': 70.7248764038086,\n",
      " 'mean_episode_return_1': 81.057861328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:27,086] After 2934400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.6296691894531,\n",
      " 'loss_1': 458.7017517089844,\n",
      " 'mean_episode_return_0': 70.7248764038086,\n",
      " 'mean_episode_return_1': 81.057861328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:32,092] After 2940800 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 401.68975830078125,\n",
      " 'loss_1': 448.6226501464844,\n",
      " 'mean_episode_return_0': 70.7795639038086,\n",
      " 'mean_episode_return_1': 81.03773498535156}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:37,098] After 2940800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.68975830078125,\n",
      " 'loss_1': 448.6226501464844,\n",
      " 'mean_episode_return_0': 70.7795639038086,\n",
      " 'mean_episode_return_1': 81.03773498535156}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:42,103] After 2940800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.68975830078125,\n",
      " 'loss_1': 448.6226501464844,\n",
      " 'mean_episode_return_0': 70.7795639038086,\n",
      " 'mean_episode_return_1': 81.03773498535156}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:47,108] After 2947200 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 398.6217956542969,\n",
      " 'loss_1': 450.4615478515625,\n",
      " 'mean_episode_return_0': 70.80962371826172,\n",
      " 'mean_episode_return_1': 81.02754974365234}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:07:52,113] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:52,208] After 2947200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 398.6217956542969,\n",
      " 'loss_1': 450.4615478515625,\n",
      " 'mean_episode_return_0': 70.80962371826172,\n",
      " 'mean_episode_return_1': 81.02754974365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:07:57,214] After 2947200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 398.6217956542969,\n",
      " 'loss_1': 450.4615478515625,\n",
      " 'mean_episode_return_0': 70.80962371826172,\n",
      " 'mean_episode_return_1': 81.02754974365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:02,220] After 2953600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 375.24261474609375,\n",
      " 'loss_1': 553.041259765625,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.05413055419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:07,224] After 2953600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.24261474609375,\n",
      " 'loss_1': 553.041259765625,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.05413055419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:12,228] After 2953600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.24261474609375,\n",
      " 'loss_1': 553.041259765625,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.05413055419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:17,233] After 2960000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 389.4588928222656,\n",
      " 'loss_1': 535.215087890625,\n",
      " 'mean_episode_return_0': 70.74837493896484,\n",
      " 'mean_episode_return_1': 81.02774047851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:22,239] After 2960000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.4588928222656,\n",
      " 'loss_1': 535.215087890625,\n",
      " 'mean_episode_return_0': 70.74837493896484,\n",
      " 'mean_episode_return_1': 81.02774047851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:27,241] After 2960000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.4588928222656,\n",
      " 'loss_1': 535.215087890625,\n",
      " 'mean_episode_return_0': 70.74837493896484,\n",
      " 'mean_episode_return_1': 81.02774047851562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:32,242] After 2966400 frames: @ 1279.8 fps Stats:\n",
      "{'loss_0': 472.9178466796875,\n",
      " 'loss_1': 457.72906494140625,\n",
      " 'mean_episode_return_0': 70.7543716430664,\n",
      " 'mean_episode_return_1': 81.06951141357422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:37,246] After 2966400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 472.9178466796875,\n",
      " 'loss_1': 457.72906494140625,\n",
      " 'mean_episode_return_0': 70.7543716430664,\n",
      " 'mean_episode_return_1': 81.06951141357422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:42,247] After 2966400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 472.9178466796875,\n",
      " 'loss_1': 457.72906494140625,\n",
      " 'mean_episode_return_0': 70.7543716430664,\n",
      " 'mean_episode_return_1': 81.06951141357422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:47,250] After 2972800 frames: @ 1279.6 fps Stats:\n",
      "{'loss_0': 359.8747253417969,\n",
      " 'loss_1': 540.6338500976562,\n",
      " 'mean_episode_return_0': 70.76519012451172,\n",
      " 'mean_episode_return_1': 81.07223510742188}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:08:52,256] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:52,348] After 2972800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 359.8747253417969,\n",
      " 'loss_1': 540.6338500976562,\n",
      " 'mean_episode_return_0': 70.76519012451172,\n",
      " 'mean_episode_return_1': 81.07223510742188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:08:57,354] After 2976000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 369.09771728515625,\n",
      " 'loss_1': 540.6338500976562,\n",
      " 'mean_episode_return_0': 70.75944519042969,\n",
      " 'mean_episode_return_1': 81.07223510742188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:02,360] After 2979200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 369.09771728515625,\n",
      " 'loss_1': 515.9363403320312,\n",
      " 'mean_episode_return_0': 70.75944519042969,\n",
      " 'mean_episode_return_1': 81.09927368164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:07,364] After 2979200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 369.09771728515625,\n",
      " 'loss_1': 515.9363403320312,\n",
      " 'mean_episode_return_0': 70.75944519042969,\n",
      " 'mean_episode_return_1': 81.09927368164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:12,369] After 2982400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 503.3932800292969,\n",
      " 'loss_1': 515.9363403320312,\n",
      " 'mean_episode_return_0': 70.77169036865234,\n",
      " 'mean_episode_return_1': 81.09927368164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:17,375] After 2985600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 503.3932800292969,\n",
      " 'loss_1': 431.2398986816406,\n",
      " 'mean_episode_return_0': 70.77169036865234,\n",
      " 'mean_episode_return_1': 81.14093780517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:22,381] After 2985600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 503.3932800292969,\n",
      " 'loss_1': 431.2398986816406,\n",
      " 'mean_episode_return_0': 70.77169036865234,\n",
      " 'mean_episode_return_1': 81.14093780517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:27,387] After 2992000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 460.6304626464844,\n",
      " 'loss_1': 411.8828125,\n",
      " 'mean_episode_return_0': 70.77156066894531,\n",
      " 'mean_episode_return_1': 81.16922760009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:32,392] After 2992000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 460.6304626464844,\n",
      " 'loss_1': 411.8828125,\n",
      " 'mean_episode_return_0': 70.77156066894531,\n",
      " 'mean_episode_return_1': 81.16922760009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:37,399] After 2992000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 460.6304626464844,\n",
      " 'loss_1': 411.8828125,\n",
      " 'mean_episode_return_0': 70.77156066894531,\n",
      " 'mean_episode_return_1': 81.16922760009766}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:42,402] After 2998400 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 436.0179443359375,\n",
      " 'loss_1': 451.2616271972656,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.20294189453125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:47,408] After 2998400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 436.0179443359375,\n",
      " 'loss_1': 451.2616271972656,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.20294189453125}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:09:52,414] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:52,507] After 2998400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 436.0179443359375,\n",
      " 'loss_1': 451.2616271972656,\n",
      " 'mean_episode_return_0': 70.7734375,\n",
      " 'mean_episode_return_1': 81.20294189453125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:09:57,513] After 3004800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 442.6243591308594,\n",
      " 'loss_1': 554.7781372070312,\n",
      " 'mean_episode_return_0': 70.80569458007812,\n",
      " 'mean_episode_return_1': 81.20840454101562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:10:02,519] After 3004800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.6243591308594,\n",
      " 'loss_1': 554.7781372070312,\n",
      " 'mean_episode_return_0': 70.80569458007812,\n",
      " 'mean_episode_return_1': 81.20840454101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:07,525] After 3004800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 442.6243591308594,\n",
      " 'loss_1': 554.7781372070312,\n",
      " 'mean_episode_return_0': 70.80569458007812,\n",
      " 'mean_episode_return_1': 81.20840454101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:12,529] After 3011200 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 402.7523498535156,\n",
      " 'loss_1': 501.68939208984375,\n",
      " 'mean_episode_return_0': 70.76274871826172,\n",
      " 'mean_episode_return_1': 81.17736053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:17,535] After 3011200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.7523498535156,\n",
      " 'loss_1': 501.68939208984375,\n",
      " 'mean_episode_return_0': 70.76274871826172,\n",
      " 'mean_episode_return_1': 81.17736053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:22,541] After 3014400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.83819580078125,\n",
      " 'loss_1': 501.68939208984375,\n",
      " 'mean_episode_return_0': 70.74112701416016,\n",
      " 'mean_episode_return_1': 81.17736053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:27,547] After 3017600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.83819580078125,\n",
      " 'loss_1': 454.6506652832031,\n",
      " 'mean_episode_return_0': 70.74112701416016,\n",
      " 'mean_episode_return_1': 81.12358856201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:32,553] After 3017600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.83819580078125,\n",
      " 'loss_1': 454.6506652832031,\n",
      " 'mean_episode_return_0': 70.74112701416016,\n",
      " 'mean_episode_return_1': 81.12358856201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:37,556] After 3020800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 479.1271667480469,\n",
      " 'loss_1': 454.6506652832031,\n",
      " 'mean_episode_return_0': 70.76387786865234,\n",
      " 'mean_episode_return_1': 81.12358856201172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:42,562] After 3024000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 479.1271667480469,\n",
      " 'loss_1': 443.3818359375,\n",
      " 'mean_episode_return_0': 70.76387786865234,\n",
      " 'mean_episode_return_1': 81.11956787109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:47,568] After 3024000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 479.1271667480469,\n",
      " 'loss_1': 443.3818359375,\n",
      " 'mean_episode_return_0': 70.76387786865234,\n",
      " 'mean_episode_return_1': 81.11956787109375}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:10:52,572] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:52,663] After 3027200 frames: @ 628.2 fps Stats:\n",
      "{'loss_0': 397.8896789550781,\n",
      " 'loss_1': 443.3818359375,\n",
      " 'mean_episode_return_0': 70.7410659790039,\n",
      " 'mean_episode_return_1': 81.11956787109375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:10:57,669] After 3030400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.8896789550781,\n",
      " 'loss_1': 547.5156860351562,\n",
      " 'mean_episode_return_0': 70.7410659790039,\n",
      " 'mean_episode_return_1': 81.1612548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:02,673] After 3030400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.8896789550781,\n",
      " 'loss_1': 547.5156860351562,\n",
      " 'mean_episode_return_0': 70.7410659790039,\n",
      " 'mean_episode_return_1': 81.1612548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:07,679] After 3033600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 447.2093505859375,\n",
      " 'loss_1': 547.5156860351562,\n",
      " 'mean_episode_return_0': 70.76443481445312,\n",
      " 'mean_episode_return_1': 81.1612548828125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:12,684] After 3036800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 447.2093505859375,\n",
      " 'loss_1': 496.818115234375,\n",
      " 'mean_episode_return_0': 70.76443481445312,\n",
      " 'mean_episode_return_1': 81.1115493774414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:17,690] After 3036800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 447.2093505859375,\n",
      " 'loss_1': 496.818115234375,\n",
      " 'mean_episode_return_0': 70.76443481445312,\n",
      " 'mean_episode_return_1': 81.1115493774414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:22,696] After 3040000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 405.6066589355469,\n",
      " 'loss_1': 496.818115234375,\n",
      " 'mean_episode_return_0': 70.74093627929688,\n",
      " 'mean_episode_return_1': 81.1115493774414}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:27,701] After 3043200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 405.6066589355469,\n",
      " 'loss_1': 554.1393432617188,\n",
      " 'mean_episode_return_0': 70.74093627929688,\n",
      " 'mean_episode_return_1': 81.17141723632812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:32,706] After 3043200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.6066589355469,\n",
      " 'loss_1': 554.1393432617188,\n",
      " 'mean_episode_return_0': 70.74093627929688,\n",
      " 'mean_episode_return_1': 81.17141723632812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:37,712] After 3046400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.1179504394531,\n",
      " 'loss_1': 554.1393432617188,\n",
      " 'mean_episode_return_0': 70.70537567138672,\n",
      " 'mean_episode_return_1': 81.17141723632812}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:42,718] After 3049600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.1179504394531,\n",
      " 'loss_1': 459.3777160644531,\n",
      " 'mean_episode_return_0': 70.70537567138672,\n",
      " 'mean_episode_return_1': 81.17924499511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:47,724] After 3049600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.1179504394531,\n",
      " 'loss_1': 459.3777160644531,\n",
      " 'mean_episode_return_0': 70.70537567138672,\n",
      " 'mean_episode_return_1': 81.17924499511719}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:11:52,729] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:52,821] After 3052800 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 473.4039306640625,\n",
      " 'loss_1': 459.3777160644531,\n",
      " 'mean_episode_return_0': 70.6937484741211,\n",
      " 'mean_episode_return_1': 81.17924499511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:11:57,827] After 3056000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 473.4039306640625,\n",
      " 'loss_1': 502.39013671875,\n",
      " 'mean_episode_return_0': 70.6937484741211,\n",
      " 'mean_episode_return_1': 81.163818359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:02,833] After 3059200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.6976013183594,\n",
      " 'loss_1': 502.39013671875,\n",
      " 'mean_episode_return_0': 70.67400360107422,\n",
      " 'mean_episode_return_1': 81.163818359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:07,839] After 3059200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.6976013183594,\n",
      " 'loss_1': 502.39013671875,\n",
      " 'mean_episode_return_0': 70.67400360107422,\n",
      " 'mean_episode_return_1': 81.163818359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:12,845] After 3062400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.6976013183594,\n",
      " 'loss_1': 605.12451171875,\n",
      " 'mean_episode_return_0': 70.67400360107422,\n",
      " 'mean_episode_return_1': 81.1879653930664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:17,851] After 3065600 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 483.88177490234375,\n",
      " 'loss_1': 605.12451171875,\n",
      " 'mean_episode_return_0': 70.67581176757812,\n",
      " 'mean_episode_return_1': 81.1879653930664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:22,857] After 3065600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 483.88177490234375,\n",
      " 'loss_1': 605.12451171875,\n",
      " 'mean_episode_return_0': 70.67581176757812,\n",
      " 'mean_episode_return_1': 81.1879653930664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:27,860] After 3068800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 483.88177490234375,\n",
      " 'loss_1': 413.8314208984375,\n",
      " 'mean_episode_return_0': 70.67581176757812,\n",
      " 'mean_episode_return_1': 81.16259765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:32,865] After 3072000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 402.161865234375,\n",
      " 'loss_1': 413.8314208984375,\n",
      " 'mean_episode_return_0': 70.66175079345703,\n",
      " 'mean_episode_return_1': 81.16259765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:37,871] After 3072000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.161865234375,\n",
      " 'loss_1': 413.8314208984375,\n",
      " 'mean_episode_return_0': 70.66175079345703,\n",
      " 'mean_episode_return_1': 81.16259765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:42,877] After 3075200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 402.161865234375,\n",
      " 'loss_1': 514.09228515625,\n",
      " 'mean_episode_return_0': 70.66175079345703,\n",
      " 'mean_episode_return_1': 81.17393493652344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:47,883] After 3078400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 361.11663818359375,\n",
      " 'loss_1': 514.09228515625,\n",
      " 'mean_episode_return_0': 70.63150024414062,\n",
      " 'mean_episode_return_1': 81.17393493652344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 14:12:52,889] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:52,977] After 3081600 frames: @ 628.3 fps Stats:\n",
      "{'loss_0': 361.11663818359375,\n",
      " 'loss_1': 452.81402587890625,\n",
      " 'mean_episode_return_0': 70.63150024414062,\n",
      " 'mean_episode_return_1': 81.20264434814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:12:57,983] After 3081600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 361.11663818359375,\n",
      " 'loss_1': 452.81402587890625,\n",
      " 'mean_episode_return_0': 70.63150024414062,\n",
      " 'mean_episode_return_1': 81.20264434814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:02,988] After 3084800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 429.1346740722656,\n",
      " 'loss_1': 452.81402587890625,\n",
      " 'mean_episode_return_0': 70.65206146240234,\n",
      " 'mean_episode_return_1': 81.20264434814453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:07,994] After 3088000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 429.1346740722656,\n",
      " 'loss_1': 446.1767883300781,\n",
      " 'mean_episode_return_0': 70.65206146240234,\n",
      " 'mean_episode_return_1': 81.18681335449219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:13,000] After 3088000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 429.1346740722656,\n",
      " 'loss_1': 446.1767883300781,\n",
      " 'mean_episode_return_0': 70.65206146240234,\n",
      " 'mean_episode_return_1': 81.18681335449219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:18,006] After 3091200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.24365234375,\n",
      " 'loss_1': 446.1767883300781,\n",
      " 'mean_episode_return_0': 70.67631530761719,\n",
      " 'mean_episode_return_1': 81.18681335449219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:23,012] After 3094400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.24365234375,\n",
      " 'loss_1': 500.7497863769531,\n",
      " 'mean_episode_return_0': 70.67631530761719,\n",
      " 'mean_episode_return_1': 81.17665100097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:28,018] After 3094400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 454.24365234375,\n",
      " 'loss_1': 500.7497863769531,\n",
      " 'mean_episode_return_0': 70.67631530761719,\n",
      " 'mean_episode_return_1': 81.17665100097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:33,024] After 3097600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 349.09051513671875,\n",
      " 'loss_1': 500.7497863769531,\n",
      " 'mean_episode_return_0': 70.7109375,\n",
      " 'mean_episode_return_1': 81.17665100097656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:38,028] After 3100800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 349.09051513671875,\n",
      " 'loss_1': 384.38006591796875,\n",
      " 'mean_episode_return_0': 70.7109375,\n",
      " 'mean_episode_return_1': 81.1943359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:43,034] After 3104000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 351.8628845214844,\n",
      " 'loss_1': 384.38006591796875,\n",
      " 'mean_episode_return_0': 70.6389389038086,\n",
      " 'mean_episode_return_1': 81.1943359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:48,040] After 3104000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 351.8628845214844,\n",
      " 'loss_1': 384.38006591796875,\n",
      " 'mean_episode_return_0': 70.6389389038086,\n",
      " 'mean_episode_return_1': 81.1943359375}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:13:53,046] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:53,139] After 3107200 frames: @ 627.6 fps Stats:\n",
      "{'loss_0': 351.8628845214844,\n",
      " 'loss_1': 479.3191833496094,\n",
      " 'mean_episode_return_0': 70.6389389038086,\n",
      " 'mean_episode_return_1': 81.17662048339844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:13:58,145] After 3110400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.0643310546875,\n",
      " 'loss_1': 479.3191833496094,\n",
      " 'mean_episode_return_0': 70.6234359741211,\n",
      " 'mean_episode_return_1': 81.17662048339844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:03,151] After 3110400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.0643310546875,\n",
      " 'loss_1': 479.3191833496094,\n",
      " 'mean_episode_return_0': 70.6234359741211,\n",
      " 'mean_episode_return_1': 81.17662048339844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:08,157] After 3113600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.0643310546875,\n",
      " 'loss_1': 469.1744689941406,\n",
      " 'mean_episode_return_0': 70.6234359741211,\n",
      " 'mean_episode_return_1': 81.23681640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:13,163] After 3116800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.41937255859375,\n",
      " 'loss_1': 469.1744689941406,\n",
      " 'mean_episode_return_0': 70.61206817626953,\n",
      " 'mean_episode_return_1': 81.23681640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:18,169] After 3116800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.41937255859375,\n",
      " 'loss_1': 469.1744689941406,\n",
      " 'mean_episode_return_0': 70.61206817626953,\n",
      " 'mean_episode_return_1': 81.23681640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:23,175] After 3120000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 417.41937255859375,\n",
      " 'loss_1': 431.7069091796875,\n",
      " 'mean_episode_return_0': 70.61206817626953,\n",
      " 'mean_episode_return_1': 81.29991912841797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:28,181] After 3123200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 495.95263671875,\n",
      " 'loss_1': 431.7069091796875,\n",
      " 'mean_episode_return_0': 70.63137817382812,\n",
      " 'mean_episode_return_1': 81.29991912841797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:33,185] After 3123200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 495.95263671875,\n",
      " 'loss_1': 431.7069091796875,\n",
      " 'mean_episode_return_0': 70.63137817382812,\n",
      " 'mean_episode_return_1': 81.29991912841797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:38,188] After 3126400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 495.95263671875,\n",
      " 'loss_1': 494.0085144042969,\n",
      " 'mean_episode_return_0': 70.63137817382812,\n",
      " 'mean_episode_return_1': 81.27320861816406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:43,194] After 3129600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 433.1861572265625,\n",
      " 'loss_1': 494.0085144042969,\n",
      " 'mean_episode_return_0': 70.65987396240234,\n",
      " 'mean_episode_return_1': 81.27320861816406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:48,200] After 3129600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.1861572265625,\n",
      " 'loss_1': 494.0085144042969,\n",
      " 'mean_episode_return_0': 70.65987396240234,\n",
      " 'mean_episode_return_1': 81.27320861816406}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:14:53,205] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:53,296] After 3132800 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 433.1861572265625,\n",
      " 'loss_1': 435.4613952636719,\n",
      " 'mean_episode_return_0': 70.65987396240234,\n",
      " 'mean_episode_return_1': 81.28987121582031}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:14:58,302] After 3136000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 474.6715393066406,\n",
      " 'loss_1': 435.4613952636719,\n",
      " 'mean_episode_return_0': 70.71837615966797,\n",
      " 'mean_episode_return_1': 81.28987121582031}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:03,309] After 3136000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 474.6715393066406,\n",
      " 'loss_1': 435.4613952636719,\n",
      " 'mean_episode_return_0': 70.71837615966797,\n",
      " 'mean_episode_return_1': 81.28987121582031}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:08,315] After 3139200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 474.6715393066406,\n",
      " 'loss_1': 432.72802734375,\n",
      " 'mean_episode_return_0': 70.71837615966797,\n",
      " 'mean_episode_return_1': 81.27839660644531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:13,320] After 3142400 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 434.0196838378906,\n",
      " 'loss_1': 432.72802734375,\n",
      " 'mean_episode_return_0': 70.76881408691406,\n",
      " 'mean_episode_return_1': 81.27839660644531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:18,326] After 3142400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 434.0196838378906,\n",
      " 'loss_1': 432.72802734375,\n",
      " 'mean_episode_return_0': 70.76881408691406,\n",
      " 'mean_episode_return_1': 81.27839660644531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:23,332] After 3145600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 434.0196838378906,\n",
      " 'loss_1': 446.91156005859375,\n",
      " 'mean_episode_return_0': 70.76881408691406,\n",
      " 'mean_episode_return_1': 81.3017578125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:28,338] After 3148800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 452.04998779296875,\n",
      " 'loss_1': 446.91156005859375,\n",
      " 'mean_episode_return_0': 70.70024871826172,\n",
      " 'mean_episode_return_1': 81.3017578125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:33,344] After 3148800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 452.04998779296875,\n",
      " 'loss_1': 446.91156005859375,\n",
      " 'mean_episode_return_0': 70.70024871826172,\n",
      " 'mean_episode_return_1': 81.3017578125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:38,349] After 3155200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 526.3381958007812,\n",
      " 'loss_1': 536.0057983398438,\n",
      " 'mean_episode_return_0': 70.75312042236328,\n",
      " 'mean_episode_return_1': 81.31746673583984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:15:43,355] After 3155200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 526.3381958007812,\n",
      " 'loss_1': 536.0057983398438,\n",
      " 'mean_episode_return_0': 70.75312042236328,\n",
      " 'mean_episode_return_1': 81.31746673583984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:48,360] After 3155200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 526.3381958007812,\n",
      " 'loss_1': 536.0057983398438,\n",
      " 'mean_episode_return_0': 70.75312042236328,\n",
      " 'mean_episode_return_1': 81.31746673583984}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:15:53,364] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:53,457] After 3161600 frames: @ 1255.8 fps Stats:\n",
      "{'loss_0': 410.02154541015625,\n",
      " 'loss_1': 502.99407958984375,\n",
      " 'mean_episode_return_0': 70.77300262451172,\n",
      " 'mean_episode_return_1': 81.3948745727539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:15:58,463] After 3161600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 410.02154541015625,\n",
      " 'loss_1': 502.99407958984375,\n",
      " 'mean_episode_return_0': 70.77300262451172,\n",
      " 'mean_episode_return_1': 81.3948745727539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:03,469] After 3164800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.02154541015625,\n",
      " 'loss_1': 467.9106140136719,\n",
      " 'mean_episode_return_0': 70.77300262451172,\n",
      " 'mean_episode_return_1': 81.41351318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:08,474] After 3168000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.1068420410156,\n",
      " 'loss_1': 467.9106140136719,\n",
      " 'mean_episode_return_0': 70.79556274414062,\n",
      " 'mean_episode_return_1': 81.41351318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:13,480] After 3168000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.1068420410156,\n",
      " 'loss_1': 467.9106140136719,\n",
      " 'mean_episode_return_0': 70.79556274414062,\n",
      " 'mean_episode_return_1': 81.41351318359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:18,486] After 3171200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.1068420410156,\n",
      " 'loss_1': 468.5820617675781,\n",
      " 'mean_episode_return_0': 70.79556274414062,\n",
      " 'mean_episode_return_1': 81.41123962402344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:23,492] After 3174400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.4332580566406,\n",
      " 'loss_1': 468.5820617675781,\n",
      " 'mean_episode_return_0': 70.8021240234375,\n",
      " 'mean_episode_return_1': 81.41123962402344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:28,498] After 3174400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.4332580566406,\n",
      " 'loss_1': 468.5820617675781,\n",
      " 'mean_episode_return_0': 70.8021240234375,\n",
      " 'mean_episode_return_1': 81.41123962402344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:33,504] After 3180800 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 441.63238525390625,\n",
      " 'loss_1': 474.9220886230469,\n",
      " 'mean_episode_return_0': 70.75556182861328,\n",
      " 'mean_episode_return_1': 81.47710418701172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:38,508] After 3180800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 441.63238525390625,\n",
      " 'loss_1': 474.9220886230469,\n",
      " 'mean_episode_return_0': 70.75556182861328,\n",
      " 'mean_episode_return_1': 81.47710418701172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:43,512] After 3180800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 441.63238525390625,\n",
      " 'loss_1': 474.9220886230469,\n",
      " 'mean_episode_return_0': 70.75556182861328,\n",
      " 'mean_episode_return_1': 81.47710418701172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:48,516] After 3187200 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 455.38427734375,\n",
      " 'loss_1': 453.4809265136719,\n",
      " 'mean_episode_return_0': 70.77825164794922,\n",
      " 'mean_episode_return_1': 81.45162200927734}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:16:53,521] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:53,613] After 3187200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.38427734375,\n",
      " 'loss_1': 453.4809265136719,\n",
      " 'mean_episode_return_0': 70.77825164794922,\n",
      " 'mean_episode_return_1': 81.45162200927734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:16:58,619] After 3190400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 455.38427734375,\n",
      " 'loss_1': 545.0259399414062,\n",
      " 'mean_episode_return_0': 70.77825164794922,\n",
      " 'mean_episode_return_1': 81.40192413330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:03,625] After 3193600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 484.54327392578125,\n",
      " 'loss_1': 545.0259399414062,\n",
      " 'mean_episode_return_0': 70.7718734741211,\n",
      " 'mean_episode_return_1': 81.40192413330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:08,630] After 3193600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 484.54327392578125,\n",
      " 'loss_1': 545.0259399414062,\n",
      " 'mean_episode_return_0': 70.7718734741211,\n",
      " 'mean_episode_return_1': 81.40192413330078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:13,636] After 3200000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 448.54803466796875,\n",
      " 'loss_1': 390.05914306640625,\n",
      " 'mean_episode_return_0': 70.85662078857422,\n",
      " 'mean_episode_return_1': 81.43428039550781}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:18,642] After 3200000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 448.54803466796875,\n",
      " 'loss_1': 390.05914306640625,\n",
      " 'mean_episode_return_0': 70.85662078857422,\n",
      " 'mean_episode_return_1': 81.43428039550781}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:23,648] After 3200000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 448.54803466796875,\n",
      " 'loss_1': 390.05914306640625,\n",
      " 'mean_episode_return_0': 70.85662078857422,\n",
      " 'mean_episode_return_1': 81.43428039550781}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:28,654] After 3206400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 433.7408447265625,\n",
      " 'loss_1': 430.33538818359375,\n",
      " 'mean_episode_return_0': 70.8524398803711,\n",
      " 'mean_episode_return_1': 81.52884674072266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:33,660] After 3206400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.7408447265625,\n",
      " 'loss_1': 430.33538818359375,\n",
      " 'mean_episode_return_0': 70.8524398803711,\n",
      " 'mean_episode_return_1': 81.52884674072266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:38,666] After 3206400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 433.7408447265625,\n",
      " 'loss_1': 430.33538818359375,\n",
      " 'mean_episode_return_0': 70.8524398803711,\n",
      " 'mean_episode_return_1': 81.52884674072266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:43,672] After 3212800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 492.3004455566406,\n",
      " 'loss_1': 488.3695068359375,\n",
      " 'mean_episode_return_0': 70.8793716430664,\n",
      " 'mean_episode_return_1': 81.53408813476562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:48,678] After 3212800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 492.3004455566406,\n",
      " 'loss_1': 488.3695068359375,\n",
      " 'mean_episode_return_0': 70.8793716430664,\n",
      " 'mean_episode_return_1': 81.53408813476562}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:17:53,683] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:53,779] After 3216000 frames: @ 627.4 fps Stats:\n",
      "{'loss_0': 492.3004455566406,\n",
      " 'loss_1': 468.9195861816406,\n",
      " 'mean_episode_return_0': 70.8793716430664,\n",
      " 'mean_episode_return_1': 81.52690124511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:17:58,785] After 3219200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 341.55120849609375,\n",
      " 'loss_1': 468.9195861816406,\n",
      " 'mean_episode_return_0': 70.90462493896484,\n",
      " 'mean_episode_return_1': 81.52690124511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:03,789] After 3219200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.55120849609375,\n",
      " 'loss_1': 468.9195861816406,\n",
      " 'mean_episode_return_0': 70.90462493896484,\n",
      " 'mean_episode_return_1': 81.52690124511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:08,793] After 3219200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 341.55120849609375,\n",
      " 'loss_1': 468.9195861816406,\n",
      " 'mean_episode_return_0': 70.90462493896484,\n",
      " 'mean_episode_return_1': 81.52690124511719}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:13,796] After 3225600 frames: @ 1279.6 fps Stats:\n",
      "{'loss_0': 452.5732727050781,\n",
      " 'loss_1': 456.94622802734375,\n",
      " 'mean_episode_return_0': 70.86268615722656,\n",
      " 'mean_episode_return_1': 81.55616760253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:18,800] After 3225600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 452.5732727050781,\n",
      " 'loss_1': 456.94622802734375,\n",
      " 'mean_episode_return_0': 70.86268615722656,\n",
      " 'mean_episode_return_1': 81.55616760253906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:23,806] After 3225600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 452.5732727050781,\n",
      " 'loss_1': 456.94622802734375,\n",
      " 'mean_episode_return_0': 70.86268615722656,\n",
      " 'mean_episode_return_1': 81.55616760253906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:18:28,812] After 3232000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 350.068115234375,\n",
      " 'loss_1': 474.6150817871094,\n",
      " 'mean_episode_return_0': 70.82499694824219,\n",
      " 'mean_episode_return_1': 81.51325225830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:33,816] After 3232000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.068115234375,\n",
      " 'loss_1': 474.6150817871094,\n",
      " 'mean_episode_return_0': 70.82499694824219,\n",
      " 'mean_episode_return_1': 81.51325225830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:38,820] After 3232000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 350.068115234375,\n",
      " 'loss_1': 474.6150817871094,\n",
      " 'mean_episode_return_0': 70.82499694824219,\n",
      " 'mean_episode_return_1': 81.51325225830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:43,824] After 3238400 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 438.4068603515625,\n",
      " 'loss_1': 482.7747497558594,\n",
      " 'mean_episode_return_0': 70.7697525024414,\n",
      " 'mean_episode_return_1': 81.57408905029297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:48,830] After 3238400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 438.4068603515625,\n",
      " 'loss_1': 482.7747497558594,\n",
      " 'mean_episode_return_0': 70.7697525024414,\n",
      " 'mean_episode_return_1': 81.57408905029297}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:18:53,835] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:53,927] After 3238400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 438.4068603515625,\n",
      " 'loss_1': 482.7747497558594,\n",
      " 'mean_episode_return_0': 70.7697525024414,\n",
      " 'mean_episode_return_1': 81.57408905029297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:18:58,933] After 3244800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 484.1861267089844,\n",
      " 'loss_1': 432.6719055175781,\n",
      " 'mean_episode_return_0': 70.76012420654297,\n",
      " 'mean_episode_return_1': 81.5994873046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:03,939] After 3244800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 484.1861267089844,\n",
      " 'loss_1': 432.6719055175781,\n",
      " 'mean_episode_return_0': 70.76012420654297,\n",
      " 'mean_episode_return_1': 81.5994873046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:08,943] After 3244800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 484.1861267089844,\n",
      " 'loss_1': 432.6719055175781,\n",
      " 'mean_episode_return_0': 70.76012420654297,\n",
      " 'mean_episode_return_1': 81.5994873046875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:13,949] After 3251200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 480.54705810546875,\n",
      " 'loss_1': 510.2707824707031,\n",
      " 'mean_episode_return_0': 70.78475189208984,\n",
      " 'mean_episode_return_1': 81.6368637084961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:18,955] After 3251200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 480.54705810546875,\n",
      " 'loss_1': 510.2707824707031,\n",
      " 'mean_episode_return_0': 70.78475189208984,\n",
      " 'mean_episode_return_1': 81.6368637084961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:23,961] After 3254400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.9005126953125,\n",
      " 'loss_1': 510.2707824707031,\n",
      " 'mean_episode_return_0': 70.73512268066406,\n",
      " 'mean_episode_return_1': 81.6368637084961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:28,967] After 3257600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 340.9005126953125,\n",
      " 'loss_1': 532.6207275390625,\n",
      " 'mean_episode_return_0': 70.73512268066406,\n",
      " 'mean_episode_return_1': 81.6700210571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:33,972] After 3257600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 340.9005126953125,\n",
      " 'loss_1': 532.6207275390625,\n",
      " 'mean_episode_return_0': 70.73512268066406,\n",
      " 'mean_episode_return_1': 81.6700210571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:38,978] After 3260800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 436.4399108886719,\n",
      " 'loss_1': 532.6207275390625,\n",
      " 'mean_episode_return_0': 70.72162628173828,\n",
      " 'mean_episode_return_1': 81.6700210571289}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:43,984] After 3264000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 436.4399108886719,\n",
      " 'loss_1': 479.54925537109375,\n",
      " 'mean_episode_return_0': 70.72162628173828,\n",
      " 'mean_episode_return_1': 81.67781066894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:48,990] After 3264000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 436.4399108886719,\n",
      " 'loss_1': 479.54925537109375,\n",
      " 'mean_episode_return_0': 70.72162628173828,\n",
      " 'mean_episode_return_1': 81.67781066894531}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:19:53,994] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:54,088] After 3270400 frames: @ 1255.6 fps Stats:\n",
      "{'loss_0': 432.1916198730469,\n",
      " 'loss_1': 500.095703125,\n",
      " 'mean_episode_return_0': 70.71831512451172,\n",
      " 'mean_episode_return_1': 81.7203140258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:19:59,094] After 3270400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.1916198730469,\n",
      " 'loss_1': 500.095703125,\n",
      " 'mean_episode_return_0': 70.71831512451172,\n",
      " 'mean_episode_return_1': 81.7203140258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:04,100] After 3270400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.1916198730469,\n",
      " 'loss_1': 500.095703125,\n",
      " 'mean_episode_return_0': 70.71831512451172,\n",
      " 'mean_episode_return_1': 81.7203140258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:09,106] After 3276800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 396.3851013183594,\n",
      " 'loss_1': 458.8407287597656,\n",
      " 'mean_episode_return_0': 70.73987579345703,\n",
      " 'mean_episode_return_1': 81.74513244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:14,112] After 3276800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 396.3851013183594,\n",
      " 'loss_1': 458.8407287597656,\n",
      " 'mean_episode_return_0': 70.73987579345703,\n",
      " 'mean_episode_return_1': 81.74513244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:19,116] After 3280000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 395.0025939941406,\n",
      " 'loss_1': 458.8407287597656,\n",
      " 'mean_episode_return_0': 70.7541275024414,\n",
      " 'mean_episode_return_1': 81.74513244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:24,120] After 3283200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 395.0025939941406,\n",
      " 'loss_1': 503.6260070800781,\n",
      " 'mean_episode_return_0': 70.7541275024414,\n",
      " 'mean_episode_return_1': 81.78433227539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:29,124] After 3283200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.0025939941406,\n",
      " 'loss_1': 503.6260070800781,\n",
      " 'mean_episode_return_0': 70.7541275024414,\n",
      " 'mean_episode_return_1': 81.78433227539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:34,129] After 3283200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.0025939941406,\n",
      " 'loss_1': 503.6260070800781,\n",
      " 'mean_episode_return_0': 70.7541275024414,\n",
      " 'mean_episode_return_1': 81.78433227539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:39,135] After 3286400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 376.2377624511719,\n",
      " 'loss_1': 503.6260070800781,\n",
      " 'mean_episode_return_0': 70.7829360961914,\n",
      " 'mean_episode_return_1': 81.78433227539062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:44,141] After 3289600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 376.2377624511719,\n",
      " 'loss_1': 523.5560913085938,\n",
      " 'mean_episode_return_0': 70.7829360961914,\n",
      " 'mean_episode_return_1': 81.7618408203125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:49,145] After 3289600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 376.2377624511719,\n",
      " 'loss_1': 523.5560913085938,\n",
      " 'mean_episode_return_0': 70.7829360961914,\n",
      " 'mean_episode_return_1': 81.7618408203125}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:20:54,148] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:54,232] After 3292800 frames: @ 629.1 fps Stats:\n",
      "{'loss_0': 402.41412353515625,\n",
      " 'loss_1': 523.5560913085938,\n",
      " 'mean_episode_return_0': 70.74512481689453,\n",
      " 'mean_episode_return_1': 81.7618408203125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:20:59,238] After 3296000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 402.41412353515625,\n",
      " 'loss_1': 497.2254943847656,\n",
      " 'mean_episode_return_0': 70.74512481689453,\n",
      " 'mean_episode_return_1': 81.74076080322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:04,244] After 3296000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.41412353515625,\n",
      " 'loss_1': 497.2254943847656,\n",
      " 'mean_episode_return_0': 70.74512481689453,\n",
      " 'mean_episode_return_1': 81.74076080322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:09,250] After 3302400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 526.94921875,\n",
      " 'loss_1': 546.09326171875,\n",
      " 'mean_episode_return_0': 70.76762390136719,\n",
      " 'mean_episode_return_1': 81.74143981933594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:14,256] After 3302400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 526.94921875,\n",
      " 'loss_1': 546.09326171875,\n",
      " 'mean_episode_return_0': 70.76762390136719,\n",
      " 'mean_episode_return_1': 81.74143981933594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:21:19,257] After 3305600 frames: @ 640.0 fps Stats:\n",
      "{'loss_0': 400.4410705566406,\n",
      " 'loss_1': 546.09326171875,\n",
      " 'mean_episode_return_0': 70.82080841064453,\n",
      " 'mean_episode_return_1': 81.74143981933594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:24,263] After 3308800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 400.4410705566406,\n",
      " 'loss_1': 500.896484375,\n",
      " 'mean_episode_return_0': 70.82080841064453,\n",
      " 'mean_episode_return_1': 81.72509765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:29,269] After 3308800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.4410705566406,\n",
      " 'loss_1': 500.896484375,\n",
      " 'mean_episode_return_0': 70.82080841064453,\n",
      " 'mean_episode_return_1': 81.72509765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:34,275] After 3312000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.4137268066406,\n",
      " 'loss_1': 500.896484375,\n",
      " 'mean_episode_return_0': 70.80787658691406,\n",
      " 'mean_episode_return_1': 81.72509765625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:39,281] After 3315200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.4137268066406,\n",
      " 'loss_1': 616.1526489257812,\n",
      " 'mean_episode_return_0': 70.80787658691406,\n",
      " 'mean_episode_return_1': 81.74237823486328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:44,287] After 3315200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.4137268066406,\n",
      " 'loss_1': 616.1526489257812,\n",
      " 'mean_episode_return_0': 70.80787658691406,\n",
      " 'mean_episode_return_1': 81.74237823486328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:49,293] After 3318400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 498.7002868652344,\n",
      " 'loss_1': 616.1526489257812,\n",
      " 'mean_episode_return_0': 70.8140640258789,\n",
      " 'mean_episode_return_1': 81.74237823486328}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:21:54,298] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:54,390] After 3321600 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 498.7002868652344,\n",
      " 'loss_1': 531.9676513671875,\n",
      " 'mean_episode_return_0': 70.8140640258789,\n",
      " 'mean_episode_return_1': 81.78105163574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:21:59,396] After 3321600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 498.7002868652344,\n",
      " 'loss_1': 531.9676513671875,\n",
      " 'mean_episode_return_0': 70.8140640258789,\n",
      " 'mean_episode_return_1': 81.78105163574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:04,402] After 3324800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 410.8205871582031,\n",
      " 'loss_1': 531.9676513671875,\n",
      " 'mean_episode_return_0': 70.83699798583984,\n",
      " 'mean_episode_return_1': 81.78105163574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:09,408] After 3328000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.8205871582031,\n",
      " 'loss_1': 456.38909912109375,\n",
      " 'mean_episode_return_0': 70.83699798583984,\n",
      " 'mean_episode_return_1': 81.74002075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:14,414] After 3331200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 440.07177734375,\n",
      " 'loss_1': 456.38909912109375,\n",
      " 'mean_episode_return_0': 70.85281372070312,\n",
      " 'mean_episode_return_1': 81.74002075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:19,420] After 3334400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 440.07177734375,\n",
      " 'loss_1': 504.7577209472656,\n",
      " 'mean_episode_return_0': 70.85281372070312,\n",
      " 'mean_episode_return_1': 81.75349426269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:24,426] After 3334400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 440.07177734375,\n",
      " 'loss_1': 504.7577209472656,\n",
      " 'mean_episode_return_0': 70.85281372070312,\n",
      " 'mean_episode_return_1': 81.75349426269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:29,428] After 3337600 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 362.06451416015625,\n",
      " 'loss_1': 504.7577209472656,\n",
      " 'mean_episode_return_0': 70.8138198852539,\n",
      " 'mean_episode_return_1': 81.75349426269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:34,432] After 3337600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 362.06451416015625,\n",
      " 'loss_1': 504.7577209472656,\n",
      " 'mean_episode_return_0': 70.8138198852539,\n",
      " 'mean_episode_return_1': 81.75349426269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:39,436] After 3340800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 362.06451416015625,\n",
      " 'loss_1': 439.1575622558594,\n",
      " 'mean_episode_return_0': 70.8138198852539,\n",
      " 'mean_episode_return_1': 81.75524139404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:44,440] After 3340800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 362.06451416015625,\n",
      " 'loss_1': 439.1575622558594,\n",
      " 'mean_episode_return_0': 70.8138198852539,\n",
      " 'mean_episode_return_1': 81.75524139404297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:49,446] After 3344000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 403.9402770996094,\n",
      " 'loss_1': 439.1575622558594,\n",
      " 'mean_episode_return_0': 70.84993743896484,\n",
      " 'mean_episode_return_1': 81.75524139404297}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:22:54,452] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:54,530] After 3347200 frames: @ 629.6 fps Stats:\n",
      "{'loss_0': 403.9402770996094,\n",
      " 'loss_1': 466.5369567871094,\n",
      " 'mean_episode_return_0': 70.84993743896484,\n",
      " 'mean_episode_return_1': 81.73834991455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:22:59,536] After 3347200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.9402770996094,\n",
      " 'loss_1': 466.5369567871094,\n",
      " 'mean_episode_return_0': 70.84993743896484,\n",
      " 'mean_episode_return_1': 81.73834991455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:04,542] After 3350400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 477.1458740234375,\n",
      " 'loss_1': 466.5369567871094,\n",
      " 'mean_episode_return_0': 70.94581604003906,\n",
      " 'mean_episode_return_1': 81.73834991455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:09,548] After 3353600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 477.1458740234375,\n",
      " 'loss_1': 425.4483642578125,\n",
      " 'mean_episode_return_0': 70.94581604003906,\n",
      " 'mean_episode_return_1': 81.75281524658203}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:14,552] After 3353600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 477.1458740234375,\n",
      " 'loss_1': 425.4483642578125,\n",
      " 'mean_episode_return_0': 70.94581604003906,\n",
      " 'mean_episode_return_1': 81.75281524658203}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:19,556] After 3356800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 399.8600769042969,\n",
      " 'loss_1': 425.4483642578125,\n",
      " 'mean_episode_return_0': 70.9572525024414,\n",
      " 'mean_episode_return_1': 81.75281524658203}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:24,561] After 3360000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.8600769042969,\n",
      " 'loss_1': 485.47509765625,\n",
      " 'mean_episode_return_0': 70.9572525024414,\n",
      " 'mean_episode_return_1': 81.71217346191406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:29,564] After 3363200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 367.3451232910156,\n",
      " 'loss_1': 485.47509765625,\n",
      " 'mean_episode_return_0': 70.9310073852539,\n",
      " 'mean_episode_return_1': 81.71217346191406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:34,570] After 3363200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.3451232910156,\n",
      " 'loss_1': 485.47509765625,\n",
      " 'mean_episode_return_0': 70.9310073852539,\n",
      " 'mean_episode_return_1': 81.71217346191406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:39,572] After 3366400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 367.3451232910156,\n",
      " 'loss_1': 530.1112060546875,\n",
      " 'mean_episode_return_0': 70.9310073852539,\n",
      " 'mean_episode_return_1': 81.69805145263672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:44,578] After 3369600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 412.05328369140625,\n",
      " 'loss_1': 530.1112060546875,\n",
      " 'mean_episode_return_0': 70.9530029296875,\n",
      " 'mean_episode_return_1': 81.69805145263672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:49,583] After 3369600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 412.05328369140625,\n",
      " 'loss_1': 530.1112060546875,\n",
      " 'mean_episode_return_0': 70.9530029296875,\n",
      " 'mean_episode_return_1': 81.69805145263672}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:23:54,589] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:54,682] After 3372800 frames: @ 627.7 fps Stats:\n",
      "{'loss_0': 412.05328369140625,\n",
      " 'loss_1': 504.9268493652344,\n",
      " 'mean_episode_return_0': 70.9530029296875,\n",
      " 'mean_episode_return_1': 81.78379821777344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:23:59,688] After 3376000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 446.3583984375,\n",
      " 'loss_1': 504.9268493652344,\n",
      " 'mean_episode_return_0': 70.9254379272461,\n",
      " 'mean_episode_return_1': 81.78379821777344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:04,694] After 3376000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 446.3583984375,\n",
      " 'loss_1': 504.9268493652344,\n",
      " 'mean_episode_return_0': 70.9254379272461,\n",
      " 'mean_episode_return_1': 81.78379821777344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:24:09,700] After 3379200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 446.3583984375,\n",
      " 'loss_1': 504.28509521484375,\n",
      " 'mean_episode_return_0': 70.9254379272461,\n",
      " 'mean_episode_return_1': 81.7935562133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:14,706] After 3382400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 473.173583984375,\n",
      " 'loss_1': 504.28509521484375,\n",
      " 'mean_episode_return_0': 70.91156768798828,\n",
      " 'mean_episode_return_1': 81.7935562133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:19,712] After 3382400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 473.173583984375,\n",
      " 'loss_1': 504.28509521484375,\n",
      " 'mean_episode_return_0': 70.91156768798828,\n",
      " 'mean_episode_return_1': 81.7935562133789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:24,718] After 3385600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 473.173583984375,\n",
      " 'loss_1': 521.1240844726562,\n",
      " 'mean_episode_return_0': 70.91156768798828,\n",
      " 'mean_episode_return_1': 81.83837890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:29,724] After 3388800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 439.1789855957031,\n",
      " 'loss_1': 521.1240844726562,\n",
      " 'mean_episode_return_0': 70.91731262207031,\n",
      " 'mean_episode_return_1': 81.83837890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:34,728] After 3388800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.1789855957031,\n",
      " 'loss_1': 521.1240844726562,\n",
      " 'mean_episode_return_0': 70.91731262207031,\n",
      " 'mean_episode_return_1': 81.83837890625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:39,734] After 3392000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 439.1789855957031,\n",
      " 'loss_1': 486.5099182128906,\n",
      " 'mean_episode_return_0': 70.91731262207031,\n",
      " 'mean_episode_return_1': 81.86014556884766}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:44,740] After 3395200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 457.5597229003906,\n",
      " 'loss_1': 486.5099182128906,\n",
      " 'mean_episode_return_0': 70.93099975585938,\n",
      " 'mean_episode_return_1': 81.86014556884766}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:49,744] After 3395200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 457.5597229003906,\n",
      " 'loss_1': 486.5099182128906,\n",
      " 'mean_episode_return_0': 70.93099975585938,\n",
      " 'mean_episode_return_1': 81.86014556884766}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:24:54,749] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:54,811] After 3398400 frames: @ 631.5 fps Stats:\n",
      "{'loss_0': 457.5597229003906,\n",
      " 'loss_1': 474.89984130859375,\n",
      " 'mean_episode_return_0': 70.93099975585938,\n",
      " 'mean_episode_return_1': 81.8541030883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:24:59,817] After 3401600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 361.8737487792969,\n",
      " 'loss_1': 474.89984130859375,\n",
      " 'mean_episode_return_0': 70.86931610107422,\n",
      " 'mean_episode_return_1': 81.8541030883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:04,820] After 3404800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 361.8737487792969,\n",
      " 'loss_1': 507.53118896484375,\n",
      " 'mean_episode_return_0': 70.86931610107422,\n",
      " 'mean_episode_return_1': 81.88177490234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:09,826] After 3408000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 393.4996643066406,\n",
      " 'loss_1': 507.53118896484375,\n",
      " 'mean_episode_return_0': 70.7934341430664,\n",
      " 'mean_episode_return_1': 81.88177490234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:14,832] After 3408000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 393.4996643066406,\n",
      " 'loss_1': 507.53118896484375,\n",
      " 'mean_episode_return_0': 70.7934341430664,\n",
      " 'mean_episode_return_1': 81.88177490234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:19,837] After 3411200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 393.4996643066406,\n",
      " 'loss_1': 463.51092529296875,\n",
      " 'mean_episode_return_0': 70.7934341430664,\n",
      " 'mean_episode_return_1': 81.89292907714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:24,843] After 3414400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 447.1925048828125,\n",
      " 'loss_1': 463.51092529296875,\n",
      " 'mean_episode_return_0': 70.77725219726562,\n",
      " 'mean_episode_return_1': 81.89292907714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:29,849] After 3414400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 447.1925048828125,\n",
      " 'loss_1': 463.51092529296875,\n",
      " 'mean_episode_return_0': 70.77725219726562,\n",
      " 'mean_episode_return_1': 81.89292907714844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:34,855] After 3417600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 447.1925048828125,\n",
      " 'loss_1': 479.60357666015625,\n",
      " 'mean_episode_return_0': 70.77725219726562,\n",
      " 'mean_episode_return_1': 81.91188049316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:39,861] After 3420800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 426.8473205566406,\n",
      " 'loss_1': 479.60357666015625,\n",
      " 'mean_episode_return_0': 70.74568939208984,\n",
      " 'mean_episode_return_1': 81.91188049316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:44,867] After 3420800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.8473205566406,\n",
      " 'loss_1': 479.60357666015625,\n",
      " 'mean_episode_return_0': 70.74568939208984,\n",
      " 'mean_episode_return_1': 81.91188049316406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:49,872] After 3424000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 426.8473205566406,\n",
      " 'loss_1': 467.02044677734375,\n",
      " 'mean_episode_return_0': 70.74568939208984,\n",
      " 'mean_episode_return_1': 81.94804382324219}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:25:54,875] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:54,969] After 3427200 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 410.28131103515625,\n",
      " 'loss_1': 467.02044677734375,\n",
      " 'mean_episode_return_0': 70.70306396484375,\n",
      " 'mean_episode_return_1': 81.94804382324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:25:59,972] After 3427200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 410.28131103515625,\n",
      " 'loss_1': 467.02044677734375,\n",
      " 'mean_episode_return_0': 70.70306396484375,\n",
      " 'mean_episode_return_1': 81.94804382324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:04,978] After 3430400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.28131103515625,\n",
      " 'loss_1': 401.8902282714844,\n",
      " 'mean_episode_return_0': 70.70306396484375,\n",
      " 'mean_episode_return_1': 81.98683166503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:09,984] After 3433600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 477.1932067871094,\n",
      " 'loss_1': 401.8902282714844,\n",
      " 'mean_episode_return_0': 70.73119354248047,\n",
      " 'mean_episode_return_1': 81.98683166503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:14,988] After 3433600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 477.1932067871094,\n",
      " 'loss_1': 401.8902282714844,\n",
      " 'mean_episode_return_0': 70.73119354248047,\n",
      " 'mean_episode_return_1': 81.98683166503906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:19,994] After 3440000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 466.5328674316406,\n",
      " 'loss_1': 432.21685791015625,\n",
      " 'mean_episode_return_0': 70.72918701171875,\n",
      " 'mean_episode_return_1': 82.01931762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:25,000] After 3440000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 466.5328674316406,\n",
      " 'loss_1': 432.21685791015625,\n",
      " 'mean_episode_return_0': 70.72918701171875,\n",
      " 'mean_episode_return_1': 82.01931762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:30,006] After 3440000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 466.5328674316406,\n",
      " 'loss_1': 432.21685791015625,\n",
      " 'mean_episode_return_0': 70.72918701171875,\n",
      " 'mean_episode_return_1': 82.01931762695312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:35,011] After 3446400 frames: @ 1279.0 fps Stats:\n",
      "{'loss_0': 494.6369323730469,\n",
      " 'loss_1': 514.9927368164062,\n",
      " 'mean_episode_return_0': 70.72237396240234,\n",
      " 'mean_episode_return_1': 82.09149932861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:40,017] After 3446400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 494.6369323730469,\n",
      " 'loss_1': 514.9927368164062,\n",
      " 'mean_episode_return_0': 70.72237396240234,\n",
      " 'mean_episode_return_1': 82.09149932861328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:45,023] After 3449600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 494.6369323730469,\n",
      " 'loss_1': 568.0875854492188,\n",
      " 'mean_episode_return_0': 70.72237396240234,\n",
      " 'mean_episode_return_1': 82.16441345214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:26:50,029] After 3452800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 390.01470947265625,\n",
      " 'loss_1': 568.0875854492188,\n",
      " 'mean_episode_return_0': 70.74394226074219,\n",
      " 'mean_episode_return_1': 82.16441345214844}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:26:55,035] Saving checkpoint to dmc_results/tute/model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:26:55,125] After 3452800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 390.01470947265625,\n",
      " 'loss_1': 568.0875854492188,\n",
      " 'mean_episode_return_0': 70.74394226074219,\n",
      " 'mean_episode_return_1': 82.16441345214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:00,131] After 3456000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 390.01470947265625,\n",
      " 'loss_1': 513.5279541015625,\n",
      " 'mean_episode_return_0': 70.74394226074219,\n",
      " 'mean_episode_return_1': 82.17643737792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:05,137] After 3459200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 405.0500793457031,\n",
      " 'loss_1': 513.5279541015625,\n",
      " 'mean_episode_return_0': 70.73088073730469,\n",
      " 'mean_episode_return_1': 82.17643737792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:10,143] After 3459200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.0500793457031,\n",
      " 'loss_1': 513.5279541015625,\n",
      " 'mean_episode_return_0': 70.73088073730469,\n",
      " 'mean_episode_return_1': 82.17643737792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:15,149] After 3465600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 436.06695556640625,\n",
      " 'loss_1': 391.333740234375,\n",
      " 'mean_episode_return_0': 70.72781372070312,\n",
      " 'mean_episode_return_1': 82.2433090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:20,155] After 3465600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 436.06695556640625,\n",
      " 'loss_1': 391.333740234375,\n",
      " 'mean_episode_return_0': 70.72781372070312,\n",
      " 'mean_episode_return_1': 82.2433090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:25,161] After 3465600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 436.06695556640625,\n",
      " 'loss_1': 391.333740234375,\n",
      " 'mean_episode_return_0': 70.72781372070312,\n",
      " 'mean_episode_return_1': 82.2433090209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:30,167] After 3472000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 345.6215515136719,\n",
      " 'loss_1': 480.669677734375,\n",
      " 'mean_episode_return_0': 70.6624984741211,\n",
      " 'mean_episode_return_1': 82.2620849609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:35,173] After 3472000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.6215515136719,\n",
      " 'loss_1': 480.669677734375,\n",
      " 'mean_episode_return_0': 70.6624984741211,\n",
      " 'mean_episode_return_1': 82.2620849609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:40,176] After 3472000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.6215515136719,\n",
      " 'loss_1': 480.669677734375,\n",
      " 'mean_episode_return_0': 70.6624984741211,\n",
      " 'mean_episode_return_1': 82.2620849609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:45,182] After 3478400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 460.90765380859375,\n",
      " 'loss_1': 492.3021240234375,\n",
      " 'mean_episode_return_0': 70.67525482177734,\n",
      " 'mean_episode_return_1': 82.2836685180664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:50,188] After 3478400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 460.90765380859375,\n",
      " 'loss_1': 492.3021240234375,\n",
      " 'mean_episode_return_0': 70.67525482177734,\n",
      " 'mean_episode_return_1': 82.2836685180664}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:27:55,194] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:27:55,283] After 3481600 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 460.90765380859375,\n",
      " 'loss_1': 493.98358154296875,\n",
      " 'mean_episode_return_0': 70.67525482177734,\n",
      " 'mean_episode_return_1': 82.2909927368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:00,288] After 3484800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 429.0933532714844,\n",
      " 'loss_1': 493.98358154296875,\n",
      " 'mean_episode_return_0': 70.69918823242188,\n",
      " 'mean_episode_return_1': 82.2909927368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:05,294] After 3484800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 429.0933532714844,\n",
      " 'loss_1': 493.98358154296875,\n",
      " 'mean_episode_return_0': 70.69918823242188,\n",
      " 'mean_episode_return_1': 82.2909927368164}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:10,300] After 3491200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 413.5587463378906,\n",
      " 'loss_1': 523.4183959960938,\n",
      " 'mean_episode_return_0': 70.72624969482422,\n",
      " 'mean_episode_return_1': 82.26001739501953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:15,304] After 3491200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.5587463378906,\n",
      " 'loss_1': 523.4183959960938,\n",
      " 'mean_episode_return_0': 70.72624969482422,\n",
      " 'mean_episode_return_1': 82.26001739501953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:20,309] After 3491200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.5587463378906,\n",
      " 'loss_1': 523.4183959960938,\n",
      " 'mean_episode_return_0': 70.72624969482422,\n",
      " 'mean_episode_return_1': 82.26001739501953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:25,315] After 3497600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 515.9041137695312,\n",
      " 'loss_1': 417.53070068359375,\n",
      " 'mean_episode_return_0': 70.74262237548828,\n",
      " 'mean_episode_return_1': 82.26424407958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:30,321] After 3497600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 515.9041137695312,\n",
      " 'loss_1': 417.53070068359375,\n",
      " 'mean_episode_return_0': 70.74262237548828,\n",
      " 'mean_episode_return_1': 82.26424407958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:35,324] After 3497600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 515.9041137695312,\n",
      " 'loss_1': 417.53070068359375,\n",
      " 'mean_episode_return_0': 70.74262237548828,\n",
      " 'mean_episode_return_1': 82.26424407958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:40,330] After 3504000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 416.11669921875,\n",
      " 'loss_1': 463.2985534667969,\n",
      " 'mean_episode_return_0': 70.82050323486328,\n",
      " 'mean_episode_return_1': 82.26496124267578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:45,336] After 3504000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.11669921875,\n",
      " 'loss_1': 463.2985534667969,\n",
      " 'mean_episode_return_0': 70.82050323486328,\n",
      " 'mean_episode_return_1': 82.26496124267578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:50,342] After 3504000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.11669921875,\n",
      " 'loss_1': 463.2985534667969,\n",
      " 'mean_episode_return_0': 70.82050323486328,\n",
      " 'mean_episode_return_1': 82.26496124267578}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:28:55,348] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:28:55,443] After 3510400 frames: @ 1254.9 fps Stats:\n",
      "{'loss_0': 501.2822570800781,\n",
      " 'loss_1': 510.0213317871094,\n",
      " 'mean_episode_return_0': 70.81082153320312,\n",
      " 'mean_episode_return_1': 82.26029205322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:00,449] After 3510400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 501.2822570800781,\n",
      " 'loss_1': 510.0213317871094,\n",
      " 'mean_episode_return_0': 70.81082153320312,\n",
      " 'mean_episode_return_1': 82.26029205322266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:05,455] After 3516800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 413.9681091308594,\n",
      " 'loss_1': 597.9434204101562,\n",
      " 'mean_episode_return_0': 70.76775360107422,\n",
      " 'mean_episode_return_1': 82.27479553222656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:10,461] After 3516800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.9681091308594,\n",
      " 'loss_1': 597.9434204101562,\n",
      " 'mean_episode_return_0': 70.76775360107422,\n",
      " 'mean_episode_return_1': 82.27479553222656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:15,467] After 3516800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 413.9681091308594,\n",
      " 'loss_1': 597.9434204101562,\n",
      " 'mean_episode_return_0': 70.76775360107422,\n",
      " 'mean_episode_return_1': 82.27479553222656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:20,473] After 3523200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 345.7576599121094,\n",
      " 'loss_1': 530.9637451171875,\n",
      " 'mean_episode_return_0': 70.75756072998047,\n",
      " 'mean_episode_return_1': 82.23700714111328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:25,479] After 3523200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.7576599121094,\n",
      " 'loss_1': 530.9637451171875,\n",
      " 'mean_episode_return_0': 70.75756072998047,\n",
      " 'mean_episode_return_1': 82.23700714111328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:30,485] After 3523200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 345.7576599121094,\n",
      " 'loss_1': 530.9637451171875,\n",
      " 'mean_episode_return_0': 70.75756072998047,\n",
      " 'mean_episode_return_1': 82.23700714111328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:35,488] After 3529600 frames: @ 1279.6 fps Stats:\n",
      "{'loss_0': 400.4266357421875,\n",
      " 'loss_1': 499.78021240234375,\n",
      " 'mean_episode_return_0': 70.73487854003906,\n",
      " 'mean_episode_return_1': 82.26299285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:40,492] After 3529600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.4266357421875,\n",
      " 'loss_1': 499.78021240234375,\n",
      " 'mean_episode_return_0': 70.73487854003906,\n",
      " 'mean_episode_return_1': 82.26299285888672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:29:45,496] After 3529600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.4266357421875,\n",
      " 'loss_1': 499.78021240234375,\n",
      " 'mean_episode_return_0': 70.73487854003906,\n",
      " 'mean_episode_return_1': 82.26299285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:50,500] After 3529600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 400.4266357421875,\n",
      " 'loss_1': 499.78021240234375,\n",
      " 'mean_episode_return_0': 70.73487854003906,\n",
      " 'mean_episode_return_1': 82.26299285888672}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:29:55,504] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:29:55,602] After 3536000 frames: @ 1254.9 fps Stats:\n",
      "{'loss_0': 415.00732421875,\n",
      " 'loss_1': 532.7283325195312,\n",
      " 'mean_episode_return_0': 70.77837371826172,\n",
      " 'mean_episode_return_1': 82.3072280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:00,609] After 3536000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.00732421875,\n",
      " 'loss_1': 532.7283325195312,\n",
      " 'mean_episode_return_0': 70.77837371826172,\n",
      " 'mean_episode_return_1': 82.3072280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:05,617] After 3536000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.00732421875,\n",
      " 'loss_1': 532.7283325195312,\n",
      " 'mean_episode_return_0': 70.77837371826172,\n",
      " 'mean_episode_return_1': 82.3072280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:10,620] After 3536000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.00732421875,\n",
      " 'loss_1': 532.7283325195312,\n",
      " 'mean_episode_return_0': 70.77837371826172,\n",
      " 'mean_episode_return_1': 82.3072280883789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:15,624] After 3542400 frames: @ 1279.4 fps Stats:\n",
      "{'loss_0': 455.0665588378906,\n",
      " 'loss_1': 481.96630859375,\n",
      " 'mean_episode_return_0': 70.77562713623047,\n",
      " 'mean_episode_return_1': 82.31906127929688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:20,630] After 3542400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.0665588378906,\n",
      " 'loss_1': 481.96630859375,\n",
      " 'mean_episode_return_0': 70.77562713623047,\n",
      " 'mean_episode_return_1': 82.31906127929688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:25,635] After 3542400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.0665588378906,\n",
      " 'loss_1': 481.96630859375,\n",
      " 'mean_episode_return_0': 70.77562713623047,\n",
      " 'mean_episode_return_1': 82.31906127929688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:30,641] After 3548800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 439.2680358886719,\n",
      " 'loss_1': 503.32733154296875,\n",
      " 'mean_episode_return_0': 70.76856231689453,\n",
      " 'mean_episode_return_1': 82.28499603271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:35,644] After 3548800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.2680358886719,\n",
      " 'loss_1': 503.32733154296875,\n",
      " 'mean_episode_return_0': 70.76856231689453,\n",
      " 'mean_episode_return_1': 82.28499603271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:40,647] After 3548800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.2680358886719,\n",
      " 'loss_1': 503.32733154296875,\n",
      " 'mean_episode_return_0': 70.76856231689453,\n",
      " 'mean_episode_return_1': 82.28499603271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:45,652] After 3555200 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 432.5970153808594,\n",
      " 'loss_1': 519.4848022460938,\n",
      " 'mean_episode_return_0': 70.71562194824219,\n",
      " 'mean_episode_return_1': 82.31706237792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:50,656] After 3555200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.5970153808594,\n",
      " 'loss_1': 519.4848022460938,\n",
      " 'mean_episode_return_0': 70.71562194824219,\n",
      " 'mean_episode_return_1': 82.31706237792969}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:30:55,661] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:30:55,749] After 3555200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 432.5970153808594,\n",
      " 'loss_1': 519.4848022460938,\n",
      " 'mean_episode_return_0': 70.71562194824219,\n",
      " 'mean_episode_return_1': 82.31706237792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:00,752] After 3561600 frames: @ 1279.7 fps Stats:\n",
      "{'loss_0': 415.30419921875,\n",
      " 'loss_1': 501.8049621582031,\n",
      " 'mean_episode_return_0': 70.7239990234375,\n",
      " 'mean_episode_return_1': 82.3612289428711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:05,758] After 3561600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.30419921875,\n",
      " 'loss_1': 501.8049621582031,\n",
      " 'mean_episode_return_0': 70.7239990234375,\n",
      " 'mean_episode_return_1': 82.3612289428711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:10,759] After 3564800 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 465.95794677734375,\n",
      " 'loss_1': 501.8049621582031,\n",
      " 'mean_episode_return_0': 70.75062561035156,\n",
      " 'mean_episode_return_1': 82.3612289428711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:15,764] After 3568000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 465.95794677734375,\n",
      " 'loss_1': 519.9453125,\n",
      " 'mean_episode_return_0': 70.75062561035156,\n",
      " 'mean_episode_return_1': 82.39299011230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:20,768] After 3568000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 465.95794677734375,\n",
      " 'loss_1': 519.9453125,\n",
      " 'mean_episode_return_0': 70.75062561035156,\n",
      " 'mean_episode_return_1': 82.39299011230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:25,774] After 3571200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 459.4435119628906,\n",
      " 'loss_1': 519.9453125,\n",
      " 'mean_episode_return_0': 70.71830749511719,\n",
      " 'mean_episode_return_1': 82.39299011230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:30,776] After 3574400 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 459.4435119628906,\n",
      " 'loss_1': 455.53179931640625,\n",
      " 'mean_episode_return_0': 70.71830749511719,\n",
      " 'mean_episode_return_1': 82.39427185058594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:35,781] After 3574400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.4435119628906,\n",
      " 'loss_1': 455.53179931640625,\n",
      " 'mean_episode_return_0': 70.71830749511719,\n",
      " 'mean_episode_return_1': 82.39427185058594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:40,787] After 3577600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.8746032714844,\n",
      " 'loss_1': 455.53179931640625,\n",
      " 'mean_episode_return_0': 70.67068481445312,\n",
      " 'mean_episode_return_1': 82.39427185058594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:45,793] After 3580800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.8746032714844,\n",
      " 'loss_1': 522.3158569335938,\n",
      " 'mean_episode_return_0': 70.67068481445312,\n",
      " 'mean_episode_return_1': 82.41187286376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:50,799] After 3580800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.8746032714844,\n",
      " 'loss_1': 522.3158569335938,\n",
      " 'mean_episode_return_0': 70.67068481445312,\n",
      " 'mean_episode_return_1': 82.41187286376953}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:31:55,804] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:31:55,896] After 3584000 frames: @ 627.9 fps Stats:\n",
      "{'loss_0': 403.9059143066406,\n",
      " 'loss_1': 522.3158569335938,\n",
      " 'mean_episode_return_0': 70.73574829101562,\n",
      " 'mean_episode_return_1': 82.41187286376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:00,902] After 3587200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 403.9059143066406,\n",
      " 'loss_1': 414.8165283203125,\n",
      " 'mean_episode_return_0': 70.73574829101562,\n",
      " 'mean_episode_return_1': 82.44517517089844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:05,908] After 3587200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 403.9059143066406,\n",
      " 'loss_1': 414.8165283203125,\n",
      " 'mean_episode_return_0': 70.73574829101562,\n",
      " 'mean_episode_return_1': 82.44517517089844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:10,912] After 3590400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 477.6265869140625,\n",
      " 'loss_1': 414.8165283203125,\n",
      " 'mean_episode_return_0': 70.73993682861328,\n",
      " 'mean_episode_return_1': 82.44517517089844}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:15,918] After 3593600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 477.6265869140625,\n",
      " 'loss_1': 531.6096801757812,\n",
      " 'mean_episode_return_0': 70.73993682861328,\n",
      " 'mean_episode_return_1': 82.44409942626953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:20,924] After 3593600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 477.6265869140625,\n",
      " 'loss_1': 531.6096801757812,\n",
      " 'mean_episode_return_0': 70.73993682861328,\n",
      " 'mean_episode_return_1': 82.44409942626953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:25,928] After 3596800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 439.7615051269531,\n",
      " 'loss_1': 531.6096801757812,\n",
      " 'mean_episode_return_0': 70.71156311035156,\n",
      " 'mean_episode_return_1': 82.44409942626953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:30,932] After 3600000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 439.7615051269531,\n",
      " 'loss_1': 467.2864685058594,\n",
      " 'mean_episode_return_0': 70.71156311035156,\n",
      " 'mean_episode_return_1': 82.41336059570312}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:32:35,936] After 3600000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.7615051269531,\n",
      " 'loss_1': 467.2864685058594,\n",
      " 'mean_episode_return_0': 70.71156311035156,\n",
      " 'mean_episode_return_1': 82.41336059570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:40,942] After 3603200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 408.90679931640625,\n",
      " 'loss_1': 467.2864685058594,\n",
      " 'mean_episode_return_0': 70.70549774169922,\n",
      " 'mean_episode_return_1': 82.41336059570312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:45,948] After 3606400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 408.90679931640625,\n",
      " 'loss_1': 487.823974609375,\n",
      " 'mean_episode_return_0': 70.70549774169922,\n",
      " 'mean_episode_return_1': 82.46330261230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:50,952] After 3606400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 408.90679931640625,\n",
      " 'loss_1': 487.823974609375,\n",
      " 'mean_episode_return_0': 70.70549774169922,\n",
      " 'mean_episode_return_1': 82.46330261230469}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:32:55,953] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:32:56,029] After 3609600 frames: @ 630.4 fps Stats:\n",
      "{'loss_0': 392.9519348144531,\n",
      " 'loss_1': 487.823974609375,\n",
      " 'mean_episode_return_0': 70.69481658935547,\n",
      " 'mean_episode_return_1': 82.46330261230469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:01,032] After 3612800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 392.9519348144531,\n",
      " 'loss_1': 431.18670654296875,\n",
      " 'mean_episode_return_0': 70.69481658935547,\n",
      " 'mean_episode_return_1': 82.45049285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:06,036] After 3616000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 440.5706481933594,\n",
      " 'loss_1': 431.18670654296875,\n",
      " 'mean_episode_return_0': 70.68687438964844,\n",
      " 'mean_episode_return_1': 82.45049285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:11,040] After 3616000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 440.5706481933594,\n",
      " 'loss_1': 431.18670654296875,\n",
      " 'mean_episode_return_0': 70.68687438964844,\n",
      " 'mean_episode_return_1': 82.45049285888672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:16,044] After 3619200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 440.5706481933594,\n",
      " 'loss_1': 573.37060546875,\n",
      " 'mean_episode_return_0': 70.68687438964844,\n",
      " 'mean_episode_return_1': 82.45327758789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:21,050] After 3622400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.30169677734375,\n",
      " 'loss_1': 573.37060546875,\n",
      " 'mean_episode_return_0': 70.70193481445312,\n",
      " 'mean_episode_return_1': 82.45327758789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:26,056] After 3625600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.30169677734375,\n",
      " 'loss_1': 600.3090209960938,\n",
      " 'mean_episode_return_0': 70.70193481445312,\n",
      " 'mean_episode_return_1': 82.52851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:31,062] After 3625600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 399.30169677734375,\n",
      " 'loss_1': 600.3090209960938,\n",
      " 'mean_episode_return_0': 70.70193481445312,\n",
      " 'mean_episode_return_1': 82.52851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:36,067] After 3628800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 467.9320983886719,\n",
      " 'loss_1': 600.3090209960938,\n",
      " 'mean_episode_return_0': 70.69506072998047,\n",
      " 'mean_episode_return_1': 82.52851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:41,073] After 3632000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 467.9320983886719,\n",
      " 'loss_1': 482.8419494628906,\n",
      " 'mean_episode_return_0': 70.69506072998047,\n",
      " 'mean_episode_return_1': 82.5932388305664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:46,079] After 3632000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 467.9320983886719,\n",
      " 'loss_1': 482.8419494628906,\n",
      " 'mean_episode_return_0': 70.69506072998047,\n",
      " 'mean_episode_return_1': 82.5932388305664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:51,085] After 3635200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 453.394287109375,\n",
      " 'loss_1': 482.8419494628906,\n",
      " 'mean_episode_return_0': 70.68587493896484,\n",
      " 'mean_episode_return_1': 82.5932388305664}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:33:56,091] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:33:56,185] After 3638400 frames: @ 627.6 fps Stats:\n",
      "{'loss_0': 453.394287109375,\n",
      " 'loss_1': 638.77880859375,\n",
      " 'mean_episode_return_0': 70.68587493896484,\n",
      " 'mean_episode_return_1': 82.6333999633789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:01,187] After 3638400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.394287109375,\n",
      " 'loss_1': 638.77880859375,\n",
      " 'mean_episode_return_0': 70.68587493896484,\n",
      " 'mean_episode_return_1': 82.6333999633789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:06,192] After 3641600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 415.6027526855469,\n",
      " 'loss_1': 638.77880859375,\n",
      " 'mean_episode_return_0': 70.69775390625,\n",
      " 'mean_episode_return_1': 82.6333999633789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:11,198] After 3644800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 415.6027526855469,\n",
      " 'loss_1': 543.784423828125,\n",
      " 'mean_episode_return_0': 70.69775390625,\n",
      " 'mean_episode_return_1': 82.63933563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:16,204] After 3644800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.6027526855469,\n",
      " 'loss_1': 543.784423828125,\n",
      " 'mean_episode_return_0': 70.69775390625,\n",
      " 'mean_episode_return_1': 82.63933563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:21,209] After 3648000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 354.53729248046875,\n",
      " 'loss_1': 543.784423828125,\n",
      " 'mean_episode_return_0': 70.67774963378906,\n",
      " 'mean_episode_return_1': 82.63933563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:26,215] After 3651200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 354.53729248046875,\n",
      " 'loss_1': 535.3034057617188,\n",
      " 'mean_episode_return_0': 70.67774963378906,\n",
      " 'mean_episode_return_1': 82.6683578491211}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:31,220] After 3651200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 354.53729248046875,\n",
      " 'loss_1': 535.3034057617188,\n",
      " 'mean_episode_return_0': 70.67774963378906,\n",
      " 'mean_episode_return_1': 82.6683578491211}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:36,224] After 3654400 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 317.4445495605469,\n",
      " 'loss_1': 535.3034057617188,\n",
      " 'mean_episode_return_0': 70.59744262695312,\n",
      " 'mean_episode_return_1': 82.6683578491211}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:41,229] After 3657600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 317.4445495605469,\n",
      " 'loss_1': 544.4306030273438,\n",
      " 'mean_episode_return_0': 70.59744262695312,\n",
      " 'mean_episode_return_1': 82.73921966552734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:46,232] After 3657600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 317.4445495605469,\n",
      " 'loss_1': 544.4306030273438,\n",
      " 'mean_episode_return_0': 70.59744262695312,\n",
      " 'mean_episode_return_1': 82.73921966552734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:51,238] After 3660800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 391.7342224121094,\n",
      " 'loss_1': 544.4306030273438,\n",
      " 'mean_episode_return_0': 70.5816879272461,\n",
      " 'mean_episode_return_1': 82.73921966552734}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:34:56,243] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:34:56,334] After 3664000 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 391.7342224121094,\n",
      " 'loss_1': 606.013916015625,\n",
      " 'mean_episode_return_0': 70.5816879272461,\n",
      " 'mean_episode_return_1': 82.73348236083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:01,340] After 3667200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 443.7496032714844,\n",
      " 'loss_1': 606.013916015625,\n",
      " 'mean_episode_return_0': 70.55437469482422,\n",
      " 'mean_episode_return_1': 82.73348236083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:06,341] After 3667200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 443.7496032714844,\n",
      " 'loss_1': 606.013916015625,\n",
      " 'mean_episode_return_0': 70.55437469482422,\n",
      " 'mean_episode_return_1': 82.73348236083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:11,347] After 3670400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 443.7496032714844,\n",
      " 'loss_1': 503.1629638671875,\n",
      " 'mean_episode_return_0': 70.55437469482422,\n",
      " 'mean_episode_return_1': 82.7835693359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:16,353] After 3673600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 408.57427978515625,\n",
      " 'loss_1': 503.1629638671875,\n",
      " 'mean_episode_return_0': 70.52281188964844,\n",
      " 'mean_episode_return_1': 82.7835693359375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:35:21,359] After 3673600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 408.57427978515625,\n",
      " 'loss_1': 503.1629638671875,\n",
      " 'mean_episode_return_0': 70.52281188964844,\n",
      " 'mean_episode_return_1': 82.7835693359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:26,364] After 3676800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 408.57427978515625,\n",
      " 'loss_1': 593.9207153320312,\n",
      " 'mean_episode_return_0': 70.52281188964844,\n",
      " 'mean_episode_return_1': 82.76583099365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:31,369] After 3680000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 426.1579284667969,\n",
      " 'loss_1': 593.9207153320312,\n",
      " 'mean_episode_return_0': 70.5278091430664,\n",
      " 'mean_episode_return_1': 82.76583099365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:36,375] After 3680000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.1579284667969,\n",
      " 'loss_1': 593.9207153320312,\n",
      " 'mean_episode_return_0': 70.5278091430664,\n",
      " 'mean_episode_return_1': 82.76583099365234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:41,381] After 3683200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 426.1579284667969,\n",
      " 'loss_1': 513.9955444335938,\n",
      " 'mean_episode_return_0': 70.5278091430664,\n",
      " 'mean_episode_return_1': 82.83429718017578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:46,387] After 3686400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 426.26873779296875,\n",
      " 'loss_1': 513.9955444335938,\n",
      " 'mean_episode_return_0': 70.54150390625,\n",
      " 'mean_episode_return_1': 82.83429718017578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:51,392] After 3686400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.26873779296875,\n",
      " 'loss_1': 513.9955444335938,\n",
      " 'mean_episode_return_0': 70.54150390625,\n",
      " 'mean_episode_return_1': 82.83429718017578}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:35:56,397] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:35:56,490] After 3689600 frames: @ 627.7 fps Stats:\n",
      "{'loss_0': 426.26873779296875,\n",
      " 'loss_1': 601.3795166015625,\n",
      " 'mean_episode_return_0': 70.54150390625,\n",
      " 'mean_episode_return_1': 82.7641372680664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:01,496] After 3692800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.2445983886719,\n",
      " 'loss_1': 601.3795166015625,\n",
      " 'mean_episode_return_0': 70.57256317138672,\n",
      " 'mean_episode_return_1': 82.7641372680664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:06,502] After 3692800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.2445983886719,\n",
      " 'loss_1': 601.3795166015625,\n",
      " 'mean_episode_return_0': 70.57256317138672,\n",
      " 'mean_episode_return_1': 82.7641372680664}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:11,508] After 3696000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.2445983886719,\n",
      " 'loss_1': 501.73162841796875,\n",
      " 'mean_episode_return_0': 70.57256317138672,\n",
      " 'mean_episode_return_1': 82.75724792480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:16,512] After 3699200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 422.4441223144531,\n",
      " 'loss_1': 501.73162841796875,\n",
      " 'mean_episode_return_0': 70.5521240234375,\n",
      " 'mean_episode_return_1': 82.75724792480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:21,516] After 3699200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 422.4441223144531,\n",
      " 'loss_1': 501.73162841796875,\n",
      " 'mean_episode_return_0': 70.5521240234375,\n",
      " 'mean_episode_return_1': 82.75724792480469}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:26,522] After 3702400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 422.4441223144531,\n",
      " 'loss_1': 488.3634948730469,\n",
      " 'mean_episode_return_0': 70.5521240234375,\n",
      " 'mean_episode_return_1': 82.83171844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:31,528] After 3705600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 443.7795104980469,\n",
      " 'loss_1': 488.3634948730469,\n",
      " 'mean_episode_return_0': 70.48812103271484,\n",
      " 'mean_episode_return_1': 82.83171844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:36,533] After 3705600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 443.7795104980469,\n",
      " 'loss_1': 488.3634948730469,\n",
      " 'mean_episode_return_0': 70.48812103271484,\n",
      " 'mean_episode_return_1': 82.83171844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:41,539] After 3712000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 450.9903869628906,\n",
      " 'loss_1': 569.1636352539062,\n",
      " 'mean_episode_return_0': 70.47999572753906,\n",
      " 'mean_episode_return_1': 82.81793975830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:46,545] After 3712000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 450.9903869628906,\n",
      " 'loss_1': 569.1636352539062,\n",
      " 'mean_episode_return_0': 70.47999572753906,\n",
      " 'mean_episode_return_1': 82.81793975830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:51,548] After 3712000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 450.9903869628906,\n",
      " 'loss_1': 569.1636352539062,\n",
      " 'mean_episode_return_0': 70.47999572753906,\n",
      " 'mean_episode_return_1': 82.81793975830078}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:36:56,553] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:36:56,643] After 3718400 frames: @ 1256.2 fps Stats:\n",
      "{'loss_0': 428.99365234375,\n",
      " 'loss_1': 516.2263793945312,\n",
      " 'mean_episode_return_0': 70.44775390625,\n",
      " 'mean_episode_return_1': 82.8634262084961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:01,649] After 3718400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 428.99365234375,\n",
      " 'loss_1': 516.2263793945312,\n",
      " 'mean_episode_return_0': 70.44775390625,\n",
      " 'mean_episode_return_1': 82.8634262084961}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:06,655] After 3721600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 428.99365234375,\n",
      " 'loss_1': 484.2353820800781,\n",
      " 'mean_episode_return_0': 70.44775390625,\n",
      " 'mean_episode_return_1': 82.85335540771484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:11,660] After 3724800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 461.66888427734375,\n",
      " 'loss_1': 484.2353820800781,\n",
      " 'mean_episode_return_0': 70.42056274414062,\n",
      " 'mean_episode_return_1': 82.85335540771484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:16,666] After 3724800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 461.66888427734375,\n",
      " 'loss_1': 484.2353820800781,\n",
      " 'mean_episode_return_0': 70.42056274414062,\n",
      " 'mean_episode_return_1': 82.85335540771484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:21,668] After 3728000 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 461.66888427734375,\n",
      " 'loss_1': 503.77740478515625,\n",
      " 'mean_episode_return_0': 70.42056274414062,\n",
      " 'mean_episode_return_1': 82.84217834472656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:26,672] After 3731200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 379.3070983886719,\n",
      " 'loss_1': 503.77740478515625,\n",
      " 'mean_episode_return_0': 70.377685546875,\n",
      " 'mean_episode_return_1': 82.84217834472656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:31,678] After 3731200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 379.3070983886719,\n",
      " 'loss_1': 503.77740478515625,\n",
      " 'mean_episode_return_0': 70.377685546875,\n",
      " 'mean_episode_return_1': 82.84217834472656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:36,680] After 3734400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 379.3070983886719,\n",
      " 'loss_1': 449.8363952636719,\n",
      " 'mean_episode_return_0': 70.377685546875,\n",
      " 'mean_episode_return_1': 82.90218353271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:41,684] After 3737600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 404.9254455566406,\n",
      " 'loss_1': 449.8363952636719,\n",
      " 'mean_episode_return_0': 70.3648681640625,\n",
      " 'mean_episode_return_1': 82.90218353271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:46,689] After 3737600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.9254455566406,\n",
      " 'loss_1': 449.8363952636719,\n",
      " 'mean_episode_return_0': 70.3648681640625,\n",
      " 'mean_episode_return_1': 82.90218353271484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:51,695] After 3740800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.9254455566406,\n",
      " 'loss_1': 581.2553100585938,\n",
      " 'mean_episode_return_0': 70.3648681640625,\n",
      " 'mean_episode_return_1': 82.92253112792969}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:37:56,700] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:37:56,795] After 3744000 frames: @ 627.5 fps Stats:\n",
      "{'loss_0': 416.9884948730469,\n",
      " 'loss_1': 581.2553100585938,\n",
      " 'mean_episode_return_0': 70.37200164794922,\n",
      " 'mean_episode_return_1': 82.92253112792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:01,801] After 3744000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.9884948730469,\n",
      " 'loss_1': 581.2553100585938,\n",
      " 'mean_episode_return_0': 70.37200164794922,\n",
      " 'mean_episode_return_1': 82.92253112792969}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:06,807] After 3750400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 429.62322998046875,\n",
      " 'loss_1': 569.3200073242188,\n",
      " 'mean_episode_return_0': 70.33968353271484,\n",
      " 'mean_episode_return_1': 82.95343780517578}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:38:11,812] After 3750400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 429.62322998046875,\n",
      " 'loss_1': 569.3200073242188,\n",
      " 'mean_episode_return_0': 70.33968353271484,\n",
      " 'mean_episode_return_1': 82.95343780517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:16,817] After 3750400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 429.62322998046875,\n",
      " 'loss_1': 569.3200073242188,\n",
      " 'mean_episode_return_0': 70.33968353271484,\n",
      " 'mean_episode_return_1': 82.95343780517578}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:21,823] After 3756800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 459.2621765136719,\n",
      " 'loss_1': 470.10491943359375,\n",
      " 'mean_episode_return_0': 70.31169128417969,\n",
      " 'mean_episode_return_1': 82.96294403076172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:26,828] After 3756800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.2621765136719,\n",
      " 'loss_1': 470.10491943359375,\n",
      " 'mean_episode_return_0': 70.31169128417969,\n",
      " 'mean_episode_return_1': 82.96294403076172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:31,834] After 3756800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.2621765136719,\n",
      " 'loss_1': 470.10491943359375,\n",
      " 'mean_episode_return_0': 70.31169128417969,\n",
      " 'mean_episode_return_1': 82.96294403076172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:36,840] After 3763200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 447.3553771972656,\n",
      " 'loss_1': 482.07659912109375,\n",
      " 'mean_episode_return_0': 70.24874877929688,\n",
      " 'mean_episode_return_1': 82.99994659423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:41,846] After 3763200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 447.3553771972656,\n",
      " 'loss_1': 482.07659912109375,\n",
      " 'mean_episode_return_0': 70.24874877929688,\n",
      " 'mean_episode_return_1': 82.99994659423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:46,852] After 3763200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 447.3553771972656,\n",
      " 'loss_1': 482.07659912109375,\n",
      " 'mean_episode_return_0': 70.24874877929688,\n",
      " 'mean_episode_return_1': 82.99994659423828}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:51,858] After 3769600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 455.8839416503906,\n",
      " 'loss_1': 539.2061767578125,\n",
      " 'mean_episode_return_0': 70.2093734741211,\n",
      " 'mean_episode_return_1': 82.9896011352539}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:38:56,864] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:38:56,937] After 3769600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.8839416503906,\n",
      " 'loss_1': 539.2061767578125,\n",
      " 'mean_episode_return_0': 70.2093734741211,\n",
      " 'mean_episode_return_1': 82.9896011352539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:01,940] After 3769600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.8839416503906,\n",
      " 'loss_1': 539.2061767578125,\n",
      " 'mean_episode_return_0': 70.2093734741211,\n",
      " 'mean_episode_return_1': 82.9896011352539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:06,946] After 3776000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 402.1470947265625,\n",
      " 'loss_1': 548.710205078125,\n",
      " 'mean_episode_return_0': 70.11831665039062,\n",
      " 'mean_episode_return_1': 83.03478240966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:11,952] After 3776000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.1470947265625,\n",
      " 'loss_1': 548.710205078125,\n",
      " 'mean_episode_return_0': 70.11831665039062,\n",
      " 'mean_episode_return_1': 83.03478240966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:16,956] After 3776000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.1470947265625,\n",
      " 'loss_1': 548.710205078125,\n",
      " 'mean_episode_return_0': 70.11831665039062,\n",
      " 'mean_episode_return_1': 83.03478240966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:21,960] After 3782400 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 453.77490234375,\n",
      " 'loss_1': 498.449951171875,\n",
      " 'mean_episode_return_0': 70.1312484741211,\n",
      " 'mean_episode_return_1': 83.01775360107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:26,966] After 3782400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.77490234375,\n",
      " 'loss_1': 498.449951171875,\n",
      " 'mean_episode_return_0': 70.1312484741211,\n",
      " 'mean_episode_return_1': 83.01775360107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:31,972] After 3785600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.9940490722656,\n",
      " 'loss_1': 498.449951171875,\n",
      " 'mean_episode_return_0': 70.09938049316406,\n",
      " 'mean_episode_return_1': 83.01775360107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:36,978] After 3788800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 378.9940490722656,\n",
      " 'loss_1': 456.04437255859375,\n",
      " 'mean_episode_return_0': 70.09938049316406,\n",
      " 'mean_episode_return_1': 83.04043579101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:41,983] After 3788800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 378.9940490722656,\n",
      " 'loss_1': 456.04437255859375,\n",
      " 'mean_episode_return_0': 70.09938049316406,\n",
      " 'mean_episode_return_1': 83.04043579101562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:46,989] After 3795200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 463.04638671875,\n",
      " 'loss_1': 573.6735229492188,\n",
      " 'mean_episode_return_0': 70.06806182861328,\n",
      " 'mean_episode_return_1': 83.0125961303711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:51,995] After 3795200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 463.04638671875,\n",
      " 'loss_1': 573.6735229492188,\n",
      " 'mean_episode_return_0': 70.06806182861328,\n",
      " 'mean_episode_return_1': 83.0125961303711}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:39:56,999] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:39:57,091] After 3795200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 463.04638671875,\n",
      " 'loss_1': 573.6735229492188,\n",
      " 'mean_episode_return_0': 70.06806182861328,\n",
      " 'mean_episode_return_1': 83.0125961303711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:02,097] After 3801600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 372.171630859375,\n",
      " 'loss_1': 526.4773559570312,\n",
      " 'mean_episode_return_0': 70.04106140136719,\n",
      " 'mean_episode_return_1': 83.06655883789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:07,103] After 3801600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.171630859375,\n",
      " 'loss_1': 526.4773559570312,\n",
      " 'mean_episode_return_0': 70.04106140136719,\n",
      " 'mean_episode_return_1': 83.06655883789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:12,109] After 3801600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.171630859375,\n",
      " 'loss_1': 526.4773559570312,\n",
      " 'mean_episode_return_0': 70.04106140136719,\n",
      " 'mean_episode_return_1': 83.06655883789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:17,115] After 3808000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 390.552734375,\n",
      " 'loss_1': 481.8751525878906,\n",
      " 'mean_episode_return_0': 70.0379409790039,\n",
      " 'mean_episode_return_1': 83.02678680419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:22,121] After 3808000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 390.552734375,\n",
      " 'loss_1': 481.8751525878906,\n",
      " 'mean_episode_return_0': 70.0379409790039,\n",
      " 'mean_episode_return_1': 83.02678680419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:27,127] After 3811200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 409.4811096191406,\n",
      " 'loss_1': 481.8751525878906,\n",
      " 'mean_episode_return_0': 70.08406829833984,\n",
      " 'mean_episode_return_1': 83.02678680419922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:32,133] After 3814400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 409.4811096191406,\n",
      " 'loss_1': 654.1520385742188,\n",
      " 'mean_episode_return_0': 70.08406829833984,\n",
      " 'mean_episode_return_1': 83.01375579833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:37,139] After 3814400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 409.4811096191406,\n",
      " 'loss_1': 654.1520385742188,\n",
      " 'mean_episode_return_0': 70.08406829833984,\n",
      " 'mean_episode_return_1': 83.01375579833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:42,144] After 3817600 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 415.43597412109375,\n",
      " 'loss_1': 654.1520385742188,\n",
      " 'mean_episode_return_0': 70.11244201660156,\n",
      " 'mean_episode_return_1': 83.01375579833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:47,149] After 3820800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 415.43597412109375,\n",
      " 'loss_1': 500.1274108886719,\n",
      " 'mean_episode_return_0': 70.11244201660156,\n",
      " 'mean_episode_return_1': 82.979736328125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:52,152] After 3820800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.43597412109375,\n",
      " 'loss_1': 500.1274108886719,\n",
      " 'mean_episode_return_0': 70.11244201660156,\n",
      " 'mean_episode_return_1': 82.979736328125}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:40:57,159] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:40:57,262] After 3824000 frames: @ 626.4 fps Stats:\n",
      "{'loss_0': 458.01995849609375,\n",
      " 'loss_1': 500.1274108886719,\n",
      " 'mean_episode_return_0': 70.06194305419922,\n",
      " 'mean_episode_return_1': 82.979736328125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:41:02,268] After 3827200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 458.01995849609375,\n",
      " 'loss_1': 470.3446044921875,\n",
      " 'mean_episode_return_0': 70.06194305419922,\n",
      " 'mean_episode_return_1': 82.92568969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:07,274] After 3827200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 458.01995849609375,\n",
      " 'loss_1': 470.3446044921875,\n",
      " 'mean_episode_return_0': 70.06194305419922,\n",
      " 'mean_episode_return_1': 82.92568969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:12,276] After 3827200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 458.01995849609375,\n",
      " 'loss_1': 470.3446044921875,\n",
      " 'mean_episode_return_0': 70.06194305419922,\n",
      " 'mean_episode_return_1': 82.92568969726562}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:17,282] After 3833600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 478.8523864746094,\n",
      " 'loss_1': 551.4365234375,\n",
      " 'mean_episode_return_0': 70.03687286376953,\n",
      " 'mean_episode_return_1': 82.92774963378906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:22,288] After 3833600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 478.8523864746094,\n",
      " 'loss_1': 551.4365234375,\n",
      " 'mean_episode_return_0': 70.03687286376953,\n",
      " 'mean_episode_return_1': 82.92774963378906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:27,292] After 3836800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 431.33428955078125,\n",
      " 'loss_1': 551.4365234375,\n",
      " 'mean_episode_return_0': 69.98881530761719,\n",
      " 'mean_episode_return_1': 82.92774963378906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:32,295] After 3840000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 431.33428955078125,\n",
      " 'loss_1': 531.8819580078125,\n",
      " 'mean_episode_return_0': 69.98881530761719,\n",
      " 'mean_episode_return_1': 83.022705078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:37,301] After 3840000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 431.33428955078125,\n",
      " 'loss_1': 531.8819580078125,\n",
      " 'mean_episode_return_0': 69.98881530761719,\n",
      " 'mean_episode_return_1': 83.022705078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:42,304] After 3843200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 443.2317810058594,\n",
      " 'loss_1': 531.8819580078125,\n",
      " 'mean_episode_return_0': 69.97637176513672,\n",
      " 'mean_episode_return_1': 83.022705078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:47,311] After 3846400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 443.2317810058594,\n",
      " 'loss_1': 507.3927307128906,\n",
      " 'mean_episode_return_0': 69.97637176513672,\n",
      " 'mean_episode_return_1': 83.0248031616211}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:52,317] After 3846400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 443.2317810058594,\n",
      " 'loss_1': 507.3927307128906,\n",
      " 'mean_episode_return_0': 69.97637176513672,\n",
      " 'mean_episode_return_1': 83.0248031616211}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:41:57,323] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:41:57,405] After 3849600 frames: @ 629.0 fps Stats:\n",
      "{'loss_0': 416.9127502441406,\n",
      " 'loss_1': 507.3927307128906,\n",
      " 'mean_episode_return_0': 69.9930648803711,\n",
      " 'mean_episode_return_1': 83.0248031616211}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:02,408] After 3852800 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 416.9127502441406,\n",
      " 'loss_1': 554.33984375,\n",
      " 'mean_episode_return_0': 69.9930648803711,\n",
      " 'mean_episode_return_1': 82.98382568359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:07,412] After 3852800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.9127502441406,\n",
      " 'loss_1': 554.33984375,\n",
      " 'mean_episode_return_0': 69.9930648803711,\n",
      " 'mean_episode_return_1': 82.98382568359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:12,418] After 3856000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 437.671875,\n",
      " 'loss_1': 554.33984375,\n",
      " 'mean_episode_return_0': 69.98987579345703,\n",
      " 'mean_episode_return_1': 82.98382568359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:17,424] After 3859200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 437.671875,\n",
      " 'loss_1': 504.8563232421875,\n",
      " 'mean_episode_return_0': 69.98987579345703,\n",
      " 'mean_episode_return_1': 82.99882507324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:22,429] After 3859200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.671875,\n",
      " 'loss_1': 504.8563232421875,\n",
      " 'mean_episode_return_0': 69.98987579345703,\n",
      " 'mean_episode_return_1': 82.99882507324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:27,436] After 3862400 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 353.9934387207031,\n",
      " 'loss_1': 504.8563232421875,\n",
      " 'mean_episode_return_0': 70.04949951171875,\n",
      " 'mean_episode_return_1': 82.99882507324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:32,441] After 3865600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 353.9934387207031,\n",
      " 'loss_1': 460.79296875,\n",
      " 'mean_episode_return_0': 70.04949951171875,\n",
      " 'mean_episode_return_1': 82.98040008544922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:37,448] After 3865600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 353.9934387207031,\n",
      " 'loss_1': 460.79296875,\n",
      " 'mean_episode_return_0': 70.04949951171875,\n",
      " 'mean_episode_return_1': 82.98040008544922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:42,453] After 3868800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 430.9771728515625,\n",
      " 'loss_1': 460.79296875,\n",
      " 'mean_episode_return_0': 70.06212615966797,\n",
      " 'mean_episode_return_1': 82.98040008544922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:47,459] After 3872000 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 430.9771728515625,\n",
      " 'loss_1': 494.5435791015625,\n",
      " 'mean_episode_return_0': 70.06212615966797,\n",
      " 'mean_episode_return_1': 82.9510726928711}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:52,464] After 3875200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 501.8293762207031,\n",
      " 'loss_1': 494.5435791015625,\n",
      " 'mean_episode_return_0': 70.03643035888672,\n",
      " 'mean_episode_return_1': 82.9510726928711}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:42:57,468] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:42:57,558] After 3878400 frames: @ 628.2 fps Stats:\n",
      "{'loss_0': 501.8293762207031,\n",
      " 'loss_1': 523.178955078125,\n",
      " 'mean_episode_return_0': 70.03643035888672,\n",
      " 'mean_episode_return_1': 82.9599609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:02,564] After 3878400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 501.8293762207031,\n",
      " 'loss_1': 523.178955078125,\n",
      " 'mean_episode_return_0': 70.03643035888672,\n",
      " 'mean_episode_return_1': 82.9599609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:07,570] After 3881600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 423.7825622558594,\n",
      " 'loss_1': 523.178955078125,\n",
      " 'mean_episode_return_0': 70.04769134521484,\n",
      " 'mean_episode_return_1': 82.9599609375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:12,576] After 3884800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 423.7825622558594,\n",
      " 'loss_1': 500.2557067871094,\n",
      " 'mean_episode_return_0': 70.04769134521484,\n",
      " 'mean_episode_return_1': 82.92506408691406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:17,580] After 3884800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 423.7825622558594,\n",
      " 'loss_1': 500.2557067871094,\n",
      " 'mean_episode_return_0': 70.04769134521484,\n",
      " 'mean_episode_return_1': 82.92506408691406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:22,584] After 3888000 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 417.4928894042969,\n",
      " 'loss_1': 500.2557067871094,\n",
      " 'mean_episode_return_0': 70.02156829833984,\n",
      " 'mean_episode_return_1': 82.92506408691406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:27,588] After 3891200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 417.4928894042969,\n",
      " 'loss_1': 632.955322265625,\n",
      " 'mean_episode_return_0': 70.02156829833984,\n",
      " 'mean_episode_return_1': 82.93348693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:32,594] After 3891200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.4928894042969,\n",
      " 'loss_1': 632.955322265625,\n",
      " 'mean_episode_return_0': 70.02156829833984,\n",
      " 'mean_episode_return_1': 82.93348693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:37,599] After 3894400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.1742248535156,\n",
      " 'loss_1': 632.955322265625,\n",
      " 'mean_episode_return_0': 70.01018524169922,\n",
      " 'mean_episode_return_1': 82.93348693847656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:42,604] After 3897600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 389.1742248535156,\n",
      " 'loss_1': 550.2080078125,\n",
      " 'mean_episode_return_0': 70.01018524169922,\n",
      " 'mean_episode_return_1': 82.92340850830078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:47,608] After 3897600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.1742248535156,\n",
      " 'loss_1': 550.2080078125,\n",
      " 'mean_episode_return_0': 70.01018524169922,\n",
      " 'mean_episode_return_1': 82.92340850830078}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:43:52,612] After 3900800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 417.5545959472656,\n",
      " 'loss_1': 550.2080078125,\n",
      " 'mean_episode_return_0': 69.98025512695312,\n",
      " 'mean_episode_return_1': 82.92340850830078}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:43:57,618] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:43:57,709] After 3904000 frames: @ 628.0 fps Stats:\n",
      "{'loss_0': 417.5545959472656,\n",
      " 'loss_1': 572.7809448242188,\n",
      " 'mean_episode_return_0': 69.98025512695312,\n",
      " 'mean_episode_return_1': 82.94353485107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:02,715] After 3904000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.5545959472656,\n",
      " 'loss_1': 572.7809448242188,\n",
      " 'mean_episode_return_0': 69.98025512695312,\n",
      " 'mean_episode_return_1': 82.94353485107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:07,720] After 3907200 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 388.2060546875,\n",
      " 'loss_1': 572.7809448242188,\n",
      " 'mean_episode_return_0': 69.96018981933594,\n",
      " 'mean_episode_return_1': 82.94353485107422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:12,724] After 3910400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 388.2060546875,\n",
      " 'loss_1': 536.158203125,\n",
      " 'mean_episode_return_0': 69.96018981933594,\n",
      " 'mean_episode_return_1': 82.9546890258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:17,730] After 3910400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 388.2060546875,\n",
      " 'loss_1': 536.158203125,\n",
      " 'mean_episode_return_0': 69.96018981933594,\n",
      " 'mean_episode_return_1': 82.9546890258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:22,736] After 3913600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 386.54937744140625,\n",
      " 'loss_1': 536.158203125,\n",
      " 'mean_episode_return_0': 70.00093841552734,\n",
      " 'mean_episode_return_1': 82.9546890258789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:27,741] After 3916800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 386.54937744140625,\n",
      " 'loss_1': 494.34100341796875,\n",
      " 'mean_episode_return_0': 70.00093841552734,\n",
      " 'mean_episode_return_1': 82.91200256347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:32,747] After 3916800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.54937744140625,\n",
      " 'loss_1': 494.34100341796875,\n",
      " 'mean_episode_return_0': 70.00093841552734,\n",
      " 'mean_episode_return_1': 82.91200256347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:37,753] After 3920000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 382.09466552734375,\n",
      " 'loss_1': 494.34100341796875,\n",
      " 'mean_episode_return_0': 69.98187255859375,\n",
      " 'mean_episode_return_1': 82.91200256347656}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:42,759] After 3923200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 382.09466552734375,\n",
      " 'loss_1': 534.5548095703125,\n",
      " 'mean_episode_return_0': 69.98187255859375,\n",
      " 'mean_episode_return_1': 82.90699768066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:47,765] After 3923200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 382.09466552734375,\n",
      " 'loss_1': 534.5548095703125,\n",
      " 'mean_episode_return_0': 69.98187255859375,\n",
      " 'mean_episode_return_1': 82.90699768066406}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:52,768] After 3926400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 411.00390625,\n",
      " 'loss_1': 534.5548095703125,\n",
      " 'mean_episode_return_0': 69.99349975585938,\n",
      " 'mean_episode_return_1': 82.90699768066406}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:44:57,773] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:44:57,866] After 3929600 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 411.00390625,\n",
      " 'loss_1': 622.6267700195312,\n",
      " 'mean_episode_return_0': 69.99349975585938,\n",
      " 'mean_episode_return_1': 82.90601348876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:02,872] After 3932800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 360.4625549316406,\n",
      " 'loss_1': 622.6267700195312,\n",
      " 'mean_episode_return_0': 69.92768096923828,\n",
      " 'mean_episode_return_1': 82.90601348876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:07,878] After 3932800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 360.4625549316406,\n",
      " 'loss_1': 622.6267700195312,\n",
      " 'mean_episode_return_0': 69.92768096923828,\n",
      " 'mean_episode_return_1': 82.90601348876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:12,883] After 3936000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 360.4625549316406,\n",
      " 'loss_1': 567.8720092773438,\n",
      " 'mean_episode_return_0': 69.92768096923828,\n",
      " 'mean_episode_return_1': 82.94918060302734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:17,889] After 3939200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.1783447265625,\n",
      " 'loss_1': 567.8720092773438,\n",
      " 'mean_episode_return_0': 69.84506225585938,\n",
      " 'mean_episode_return_1': 82.94918060302734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:22,895] After 3939200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.1783447265625,\n",
      " 'loss_1': 567.8720092773438,\n",
      " 'mean_episode_return_0': 69.84506225585938,\n",
      " 'mean_episode_return_1': 82.94918060302734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:27,901] After 3942400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.1783447265625,\n",
      " 'loss_1': 472.3545837402344,\n",
      " 'mean_episode_return_0': 69.84506225585938,\n",
      " 'mean_episode_return_1': 83.02001953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:32,907] After 3945600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 396.1219482421875,\n",
      " 'loss_1': 472.3545837402344,\n",
      " 'mean_episode_return_0': 69.83244323730469,\n",
      " 'mean_episode_return_1': 83.02001953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:37,912] After 3945600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 396.1219482421875,\n",
      " 'loss_1': 472.3545837402344,\n",
      " 'mean_episode_return_0': 69.83244323730469,\n",
      " 'mean_episode_return_1': 83.02001953125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:42,913] After 3948800 frames: @ 640.0 fps Stats:\n",
      "{'loss_0': 396.1219482421875,\n",
      " 'loss_1': 519.4532470703125,\n",
      " 'mean_episode_return_0': 69.83244323730469,\n",
      " 'mean_episode_return_1': 83.0191421508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:47,916] After 3952000 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 417.9217834472656,\n",
      " 'loss_1': 519.4532470703125,\n",
      " 'mean_episode_return_0': 69.786376953125,\n",
      " 'mean_episode_return_1': 83.0191421508789}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:52,920] After 3952000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.9217834472656,\n",
      " 'loss_1': 519.4532470703125,\n",
      " 'mean_episode_return_0': 69.786376953125,\n",
      " 'mean_episode_return_1': 83.0191421508789}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:45:57,925] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:45:58,018] After 3955200 frames: @ 627.7 fps Stats:\n",
      "{'loss_0': 417.9217834472656,\n",
      " 'loss_1': 517.55126953125,\n",
      " 'mean_episode_return_0': 69.786376953125,\n",
      " 'mean_episode_return_1': 83.05058288574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:03,024] After 3958400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 514.3993530273438,\n",
      " 'loss_1': 517.55126953125,\n",
      " 'mean_episode_return_0': 69.7802505493164,\n",
      " 'mean_episode_return_1': 83.05058288574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:08,029] After 3958400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 514.3993530273438,\n",
      " 'loss_1': 517.55126953125,\n",
      " 'mean_episode_return_0': 69.7802505493164,\n",
      " 'mean_episode_return_1': 83.05058288574219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:13,035] After 3961600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 514.3993530273438,\n",
      " 'loss_1': 501.32672119140625,\n",
      " 'mean_episode_return_0': 69.7802505493164,\n",
      " 'mean_episode_return_1': 83.00562286376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:18,041] After 3964800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 392.9974670410156,\n",
      " 'loss_1': 501.32672119140625,\n",
      " 'mean_episode_return_0': 69.79656219482422,\n",
      " 'mean_episode_return_1': 83.00562286376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:23,047] After 3964800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 392.9974670410156,\n",
      " 'loss_1': 501.32672119140625,\n",
      " 'mean_episode_return_0': 69.79656219482422,\n",
      " 'mean_episode_return_1': 83.00562286376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:28,053] After 3968000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 392.9974670410156,\n",
      " 'loss_1': 561.1010131835938,\n",
      " 'mean_episode_return_0': 69.79656219482422,\n",
      " 'mean_episode_return_1': 83.03538513183594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:33,055] After 3971200 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 469.4041748046875,\n",
      " 'loss_1': 561.1010131835938,\n",
      " 'mean_episode_return_0': 69.79093933105469,\n",
      " 'mean_episode_return_1': 83.03538513183594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:38,061] After 3971200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 469.4041748046875,\n",
      " 'loss_1': 561.1010131835938,\n",
      " 'mean_episode_return_0': 69.79093933105469,\n",
      " 'mean_episode_return_1': 83.03538513183594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:46:43,066] After 3977600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 395.894287109375,\n",
      " 'loss_1': 490.0771789550781,\n",
      " 'mean_episode_return_0': 69.76868438720703,\n",
      " 'mean_episode_return_1': 83.03913116455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:48,072] After 3977600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.894287109375,\n",
      " 'loss_1': 490.0771789550781,\n",
      " 'mean_episode_return_0': 69.76868438720703,\n",
      " 'mean_episode_return_1': 83.03913116455078}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:53,078] After 3977600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.894287109375,\n",
      " 'loss_1': 490.0771789550781,\n",
      " 'mean_episode_return_0': 69.76868438720703,\n",
      " 'mean_episode_return_1': 83.03913116455078}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:46:58,083] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:46:58,171] After 3984000 frames: @ 1256.9 fps Stats:\n",
      "{'loss_0': 402.58209228515625,\n",
      " 'loss_1': 577.8986206054688,\n",
      " 'mean_episode_return_0': 69.6936264038086,\n",
      " 'mean_episode_return_1': 83.02851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:03,177] After 3984000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.58209228515625,\n",
      " 'loss_1': 577.8986206054688,\n",
      " 'mean_episode_return_0': 69.6936264038086,\n",
      " 'mean_episode_return_1': 83.02851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:08,183] After 3984000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.58209228515625,\n",
      " 'loss_1': 577.8986206054688,\n",
      " 'mean_episode_return_0': 69.6936264038086,\n",
      " 'mean_episode_return_1': 83.02851104736328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:13,188] After 3990400 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 402.9127197265625,\n",
      " 'loss_1': 471.4504699707031,\n",
      " 'mean_episode_return_0': 69.6707534790039,\n",
      " 'mean_episode_return_1': 83.02588653564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:18,192] After 3990400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.9127197265625,\n",
      " 'loss_1': 471.4504699707031,\n",
      " 'mean_episode_return_0': 69.6707534790039,\n",
      " 'mean_episode_return_1': 83.02588653564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:23,198] After 3990400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 402.9127197265625,\n",
      " 'loss_1': 471.4504699707031,\n",
      " 'mean_episode_return_0': 69.6707534790039,\n",
      " 'mean_episode_return_1': 83.02588653564453}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:28,204] After 3996800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 371.4068298339844,\n",
      " 'loss_1': 456.54595947265625,\n",
      " 'mean_episode_return_0': 69.6571273803711,\n",
      " 'mean_episode_return_1': 83.0574951171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:33,209] After 3996800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 371.4068298339844,\n",
      " 'loss_1': 456.54595947265625,\n",
      " 'mean_episode_return_0': 69.6571273803711,\n",
      " 'mean_episode_return_1': 83.0574951171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:38,215] After 3996800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 371.4068298339844,\n",
      " 'loss_1': 456.54595947265625,\n",
      " 'mean_episode_return_0': 69.6571273803711,\n",
      " 'mean_episode_return_1': 83.0574951171875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:43,221] After 4003200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 375.1608581542969,\n",
      " 'loss_1': 551.0327758789062,\n",
      " 'mean_episode_return_0': 69.58775329589844,\n",
      " 'mean_episode_return_1': 83.11502075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:48,224] After 4003200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.1608581542969,\n",
      " 'loss_1': 551.0327758789062,\n",
      " 'mean_episode_return_0': 69.58775329589844,\n",
      " 'mean_episode_return_1': 83.11502075195312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:53,229] After 4003200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 375.1608581542969,\n",
      " 'loss_1': 551.0327758789062,\n",
      " 'mean_episode_return_0': 69.58775329589844,\n",
      " 'mean_episode_return_1': 83.11502075195312}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:47:58,233] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:47:58,339] After 4009600 frames: @ 1252.8 fps Stats:\n",
      "{'loss_0': 444.2307434082031,\n",
      " 'loss_1': 535.3621215820312,\n",
      " 'mean_episode_return_0': 69.58037567138672,\n",
      " 'mean_episode_return_1': 83.17189025878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:03,345] After 4009600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.2307434082031,\n",
      " 'loss_1': 535.3621215820312,\n",
      " 'mean_episode_return_0': 69.58037567138672,\n",
      " 'mean_episode_return_1': 83.17189025878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:08,348] After 4009600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.2307434082031,\n",
      " 'loss_1': 535.3621215820312,\n",
      " 'mean_episode_return_0': 69.58037567138672,\n",
      " 'mean_episode_return_1': 83.17189025878906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:13,354] After 4016000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 431.6197509765625,\n",
      " 'loss_1': 530.5455322265625,\n",
      " 'mean_episode_return_0': 69.5900650024414,\n",
      " 'mean_episode_return_1': 83.19546508789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:18,359] After 4016000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 431.6197509765625,\n",
      " 'loss_1': 530.5455322265625,\n",
      " 'mean_episode_return_0': 69.5900650024414,\n",
      " 'mean_episode_return_1': 83.19546508789062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:23,366] After 4019200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 431.6197509765625,\n",
      " 'loss_1': 556.4583740234375,\n",
      " 'mean_episode_return_0': 69.5900650024414,\n",
      " 'mean_episode_return_1': 83.16744995117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:28,371] After 4022400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 383.9605712890625,\n",
      " 'loss_1': 556.4583740234375,\n",
      " 'mean_episode_return_0': 69.58112335205078,\n",
      " 'mean_episode_return_1': 83.16744995117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:33,377] After 4022400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.9605712890625,\n",
      " 'loss_1': 556.4583740234375,\n",
      " 'mean_episode_return_0': 69.58112335205078,\n",
      " 'mean_episode_return_1': 83.16744995117188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:38,383] After 4028800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 380.4422912597656,\n",
      " 'loss_1': 536.8973388671875,\n",
      " 'mean_episode_return_0': 69.53412628173828,\n",
      " 'mean_episode_return_1': 83.15587615966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:43,389] After 4028800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 380.4422912597656,\n",
      " 'loss_1': 536.8973388671875,\n",
      " 'mean_episode_return_0': 69.53412628173828,\n",
      " 'mean_episode_return_1': 83.15587615966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:48,395] After 4028800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 380.4422912597656,\n",
      " 'loss_1': 536.8973388671875,\n",
      " 'mean_episode_return_0': 69.53412628173828,\n",
      " 'mean_episode_return_1': 83.15587615966797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:53,401] After 4035200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 467.19866943359375,\n",
      " 'loss_1': 564.8887329101562,\n",
      " 'mean_episode_return_0': 69.52481079101562,\n",
      " 'mean_episode_return_1': 83.16997528076172}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:48:58,407] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:48:58,496] After 4035200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 467.19866943359375,\n",
      " 'loss_1': 564.8887329101562,\n",
      " 'mean_episode_return_0': 69.52481079101562,\n",
      " 'mean_episode_return_1': 83.16997528076172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:03,502] After 4035200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 467.19866943359375,\n",
      " 'loss_1': 564.8887329101562,\n",
      " 'mean_episode_return_0': 69.52481079101562,\n",
      " 'mean_episode_return_1': 83.16997528076172}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:08,508] After 4041600 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 412.52178955078125,\n",
      " 'loss_1': 633.3709106445312,\n",
      " 'mean_episode_return_0': 69.52218627929688,\n",
      " 'mean_episode_return_1': 83.19609069824219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:13,514] After 4041600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 412.52178955078125,\n",
      " 'loss_1': 633.3709106445312,\n",
      " 'mean_episode_return_0': 69.52218627929688,\n",
      " 'mean_episode_return_1': 83.19609069824219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:18,516] After 4041600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 412.52178955078125,\n",
      " 'loss_1': 633.3709106445312,\n",
      " 'mean_episode_return_0': 69.52218627929688,\n",
      " 'mean_episode_return_1': 83.19609069824219}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:23,522] After 4048000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 417.79522705078125,\n",
      " 'loss_1': 430.1286315917969,\n",
      " 'mean_episode_return_0': 69.53087615966797,\n",
      " 'mean_episode_return_1': 83.15293884277344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:49:28,524] After 4048000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.79522705078125,\n",
      " 'loss_1': 430.1286315917969,\n",
      " 'mean_episode_return_0': 69.53087615966797,\n",
      " 'mean_episode_return_1': 83.15293884277344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:33,530] After 4048000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 417.79522705078125,\n",
      " 'loss_1': 430.1286315917969,\n",
      " 'mean_episode_return_0': 69.53087615966797,\n",
      " 'mean_episode_return_1': 83.15293884277344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:38,536] After 4054400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 499.78717041015625,\n",
      " 'loss_1': 636.1228637695312,\n",
      " 'mean_episode_return_0': 69.50749969482422,\n",
      " 'mean_episode_return_1': 83.13533782958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:43,540] After 4054400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 499.78717041015625,\n",
      " 'loss_1': 636.1228637695312,\n",
      " 'mean_episode_return_0': 69.50749969482422,\n",
      " 'mean_episode_return_1': 83.13533782958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:48,546] After 4054400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 499.78717041015625,\n",
      " 'loss_1': 636.1228637695312,\n",
      " 'mean_episode_return_0': 69.50749969482422,\n",
      " 'mean_episode_return_1': 83.13533782958984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:53,552] After 4060800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 404.5205383300781,\n",
      " 'loss_1': 543.7625122070312,\n",
      " 'mean_episode_return_0': 69.51119232177734,\n",
      " 'mean_episode_return_1': 83.09539031982422}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:49:58,558] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:49:58,646] After 4060800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.5205383300781,\n",
      " 'loss_1': 543.7625122070312,\n",
      " 'mean_episode_return_0': 69.51119232177734,\n",
      " 'mean_episode_return_1': 83.09539031982422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:03,652] After 4060800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.5205383300781,\n",
      " 'loss_1': 543.7625122070312,\n",
      " 'mean_episode_return_0': 69.51119232177734,\n",
      " 'mean_episode_return_1': 83.09539031982422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:08,658] After 4067200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 459.07421875,\n",
      " 'loss_1': 557.0629272460938,\n",
      " 'mean_episode_return_0': 69.43775177001953,\n",
      " 'mean_episode_return_1': 83.13753509521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:13,664] After 4067200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.07421875,\n",
      " 'loss_1': 557.0629272460938,\n",
      " 'mean_episode_return_0': 69.43775177001953,\n",
      " 'mean_episode_return_1': 83.13753509521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:18,670] After 4067200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.07421875,\n",
      " 'loss_1': 557.0629272460938,\n",
      " 'mean_episode_return_0': 69.43775177001953,\n",
      " 'mean_episode_return_1': 83.13753509521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:23,676] After 4073600 frames: @ 1278.8 fps Stats:\n",
      "{'loss_0': 427.035888671875,\n",
      " 'loss_1': 583.7116088867188,\n",
      " 'mean_episode_return_0': 69.39256286621094,\n",
      " 'mean_episode_return_1': 83.15000915527344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:28,682] After 4073600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 427.035888671875,\n",
      " 'loss_1': 583.7116088867188,\n",
      " 'mean_episode_return_0': 69.39256286621094,\n",
      " 'mean_episode_return_1': 83.15000915527344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:33,688] After 4076800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.244140625,\n",
      " 'loss_1': 583.7116088867188,\n",
      " 'mean_episode_return_0': 69.36912536621094,\n",
      " 'mean_episode_return_1': 83.15000915527344}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:38,694] After 4080000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.244140625,\n",
      " 'loss_1': 528.1411743164062,\n",
      " 'mean_episode_return_0': 69.36912536621094,\n",
      " 'mean_episode_return_1': 83.14964294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:43,700] After 4080000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.244140625,\n",
      " 'loss_1': 528.1411743164062,\n",
      " 'mean_episode_return_0': 69.36912536621094,\n",
      " 'mean_episode_return_1': 83.14964294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:48,705] After 4083200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 376.3550720214844,\n",
      " 'loss_1': 528.1411743164062,\n",
      " 'mean_episode_return_0': 69.35362243652344,\n",
      " 'mean_episode_return_1': 83.14964294433594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:53,708] After 4086400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 376.3550720214844,\n",
      " 'loss_1': 539.1039428710938,\n",
      " 'mean_episode_return_0': 69.35362243652344,\n",
      " 'mean_episode_return_1': 83.16873168945312}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:50:58,713] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:50:58,792] After 4086400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 376.3550720214844,\n",
      " 'loss_1': 539.1039428710938,\n",
      " 'mean_episode_return_0': 69.35362243652344,\n",
      " 'mean_episode_return_1': 83.16873168945312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:03,798] After 4089600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 386.0705871582031,\n",
      " 'loss_1': 539.1039428710938,\n",
      " 'mean_episode_return_0': 69.30106353759766,\n",
      " 'mean_episode_return_1': 83.16873168945312}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:08,804] After 4092800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 386.0705871582031,\n",
      " 'loss_1': 532.7762451171875,\n",
      " 'mean_episode_return_0': 69.30106353759766,\n",
      " 'mean_episode_return_1': 83.12013244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:13,810] After 4092800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.0705871582031,\n",
      " 'loss_1': 532.7762451171875,\n",
      " 'mean_episode_return_0': 69.30106353759766,\n",
      " 'mean_episode_return_1': 83.12013244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:18,816] After 4096000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 431.1799621582031,\n",
      " 'loss_1': 532.7762451171875,\n",
      " 'mean_episode_return_0': 69.34330749511719,\n",
      " 'mean_episode_return_1': 83.12013244628906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:23,821] After 4099200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 431.1799621582031,\n",
      " 'loss_1': 562.777587890625,\n",
      " 'mean_episode_return_0': 69.34330749511719,\n",
      " 'mean_episode_return_1': 83.084228515625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:28,827] After 4099200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 431.1799621582031,\n",
      " 'loss_1': 562.777587890625,\n",
      " 'mean_episode_return_0': 69.34330749511719,\n",
      " 'mean_episode_return_1': 83.084228515625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:33,833] After 4102400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 501.673828125,\n",
      " 'loss_1': 562.777587890625,\n",
      " 'mean_episode_return_0': 69.40699768066406,\n",
      " 'mean_episode_return_1': 83.084228515625}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:38,839] After 4105600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 501.673828125,\n",
      " 'loss_1': 582.960693359375,\n",
      " 'mean_episode_return_0': 69.40699768066406,\n",
      " 'mean_episode_return_1': 83.05438995361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:43,844] After 4105600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 501.673828125,\n",
      " 'loss_1': 582.960693359375,\n",
      " 'mean_episode_return_0': 69.40699768066406,\n",
      " 'mean_episode_return_1': 83.05438995361328}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:48,848] After 4112000 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 372.58990478515625,\n",
      " 'loss_1': 404.0924072265625,\n",
      " 'mean_episode_return_0': 69.3707504272461,\n",
      " 'mean_episode_return_1': 83.03142547607422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:53,855] After 4112000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.58990478515625,\n",
      " 'loss_1': 404.0924072265625,\n",
      " 'mean_episode_return_0': 69.3707504272461,\n",
      " 'mean_episode_return_1': 83.03142547607422}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:51:58,861] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:51:58,950] After 4112000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.58990478515625,\n",
      " 'loss_1': 404.0924072265625,\n",
      " 'mean_episode_return_0': 69.3707504272461,\n",
      " 'mean_episode_return_1': 83.03142547607422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:03,956] After 4118400 frames: @ 1278.9 fps Stats:\n",
      "{'loss_0': 405.6611633300781,\n",
      " 'loss_1': 509.7304992675781,\n",
      " 'mean_episode_return_0': 69.3446273803711,\n",
      " 'mean_episode_return_1': 82.95093536376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:08,960] After 4118400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.6611633300781,\n",
      " 'loss_1': 509.7304992675781,\n",
      " 'mean_episode_return_0': 69.3446273803711,\n",
      " 'mean_episode_return_1': 82.95093536376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:13,966] After 4121600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.6575012207031,\n",
      " 'loss_1': 509.7304992675781,\n",
      " 'mean_episode_return_0': 69.27037811279297,\n",
      " 'mean_episode_return_1': 82.95093536376953}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:52:18,972] After 4124800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 384.6575012207031,\n",
      " 'loss_1': 503.7451477050781,\n",
      " 'mean_episode_return_0': 69.27037811279297,\n",
      " 'mean_episode_return_1': 83.00657653808594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:23,978] After 4124800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.6575012207031,\n",
      " 'loss_1': 503.7451477050781,\n",
      " 'mean_episode_return_0': 69.27037811279297,\n",
      " 'mean_episode_return_1': 83.00657653808594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:28,980] After 4128000 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 445.0869140625,\n",
      " 'loss_1': 503.7451477050781,\n",
      " 'mean_episode_return_0': 69.2177505493164,\n",
      " 'mean_episode_return_1': 83.00657653808594}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:33,984] After 4131200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 445.0869140625,\n",
      " 'loss_1': 580.4701538085938,\n",
      " 'mean_episode_return_0': 69.2177505493164,\n",
      " 'mean_episode_return_1': 83.00202941894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:38,991] After 4131200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 445.0869140625,\n",
      " 'loss_1': 580.4701538085938,\n",
      " 'mean_episode_return_0': 69.2177505493164,\n",
      " 'mean_episode_return_1': 83.00202941894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:43,999] After 4134400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.34320068359375,\n",
      " 'loss_1': 580.4701538085938,\n",
      " 'mean_episode_return_0': 69.20362091064453,\n",
      " 'mean_episode_return_1': 83.00202941894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:49,004] After 4137600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.34320068359375,\n",
      " 'loss_1': 415.4865417480469,\n",
      " 'mean_episode_return_0': 69.20362091064453,\n",
      " 'mean_episode_return_1': 83.05427551269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:54,010] After 4137600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.34320068359375,\n",
      " 'loss_1': 415.4865417480469,\n",
      " 'mean_episode_return_0': 69.20362091064453,\n",
      " 'mean_episode_return_1': 83.05427551269531}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:52:59,016] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:52:59,106] After 4140800 frames: @ 628.1 fps Stats:\n",
      "{'loss_0': 454.4259033203125,\n",
      " 'loss_1': 415.4865417480469,\n",
      " 'mean_episode_return_0': 69.13919067382812,\n",
      " 'mean_episode_return_1': 83.05427551269531}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:04,112] After 4144000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 454.4259033203125,\n",
      " 'loss_1': 595.486572265625,\n",
      " 'mean_episode_return_0': 69.13919067382812,\n",
      " 'mean_episode_return_1': 83.11056518554688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:09,116] After 4144000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 454.4259033203125,\n",
      " 'loss_1': 595.486572265625,\n",
      " 'mean_episode_return_0': 69.13919067382812,\n",
      " 'mean_episode_return_1': 83.11056518554688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:14,122] After 4147200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 387.519287109375,\n",
      " 'loss_1': 595.486572265625,\n",
      " 'mean_episode_return_0': 69.122314453125,\n",
      " 'mean_episode_return_1': 83.11056518554688}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:19,123] After 4150400 frames: @ 640.0 fps Stats:\n",
      "{'loss_0': 387.519287109375,\n",
      " 'loss_1': 443.7204895019531,\n",
      " 'mean_episode_return_0': 69.122314453125,\n",
      " 'mean_episode_return_1': 83.14925384521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:24,128] After 4150400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 387.519287109375,\n",
      " 'loss_1': 443.7204895019531,\n",
      " 'mean_episode_return_0': 69.122314453125,\n",
      " 'mean_episode_return_1': 83.14925384521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:29,132] After 4153600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 473.18585205078125,\n",
      " 'loss_1': 443.7204895019531,\n",
      " 'mean_episode_return_0': 69.08474731445312,\n",
      " 'mean_episode_return_1': 83.14925384521484}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:34,136] After 4156800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 473.18585205078125,\n",
      " 'loss_1': 520.23828125,\n",
      " 'mean_episode_return_0': 69.08474731445312,\n",
      " 'mean_episode_return_1': 83.19969177246094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:39,142] After 4156800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 473.18585205078125,\n",
      " 'loss_1': 520.23828125,\n",
      " 'mean_episode_return_0': 69.08474731445312,\n",
      " 'mean_episode_return_1': 83.19969177246094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:44,147] After 4160000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 423.6602783203125,\n",
      " 'loss_1': 520.23828125,\n",
      " 'mean_episode_return_0': 69.0340576171875,\n",
      " 'mean_episode_return_1': 83.19969177246094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:49,153] After 4163200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 423.6602783203125,\n",
      " 'loss_1': 585.0142211914062,\n",
      " 'mean_episode_return_0': 69.0340576171875,\n",
      " 'mean_episode_return_1': 83.2234878540039}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:54,160] After 4163200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 423.6602783203125,\n",
      " 'loss_1': 585.0142211914062,\n",
      " 'mean_episode_return_0': 69.0340576171875,\n",
      " 'mean_episode_return_1': 83.2234878540039}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:53:59,164] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:53:59,264] After 4166400 frames: @ 627.0 fps Stats:\n",
      "{'loss_0': 425.9387512207031,\n",
      " 'loss_1': 585.0142211914062,\n",
      " 'mean_episode_return_0': 69.02912139892578,\n",
      " 'mean_episode_return_1': 83.2234878540039}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:04,270] After 4169600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 425.9387512207031,\n",
      " 'loss_1': 501.43292236328125,\n",
      " 'mean_episode_return_0': 69.02912139892578,\n",
      " 'mean_episode_return_1': 83.25335693359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:09,276] After 4172800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 444.1387939453125,\n",
      " 'loss_1': 501.43292236328125,\n",
      " 'mean_episode_return_0': 68.99087524414062,\n",
      " 'mean_episode_return_1': 83.25335693359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:14,282] After 4172800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.1387939453125,\n",
      " 'loss_1': 501.43292236328125,\n",
      " 'mean_episode_return_0': 68.99087524414062,\n",
      " 'mean_episode_return_1': 83.25335693359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:19,288] After 4176000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 444.1387939453125,\n",
      " 'loss_1': 483.0910949707031,\n",
      " 'mean_episode_return_0': 68.99087524414062,\n",
      " 'mean_episode_return_1': 83.2542495727539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:24,292] After 4179200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 395.80560302734375,\n",
      " 'loss_1': 483.0910949707031,\n",
      " 'mean_episode_return_0': 68.95111846923828,\n",
      " 'mean_episode_return_1': 83.2542495727539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:29,298] After 4179200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 395.80560302734375,\n",
      " 'loss_1': 483.0910949707031,\n",
      " 'mean_episode_return_0': 68.95111846923828,\n",
      " 'mean_episode_return_1': 83.2542495727539}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:34,304] After 4182400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 395.80560302734375,\n",
      " 'loss_1': 561.5414428710938,\n",
      " 'mean_episode_return_0': 68.95111846923828,\n",
      " 'mean_episode_return_1': 83.24771118164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:39,306] After 4185600 frames: @ 639.9 fps Stats:\n",
      "{'loss_0': 387.24365234375,\n",
      " 'loss_1': 561.5414428710938,\n",
      " 'mean_episode_return_0': 68.96611785888672,\n",
      " 'mean_episode_return_1': 83.24771118164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:44,308] After 4185600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 387.24365234375,\n",
      " 'loss_1': 561.5414428710938,\n",
      " 'mean_episode_return_0': 68.96611785888672,\n",
      " 'mean_episode_return_1': 83.24771118164062}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:49,312] After 4188800 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 387.24365234375,\n",
      " 'loss_1': 506.50885009765625,\n",
      " 'mean_episode_return_0': 68.96611785888672,\n",
      " 'mean_episode_return_1': 83.24212646484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:54,318] After 4192000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 429.239990234375,\n",
      " 'loss_1': 506.50885009765625,\n",
      " 'mean_episode_return_0': 68.94843292236328,\n",
      " 'mean_episode_return_1': 83.24212646484375}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:54:59,324] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:54:59,412] After 4192000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 429.239990234375,\n",
      " 'loss_1': 506.50885009765625,\n",
      " 'mean_episode_return_0': 68.94843292236328,\n",
      " 'mean_episode_return_1': 83.24212646484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:04,416] After 4195200 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 429.239990234375,\n",
      " 'loss_1': 534.6259765625,\n",
      " 'mean_episode_return_0': 68.94843292236328,\n",
      " 'mean_episode_return_1': 83.2784194946289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 14:55:09,422] After 4198400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 491.5988464355469,\n",
      " 'loss_1': 534.6259765625,\n",
      " 'mean_episode_return_0': 68.92743682861328,\n",
      " 'mean_episode_return_1': 83.2784194946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:14,428] After 4198400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 491.5988464355469,\n",
      " 'loss_1': 534.6259765625,\n",
      " 'mean_episode_return_0': 68.92743682861328,\n",
      " 'mean_episode_return_1': 83.2784194946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:19,434] After 4201600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 491.5988464355469,\n",
      " 'loss_1': 475.7994384765625,\n",
      " 'mean_episode_return_0': 68.92743682861328,\n",
      " 'mean_episode_return_1': 83.24222564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:24,440] After 4204800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.98968505859375,\n",
      " 'loss_1': 475.7994384765625,\n",
      " 'mean_episode_return_0': 68.91637420654297,\n",
      " 'mean_episode_return_1': 83.24222564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:29,446] After 4204800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 401.98968505859375,\n",
      " 'loss_1': 475.7994384765625,\n",
      " 'mean_episode_return_0': 68.91637420654297,\n",
      " 'mean_episode_return_1': 83.24222564697266}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:34,452] After 4208000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 401.98968505859375,\n",
      " 'loss_1': 508.3684387207031,\n",
      " 'mean_episode_return_0': 68.91637420654297,\n",
      " 'mean_episode_return_1': 83.25811004638672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:39,458] After 4211200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 353.329833984375,\n",
      " 'loss_1': 508.3684387207031,\n",
      " 'mean_episode_return_0': 68.9233169555664,\n",
      " 'mean_episode_return_1': 83.25811004638672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:44,463] After 4211200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 353.329833984375,\n",
      " 'loss_1': 508.3684387207031,\n",
      " 'mean_episode_return_0': 68.9233169555664,\n",
      " 'mean_episode_return_1': 83.25811004638672}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:49,469] After 4217600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 448.70452880859375,\n",
      " 'loss_1': 475.8211364746094,\n",
      " 'mean_episode_return_0': 68.86930847167969,\n",
      " 'mean_episode_return_1': 83.26403045654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:54,475] After 4217600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 448.70452880859375,\n",
      " 'loss_1': 475.8211364746094,\n",
      " 'mean_episode_return_0': 68.86930847167969,\n",
      " 'mean_episode_return_1': 83.26403045654297}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:55:59,481] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:55:59,578] After 4220800 frames: @ 627.2 fps Stats:\n",
      "{'loss_0': 448.70452880859375,\n",
      " 'loss_1': 545.9406127929688,\n",
      " 'mean_episode_return_0': 68.86930847167969,\n",
      " 'mean_episode_return_1': 83.28624725341797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:04,584] After 4224000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 504.404296875,\n",
      " 'loss_1': 545.9406127929688,\n",
      " 'mean_episode_return_0': 68.86244201660156,\n",
      " 'mean_episode_return_1': 83.28624725341797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:09,588] After 4224000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 504.404296875,\n",
      " 'loss_1': 545.9406127929688,\n",
      " 'mean_episode_return_0': 68.86244201660156,\n",
      " 'mean_episode_return_1': 83.28624725341797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:14,592] After 4224000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 504.404296875,\n",
      " 'loss_1': 545.9406127929688,\n",
      " 'mean_episode_return_0': 68.86244201660156,\n",
      " 'mean_episode_return_1': 83.28624725341797}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:19,598] After 4230400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 386.6648254394531,\n",
      " 'loss_1': 448.0224914550781,\n",
      " 'mean_episode_return_0': 68.83943939208984,\n",
      " 'mean_episode_return_1': 83.23873138427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:24,600] After 4230400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 386.6648254394531,\n",
      " 'loss_1': 448.0224914550781,\n",
      " 'mean_episode_return_0': 68.83943939208984,\n",
      " 'mean_episode_return_1': 83.23873138427734}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:29,604] After 4233600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 386.6648254394531,\n",
      " 'loss_1': 561.6558837890625,\n",
      " 'mean_episode_return_0': 68.83943939208984,\n",
      " 'mean_episode_return_1': 83.27293395996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:34,610] After 4236800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 396.6102294921875,\n",
      " 'loss_1': 561.6558837890625,\n",
      " 'mean_episode_return_0': 68.81212615966797,\n",
      " 'mean_episode_return_1': 83.27293395996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:39,616] After 4236800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 396.6102294921875,\n",
      " 'loss_1': 561.6558837890625,\n",
      " 'mean_episode_return_0': 68.81212615966797,\n",
      " 'mean_episode_return_1': 83.27293395996094}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:44,622] After 4240000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 396.6102294921875,\n",
      " 'loss_1': 505.7156066894531,\n",
      " 'mean_episode_return_0': 68.81212615966797,\n",
      " 'mean_episode_return_1': 83.31593322753906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:49,628] After 4243200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 459.44757080078125,\n",
      " 'loss_1': 505.7156066894531,\n",
      " 'mean_episode_return_0': 68.79106140136719,\n",
      " 'mean_episode_return_1': 83.31593322753906}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:54,632] After 4243200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 459.44757080078125,\n",
      " 'loss_1': 505.7156066894531,\n",
      " 'mean_episode_return_0': 68.79106140136719,\n",
      " 'mean_episode_return_1': 83.31593322753906}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:56:59,637] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:56:59,729] After 4246400 frames: @ 627.8 fps Stats:\n",
      "{'loss_0': 459.44757080078125,\n",
      " 'loss_1': 527.38037109375,\n",
      " 'mean_episode_return_0': 68.79106140136719,\n",
      " 'mean_episode_return_1': 83.333740234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:04,735] After 4249600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.1143798828125,\n",
      " 'loss_1': 527.38037109375,\n",
      " 'mean_episode_return_0': 68.79949951171875,\n",
      " 'mean_episode_return_1': 83.333740234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:09,741] After 4249600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 374.1143798828125,\n",
      " 'loss_1': 527.38037109375,\n",
      " 'mean_episode_return_0': 68.79949951171875,\n",
      " 'mean_episode_return_1': 83.333740234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:14,747] After 4252800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 374.1143798828125,\n",
      " 'loss_1': 590.5895385742188,\n",
      " 'mean_episode_return_0': 68.79949951171875,\n",
      " 'mean_episode_return_1': 83.3350830078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:19,753] After 4256000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.35589599609375,\n",
      " 'loss_1': 590.5895385742188,\n",
      " 'mean_episode_return_0': 68.78843688964844,\n",
      " 'mean_episode_return_1': 83.3350830078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:24,759] After 4256000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 389.35589599609375,\n",
      " 'loss_1': 590.5895385742188,\n",
      " 'mean_episode_return_0': 68.78843688964844,\n",
      " 'mean_episode_return_1': 83.3350830078125}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:29,765] After 4259200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 389.35589599609375,\n",
      " 'loss_1': 450.5556945800781,\n",
      " 'mean_episode_return_0': 68.78843688964844,\n",
      " 'mean_episode_return_1': 83.3328857421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:34,768] After 4262400 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 438.0314025878906,\n",
      " 'loss_1': 450.5556945800781,\n",
      " 'mean_episode_return_0': 68.81387329101562,\n",
      " 'mean_episode_return_1': 83.3328857421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:39,774] After 4262400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 438.0314025878906,\n",
      " 'loss_1': 450.5556945800781,\n",
      " 'mean_episode_return_0': 68.81387329101562,\n",
      " 'mean_episode_return_1': 83.3328857421875}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:44,780] After 4265600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 438.0314025878906,\n",
      " 'loss_1': 571.8870239257812,\n",
      " 'mean_episode_return_0': 68.81387329101562,\n",
      " 'mean_episode_return_1': 83.28719329833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:49,784] After 4268800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 453.1714782714844,\n",
      " 'loss_1': 571.8870239257812,\n",
      " 'mean_episode_return_0': 68.83512878417969,\n",
      " 'mean_episode_return_1': 83.28719329833984}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:54,789] After 4268800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 453.1714782714844,\n",
      " 'loss_1': 571.8870239257812,\n",
      " 'mean_episode_return_0': 68.83512878417969,\n",
      " 'mean_episode_return_1': 83.28719329833984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:335 2022-09-17 14:57:59,795] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:57:59,889] After 4275200 frames: @ 1255.2 fps Stats:\n",
      "{'loss_0': 383.1789855957031,\n",
      " 'loss_1': 572.2518920898438,\n",
      " 'mean_episode_return_0': 68.82237243652344,\n",
      " 'mean_episode_return_1': 83.24480438232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:04,892] After 4275200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.1789855957031,\n",
      " 'loss_1': 572.2518920898438,\n",
      " 'mean_episode_return_0': 68.82237243652344,\n",
      " 'mean_episode_return_1': 83.24480438232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:09,898] After 4275200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 383.1789855957031,\n",
      " 'loss_1': 572.2518920898438,\n",
      " 'mean_episode_return_0': 68.82237243652344,\n",
      " 'mean_episode_return_1': 83.24480438232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:14,903] After 4281600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 451.47216796875,\n",
      " 'loss_1': 514.0507202148438,\n",
      " 'mean_episode_return_0': 68.86687469482422,\n",
      " 'mean_episode_return_1': 83.18743896484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:19,910] After 4281600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.47216796875,\n",
      " 'loss_1': 514.0507202148438,\n",
      " 'mean_episode_return_0': 68.86687469482422,\n",
      " 'mean_episode_return_1': 83.18743896484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:24,915] After 4281600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 451.47216796875,\n",
      " 'loss_1': 514.0507202148438,\n",
      " 'mean_episode_return_0': 68.86687469482422,\n",
      " 'mean_episode_return_1': 83.18743896484375}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:29,921] After 4288000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 502.8270263671875,\n",
      " 'loss_1': 492.74139404296875,\n",
      " 'mean_episode_return_0': 68.91812133789062,\n",
      " 'mean_episode_return_1': 83.18157196044922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:34,927] After 4288000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 502.8270263671875,\n",
      " 'loss_1': 492.74139404296875,\n",
      " 'mean_episode_return_0': 68.91812133789062,\n",
      " 'mean_episode_return_1': 83.18157196044922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:39,933] After 4288000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 502.8270263671875,\n",
      " 'loss_1': 492.74139404296875,\n",
      " 'mean_episode_return_0': 68.91812133789062,\n",
      " 'mean_episode_return_1': 83.18157196044922}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:44,936] After 4294400 frames: @ 1279.5 fps Stats:\n",
      "{'loss_0': 437.5467834472656,\n",
      " 'loss_1': 476.9008483886719,\n",
      " 'mean_episode_return_0': 68.92481231689453,\n",
      " 'mean_episode_return_1': 83.19762420654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:49,942] After 4294400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.5467834472656,\n",
      " 'loss_1': 476.9008483886719,\n",
      " 'mean_episode_return_0': 68.92481231689453,\n",
      " 'mean_episode_return_1': 83.19762420654297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:58:54,948] After 4294400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 437.5467834472656,\n",
      " 'loss_1': 476.9008483886719,\n",
      " 'mean_episode_return_0': 68.92481231689453,\n",
      " 'mean_episode_return_1': 83.19762420654297}\n",
      "[INFO:417825 trainer:335 2022-09-17 14:58:59,954] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:00,050] After 4300800 frames: @ 1254.8 fps Stats:\n",
      "{'loss_0': 405.0926208496094,\n",
      " 'loss_1': 451.5455322265625,\n",
      " 'mean_episode_return_0': 68.93787384033203,\n",
      " 'mean_episode_return_1': 83.19967651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:05,052] After 4300800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.0926208496094,\n",
      " 'loss_1': 451.5455322265625,\n",
      " 'mean_episode_return_0': 68.93787384033203,\n",
      " 'mean_episode_return_1': 83.19967651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:10,058] After 4300800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 405.0926208496094,\n",
      " 'loss_1': 451.5455322265625,\n",
      " 'mean_episode_return_0': 68.93787384033203,\n",
      " 'mean_episode_return_1': 83.19967651367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:15,063] After 4307200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 351.7442932128906,\n",
      " 'loss_1': 516.1815795898438,\n",
      " 'mean_episode_return_0': 68.97663116455078,\n",
      " 'mean_episode_return_1': 83.16881561279297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:20,069] After 4307200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 351.7442932128906,\n",
      " 'loss_1': 516.1815795898438,\n",
      " 'mean_episode_return_0': 68.97663116455078,\n",
      " 'mean_episode_return_1': 83.16881561279297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:25,072] After 4307200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 351.7442932128906,\n",
      " 'loss_1': 516.1815795898438,\n",
      " 'mean_episode_return_0': 68.97663116455078,\n",
      " 'mean_episode_return_1': 83.16881561279297}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:30,077] After 4313600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 384.720703125,\n",
      " 'loss_1': 480.4418640136719,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.21656036376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:35,083] After 4313600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.720703125,\n",
      " 'loss_1': 480.4418640136719,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.21656036376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:40,089] After 4313600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 384.720703125,\n",
      " 'loss_1': 480.4418640136719,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.21656036376953}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:45,095] After 4320000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 444.183349609375,\n",
      " 'loss_1': 577.3103637695312,\n",
      " 'mean_episode_return_0': 68.9322509765625,\n",
      " 'mean_episode_return_1': 83.19405364990234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:50,101] After 4320000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.183349609375,\n",
      " 'loss_1': 577.3103637695312,\n",
      " 'mean_episode_return_0': 68.9322509765625,\n",
      " 'mean_episode_return_1': 83.19405364990234}\n",
      "[INFO:417825 trainer:367 2022-09-17 14:59:55,107] After 4320000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 444.183349609375,\n",
      " 'loss_1': 577.3103637695312,\n",
      " 'mean_episode_return_0': 68.9322509765625,\n",
      " 'mean_episode_return_1': 83.19405364990234}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:00:00,113] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:00,205] After 4326400 frames: @ 1255.6 fps Stats:\n",
      "{'loss_0': 416.7646789550781,\n",
      " 'loss_1': 529.7528686523438,\n",
      " 'mean_episode_return_0': 68.94824981689453,\n",
      " 'mean_episode_return_1': 83.1795883178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:05,211] After 4326400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.7646789550781,\n",
      " 'loss_1': 529.7528686523438,\n",
      " 'mean_episode_return_0': 68.94824981689453,\n",
      " 'mean_episode_return_1': 83.1795883178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:10,217] After 4326400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 416.7646789550781,\n",
      " 'loss_1': 529.7528686523438,\n",
      " 'mean_episode_return_0': 68.94824981689453,\n",
      " 'mean_episode_return_1': 83.1795883178711}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:15,221] After 4332800 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 398.12615966796875,\n",
      " 'loss_1': 542.103759765625,\n",
      " 'mean_episode_return_0': 68.94306182861328,\n",
      " 'mean_episode_return_1': 83.17310333251953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:20,227] After 4332800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 398.12615966796875,\n",
      " 'loss_1': 542.103759765625,\n",
      " 'mean_episode_return_0': 68.94306182861328,\n",
      " 'mean_episode_return_1': 83.17310333251953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:25,232] After 4336000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 342.65374755859375,\n",
      " 'loss_1': 542.103759765625,\n",
      " 'mean_episode_return_0': 69.00093841552734,\n",
      " 'mean_episode_return_1': 83.17310333251953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:30,238] After 4339200 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 342.65374755859375,\n",
      " 'loss_1': 406.25714111328125,\n",
      " 'mean_episode_return_0': 69.00093841552734,\n",
      " 'mean_episode_return_1': 83.211181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:35,244] After 4339200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 342.65374755859375,\n",
      " 'loss_1': 406.25714111328125,\n",
      " 'mean_episode_return_0': 69.00093841552734,\n",
      " 'mean_episode_return_1': 83.211181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:40,248] After 4342400 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 520.9799194335938,\n",
      " 'loss_1': 406.25714111328125,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.211181640625}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:45,253] After 4345600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 520.9799194335938,\n",
      " 'loss_1': 494.9578857421875,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.23928833007812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 15:00:50,256] After 4345600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 520.9799194335938,\n",
      " 'loss_1': 494.9578857421875,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.23928833007812}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:00:55,262] After 4348800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 479.4306945800781,\n",
      " 'loss_1': 494.9578857421875,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.23928833007812}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:01:00,265] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:00,350] After 4352000 frames: @ 629.0 fps Stats:\n",
      "{'loss_0': 479.4306945800781,\n",
      " 'loss_1': 496.7416687011719,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.20183563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:05,356] After 4352000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 479.4306945800781,\n",
      " 'loss_1': 496.7416687011719,\n",
      " 'mean_episode_return_0': 68.99800109863281,\n",
      " 'mean_episode_return_1': 83.20183563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:10,362] After 4355200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.11077880859375,\n",
      " 'loss_1': 496.7416687011719,\n",
      " 'mean_episode_return_0': 68.93043518066406,\n",
      " 'mean_episode_return_1': 83.20183563232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:15,368] After 4358400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.11077880859375,\n",
      " 'loss_1': 509.6692199707031,\n",
      " 'mean_episode_return_0': 68.93043518066406,\n",
      " 'mean_episode_return_1': 83.26140594482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:20,372] After 4358400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.11077880859375,\n",
      " 'loss_1': 509.6692199707031,\n",
      " 'mean_episode_return_0': 68.93043518066406,\n",
      " 'mean_episode_return_1': 83.26140594482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:25,378] After 4361600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.745849609375,\n",
      " 'loss_1': 509.6692199707031,\n",
      " 'mean_episode_return_0': 68.90706634521484,\n",
      " 'mean_episode_return_1': 83.26140594482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:30,384] After 4364800 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 404.745849609375,\n",
      " 'loss_1': 602.3550415039062,\n",
      " 'mean_episode_return_0': 68.90706634521484,\n",
      " 'mean_episode_return_1': 83.28665924072266}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:35,390] After 4364800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.745849609375,\n",
      " 'loss_1': 602.3550415039062,\n",
      " 'mean_episode_return_0': 68.90706634521484,\n",
      " 'mean_episode_return_1': 83.28665924072266}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:40,396] After 4371200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 421.4773864746094,\n",
      " 'loss_1': 508.51177978515625,\n",
      " 'mean_episode_return_0': 68.93575286865234,\n",
      " 'mean_episode_return_1': 83.27378845214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:45,401] After 4371200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 421.4773864746094,\n",
      " 'loss_1': 508.51177978515625,\n",
      " 'mean_episode_return_0': 68.93575286865234,\n",
      " 'mean_episode_return_1': 83.27378845214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:50,407] After 4374400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 409.8891296386719,\n",
      " 'loss_1': 508.51177978515625,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.27378845214844}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:01:55,413] After 4377600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 409.8891296386719,\n",
      " 'loss_1': 443.5785827636719,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.29851531982422}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:02:00,419] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:00,499] After 4377600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 409.8891296386719,\n",
      " 'loss_1': 443.5785827636719,\n",
      " 'mean_episode_return_0': 68.93769073486328,\n",
      " 'mean_episode_return_1': 83.29851531982422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:05,505] After 4380800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 455.42584228515625,\n",
      " 'loss_1': 443.5785827636719,\n",
      " 'mean_episode_return_0': 68.9573745727539,\n",
      " 'mean_episode_return_1': 83.29851531982422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:10,511] After 4384000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 455.42584228515625,\n",
      " 'loss_1': 480.43212890625,\n",
      " 'mean_episode_return_0': 68.9573745727539,\n",
      " 'mean_episode_return_1': 83.2999038696289}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:15,516] After 4384000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 455.42584228515625,\n",
      " 'loss_1': 480.43212890625,\n",
      " 'mean_episode_return_0': 68.9573745727539,\n",
      " 'mean_episode_return_1': 83.2999038696289}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:20,522] After 4387200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 486.74249267578125,\n",
      " 'loss_1': 480.43212890625,\n",
      " 'mean_episode_return_0': 68.94937133789062,\n",
      " 'mean_episode_return_1': 83.2999038696289}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:25,528] After 4390400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 486.74249267578125,\n",
      " 'loss_1': 502.6272277832031,\n",
      " 'mean_episode_return_0': 68.94937133789062,\n",
      " 'mean_episode_return_1': 83.29207611083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:30,534] After 4393600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.34783935546875,\n",
      " 'loss_1': 502.6272277832031,\n",
      " 'mean_episode_return_0': 68.97706604003906,\n",
      " 'mean_episode_return_1': 83.29207611083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:35,540] After 4393600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 397.34783935546875,\n",
      " 'loss_1': 502.6272277832031,\n",
      " 'mean_episode_return_0': 68.97706604003906,\n",
      " 'mean_episode_return_1': 83.29207611083984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:40,546] After 4396800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 397.34783935546875,\n",
      " 'loss_1': 547.8582763671875,\n",
      " 'mean_episode_return_0': 68.97706604003906,\n",
      " 'mean_episode_return_1': 83.30134582519531}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:45,552] After 4400000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 539.178955078125,\n",
      " 'loss_1': 547.8582763671875,\n",
      " 'mean_episode_return_0': 68.9591293334961,\n",
      " 'mean_episode_return_1': 83.30134582519531}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:50,558] After 4403200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 539.178955078125,\n",
      " 'loss_1': 467.999755859375,\n",
      " 'mean_episode_return_0': 68.9591293334961,\n",
      " 'mean_episode_return_1': 83.33855438232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:02:55,564] After 4403200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 539.178955078125,\n",
      " 'loss_1': 467.999755859375,\n",
      " 'mean_episode_return_0': 68.9591293334961,\n",
      " 'mean_episode_return_1': 83.33855438232422}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:03:00,570] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:00,645] After 4406400 frames: @ 629.9 fps Stats:\n",
      "{'loss_0': 404.8055725097656,\n",
      " 'loss_1': 467.999755859375,\n",
      " 'mean_episode_return_0': 68.94793701171875,\n",
      " 'mean_episode_return_1': 83.33855438232422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:05,651] After 4409600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 404.8055725097656,\n",
      " 'loss_1': 484.17327880859375,\n",
      " 'mean_episode_return_0': 68.94793701171875,\n",
      " 'mean_episode_return_1': 83.29273223876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:10,657] After 4409600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 404.8055725097656,\n",
      " 'loss_1': 484.17327880859375,\n",
      " 'mean_episode_return_0': 68.94793701171875,\n",
      " 'mean_episode_return_1': 83.29273223876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:15,662] After 4412800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 399.4638977050781,\n",
      " 'loss_1': 484.17327880859375,\n",
      " 'mean_episode_return_0': 68.93262481689453,\n",
      " 'mean_episode_return_1': 83.29273223876953}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:20,668] After 4416000 frames: @ 639.5 fps Stats:\n",
      "{'loss_0': 399.4638977050781,\n",
      " 'loss_1': 737.1671142578125,\n",
      " 'mean_episode_return_0': 68.93262481689453,\n",
      " 'mean_episode_return_1': 83.3604965209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:25,672] After 4416000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 399.4638977050781,\n",
      " 'loss_1': 737.1671142578125,\n",
      " 'mean_episode_return_0': 68.93262481689453,\n",
      " 'mean_episode_return_1': 83.3604965209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:30,676] After 4419200 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 438.5535888671875,\n",
      " 'loss_1': 737.1671142578125,\n",
      " 'mean_episode_return_0': 68.93562316894531,\n",
      " 'mean_episode_return_1': 83.3604965209961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 15:03:35,680] After 4419200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 438.5535888671875,\n",
      " 'loss_1': 737.1671142578125,\n",
      " 'mean_episode_return_0': 68.93562316894531,\n",
      " 'mean_episode_return_1': 83.3604965209961}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:40,684] After 4422400 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 438.5535888671875,\n",
      " 'loss_1': 479.75030517578125,\n",
      " 'mean_episode_return_0': 68.93562316894531,\n",
      " 'mean_episode_return_1': 83.31183624267578}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:45,690] After 4425600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 415.9787292480469,\n",
      " 'loss_1': 479.75030517578125,\n",
      " 'mean_episode_return_0': 68.9390640258789,\n",
      " 'mean_episode_return_1': 83.31183624267578}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:50,692] After 4425600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 415.9787292480469,\n",
      " 'loss_1': 479.75030517578125,\n",
      " 'mean_episode_return_0': 68.9390640258789,\n",
      " 'mean_episode_return_1': 83.31183624267578}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:03:55,698] After 4428800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 415.9787292480469,\n",
      " 'loss_1': 435.8148193359375,\n",
      " 'mean_episode_return_0': 68.9390640258789,\n",
      " 'mean_episode_return_1': 83.30171966552734}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:04:00,703] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:00,825] After 4432000 frames: @ 624.2 fps Stats:\n",
      "{'loss_0': 388.08709716796875,\n",
      " 'loss_1': 435.8148193359375,\n",
      " 'mean_episode_return_0': 68.93799591064453,\n",
      " 'mean_episode_return_1': 83.30171966552734}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:05,828] After 4432000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 388.08709716796875,\n",
      " 'loss_1': 435.8148193359375,\n",
      " 'mean_episode_return_0': 68.93799591064453,\n",
      " 'mean_episode_return_1': 83.30171966552734}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:10,834] After 4435200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 388.08709716796875,\n",
      " 'loss_1': 529.966552734375,\n",
      " 'mean_episode_return_0': 68.93799591064453,\n",
      " 'mean_episode_return_1': 83.34661865234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:15,839] After 4438400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 456.51312255859375,\n",
      " 'loss_1': 529.966552734375,\n",
      " 'mean_episode_return_0': 68.93819427490234,\n",
      " 'mean_episode_return_1': 83.34661865234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:20,844] After 4438400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 456.51312255859375,\n",
      " 'loss_1': 529.966552734375,\n",
      " 'mean_episode_return_0': 68.93819427490234,\n",
      " 'mean_episode_return_1': 83.34661865234375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:25,852] After 4441600 frames: @ 639.3 fps Stats:\n",
      "{'loss_0': 456.51312255859375,\n",
      " 'loss_1': 497.0895080566406,\n",
      " 'mean_episode_return_0': 68.93819427490234,\n",
      " 'mean_episode_return_1': 83.38616943359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:30,856] After 4444800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 372.9205627441406,\n",
      " 'loss_1': 497.0895080566406,\n",
      " 'mean_episode_return_0': 68.92893981933594,\n",
      " 'mean_episode_return_1': 83.38616943359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:35,862] After 4444800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 372.9205627441406,\n",
      " 'loss_1': 497.0895080566406,\n",
      " 'mean_episode_return_0': 68.92893981933594,\n",
      " 'mean_episode_return_1': 83.38616943359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:40,868] After 4448000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 372.9205627441406,\n",
      " 'loss_1': 462.2521667480469,\n",
      " 'mean_episode_return_0': 68.92893981933594,\n",
      " 'mean_episode_return_1': 83.33879089355469}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:45,874] After 4451200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 448.988037109375,\n",
      " 'loss_1': 462.2521667480469,\n",
      " 'mean_episode_return_0': 68.83025360107422,\n",
      " 'mean_episode_return_1': 83.33879089355469}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:50,880] After 4454400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 448.988037109375,\n",
      " 'loss_1': 511.8135070800781,\n",
      " 'mean_episode_return_0': 68.83025360107422,\n",
      " 'mean_episode_return_1': 83.39421844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:04:55,886] After 4454400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 448.988037109375,\n",
      " 'loss_1': 511.8135070800781,\n",
      " 'mean_episode_return_0': 68.83025360107422,\n",
      " 'mean_episode_return_1': 83.39421844482422}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:05:00,888] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:00,976] After 4457600 frames: @ 628.7 fps Stats:\n",
      "{'loss_0': 464.06927490234375,\n",
      " 'loss_1': 511.8135070800781,\n",
      " 'mean_episode_return_0': 68.81568908691406,\n",
      " 'mean_episode_return_1': 83.39421844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:05,982] After 4457600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 464.06927490234375,\n",
      " 'loss_1': 511.8135070800781,\n",
      " 'mean_episode_return_0': 68.81568908691406,\n",
      " 'mean_episode_return_1': 83.39421844482422}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:10,988] After 4464000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 484.75933837890625,\n",
      " 'loss_1': 465.39599609375,\n",
      " 'mean_episode_return_0': 68.81743621826172,\n",
      " 'mean_episode_return_1': 83.40179443359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:15,992] After 4464000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 484.75933837890625,\n",
      " 'loss_1': 465.39599609375,\n",
      " 'mean_episode_return_0': 68.81743621826172,\n",
      " 'mean_episode_return_1': 83.40179443359375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:20,998] After 4467200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 484.75933837890625,\n",
      " 'loss_1': 460.76373291015625,\n",
      " 'mean_episode_return_0': 68.81743621826172,\n",
      " 'mean_episode_return_1': 83.41087341308594}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:26,003] After 4470400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 448.8330383300781,\n",
      " 'loss_1': 460.76373291015625,\n",
      " 'mean_episode_return_0': 68.83756256103516,\n",
      " 'mean_episode_return_1': 83.41087341308594}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:31,008] After 4470400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 448.8330383300781,\n",
      " 'loss_1': 460.76373291015625,\n",
      " 'mean_episode_return_0': 68.83756256103516,\n",
      " 'mean_episode_return_1': 83.41087341308594}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:36,012] After 4473600 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 448.8330383300781,\n",
      " 'loss_1': 497.16436767578125,\n",
      " 'mean_episode_return_0': 68.83756256103516,\n",
      " 'mean_episode_return_1': 83.4673843383789}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:41,016] After 4476800 frames: @ 639.6 fps Stats:\n",
      "{'loss_0': 410.98638916015625,\n",
      " 'loss_1': 497.16436767578125,\n",
      " 'mean_episode_return_0': 68.77062225341797,\n",
      " 'mean_episode_return_1': 83.4673843383789}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:46,020] After 4476800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 410.98638916015625,\n",
      " 'loss_1': 497.16436767578125,\n",
      " 'mean_episode_return_0': 68.77062225341797,\n",
      " 'mean_episode_return_1': 83.4673843383789}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:51,026] After 4480000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 410.98638916015625,\n",
      " 'loss_1': 648.4990844726562,\n",
      " 'mean_episode_return_0': 68.77062225341797,\n",
      " 'mean_episode_return_1': 83.50706481933594}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:05:56,032] After 4483200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.9449157714844,\n",
      " 'loss_1': 648.4990844726562,\n",
      " 'mean_episode_return_0': 68.74537658691406,\n",
      " 'mean_episode_return_1': 83.50706481933594}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:06:01,035] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:01,110] After 4483200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.9449157714844,\n",
      " 'loss_1': 648.4990844726562,\n",
      " 'mean_episode_return_0': 68.74537658691406,\n",
      " 'mean_episode_return_1': 83.50706481933594}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:06,117] After 4486400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.9449157714844,\n",
      " 'loss_1': 653.225830078125,\n",
      " 'mean_episode_return_0': 68.74537658691406,\n",
      " 'mean_episode_return_1': 83.4268569946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:11,120] After 4489600 frames: @ 639.7 fps Stats:\n",
      "{'loss_0': 426.6916198730469,\n",
      " 'loss_1': 653.225830078125,\n",
      " 'mean_episode_return_0': 68.77693939208984,\n",
      " 'mean_episode_return_1': 83.4268569946289}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:16,124] After 4489600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 426.6916198730469,\n",
      " 'loss_1': 653.225830078125,\n",
      " 'mean_episode_return_0': 68.77693939208984,\n",
      " 'mean_episode_return_1': 83.4268569946289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 15:06:21,130] After 4492800 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 426.6916198730469,\n",
      " 'loss_1': 523.192626953125,\n",
      " 'mean_episode_return_0': 68.77693939208984,\n",
      " 'mean_episode_return_1': 83.42443084716797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:26,136] After 4496000 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 406.9728088378906,\n",
      " 'loss_1': 523.192626953125,\n",
      " 'mean_episode_return_0': 68.72262573242188,\n",
      " 'mean_episode_return_1': 83.42443084716797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:31,141] After 4496000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.9728088378906,\n",
      " 'loss_1': 523.192626953125,\n",
      " 'mean_episode_return_0': 68.72262573242188,\n",
      " 'mean_episode_return_1': 83.42443084716797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:36,144] After 4499200 frames: @ 639.8 fps Stats:\n",
      "{'loss_0': 406.9728088378906,\n",
      " 'loss_1': 638.4119262695312,\n",
      " 'mean_episode_return_0': 68.72262573242188,\n",
      " 'mean_episode_return_1': 83.48529052734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:41,150] After 4502400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 359.3641357421875,\n",
      " 'loss_1': 638.4119262695312,\n",
      " 'mean_episode_return_0': 68.69606018066406,\n",
      " 'mean_episode_return_1': 83.48529052734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:46,156] After 4502400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 359.3641357421875,\n",
      " 'loss_1': 638.4119262695312,\n",
      " 'mean_episode_return_0': 68.69606018066406,\n",
      " 'mean_episode_return_1': 83.48529052734375}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:51,162] After 4508800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 406.4247131347656,\n",
      " 'loss_1': 442.0382080078125,\n",
      " 'mean_episode_return_0': 68.70731353759766,\n",
      " 'mean_episode_return_1': 83.47565460205078}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:06:56,168] After 4508800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 406.4247131347656,\n",
      " 'loss_1': 442.0382080078125,\n",
      " 'mean_episode_return_0': 68.70731353759766,\n",
      " 'mean_episode_return_1': 83.47565460205078}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:07:01,174] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:01,268] After 4512000 frames: @ 627.5 fps Stats:\n",
      "{'loss_0': 406.4247131347656,\n",
      " 'loss_1': 458.3802185058594,\n",
      " 'mean_episode_return_0': 68.70731353759766,\n",
      " 'mean_episode_return_1': 83.48986053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:06,274] After 4515200 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 471.0841369628906,\n",
      " 'loss_1': 458.3802185058594,\n",
      " 'mean_episode_return_0': 68.65293884277344,\n",
      " 'mean_episode_return_1': 83.48986053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:11,280] After 4515200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 471.0841369628906,\n",
      " 'loss_1': 458.3802185058594,\n",
      " 'mean_episode_return_0': 68.65293884277344,\n",
      " 'mean_episode_return_1': 83.48986053466797}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:16,286] After 4518400 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 471.0841369628906,\n",
      " 'loss_1': 480.7052307128906,\n",
      " 'mean_episode_return_0': 68.65293884277344,\n",
      " 'mean_episode_return_1': 83.50321197509766}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:21,292] After 4521600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 498.6394958496094,\n",
      " 'loss_1': 480.7052307128906,\n",
      " 'mean_episode_return_0': 68.69569396972656,\n",
      " 'mean_episode_return_1': 83.50321197509766}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:26,298] After 4521600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 498.6394958496094,\n",
      " 'loss_1': 480.7052307128906,\n",
      " 'mean_episode_return_0': 68.69569396972656,\n",
      " 'mean_episode_return_1': 83.50321197509766}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:31,304] After 4528000 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 464.42730712890625,\n",
      " 'loss_1': 518.9788818359375,\n",
      " 'mean_episode_return_0': 68.6684341430664,\n",
      " 'mean_episode_return_1': 83.50838470458984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:36,310] After 4528000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 464.42730712890625,\n",
      " 'loss_1': 518.9788818359375,\n",
      " 'mean_episode_return_0': 68.6684341430664,\n",
      " 'mean_episode_return_1': 83.50838470458984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:41,316] After 4528000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 464.42730712890625,\n",
      " 'loss_1': 518.9788818359375,\n",
      " 'mean_episode_return_0': 68.6684341430664,\n",
      " 'mean_episode_return_1': 83.50838470458984}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:46,322] After 4534400 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 546.8920288085938,\n",
      " 'loss_1': 607.679443359375,\n",
      " 'mean_episode_return_0': 68.6966323852539,\n",
      " 'mean_episode_return_1': 83.52643585205078}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:51,328] After 4534400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 546.8920288085938,\n",
      " 'loss_1': 607.679443359375,\n",
      " 'mean_episode_return_0': 68.6966323852539,\n",
      " 'mean_episode_return_1': 83.52643585205078}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:07:56,334] After 4537600 frames: @ 639.4 fps Stats:\n",
      "{'loss_0': 546.8920288085938,\n",
      " 'loss_1': 543.5198974609375,\n",
      " 'mean_episode_return_0': 68.6966323852539,\n",
      " 'mean_episode_return_1': 83.46662139892578}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:08:01,340] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:01,442] After 4540800 frames: @ 626.5 fps Stats:\n",
      "{'loss_0': 462.1461486816406,\n",
      " 'loss_1': 543.5198974609375,\n",
      " 'mean_episode_return_0': 68.72793579101562,\n",
      " 'mean_episode_return_1': 83.46662139892578}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:06,448] After 4540800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 462.1461486816406,\n",
      " 'loss_1': 543.5198974609375,\n",
      " 'mean_episode_return_0': 68.72793579101562,\n",
      " 'mean_episode_return_1': 83.46662139892578}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:11,454] After 4547200 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 425.2095947265625,\n",
      " 'loss_1': 483.3704528808594,\n",
      " 'mean_episode_return_0': 68.68830871582031,\n",
      " 'mean_episode_return_1': 83.48536682128906}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:16,460] After 4547200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 425.2095947265625,\n",
      " 'loss_1': 483.3704528808594,\n",
      " 'mean_episode_return_0': 68.68830871582031,\n",
      " 'mean_episode_return_1': 83.48536682128906}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:21,466] After 4547200 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 425.2095947265625,\n",
      " 'loss_1': 483.3704528808594,\n",
      " 'mean_episode_return_0': 68.68830871582031,\n",
      " 'mean_episode_return_1': 83.48536682128906}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:26,472] After 4553600 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 424.61468505859375,\n",
      " 'loss_1': 459.4637451171875,\n",
      " 'mean_episode_return_0': 68.66187286376953,\n",
      " 'mean_episode_return_1': 83.46920776367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:31,476] After 4553600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 424.61468505859375,\n",
      " 'loss_1': 459.4637451171875,\n",
      " 'mean_episode_return_0': 68.66187286376953,\n",
      " 'mean_episode_return_1': 83.46920776367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:36,481] After 4553600 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 424.61468505859375,\n",
      " 'loss_1': 459.4637451171875,\n",
      " 'mean_episode_return_0': 68.66187286376953,\n",
      " 'mean_episode_return_1': 83.46920776367188}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:41,486] After 4560000 frames: @ 1279.1 fps Stats:\n",
      "{'loss_0': 367.61029052734375,\n",
      " 'loss_1': 549.1472778320312,\n",
      " 'mean_episode_return_0': 68.63543701171875,\n",
      " 'mean_episode_return_1': 83.4757308959961}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:46,488] After 4560000 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 367.61029052734375,\n",
      " 'loss_1': 549.1472778320312,\n",
      " 'mean_episode_return_0': 68.63543701171875,\n",
      " 'mean_episode_return_1': 83.4757308959961}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:51,492] After 4566400 frames: @ 1279.2 fps Stats:\n",
      "{'loss_0': 486.5614318847656,\n",
      " 'loss_1': 527.8192138671875,\n",
      " 'mean_episode_return_0': 68.67012786865234,\n",
      " 'mean_episode_return_1': 83.52937316894531}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:08:56,496] After 4566400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 486.5614318847656,\n",
      " 'loss_1': 527.8192138671875,\n",
      " 'mean_episode_return_0': 68.67012786865234,\n",
      " 'mean_episode_return_1': 83.52937316894531}\n",
      "[INFO:417825 trainer:335 2022-09-17 15:09:01,502] Saving checkpoint to dmc_results/tute/model.tar\n",
      "[INFO:417825 trainer:367 2022-09-17 15:09:01,591] After 4566400 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 486.5614318847656,\n",
      " 'loss_1': 527.8192138671875,\n",
      " 'mean_episode_return_0': 68.67012786865234,\n",
      " 'mean_episode_return_1': 83.52937316894531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:417825 trainer:367 2022-09-17 15:09:06,597] After 4572800 frames: @ 1278.7 fps Stats:\n",
      "{'loss_0': 439.09033203125,\n",
      " 'loss_1': 512.5550537109375,\n",
      " 'mean_episode_return_0': 68.67831420898438,\n",
      " 'mean_episode_return_1': 83.55351257324219}\n",
      "[INFO:417825 trainer:367 2022-09-17 15:09:11,600] After 4572800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 439.09033203125,\n",
      " 'loss_1': 512.5550537109375,\n",
      " 'mean_episode_return_0': 68.67831420898438,\n",
      " 'mean_episode_return_1': 83.55351257324219}\n"
     ]
    }
   ],
   "source": [
    "# Train DMC Agents\n",
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc_agent = DMCAgent(\n",
    "    env.state_shape,\n",
    "    env.action_shape,\n",
    "    exp_epsilon=0.01,\n",
    "    device=\"0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5601bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc_agent = trainer.model_func(0).get_agent(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e034b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser('%s/%s/%s' % (\"dmc_results\", \"tute\", 'model.tar')))\n",
    "checkpoint_states = torch.load(checkpointpath, map_location=\"cuda:0\")\n",
    "dmc_agent.load_state_dict(checkpoint_states[\"model_state_dict\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a2c62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins = 0\n",
    "env.set_agents([dmc_agent, random_agent])\n",
    "for n in range(50):\n",
    "    trajectories, player_wins = env.run(is_training=False)\n",
    "    if player_wins[0] > player_wins[1]:\n",
    "        wins += 1\n",
    "env.set_agents([random_agent, dmc_agent])\n",
    "for n in range(50):\n",
    "    trajectories, player_wins = env.run(is_training=False)\n",
    "    if player_wins[1] > player_wins[0]:\n",
    "        wins += 1\n",
    "wins / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9157bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_agent = TuteHumanAgent(player=1, tute=env.game)\n",
    "env.set_agents([dmc_agent, human_agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8623f85f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 played el seis de oros\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el tres de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el seis de oros\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el cuatro de espadas\n",
      "2: el siete de bastos\n",
      "3: el seis de copas\n",
      "4: el siete de espadas\n",
      "5: el seis de bastos\n",
      "6: el dos de espadas\n",
      "7: el dos de oros\n",
      "8: la sota de espadas\n",
      "1  2  3  4  5  6  7  8 ? 7\n",
      "\n",
      "Player 2 played el dos de oros\n",
      "Player 1 won trick\n",
      "Player 1 played el cuatro de copas\n",
      "\n",
      "\n",
      "================= Trump =================\n",
      "el tres de copas\n",
      "\n",
      "\n",
      "================ Face up ================\n",
      "el cuatro de copas\n",
      "\n",
      "\n",
      "=============== Your Hand ===============\n",
      "1: el cuatro de espadas\n",
      "2: el siete de bastos\n",
      "3: el seis de copas\n",
      "4: el siete de espadas\n",
      "5: el seis de bastos\n",
      "6: el dos de espadas\n",
      "7: la sota de espadas\n",
      "8: la sota de bastos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trajectories, player_wins \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/rlcard/envs/env.py:144\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_over():\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Agent plays\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_training:\n\u001b[0;32m--> 144\u001b[0m         action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents[player_id]\u001b[38;5;241m.\u001b[39mstep(state)\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/agent.py:72\u001b[0m, in \u001b[0;36mTuteHumanAgent.eval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict the action given the curent state for evaluation. The same to step here.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        action (int): the action predicted (randomly chosen) by the random agent\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/rlcard/agent.py:58\u001b[0m, in \u001b[0;36mTuteHumanAgent.step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtute\u001b[38;5;241m.\u001b[39mget_hand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer)\n\u001b[1;32m     54\u001b[0m possible_cards \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m     index \u001b[38;5;28;01mfor\u001b[39;00m index, card \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hand\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39miteritems())\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m card[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_legal_actions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m ]\n\u001b[0;32m---> 58\u001b[0m card \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_card\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_cards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m card\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/ML/huggingface/tute/tute/tute.py:258\u001b[0m, in \u001b[0;36mTute.choose_card\u001b[0;34m(context, hand, possible_cards)\u001b[0m\n\u001b[1;32m    255\u001b[0m Tute\u001b[38;5;241m.\u001b[39mshow_cards(hand, with_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossible_cards\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m? \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(choice) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m possible_cards:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "trajectories, player_wins = env.run(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c94cc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 140])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fe190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
